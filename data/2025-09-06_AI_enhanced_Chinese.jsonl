{"id": "2509.04143", "categories": ["cs.GT"], "pdf": "https://arxiv.org/pdf/2509.04143", "abs": "https://arxiv.org/abs/2509.04143", "authors": ["Cedric Perret", "The Anh Han", "Elias Fern\u00e1ndez Domingos", "Theodor Cimpeanu", "Simon T. Powers"], "title": "The evolution of trust as a cognitive shortcut in repeated interactions", "comment": null, "summary": "Trust is often thought to increase cooperation. However, game-theoretic\nmodels often fail to distinguish between cooperative behaviour and trust. This\nmakes it difficult to measure trust and determine its effect in different\nsocial dilemmas. We address this here by formalising trust as a cognitive\nshortcut in repeated games. This functions by avoiding checking a partner's\nactions once a threshold level of cooperativeness has been observed. We\nconsider trust-based strategies that implement this heuristic, and\nsystematically analyse their evolution across the space of two-player symmetric\nsocial dilemma games. We find that where it is costly to check whether another\nagent's actions were cooperative, as is the case in many real-world settings,\nthen trust-based strategies can outcompete standard reciprocal strategies such\nas Tit-for-Tat in many social dilemmas. Moreover, the presence of trust\nincreases the overall level of cooperation in the population, especially in\ncases where agents can make unintentional errors in their actions. This occurs\neven in the presence of strategies designed to build and then exploit trust.\nOverall, our results demonstrate the individual adaptive benefit to an agent of\nusing a trust heuristic, and provide a formal theory for how trust can promote\ncooperation in different types of social interaction. We discuss the\nimplications of this for interactions between humans and artificial\nintelligence agents.", "AI": {"tldr": "\u8be5\u8bba\u6587\u901a\u8fc7\u5c06\u4fe1\u4efb\u5f62\u5f0f\u5316\u4e3a\u91cd\u590d\u535a\u5f08\u4e2d\u7684\u8ba4\u77e5\u6377\u5f84\uff0c\u63a2\u8ba8\u4e86\u4fe1\u4efb\u5982\u4f55\u5728\u4e0d\u540c\u793e\u4f1a\u56f0\u5883\u4e2d\u4fc3\u8fdb\u5408\u4f5c\u3002\u7814\u7a76\u53d1\u73b0\uff0c\u4fe1\u4efb\u7b56\u7565\u5728\u8bb8\u591a\u60c5\u51b5\u4e0b\u80fd\u80dc\u8fc7\u4f20\u7edf\u4e92\u60e0\u7b56\u7565\uff0c\u5e76\u63d0\u9ad8\u6574\u4f53\u5408\u4f5c\u6c34\u5e73\u3002", "motivation": "\u73b0\u6709\u535a\u5f08\u8bba\u6a21\u578b\u5f80\u5f80\u672a\u80fd\u533a\u5206\u5408\u4f5c\u884c\u4e3a\u4e0e\u4fe1\u4efb\uff0c\u5bfc\u81f4\u96be\u4ee5\u6d4b\u91cf\u4fe1\u4efb\u53ca\u5176\u5728\u793e\u4f1a\u56f0\u5883\u4e2d\u7684\u4f5c\u7528\u3002\u672c\u6587\u65e8\u5728\u901a\u8fc7\u5f62\u5f0f\u5316\u4fe1\u4efb\u6765\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u4f5c\u8005\u5c06\u4fe1\u4efb\u5b9a\u4e49\u4e3a\u91cd\u590d\u535a\u5f08\u4e2d\u7684\u8ba4\u77e5\u6377\u5f84\uff0c\u5373\u4e00\u65e6\u89c2\u5bdf\u5230\u5408\u4f5c\u4f19\u4f34\u7684\u5408\u4f5c\u884c\u4e3a\u8fbe\u5230\u9608\u503c\uff0c\u4fbf\u505c\u6b62\u68c0\u67e5\u5176\u884c\u4e3a\u3002\u7814\u7a76\u8005\u5206\u6790\u4e86\u57fa\u4e8e\u4fe1\u4efb\u7684\u7b56\u7565\u5728\u5bf9\u79f0\u53cc\u4eba\u793e\u4f1a\u56f0\u5883\u535a\u5f08\u4e2d\u7684\u6f14\u5316\u3002", "result": "\u7814\u7a76\u53d1\u73b0\uff0c\u5728\u68c0\u67e5\u5408\u4f5c\u884c\u4e3a\u6210\u672c\u9ad8\u6602\u7684\u60c5\u51b5\u4e0b\uff08\u5982\u73b0\u5b9e\u4e16\u754c\u4e2d\u7684\u8bb8\u591a\u573a\u666f\uff09\uff0c\u4fe1\u4efb\u7b56\u7565\u80fd\u80dc\u8fc7\u5982\u201c\u4ee5\u7259\u8fd8\u7259\u201d\u7b49\u4f20\u7edf\u4e92\u60e0\u7b56\u7565\u3002\u6b64\u5916\uff0c\u4fe1\u4efb\u7684\u5b58\u5728\u63d0\u9ad8\u4e86\u6574\u4f53\u5408\u4f5c\u6c34\u5e73\uff0c\u5373\u4f7f\u5728\u5b58\u5728\u7b56\u7565\u6027\u4fe1\u4efb\u6ee5\u7528\u7684\u60c5\u51b5\u4e0b\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u8868\u660e\uff0c\u4fe1\u4efb\u542f\u53d1\u5f0f\u80fd\u4e3a\u4e2a\u4f53\u5e26\u6765\u9002\u5e94\u6027\u4f18\u52bf\uff0c\u5e76\u63d0\u4f9b\u4e86\u4fe1\u4efb\u5982\u4f55\u5728\u4e0d\u540c\u793e\u4f1a\u4e92\u52a8\u4e2d\u4fc3\u8fdb\u5408\u4f5c\u7684\u5f62\u5f0f\u5316\u7406\u8bba\u3002\u4f5c\u8005\u8fd8\u8ba8\u8bba\u4e86\u8fd9\u5bf9\u4eba\u7c7b\u4e0e\u4eba\u5de5\u667a\u80fd\u4e92\u52a8\u7684\u5f71\u54cd\u3002"}}
{"id": "2509.04253", "categories": ["cs.PL"], "pdf": "https://arxiv.org/pdf/2509.04253", "abs": "https://arxiv.org/abs/2509.04253", "authors": ["Siyuan He", "Songlin Jia", "Yuyan Bao", "Tiark Rompf"], "title": "When Lifetimes Liberate: A Type System for Arenas with Higher-Order Reachability Tracking", "comment": null, "summary": "Static resource management in higher-order functional languages remains\nelusive due to tensions between control, expressiveness, and flexibility.\nRegion-based systems [Grossman et al. 2002; Tofte et al. 2001] offer control\nover lifetimes and expressive in-region sharing, but restrict resources to\nlexical scopes. Rust, an instance of ownership types [Clarke et al. 2013],\noffers non-lexical lifetimes and robust safety guarantees, yet its global\ninvariants make common sharing patterns hard to express. Reachability types\n[Wei et al. 2024] enable reasoning about sharing and separation, but lack\npractical tools for controlling resource lifetimes.\n  In this work, we try to unify their strengths. Our solution enables grouping\nresources as arenas for arbitrary sharing and static guarantees of lexically\nscoped lifetimes. Crucially, arenas and lexical lifetimes are not the only\nchoice: users may also manage resources individually, with non-lexical\nlifetimes. Regardless of mode, resources share the same type, preserving the\nhigher-order parametric nature of the language.\n  Obtaining static safety guarantee in a higher-order language with flexible\nsharing is nontrivial. To this end, we propose two new extensions atop\nreachability types [Wei et al. 2024]. First, A<: features a novel\ntwo-dimensional store model to enable coarse-grained reachability tracking for\narbitrarily shared resources within arenas. Building on this, {A}<: establishes\nlexical lifetime control with static guarantees. As the first reachability\nformalism presented for lifetime control, {A}<: avoids the complication of\nflow-sensitive reasoning and retains expressive power and simplicity. Both\ncalculi are formalized and proven type safe in Rocq.", "AI": {"tldr": "\u672c\u7814\u7a76\u7ed3\u5408\u533a\u57df\u7cfb\u7edf\u3001\u6240\u6709\u6743\u7c7b\u578b\u548c\u53ef\u8fbe\u6027\u7c7b\u578b\u7684\u4f18\u52bf\uff0c\u63d0\u51fa\u4e86\u65b0\u7684\u8d44\u6e90\u7ba1\u7406\u65b9\u6848\uff0c\u652f\u6301\u7075\u6d3b\u7684\u8d44\u6e90\u5171\u4eab\u548c\u9759\u6001\u751f\u547d\u5468\u671f\u63a7\u5236\u3002", "motivation": "\u9759\u6001\u8d44\u6e90\u7ba1\u7406\u5728\u9ad8\u7ea7\u51fd\u6570\u5f0f\u8bed\u8a00\u4e2d\u4e00\u76f4\u5177\u6709\u6311\u6218\u6027\uff0c\u73b0\u6709\u65b9\u6cd5\uff08\u5982\u533a\u57df\u7cfb\u7edf\u548c\u6240\u6709\u6743\u7c7b\u578b\uff09\u5728\u63a7\u5236\u3001\u8868\u8fbe\u6027\u548c\u7075\u6d3b\u6027\u4e4b\u95f4\u5b58\u5728\u77db\u76fe\u3002", "method": "\u63d0\u51fa\u4e86\u4e24\u79cd\u65b0\u7684\u6269\u5c55\uff1aA<:\u901a\u8fc7\u4e8c\u7ef4\u5b58\u50a8\u6a21\u578b\u5b9e\u73b0\u7c97\u7c92\u5ea6\u7684\u53ef\u8fbe\u6027\u8ddf\u8e2a\uff1b{A}<:\u5728\u6b64\u57fa\u7840\u4e0a\u63d0\u4f9b\u9759\u6001\u751f\u547d\u5468\u671f\u63a7\u5236\uff0c\u907f\u514d\u4e86\u6d41\u654f\u611f\u63a8\u7406\u7684\u590d\u6742\u6027\u3002", "result": "\u6240\u63d0\u51fa\u7684\u65b9\u6848\u5728Rocq\u4e2d\u5f62\u5f0f\u5316\u5e76\u8bc1\u660e\u4e3a\u7c7b\u578b\u5b89\u5168\u7684\uff0c\u540c\u65f6\u4fdd\u7559\u4e86\u8bed\u8a00\u7684\u53c2\u6570\u5316\u548c\u9ad8\u9636\u7279\u6027\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u9ad8\u9636\u51fd\u6570\u5f0f\u8bed\u8a00\u4e2d\u7684\u9759\u6001\u8d44\u6e90\u7ba1\u7406\u63d0\u4f9b\u4e86\u4e00\u79cd\u53ef\u884c\u7684\u7edf\u4e00\u65b9\u6848\uff0c\u517c\u5177\u7075\u6d3b\u6027\u548c\u5b89\u5168\u6027\u3002"}}
{"id": "2509.03680", "categories": ["cs.GR", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2509.03680", "abs": "https://arxiv.org/abs/2509.03680", "authors": ["Ruofan Liang", "Kai He", "Zan Gojcic", "Igor Gilitschenski", "Sanja Fidler", "Nandita Vijaykumar", "Zian Wang"], "title": "LuxDiT: Lighting Estimation with Video Diffusion Transformer", "comment": "Project page: https://research.nvidia.com/labs/toronto-ai/LuxDiT/", "summary": "Estimating scene lighting from a single image or video remains a longstanding\nchallenge in computer vision and graphics. Learning-based approaches are\nconstrained by the scarcity of ground-truth HDR environment maps, which are\nexpensive to capture and limited in diversity. While recent generative models\noffer strong priors for image synthesis, lighting estimation remains difficult\ndue to its reliance on indirect visual cues, the need to infer global\n(non-local) context, and the recovery of high-dynamic-range outputs. We propose\nLuxDiT, a novel data-driven approach that fine-tunes a video diffusion\ntransformer to generate HDR environment maps conditioned on visual input.\nTrained on a large synthetic dataset with diverse lighting conditions, our\nmodel learns to infer illumination from indirect visual cues and generalizes\neffectively to real-world scenes. To improve semantic alignment between the\ninput and the predicted environment map, we introduce a low-rank adaptation\nfinetuning strategy using a collected dataset of HDR panoramas. Our method\nproduces accurate lighting predictions with realistic angular high-frequency\ndetails, outperforming existing state-of-the-art techniques in both\nquantitative and qualitative evaluations.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86LuxDiT\uff0c\u4e00\u79cd\u57fa\u4e8e\u89c6\u9891\u6269\u6563\u53d8\u6362\u5668\u7684\u6570\u636e\u9a71\u52a8\u65b9\u6cd5\uff0c\u7528\u4e8e\u4ece\u89c6\u89c9\u8f93\u5165\u751f\u6210HDR\u73af\u5883\u5730\u56fe\uff0c\u5e76\u901a\u8fc7\u4f4e\u79e9\u9002\u5e94\u5fae\u8c03\u7b56\u7565\u63d0\u5347\u8f93\u5165\u4e0e\u9884\u6d4b\u7ed3\u679c\u7684\u8bed\u4e49\u5bf9\u9f50\u3002", "motivation": "\u4ece\u5355\u5f20\u56fe\u50cf\u6216\u89c6\u9891\u4f30\u8ba1\u573a\u666f\u5149\u7167\u662f\u8ba1\u7b97\u673a\u89c6\u89c9\u548c\u56fe\u5f62\u5b66\u4e2d\u7684\u957f\u671f\u6311\u6218\uff0c\u73b0\u6709\u7684\u5b66\u4e60\u578b\u65b9\u6cd5\u53d7\u9650\u4e8e\u771f\u5b9eHDR\u73af\u5883\u5730\u56fe\u7684\u7a00\u7f3a\u6027\u548c\u591a\u6837\u6027\u4e0d\u8db3\u3002", "method": "LuxDiT\u662f\u4e00\u79cd\u65b0\u9896\u7684\u6570\u636e\u9a71\u52a8\u65b9\u6cd5\uff0c\u901a\u8fc7\u5fae\u8c03\u89c6\u9891\u6269\u6563\u53d8\u6362\u5668\u6765\u751f\u6210\u6761\u4ef6\u4e8e\u89c6\u89c9\u8f93\u5165\u7684\u9ad8\u52a8\u6001\u8303\u56f4\uff08HDR\uff09\u73af\u5883\u5730\u56fe\uff0c\u5e76\u4f7f\u7528\u4f4e\u79e9\u9002\u5e94\u5fae\u8c03\u7b56\u7565\u4f18\u5316\u8bed\u4e49\u5bf9\u9f50\u3002", "result": "\u65b9\u6cd5\u5728\u5408\u6210\u7684\u591a\u6837\u5316\u5149\u7167\u6570\u636e\u96c6\u4e0a\u8bad\u7ec3\uff0c\u80fd\u591f\u4ece\u95f4\u63a5\u89c6\u89c9\u7ebf\u7d22\u4e2d\u63a8\u65ad\u5149\u7167\uff0c\u5e76\u6709\u6548\u6cdb\u5316\u5230\u771f\u5b9e\u573a\u666f\uff0c\u751f\u6210\u7684\u5149\u7167\u9884\u6d4b\u5177\u6709\u771f\u5b9e\u7684\u89d2\u5ea6\u9ad8\u9891\u7ec6\u8282\uff0c\u5728\u5b9a\u91cf\u548c\u5b9a\u6027\u8bc4\u4f30\u4e2d\u4f18\u4e8e\u73b0\u6709\u6280\u672f\u3002", "conclusion": "LuxDiT\u901a\u8fc7\u5b66\u4e60\u95f4\u63a5\u89c6\u89c9\u7ebf\u7d22\u548c\u5168\u5c40\u975e\u5c40\u90e8\u4e0a\u4e0b\u6587\u5173\u7cfb\uff0c\u4e3a\u5149\u7167\u4f30\u8ba1\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u663e\u8457\u63d0\u5347\u4e86\u51c6\u786e\u6027\u548c\u5b9e\u7528\u6027\u3002"}}
{"id": "2509.03753", "categories": ["cs.GR", "cs.CG", "cs.RO", "68U05", "I.3.5"], "pdf": "https://arxiv.org/pdf/2509.03753", "abs": "https://arxiv.org/abs/2509.03753", "authors": ["Michael Greer"], "title": "Memory Optimization for Convex Hull Support Point Queries", "comment": "6 pages, 15 figures", "summary": "This paper evaluates several improvements to the memory layout of convex\nhulls to improve computation times for support point queries. The support point\nquery is a fundamental part of common collision algorithms, and the work\npresented achieves a significant speedup depending on the number of vertices of\nthe convex hull.", "AI": {"tldr": "\u672c\u6587\u8bc4\u4f30\u4e86\u51e0\u79cd\u6539\u8fdb\u51f8\u5305\u5185\u5b58\u5e03\u5c40\u7684\u65b9\u6cd5\uff0c\u4ee5\u52a0\u901f\u652f\u6301\u70b9\u67e5\u8be2\u7684\u8ba1\u7b97\u65f6\u95f4\uff0c\u8fd9\u79cd\u67e5\u8be2\u662f\u5e38\u89c1\u78b0\u649e\u7b97\u6cd5\u7684\u6838\u5fc3\u90e8\u5206\u3002", "motivation": "\u652f\u6301\u70b9\u67e5\u8be2\u662f\u5e38\u89c1\u78b0\u649e\u7b97\u6cd5\u7684\u57fa\u7840\u64cd\u4f5c\uff0c\u4f46\u5176\u6027\u80fd\u53d7\u51f8\u5305\u9876\u70b9\u6570\u91cf\u7684\u5f71\u54cd\u8f83\u5927\uff0c\u56e0\u6b64\u9700\u8981\u4f18\u5316\u5185\u5b58\u5e03\u5c40\u4ee5\u63d0\u5347\u8ba1\u7b97\u6548\u7387\u3002", "method": "\u901a\u8fc7\u6539\u8fdb\u51f8\u5305\u7684\u5185\u5b58\u5e03\u5c40\uff0c\u4f18\u5316\u652f\u6301\u70b9\u67e5\u8be2\u7684\u8ba1\u7b97\u6548\u7387\u3002", "result": "\u7814\u7a76\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u4e0d\u540c\u9876\u70b9\u6570\u91cf\u7684\u51f8\u5305\u4e0a\u5747\u5b9e\u73b0\u4e86\u663e\u8457\u7684\u52a0\u901f\u6548\u679c\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u7684\u5185\u5b58\u5e03\u5c40\u6539\u8fdb\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86\u652f\u6301\u70b9\u67e5\u8be2\u7684\u8ba1\u7b97\u901f\u5ea6\uff0c\u7279\u522b\u662f\u5728\u9876\u70b9\u6570\u91cf\u8f83\u591a\u7684\u60c5\u51b5\u4e0b\u6548\u679c\u66f4\u4e3a\u660e\u663e\u3002"}}
{"id": "2509.03775", "categories": ["cs.GR", "cs.CV"], "pdf": "https://arxiv.org/pdf/2509.03775", "abs": "https://arxiv.org/abs/2509.03775", "authors": ["Sankeerth Durvasula", "Sharanshangar Muhunthan", "Zain Moustafa", "Richard Chen", "Ruofan Liang", "Yushi Guan", "Nilesh Ahuja", "Nilesh Jain", "Selvakumar Panneer", "Nandita Vijaykumar"], "title": "ContraGS: Codebook-Condensed and Trainable Gaussian Splatting for Fast, Memory-Efficient Reconstruction", "comment": null, "summary": "3D Gaussian Splatting (3DGS) is a state-of-art technique to model real-world\nscenes with high quality and real-time rendering. Typically, a higher quality\nrepresentation can be achieved by using a large number of 3D Gaussians.\nHowever, using large 3D Gaussian counts significantly increases the GPU device\nmemory for storing model parameters. A large model thus requires powerful GPUs\nwith high memory capacities for training and has slower training/rendering\nlatencies due to the inefficiencies of memory access and data movement. In this\nwork, we introduce ContraGS, a method to enable training directly on compressed\n3DGS representations without reducing the Gaussian Counts, and thus with a\nlittle loss in model quality. ContraGS leverages codebooks to compactly store a\nset of Gaussian parameter vectors throughout the training process, thereby\nsignificantly reducing memory consumption. While codebooks have been\ndemonstrated to be highly effective at compressing fully trained 3DGS models,\ndirectly training using codebook representations is an unsolved challenge.\nContraGS solves the problem of learning non-differentiable parameters in\ncodebook-compressed representations by posing parameter estimation as a\nBayesian inference problem. To this end, ContraGS provides a framework that\neffectively uses MCMC sampling to sample over a posterior distribution of these\ncompressed representations. With ContraGS, we demonstrate that ContraGS\nsignificantly reduces the peak memory during training (on average 3.49X) and\naccelerated training and rendering (1.36X and 1.88X on average, respectively),\nwhile retraining close to state-of-art quality.", "AI": {"tldr": "ContraGS\u662f\u4e00\u79cd\u901a\u8fc7\u538b\u7f293D\u9ad8\u65af\u6cfc\u6e85\uff083DGS\uff09\u8868\u793a\u6765\u51cf\u5c11\u5185\u5b58\u6d88\u8017\u548c\u52a0\u901f\u8bad\u7ec3/\u6e32\u67d3\u7684\u65b9\u6cd5\uff0c\u540c\u65f6\u4fdd\u6301\u6a21\u578b\u8d28\u91cf\u3002", "motivation": "\u4f20\u7edf\u76843DGS\u6280\u672f\u9700\u8981\u5927\u91cf3D\u9ad8\u65af\u5206\u5e03\u6765\u5b9e\u73b0\u9ad8\u8d28\u91cf\u573a\u666f\u5efa\u6a21\uff0c\u4f46\u8fd9\u4f1a\u589e\u52a0GPU\u5185\u5b58\u6d88\u8017\u5e76\u964d\u4f4e\u8bad\u7ec3/\u6e32\u67d3\u6548\u7387\u3002ContraGS\u65e8\u5728\u901a\u8fc7\u538b\u7f293DGS\u8868\u793a\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "ContraGS\u5229\u7528\u7801\u4e66\u7d27\u51d1\u5b58\u50a8\u9ad8\u65af\u53c2\u6570\u5411\u91cf\uff0c\u5e76\u901a\u8fc7\u8d1d\u53f6\u65af\u63a8\u65ad\u548c\u975e\u53ef\u5fae\u53c2\u6570\u5b66\u4e60\u7684MCMC\u91c7\u6837\u65b9\u6cd5\uff0c\u76f4\u63a5\u5728\u538b\u7f29\u8868\u793a\u4e0a\u8fdb\u884c\u8bad\u7ec3\u3002", "result": "ContraGS\u663e\u8457\u51cf\u5c11\u4e86\u8bad\u7ec3\u65f6\u7684\u5cf0\u503c\u5185\u5b58\uff08\u5e73\u57473.49\u500d\uff09\u5e76\u52a0\u901f\u4e86\u8bad\u7ec3\u548c\u6e32\u67d3\uff08\u5206\u522b\u5e73\u57471.36\u500d\u548c1.88\u500d\uff09\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u63a5\u8fd1\u73b0\u6709\u6280\u672f\u7684\u6a21\u578b\u8d28\u91cf\u3002", "conclusion": "ContraGS\u901a\u8fc7\u538b\u7f293DGS\u8868\u793a\u6210\u529f\u89e3\u51b3\u4e86\u5185\u5b58\u548c\u6548\u7387\u95ee\u9898\uff0c\u4e3a\u9ad8\u8d28\u91cf\u5b9e\u65f6\u6e32\u67d3\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.04047", "categories": ["cs.GR", "cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.04047", "abs": "https://arxiv.org/abs/2509.04047", "authors": ["Ashish Tiwari", "Satyam Bhardwaj", "Yash Bachwana", "Parag Sarvoday Sahu", "T. M. Feroz Ali", "Bhargava Chintalapati", "Shanmuganathan Raman"], "title": "TensoIS: A Step Towards Feed-Forward Tensorial Inverse Subsurface Scattering for Perlin Distributed Heterogeneous Media", "comment": "To appear in Pacific Graphics 2025 (CGF Journal Track), Project page:\n  https://yashbachwana.github.io/TensoIS/", "summary": "Estimating scattering parameters of heterogeneous media from images is a\nseverely under-constrained and challenging problem. Most of the existing\napproaches model BSSRDF either through an analysis-by-synthesis approach,\napproximating complex path integrals, or using differentiable volume rendering\ntechniques to account for heterogeneity. However, only a few studies have\napplied learning-based methods to estimate subsurface scattering parameters,\nbut they assume homogeneous media. Interestingly, no specific distribution is\nknown to us that can explicitly model the heterogeneous scattering parameters\nin the real world. Notably, procedural noise models such as Perlin and Fractal\nPerlin noise have been effective in representing intricate heterogeneities of\nnatural, organic, and inorganic surfaces. Leveraging this, we first create\nHeteroSynth, a synthetic dataset comprising photorealistic images of\nheterogeneous media whose scattering parameters are modeled using Fractal\nPerlin noise. Furthermore, we propose Tensorial Inverse Scattering (TensoIS), a\nlearning-based feed-forward framework to estimate these Perlin-distributed\nheterogeneous scattering parameters from sparse multi-view image observations.\nInstead of directly predicting the 3D scattering parameter volume, TensoIS uses\nlearnable low-rank tensor components to represent the scattering volume. We\nevaluate TensoIS on unseen heterogeneous variations over shapes from the\nHeteroSynth test set, smoke and cloud geometries obtained from open-source\nrealistic volumetric simulations, and some real-world samples to establish its\neffectiveness for inverse scattering. Overall, this study is an attempt to\nexplore Perlin noise distribution, given the lack of any such well-defined\ndistribution in literature, to potentially model real-world heterogeneous\nscattering in a feed-forward manner.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5b66\u4e60\u7684\u524d\u9988\u6846\u67b6TensoIS\uff0c\u7528\u4e8e\u4ece\u7a00\u758f\u591a\u89c6\u89d2\u56fe\u50cf\u4e2d\u4f30\u8ba1\u5f02\u8d28\u6563\u5c04\u53c2\u6570\uff0c\u5e76\u521b\u5efa\u4e86\u4e00\u4e2a\u5408\u6210\u6570\u636e\u96c6HeteroSynth\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u901a\u5e38\u5047\u8bbe\u4ecb\u8d28\u5747\u5300\u6216\u901a\u8fc7\u590d\u6742\u8def\u5f84\u79ef\u5206\u5efa\u6a21\u5f02\u8d28\u6027\uff0c\u4f46\u7f3a\u4e4f\u660e\u786e\u5efa\u6a21\u771f\u5b9e\u4e16\u754c\u5f02\u8d28\u6563\u5c04\u53c2\u6570\u7684\u65b9\u6cd5\u3002\u8bba\u6587\u5c1d\u8bd5\u5229\u7528Perlin\u566a\u58f0\u5206\u5e03\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\u3002", "method": "\u8bba\u6587\u63d0\u51faTensoIS\u6846\u67b6\uff0c\u4f7f\u7528\u53ef\u5b66\u4e60\u7684\u4f4e\u79e9\u5f20\u91cf\u5206\u91cf\u8868\u793a\u6563\u5c04\u4f53\u79ef\uff0c\u5e76\u57fa\u4e8eFractal Perlin\u566a\u58f0\u521b\u5efa\u5408\u6210\u6570\u636e\u96c6HeteroSynth\u3002", "result": "TensoIS\u5728\u5408\u6210\u6d4b\u8bd5\u96c6\u3001\u70df\u96fe\u548c\u4e91\u51e0\u4f55\u4f53\u4ee5\u53ca\u771f\u5b9e\u6837\u672c\u4e0a\u8868\u73b0\u51fa\u8272\uff0c\u9a8c\u8bc1\u4e86\u5176\u5728\u9006\u6563\u5c04\u95ee\u9898\u4e2d\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u672c\u7814\u7a76\u63a2\u7d22\u4e86Perlin\u566a\u58f0\u5206\u5e03\u5728\u524d\u9988\u65b9\u5f0f\u4e0b\u5efa\u6a21\u771f\u5b9e\u4e16\u754c\u5f02\u8d28\u6563\u5c04\u7684\u6f5c\u529b\uff0c\u586b\u8865\u4e86\u73b0\u6709\u6587\u732e\u7684\u7a7a\u767d\u3002"}}
{"id": "2509.04058", "categories": ["cs.GR", "cs.CV"], "pdf": "https://arxiv.org/pdf/2509.04058", "abs": "https://arxiv.org/abs/2509.04058", "authors": ["Lei Zhong", "Yi Yang", "Changjian Li"], "title": "SMooGPT: Stylized Motion Generation using Large Language Models", "comment": null, "summary": "Stylized motion generation is actively studied in computer graphics,\nespecially benefiting from the rapid advances in diffusion models. The goal of\nthis task is to produce a novel motion respecting both the motion content and\nthe desired motion style, e.g., ``walking in a loop like a Monkey''. Existing\nresearch attempts to address this problem via motion style transfer or\nconditional motion generation. They typically embed the motion style into a\nlatent space and guide the motion implicitly in a latent space as well. Despite\nthe progress, their methods suffer from low interpretability and control,\nlimited generalization to new styles, and fail to produce motions other than\n``walking'' due to the strong bias in the public stylization dataset. In this\npaper, we propose to solve the stylized motion generation problem from a new\nperspective of reasoning-composition-generation, based on our observations: i)\nhuman motion can often be effectively described using natural language in a\nbody-part centric manner, ii) LLMs exhibit a strong ability to understand and\nreason about human motion, and iii) human motion has an inherently\ncompositional nature, facilitating the new motion content or style generation\nvia effective recomposing. We thus propose utilizing body-part text space as an\nintermediate representation, and present SMooGPT, a fine-tuned LLM, acting as a\nreasoner, composer, and generator when generating the desired stylized motion.\nOur method executes in the body-part text space with much higher\ninterpretability, enabling fine-grained motion control, effectively resolving\npotential conflicts between motion content and style, and generalizes well to\nnew styles thanks to the open-vocabulary ability of LLMs. Comprehensive\nexperiments and evaluations, and a user perceptual study, demonstrate the\neffectiveness of our approach, especially under the pure text-driven stylized\nmotion generation.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eLLM\u7684\u65b0\u65b9\u6cd5SMooGPT\uff0c\u7528\u4e8e\u751f\u6210\u5177\u6709\u9ad8\u5ea6\u53ef\u89e3\u91ca\u6027\u548c\u63a7\u5236\u6027\u7684\u98ce\u683c\u5316\u8fd0\u52a8\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u5728\u98ce\u683c\u8fc1\u79fb\u548c\u6761\u4ef6\u751f\u6210\u4e2d\u7684\u5c40\u9650\u6027\u3002", "motivation": "\u73b0\u6709\u98ce\u683c\u5316\u52a8\u4f5c\u751f\u6210\u65b9\u6cd5\u5b58\u5728\u4f4e\u53ef\u89e3\u91ca\u6027\u3001\u63a7\u5236\u6027\u5dee\u548c\u5bf9\u65b0\u98ce\u683c\u6cdb\u5316\u80fd\u529b\u6709\u9650\u7684\u95ee\u9898\uff0c\u8bba\u6587\u65e8\u5728\u901a\u8fc7\u81ea\u7136\u8bed\u8a00\u548cLLM\u7684\u80fd\u529b\u6539\u8fdb\u8fd9\u4e9b\u95ee\u9898\u3002", "method": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u63a8\u7406-\u7ec4\u5408-\u751f\u6210\u7684\u65b0\u65b9\u6cd5\uff0c\u5229\u7528\u8eab\u4f53\u90e8\u4f4d\u6587\u672c\u7a7a\u95f4\u4f5c\u4e3a\u4e2d\u95f4\u8868\u793a\uff0c\u5e76\u5f00\u53d1\u4e86SMooGPT\uff08\u4e00\u79cd\u5fae\u8c03\u7684LLM\uff09\u4f5c\u4e3a\u63a8\u7406\u3001\u7ec4\u5408\u548c\u751f\u6210\u5de5\u5177\u3002", "result": "\u5b9e\u9a8c\u548c\u7528\u6237\u611f\u77e5\u7814\u7a76\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5177\u6709\u9ad8\u5ea6\u7684\u53ef\u89e3\u91ca\u6027\u548c\u63a7\u5236\u6027\uff0c\u80fd\u591f\u6709\u6548\u89e3\u51b3\u8fd0\u52a8\u5185\u5bb9\u4e0e\u98ce\u683c\u4e4b\u95f4\u7684\u51b2\u7a81\uff0c\u5e76\u5bf9\u65b0\u98ce\u683c\u5177\u6709\u826f\u597d\u7684\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "\u57fa\u4e8eLLM\u7684SMooGPT\u65b9\u6cd5\u5728\u98ce\u683c\u5316\u52a8\u4f5c\u751f\u6210\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u5c24\u5176\u662f\u5728\u7eaf\u6587\u672c\u9a71\u52a8\u7684\u60c5\u51b5\u4e0b\uff0c\u4e3a\u672a\u6765\u7814\u7a76\u63d0\u4f9b\u4e86\u65b0\u7684\u65b9\u5411\u548c\u53ef\u80fd\u6027\u3002"}}
{"id": "2509.04145", "categories": ["cs.GR", "cs.CV"], "pdf": "https://arxiv.org/pdf/2509.04145", "abs": "https://arxiv.org/abs/2509.04145", "authors": ["Dongliang Cao", "Guoxing Sun", "Marc Habermann", "Florian Bernard"], "title": "Hyper Diffusion Avatars: Dynamic Human Avatar Generation using Network Weight Space Diffusion", "comment": null, "summary": "Creating human avatars is a highly desirable yet challenging task. Recent\nadvancements in radiance field rendering have achieved unprecedented\nphotorealism and real-time performance for personalized dynamic human avatars.\nHowever, these approaches are typically limited to person-specific rendering\nmodels trained on multi-view video data for a single individual, limiting their\nability to generalize across different identities. On the other hand,\ngenerative approaches leveraging prior knowledge from pre-trained 2D diffusion\nmodels can produce cartoonish, static human avatars, which are animated through\nsimple skeleton-based articulation. Therefore, the avatars generated by these\nmethods suffer from lower rendering quality compared to person-specific\nrendering methods and fail to capture pose-dependent deformations such as cloth\nwrinkles. In this paper, we propose a novel approach that unites the strengths\nof person-specific rendering and diffusion-based generative modeling to enable\ndynamic human avatar generation with both high photorealism and realistic\npose-dependent deformations. Our method follows a two-stage pipeline: first, we\noptimize a set of person-specific UNets, with each network representing a\ndynamic human avatar that captures intricate pose-dependent deformations. In\nthe second stage, we train a hyper diffusion model over the optimized network\nweights. During inference, our method generates network weights for real-time,\ncontrollable rendering of dynamic human avatars. Using a large-scale,\ncross-identity, multi-view video dataset, we demonstrate that our approach\noutperforms state-of-the-art human avatar generation methods.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u4e2a\u6027\u5316\u6e32\u67d3\u548c\u6269\u6563\u751f\u6210\u6a21\u578b\u7684\u65b0\u65b9\u6cd5\uff0c\u80fd\u591f\u751f\u6210\u5177\u6709\u9ad8\u771f\u5b9e\u611f\u548c\u59ff\u6001\u4f9d\u8d56\u53d8\u5f62\u80fd\u529b\u7684\u52a8\u6001\u4eba\u7c7b\u5316\u8eab\u3002", "motivation": "\u73b0\u6709\u7684\u4eba\u7c7b\u5316\u8eab\u751f\u6210\u65b9\u6cd5\u8981\u4e48\u5c40\u9650\u4e8e\u5355\u4e00\u8eab\u4efd\u7684\u6e32\u67d3\u6a21\u578b\uff0c\u8981\u4e48\u751f\u6210\u8d28\u91cf\u8f83\u4f4e\u4e14\u65e0\u6cd5\u6355\u6349\u59ff\u6001\u4f9d\u8d56\u53d8\u5f62\u3002\u672c\u6587\u65e8\u5728\u7ed3\u5408\u4e24\u8005\u7684\u4f18\u52bf\uff0c\u5b9e\u73b0\u9ad8\u8d28\u91cf\u548c\u52a8\u6001\u5316\u7684\u5316\u8eab\u751f\u6210\u3002", "method": "\u91c7\u7528\u4e24\u9636\u6bb5\u6d41\u7a0b\uff1a\u9996\u5148\u4f18\u5316\u4e00\u7ec4\u4e2a\u6027\u5316UNet\u7f51\u7edc\u4ee5\u6355\u6349\u59ff\u6001\u4f9d\u8d56\u53d8\u5f62\uff0c\u7136\u540e\u8bad\u7ec3\u4e00\u4e2a\u8d85\u6269\u6563\u6a21\u578b\u6765\u751f\u6210\u5b9e\u65f6\u53ef\u63a7\u7684\u52a8\u6001\u5316\u8eab\u7f51\u7edc\u6743\u91cd\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u5927\u89c4\u6a21\u3001\u8de8\u8eab\u4efd\u7684\u591a\u89c6\u89d2\u89c6\u9891\u6570\u636e\u96c6\u4e0a\u4f18\u4e8e\u73b0\u6709\u7684\u4eba\u7c7b\u5316\u8eab\u751f\u6210\u65b9\u6cd5\u3002", "conclusion": "\u901a\u8fc7\u7ed3\u5408\u4e2a\u6027\u5316\u6e32\u67d3\u548c\u6269\u6563\u751f\u6210\u6a21\u578b\uff0c\u672c\u6587\u6210\u529f\u5b9e\u73b0\u4e86\u9ad8\u771f\u5b9e\u611f\u548c\u59ff\u6001\u4f9d\u8d56\u53d8\u5f62\u7684\u52a8\u6001\u4eba\u7c7b\u5316\u8eab\u751f\u6210\u3002"}}
{"id": "2509.04277", "categories": ["cs.GR", "cs.DC", "65D18", "I.3.5"], "pdf": "https://arxiv.org/pdf/2509.04277", "abs": "https://arxiv.org/abs/2509.04277", "authors": ["Przemyslaw Korzeniowski", "Niels Hald", "Fernando Bello"], "title": "Massively-Parallel Implementation of Inextensible Elastic Rods Using Inter-block GPU Synchronization", "comment": "12 pages, unpublished", "summary": "An elastic rod is a long and thin body able to sustain large global\ndeformations, even if local strains are small. The Cosserat rod is a non-linear\nelastic rod with an oriented centreline, which enables modelling of bending,\nstretching and twisting deformations. It can be used for physically-based\ncomputer simulation of threads, wires, ropes, as well as flexible surgical\ninstruments such as catheters, guidewires or sutures. We present a\nmassively-parallel implementation of the original CoRdE model as well as our\ninextensible variation. By superseding the CUDA Scalable Programming Model and\nusing inter-block synchronization, we managed to simulate multiple physics\ntime-steps per single kernel launch utilizing all the GPU's streaming\nmultiprocessors. Under some constraints, this results in nearly constant\ncomputation time, regardless of the number of Cosserat elements simulated. When\nexecuting 10 time-steps per single kernel launch, our implementation of the\noriginal, extensible CoRdE was x40.0 faster. In a number of tests, the GPU\nimplementation of our inextensible CoRdE modification achieved an average\nspeed-up of x15.11 over the corresponding CPU version. Simulating a\ncatheter/guidewire pair (2x512 Cosserat elements) in a cardiovascular\napplication resulted in a 13.5 fold performance boost, enabling for accurate\nreal-time simulation at haptic interactive rates (0.5-1kHz).", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eGPU\u7684\u5927\u89c4\u6a21\u5e76\u884c\u5b9e\u73b0\u65b9\u6cd5\uff0c\u7528\u4e8e\u6a21\u62dfCosserat\u5f39\u6027\u6746\u6a21\u578b\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u8ba1\u7b97\u901f\u5ea6\uff0c\u9002\u7528\u4e8e\u5b9e\u65f6\u6a21\u62df\u5bfc\u7ba1/\u5bfc\u4e1d\u7b49\u67d4\u6027\u533b\u7597\u5668\u68b0\u3002", "motivation": "\u4e3a\u4e86\u9ad8\u6548\u6a21\u62df\u5f39\u6027\u6746\uff08\u5982\u5bfc\u7ba1\u3001\u5bfc\u4e1d\u7b49\uff09\u7684\u5927\u8303\u56f4\u53d8\u5f62\uff0c\u5e76\u5b9e\u73b0\u5b9e\u65f6\u4ea4\u4e92\u5f0f\u6a21\u62df\uff0c\u9700\u8981\u5f00\u53d1\u4e00\u79cd\u9ad8\u6027\u80fd\u7684\u5e76\u884c\u8ba1\u7b97\u65b9\u6cd5\u3002", "method": "\u91c7\u7528GPU\u5e76\u884c\u8ba1\u7b97\u6a21\u578b\uff08\u57fa\u4e8eCUDA Scalable Programming Model\uff09\uff0c\u901a\u8fc7\u5757\u95f4\u540c\u6b65\u6280\u672f\uff0c\u5b9e\u73b0\u4e86\u6bcf\u4e2a\u5185\u6838\u542f\u52a8\u6267\u884c\u591a\u4e2a\u7269\u7406\u65f6\u95f4\u6b65\u957f\u7684\u9ad8\u6548\u8ba1\u7b97\u3002", "result": "\u5728\u6d4b\u8bd5\u4e2d\uff0cGPU\u5b9e\u73b0\u7684\u539f\u59cbCoRdE\u6a21\u578b\u901f\u5ea6\u63d0\u5347\u4e8640\u500d\uff0c\u800c\u4e0d\u53ef\u4f38\u7f29\u7684CoRdE\u53d8\u4f53\u5e73\u5747\u63d0\u5347\u4e8615.11\u500d\uff0c\u5bfc\u7ba1/\u5bfc\u4e1d\u5bf9\u7684\u6a21\u62df\u6027\u80fd\u63d0\u5347\u4e8613.5\u500d\u3002", "conclusion": "\u63d0\u51fa\u7684\u5e76\u884c\u8ba1\u7b97\u65b9\u6cd5\u663e\u8457\u63d0\u9ad8\u4e86Cosserat\u5f39\u6027\u6746\u6a21\u578b\u7684\u6a21\u62df\u6548\u7387\uff0c\u80fd\u591f\u652f\u6301\u5b9e\u65f6\u9ad8\u7cbe\u5ea6\u6a21\u62df\uff080.5-1kHz\uff09\uff0c\u9002\u7528\u4e8e\u533b\u7597\u5668\u68b0\u7684\u4ea4\u4e92\u5f0f\u5e94\u7528\u3002"}}
