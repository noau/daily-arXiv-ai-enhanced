{"id": "2511.21802", "categories": ["cs.GT", "cs.AI", "econ.GN"], "pdf": "https://arxiv.org/pdf/2511.21802", "abs": "https://arxiv.org/abs/2511.21802", "authors": ["Sriram Tolety"], "title": "Tacit Bidder-Side Collusion: Artificial Intelligence in Dynamic Auctions", "comment": null, "summary": "We study whether large language models acting as autonomous bidders can tacitly collude by coordinating when to accept platform posted payouts in repeated Dutch auctions, without any communication. We present a minimal repeated auction model that yields a simple incentive compatibility condition and a closed form threshold for sustainable collusion for subgame-perfect Nash equilibria. In controlled simulations with multiple language models, we observe systematic supra-competitive prices in small auction settings and a return to competitive behavior as the number of bidders in the market increases, consistent with the theoretical model. We also find LLMs use various mechanisms to facilitate tacit coordination, such as focal point acceptance timing versus patient strategies that track the theoretical incentives. The results provide, to our knowledge, the first evidence of bidder side tacit collusion by LLMs and show that market structure levers can be more effective than capability limits for mitigation."}
{"id": "2511.21875", "categories": ["cs.GT", "cs.MA", "econ.TH", "nlin.AO", "physics.soc-ph"], "pdf": "https://arxiv.org/pdf/2511.21875", "abs": "https://arxiv.org/abs/2511.21875", "authors": ["Hiroaki Chiba-Okabe", "Joshua B. Plotkin"], "title": "The Evolution of Trust under Institutional Moral Hazard", "comment": "26 pages, 11 figures", "summary": "We study the behavior of for-profit institutions that broadcast reputations to foster trust among market participants. We develop a theoretical model in which buyers and sellers are matched on a platform to engage in transactions involving a moral hazard: sellers can either faithfully deliver goods after receiving payment, or not. Although the buyer does not know a seller's true type, the platform maintains a reputation system that probabilistically assigns binary reputation signals. Buyers make purchase decisions based on reputation signals, which influence the payoffs to sellers who then adapt their type over time. These market dynamics ultimately shape the platform's profit from commissions on sales. Our analysis reveals that platforms inherently have an incentive for rating inflation, driven by the desire to increase commission. This introduces a second layer of moral hazard: the platform's incentive to distort reputations for its own profit. Such distortion is self-limited by the platform's need to maintain enough accuracy that trustworthy sellers remain in the market, without which rational buyers would refrain from purchases altogether. Nonetheless, the optimal strategy for the platform can be to invest in order to reduce signal accuracy. When the platform can freely set commission fees, however, maximum profit may be achieved by costly investment in an accurate reputation system. These findings highlight the intricate tensions between platform incentives and resulting social utility for marketplace participants."}
{"id": "2511.22061", "categories": ["cs.GT"], "pdf": "https://arxiv.org/pdf/2511.22061", "abs": "https://arxiv.org/abs/2511.22061", "authors": ["Boyao Peng", "Linkun Liu"], "title": "Aligning with Human Values to Enhance Interaction: An eHMI-Mediated Lane-Changing Negotiation Strategy Using Bayesian Inference", "comment": null, "summary": "As autonomous driving technology evolves, ensuring the stability and safety of Autonomous Driving Systems (ADS) through alignment with human values becomes increasingly crucial. While existing research emphasizes the adherence of AI to honest ethical principles, it overlooks the potential benefits of benevolent deception, which maximize overall payoffs. This study proposes a game-theoretic model for lane-changing scenarios, incorporating Bayesian inference to capture dynamic changes in human trust during interactions under external Human-Machine Interface (eHMI) disclosed information. Case studies reveal that benevolent deception can enhance the efficiency of interaction in up to 59.4% of scenarios and improve safety in up to 52.7%. However, in the most pronounced cases, deception also led to trust collapse in up to 36.9% of drivers, exposing a critical vulnerability in the ethical design of ADS. The findings suggest that aligning ADS with comprehensive human ethical values, including the conditional use of benevolent deception, can enhance human-machine interaction. Additionally, the risk of trust collapse remains a major ethical loophole that must be addressed in future ADS development."}
{"id": "2511.22208", "categories": ["cs.GT"], "pdf": "https://arxiv.org/pdf/2511.22208", "abs": "https://arxiv.org/abs/2511.22208", "authors": ["Shunta Yamazaki", "Tomomi Matsui"], "title": "On Computing the Shapley Value in Bankruptcy Games -llustrated by Rectified Linear Function Game-", "comment": "19 pages", "summary": "In this research, we discuss a problem of calculating the Shapley value in bankruptcy games. We show that the decision problem of computing the Shapley value in bankruptcy games is NP-complete. We also investigate the relationship between the Shapley value of bankruptcy games and the Shapley-Shubik index in weighted voting games. The relation naturally implies a dynamic programming technique for calculating the Shapley value. We also present two recursive algorithms for computing the Shapley value: the first is the recursive completion method originally proposed by O'Neill, and the second is our novel contribution based on the dual game formulation. These recursive approaches offer conceptual clarity and computational efficiency, especially when combined with memoisation technique. Finally, we propose a Fully Polynomial-Time Randomized Approximation Scheme (FPRAS) based on Monte Carlo sampling, providing an efficient approximation method for large-scale instances."}
{"id": "2511.22075", "categories": ["cs.PL"], "pdf": "https://arxiv.org/pdf/2511.22075", "abs": "https://arxiv.org/abs/2511.22075", "authors": ["Doruk Alp Mutlu"], "title": "Expanding Specification Capabilities of a Gradual Verifier with Pure Functions", "comment": "Submitted to the 53rd ACM SIGPLAN Symposium on Principles of Programming Languages (POPL 2026) Student Research Competition", "summary": "Gradual verification soundly combines static checking and dynamic checking to provide an incremental approach for software verification. With gradual verification, programs can be partially specified first, and then the full specification of a program can be achieved in incremental steps. The first and only practicable gradual verifier based on symbolic execution, Gradual C0, supports recursive heap data structures. Despite recent efforts to improve the expressivity of Gradual C0's specification language, Gradual C0's specification language is still limited in its capabilities for complex expressions. This work explores an extension to Gradual C0's design with a common construct supported by many static verification tools, pure functions, which both extend the specification capabilities of Gradual C0 and increase the ease of encoding observer methods in Gradual C0. Our approach addresses the technical challenges related to the axiomatisation of pure functions with imprecise specifications."}
{"id": "2511.21697", "categories": ["cs.GR"], "pdf": "https://arxiv.org/pdf/2511.21697", "abs": "https://arxiv.org/abs/2511.21697", "authors": ["Julien Philip", "Li Ma", "Pascal Clausen", "Wenqi Xian", "Ahmet Levent Taşel", "Mingming He", "Xueming Yu", "David M. George", "Ning Yu", "Oliver Pilarski", "Paul Debevec"], "title": "Detail Enhanced Gaussian Splatting for Large-Scale Volumetric Capture", "comment": "10 pages, Accepted as a Journal paper at Siggraph Asia 2025. Webpage: https://eyeline-labs.github.io/DEGS/", "summary": "We present a unique system for large-scale, multi-performer, high resolution 4D volumetric capture providing realistic free-viewpoint video up to and including 4K resolution facial closeups. To achieve this, we employ a novel volumetric capture, reconstruction and rendering pipeline based on Dynamic Gaussian Splatting and Diffusion-based Detail Enhancement. We design our pipeline specifically to meet the demands of high-end media production. We employ two capture rigs: the Scene Rig, which captures multi-actor performances at a resolution which falls short of 4K production quality, and the Face Rig, which records high-fidelity single-actor facial detail to serve as a reference for detail enhancement. We first reconstruct dynamic performances from the Scene Rig using 4D Gaussian Splatting, incorporating new model designs and training strategies to improve reconstruction, dynamic range, and rendering quality. Then to render high-quality images for facial closeups, we introduce a diffusion-based detail enhancement model. This model is fine-tuned with high-fidelity data from the same actors recorded in the Face Rig. We train on paired data generated from low- and high-quality Gaussian Splatting (GS) models, using the low-quality input to match the quality of the Scene Rig, with the high-quality GS as ground truth. Our results demonstrate the effectiveness of this pipeline in bridging the gap between the scalable performance capture of a large-scale rig and the high-resolution standards required for film and media production."}
{"id": "2511.22369", "categories": ["cs.GT", "cs.MA"], "pdf": "https://arxiv.org/pdf/2511.22369", "abs": "https://arxiv.org/abs/2511.22369", "authors": ["Kym Pram", "Burkhard C. Schipper"], "title": "Mechanism Design under Unawareness -- Extended Abstract", "comment": "In Proceedings TARK 2025, arXiv:2511.20540. arXiv admin note: substantial text overlap with arXiv:2504.04382", "summary": "We study the design of mechanisms under asymmetric awareness and information. While the mechanism designer cannot necessarily commit to a particular social choice function in the face of unawareness, she can at least commit to properties of social choice functions such as efficiency given ex post awareness. Assuming quasi-linear utilities and private values, we show that we can implement in conditional dominant strategies a social choice function that is utilitarian ex post efficient under pooled awareness without the need of the social planner being fully aware ex ante. To this end, we develop novel dynamic versions of Vickrey-Clarke-Groves mechanisms in which true types are revealed and subsequently elaborated at endogenous higher awareness levels. We explore how asymmetric awareness affects budget balance and participation constraints. We show that ex ante unforeseen contingencies are no excuse for deficits. Finally, we propose a dynamic elaboration reverse second price auction for efficient procurement of complex incompletely specified projects with budget balance and participation constraints."}
{"id": "2511.22419", "categories": ["cs.PL"], "pdf": "https://arxiv.org/pdf/2511.22419", "abs": "https://arxiv.org/abs/2511.22419", "authors": ["Ken Sakayori", "Andrea Colledan", "Ugo Dal Lago"], "title": "On Circuit Description Languages, Indexed Monads, and Resource Analysis", "comment": "Extended version of a paper to be published at POPL 2026", "summary": "In this paper, a monad-based denotational model is introduced and shown adequate for the Proto-Quipper family of calculi, themselves being idealized versions of the Quipper programming language. The use of a monadic approach allows us to separate the value to which a term reduces from the circuit that the term itself produces as a side effect. In turn, this enables the denotational interpretation and validation of rich type systems in which the size of the produced circuit can be controlled. Notably, the proposed semantic framework, through the novel concept of circuit algebra, suggests forms of effect typing guaranteeing quantitative properties about the resulting circuit, even in presence of optimizations."}
{"id": "2511.22288", "categories": ["cs.GR"], "pdf": "https://arxiv.org/pdf/2511.22288", "abs": "https://arxiv.org/abs/2511.22288", "authors": ["Zhaorui Meng", "Lu Yin", "Yangqing Hou", "Anjun Chen", "Shihui Guo", "Yipeng Qin"], "title": "Improving Sparse IMU-based Motion Capture with Motion Label Smoothing", "comment": "Accepted by AAAI 2026", "summary": "Sparse Inertial Measurement Units (IMUs) based human motion capture has gained significant momentum, driven by the adaptation of fundamental AI tools such as recurrent neural networks (RNNs) and transformers that are tailored for temporal and spatial modeling. Despite these achievements, current research predominantly focuses on pipeline and architectural designs, with comparatively little attention given to regularization methods, highlighting a critical gap in developing a comprehensive AI toolkit for this task. To bridge this gap, we propose motion label smoothing, a novel method that adapts the classic label smoothing strategy from classification to the sparse IMU-based motion capture task. Specifically, we first demonstrate that a naive adaptation of label smoothing, including simply blending a uniform vector or a ``uniform'' motion representation (e.g., dataset-average motion or a canonical T-pose), is suboptimal; and argue that a proper adaptation requires increasing the entropy of the smoothed labels. Second, we conduct a thorough analysis of human motion labels, identifying three critical properties: 1) Temporal Smoothness, 2) Joint Correlation, and 3) Low-Frequency Dominance, and show that conventional approaches to entropy enhancement (e.g., blending Gaussian noise) are ineffective as they disrupt these properties. Finally, we propose the blend of a novel skeleton-based Perlin noise for motion label smoothing, designed to raise label entropy while satisfying motion properties. Extensive experiments applying our motion label smoothing to three state-of-the-art methods across four real-world IMU datasets demonstrate its effectiveness and robust generalization (plug-and-play) capability."}
{"id": "2511.22370", "categories": ["cs.GT", "cs.CC"], "pdf": "https://arxiv.org/pdf/2511.22370", "abs": "https://arxiv.org/abs/2511.22370", "authors": ["Jörg Rothe", "Ildikó Schlotter"], "title": "Solving Four Open Problems about Core Stability in Altruistic Hedonic Games", "comment": "In Proceedings TARK 2025, arXiv:2511.20540", "summary": "Hedonic games -- at the interface of cooperative game theory and computational social choice -- are coalition formation games in which the players have preferences over the coalitions they can join. Kerkmann et al. [13] introduced altruistic hedonic games where the players' utilities depend not only on their own but also on their friends' valuations of coalitions. The complexity of the verification problem for core stability has remained open in four variants of altruistic hedonic games: namely, for the variants with average- and minimum-based \"equal-treatment\" and \"altruistic-treatment\" preferences. We solve these four open questions by proving the corresponding problems coNP-complete; our reductions rely on rather intricate gadgets in the related networks of friends."}
{"id": "2511.22692", "categories": ["cs.PL"], "pdf": "https://arxiv.org/pdf/2511.22692", "abs": "https://arxiv.org/abs/2511.22692", "authors": ["David Castro-Perez", "Francisco Ferreira", "Sung-Shik Jongmans"], "title": "A Synthetic Reconstruction of Multiparty Session Types (with Appendix)", "comment": null, "summary": "Multiparty session types (MPST) provide a rigorous foundation for verifying the safety and liveness of concurrent systems. However, existing approaches often force a difficult trade-off: classical, projection-based techniques are compositional but limited in expressiveness, while more recent techniques achieve higher expressiveness by relying on non-compositional, whole-system model checking, which scales poorly.\n  This paper introduces a new approach to MPST that delivers both expressiveness and compositionality, called the synthetic approach. Our key innovation is a type system that verifies each process directly against a global protocol specification, represented as a labelled transition system (LTS) in general, with global types as a special case. This approach uniquely avoids the need for intermediate local types and projection.\n  We demonstrate that our approach, while conceptually simpler, supports a benchmark of challenging protocols that were previously beyond the reach of compositional techniques in the MPST literature. We generalise our type system, showing that it can validate processes against any specification that constitutes a \"well-behaved\" LTS, supporting protocols not expressible with the standard global type syntax. The entire framework, including all theorems and many examples, has been formalised and mechanised in Agda, and we have developed a prototype implementation as an extension to VS Code."}
{"id": "2511.23029", "categories": ["cs.GR", "cs.CV"], "pdf": "https://arxiv.org/pdf/2511.23029", "abs": "https://arxiv.org/abs/2511.23029", "authors": ["Tai Inui", "Alexander Matsumura", "Edgar Simo-Serra"], "title": "Geodiffussr: Generative Terrain Texturing with Elevation Fidelity", "comment": null, "summary": "Large-scale terrain generation remains a labor-intensive task in computer graphics. We introduce Geodiffussr, a flow-matching pipeline that synthesizes text-guided texture maps while strictly adhering to a supplied Digital Elevation Map (DEM). The core mechanism is multi-scale content aggregation (MCA): DEM features from a pretrained encoder are injected into UNet blocks at multiple resolutions to enforce global-to-local elevation consistency. Compared with a non-MCA baseline, MCA markedly improves visual fidelity and strengthens height-appearance coupling (FID $\\downarrow$ 49.16%, LPIPS $\\downarrow$ 32.33%, $Δ$dCor $\\downarrow$ to 0.0016). To train and evaluate Geodiffussr, we assemble a globally distributed, biome- and climate-stratified corpus of triplets pairing SRTM-derived DEMs with Sentinel-2 imagery and vision-grounded natural-language captions that describe visible land cover. We position Geodiffussr as a strong baseline and step toward controllable 2.5D landscape generation for coarse-scale ideation and previz, complementary to physically based terrain and ecosystem simulators."}
{"id": "2511.22372", "categories": ["cs.GT", "cs.LO", "cs.MA"], "pdf": "https://arxiv.org/pdf/2511.22372", "abs": "https://arxiv.org/abs/2511.22372", "authors": ["Eric Pacuit", "Leo Yang"], "title": "Common $p$-Belief with Plausibility Measures: Extended Abstract", "comment": "In Proceedings TARK 2025, arXiv:2511.20540", "summary": "Aumann's famous Agreeing to Disagree Theorem states that if a group of agents share a common prior, update their beliefs by Bayesian conditioning based on private information, and have common knowledge of their posterior beliefs regarding some event, these posteriors must be identical. There is an elegant generalization of this theorem by Monderer and Samet, later refined by Neeman: if a group of agents share a common prior, update their beliefs using Bayesian conditioning on private information, and have common p-belief of their posteriors, these posteriors must be close (i.e., they cannot differ by more than 1 - p). Here, common p-belief generalizes the concept of common knowledge to probabilistic beliefs: agents commonly p-believe an event E if everyone believes E to at least degree p, everyone believes to at least degree p that everyone believes E to at least degree p, and so on.\n  This paper further extends the Monderer-Samet-Neeman Agreement Theorem from classical probability measures to plausibility measures -- a very general framework introduced by Halpern that unifies many formal models of belief. To facilitate this extension, we provide a new proof of the Monderer-Samet-Neeman theorem in the classical setting. Building upon both the original proof and our new proof, we offer two different generalizations of the theorem to plausibility-based structures.\n  We then apply these generalized results to several non-classical belief models, including conditional probability structures and lexicographic probability structures. Moreover, we show that whenever our generalized theorems do not apply, the Monderer-Samet-Neeman Agreement Theorem fails. These findings suggest that our results successfully identify the minimal conditions required for a belief model to satisfy the Monderer-Samet-Neeman Agreement Theorem."}
{"id": "2511.23283", "categories": ["cs.PL"], "pdf": "https://arxiv.org/pdf/2511.23283", "abs": "https://arxiv.org/abs/2511.23283", "authors": ["Alexandre Moine", "Sam Westrick", "Joseph Tassarotti"], "title": "All for One and One for All: Program Logics for Exploiting Internal Determinism in Parallel Programs", "comment": "32 pages, 26 figures, extended version of the same paper accepted at POPL 2026", "summary": "Nondeterminism makes parallel programs challenging to write and reason about. To avoid these challenges, researchers have developed techniques for internally deterministic parallel programming, in which the steps of a parallel computation proceed in a deterministic way. Internal determinism is useful because it lets a programmer reason about a program as if it executed in a sequential order. However, no verification framework exists to exploit this property and simplify formal reasoning about internally deterministic programs.\n  To capture the essence of why internally deterministic programs should be easier to reason about, this paper defines a property called schedule-independent safety. A program satisfies schedule-independent safety, if, to show that the program is safe across all orderings, it suffices to show that one terminating execution of the program is safe. We then present a separation logic called Musketeer for proving that a program satisfies schedule-independent safety. Once a parallel program has been shown to satisfy schedule-independent safety, we can verify it with a new logic called Angelic, which allows one to dynamically select and verify just one sequential ordering of the program.\n  Using Musketeer, we prove the soundness of MiniDet, an affine type system for enforcing internal determinism. MiniDet supports several core algorithmic primitives for internally deterministic programming that have been identified in the research literature, including a deterministic version of a concurrent hash set. Because any syntactically well-typed MiniDet program satisfies schedule-independent safety, we can apply Angelic to verify such programs.\n  All results in this paper have been verified in Rocq using the Iris separation logic framework."}
{"id": "2511.23131", "categories": ["cs.GR"], "pdf": "https://arxiv.org/pdf/2511.23131", "abs": "https://arxiv.org/abs/2511.23131", "authors": ["Manas Chaudhary", "Chandradeep Pokhariya", "Rahul Narain"], "title": "Towards Generalized Position-Based Dynamics", "comment": null, "summary": "The position-based dynamics (PBD) algorithm is a popular and versatile technique for real-time simulation of deformable bodies, but is only applicable to forces that can be expressed as linearly compliant constraints. In this work, we explore a generalization of PBD that is applicable to arbitrary nonlinear force models. We do this by reformulating the implicit time integration equations in terms of the individual forces in the system, to which applying Gauss-Seidel iterations naturally leads to a PBD-type algorithm. As we demonstrate, our method allows simulation of data-driven cloth models [Sperl et al. 2020] that cannot be represented by existing variations of position-based dynamics, enabling performance improvements over the baseline Newton-based solver for high mesh resolutions. We also show our method's applicability to volumetric neo-Hookean elasticity with an inversion barrier."}
{"id": "2511.22384", "categories": ["cs.GT", "cs.CC"], "pdf": "https://arxiv.org/pdf/2511.22384", "abs": "https://arxiv.org/abs/2511.22384", "authors": ["Laryssa Horn", "Paul Nüsken", "Jörg Rothe", "Tessa Seeger"], "title": "Skating System Unveiled: Exploring Preference Aggregation in Ballroom Tournaments", "comment": "In Proceedings TARK 2025, arXiv:2511.20540", "summary": "The Skating System, which originated from the scrutineering system in dance sport tournaments, can be formulated as a voting system: We introduce and formalize the Skating System Single (SkS, for short), a new voting system embedded into the framework of computational social choice. Although SkS has similarities with Bucklin voting, it differs from it because it is subject to additional constraints when determining the election winners. Through an analysis of the axiomatic properties of SkS and of its vulnerability to manipulative and electoral control attacks, we compare SkS with Bucklin voting and provide insights into its potential strengths and weaknesses. In particular, we show that SkS satisfies nondictatorship as well as the majority criterion, positive responsiveness, monotonicity, and citizens' sovereignty but violates the Condorcet criterion, strong monotonicity, independence of clones, consistency, participation, resoluteness, and strategy-proofness. Further, we study manipulation, i.e., where (groups of) voters vote strategically to improve the outcome of an election in their favor, showing that the constructive coalitional weighted manipulation problem for SkS is NP-complete, while the destructive variant can be solved in polynomial time. Lastly, we initiate the study of electoral control, where an external agent attempts to change the election outcome by interfering with the structure of the election. Here, we show NP-completeness for constructive and destructive control by deleting candidates as well as for constructive control by adding voters, whereas we show that the problem of destructive control by adding voters can be solved in polynomial time."}
{"id": "2511.23358", "categories": ["cs.PL"], "pdf": "https://arxiv.org/pdf/2511.23358", "abs": "https://arxiv.org/abs/2511.23358", "authors": ["Alexandre Moine", "Stephanie Balzer", "Alex Xu", "Sam Westrick"], "title": "TypeDis: A Type System for Disentanglement", "comment": "34 pages, 24 figures, extended version of the same paper accepted at POPL 2026", "summary": "Disentanglement is a runtime property of parallel programs guaranteeing that parallel tasks remain oblivious to each other's allocations. As demonstrated in the MaPLe compiler and run-time system, disentanglement can be exploited for fast automatic memory management, especially task-local garbage collection with no synchronization between parallel tasks. However, as a low-level property, disentanglement can be difficult to reason about for programmers. The only means of statically verifying disentanglement so far has been DisLog, an Iris-fueled variant of separation logic, mechanized in the Rocq proof assistant. DisLog is a fully-featured program logic, allowing for proof of functional correctness as well as verification of disentanglement. Yet its employment requires significant expertise and per-program proof effort.\n  This paper explores the route of automatic verification via a type system, ensuring that any well-typed program is disentangled and lifting the burden of carrying out manual proofs from the programmer. It contributes TypeDis, a type system inspired by region types, where each type is annotated with a timestamp, identifying the task that allocated it. TypeDis supports iso-recursive types as well as polymorphism over both types and timestamps. Crucially, timestamps are allowed to change during type-checking, at join points as well as via a form of subtyping, dubbed subtiming. The paper illustrates TypeDis and its features on a range of examples. The soundness of TypeDis and the examples are mechanized in the Rocq proof assistant, using an improved version of DisLog, dubbed DisLog2."}
{"id": "2511.22388", "categories": ["cs.GT"], "pdf": "https://arxiv.org/pdf/2511.22388", "abs": "https://arxiv.org/abs/2511.22388", "authors": ["Nicodemo De Vito"], "title": "Prudent Rationalizability and the Best Rationalization Principle", "comment": "In Proceedings TARK 2025, arXiv:2511.20540", "summary": "We study cautious reasoning in finite sequential games played by agents with perfect recall. Our contribution lies in formulating a definition of prudent rationalizability (Heifetz et al. 2021, BEJTE) as an iterative reduction procedure of beliefs. To this end, we represent the players' beliefs by systems of conditional non-standard probability measures. The key novelty is the notion of c-strong belief, a non-standard, \"cautious\" version of strong belief (Battigalli and Siniscalchi 2002, JET). Our formulation of prudent rationalizability embodies a \"best rationalization principle\" similar to the one that underlies the solution concept of strong rationalizability. The main results show the equivalence between the proposed definition with the one originally put forth by Heifetz et al. (2021) in terms of conditional beliefs represented by standard probabilities. In particular, it is shown that prudent rationalizability can be algorithmically characterized by iterated admissibility. Finally, our formulation can be extended to sequential games with unawareness."}
{"id": "2511.23472", "categories": ["cs.PL"], "pdf": "https://arxiv.org/pdf/2511.23472", "abs": "https://arxiv.org/abs/2511.23472", "authors": ["Yusuke Matsushita", "Kengo Hirata", "Ryo Wakizaka", "Emanuele D'Osualdo"], "title": "RapunSL: Untangling Quantum Computing with Separation, Linear Combination and Mixing", "comment": "Full version of the conference paper at POPL 2026. The first two authors contributed equally to this work", "summary": "Quantum Separation Logic (QSL) has been proposed as an effective tool to improve the scalability of deductive reasoning for quantum programs. In QSL, separation is interpreted as disentanglement, and the frame rule brings a notion of entanglement-local specification (one that only talks about the qubits entangled with those acted upon by the program). In this paper, we identify two notions of locality unique to the quantum domain, and we construct a novel quantum separation logic, RapunSL, which is able to soundly reduce reasoning about superposition states to reasoning about pure states (basis-locality), and reasoning about mixed states arising from measurement to reasoning about pure states (outcome-locality). To do so, we introduce two connectives, linear combination and mixing, which together with separation provide a dramatic improvement in the scalability of reasoning, as we demonstrate on a series of challenging case studies."}
{"id": "2511.22918", "categories": ["cs.GT", "cs.CE"], "pdf": "https://arxiv.org/pdf/2511.22918", "abs": "https://arxiv.org/abs/2511.22918", "authors": ["Nan An", "Weian Li", "Qi Qi", "Changyuan Yu", "Liang Zhang"], "title": "Beyond Last-Click: An Optimal Mechanism for Ad Attribution", "comment": "39th Conference on Neural Information Processing Systems (NeurIPS 2025)", "summary": "Accurate attribution for multiple platforms is critical for evaluating performance-based advertising. However, existing attribution methods rely heavily on the heuristic methods, e.g., Last-Click Mechanism (LCM) which always allocates the attribution to the platform with the latest report, lacking theoretical guarantees for attribution accuracy. In this work, we propose a novel theoretical model for the advertising attribution problem, in which we aim to design the optimal dominant strategy incentive compatible (DSIC) mechanisms and evaluate their performance. We first show that LCM is not DSIC and performs poorly in terms of accuracy and fairness. To address this limitation, we introduce the Peer-Validated Mechanism (PVM), a DSIC mechanism in which a platform's attribution depends solely on the reports of other platforms. We then examine the accuracy of PVM across both homogeneous and heterogeneous settings, and provide provable accuracy bounds for each case. Notably, we show that PVM is the optimal DSIC mechanism in the homogeneous setting. Finally, numerical experiments are conducted to show that PVM consistently outperforms LCM in terms of attribution accuracy and fairness."}
{"id": "2511.22925", "categories": ["cs.GT"], "pdf": "https://arxiv.org/pdf/2511.22925", "abs": "https://arxiv.org/abs/2511.22925", "authors": ["Nan An", "Weian Li", "Qi Qi", "Liang Zhang"], "title": "Merging Mechanisms for Ads and Organic Items in E-commerce Platforms", "comment": "Proceedings of the AAAI Conference on Artificial Intelligence, Vol. 39 No. 13: AAAI-25 Technical Tracks 13 (pp. 13547-13554)", "summary": "In contemporary e-commerce platforms, search result pages display two types of items: ad items and organic items. Ad items are determined through an advertising auction system, while organic items are selected by a recommendation system. These systems have distinct optimization objectives, creating the challenge of effectively merging these two components. Recent research has explored merging mechanisms for e-commerce platforms, but none have simultaneously achieved all desirable properties: incentive compatibility, individual rationality, adaptability to multiple slots, integration of inseparable candidates, and avoidance of repeated exposure for ads and organic items. This paper addresses the design of a merging mechanism that satisfies all these properties. We first provide the necessary conditions for the optimal merging mechanisms. Next, we introduce two simple and effective mechanisms, termed the generalized fix mechanism and the generalized change mechanism. Finally, we theoretically prove that both mechanisms offer guaranteed approximation ratios compared to the optimal mechanism in both simplest and general settings."}
{"id": "2511.23097", "categories": ["cs.GT", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.23097", "abs": "https://arxiv.org/abs/2511.23097", "authors": ["Georgios Papasotiropoulos", "Zein Pishbin"], "title": "Fairness in the Multi-Secretary Problem", "comment": "AAAI'26", "summary": "This paper bridges two perspectives: it studies the multi-secretary problem through the fairness lens of social choice, and examines multi-winner elections from the viewpoint of online decision making. After identifying the limitations of the prominent proportionality notion of Extended Justified Representation (EJR) in the online domain, the work proposes a set of mechanisms that merge techniques from online algorithms with rules from social choice -- such as the Method of Equal Shares and the Nash Rule -- and supports them through both theoretical analysis and extensive experimental evaluation."}
{"id": "2511.23454", "categories": ["cs.GT"], "pdf": "https://arxiv.org/pdf/2511.23454", "abs": "https://arxiv.org/abs/2511.23454", "authors": ["Alexander Heckett", "Vincent Conitzer"], "title": "Designing Rules for Choosing a Winner in a Debate", "comment": "22 pages, 3 tables", "summary": "We consider settings where an uninformed principal must hear arguments from two better-informed agents, corresponding to two possible courses of action that they argue for. The arguments are verifiable in the sense that the true state of the world restricts the arguments that can be made by the agents. Each agent simply wants to be chosen as the winner and does so strategically based on the rule set by the principal. How should the principal design the rule to choose the better action? We provide a formal framework for answering this question, exhibit some basic properties of it, study the computational problems of evaluating and optimizing the principal's policy, and provide key error bounds."}
