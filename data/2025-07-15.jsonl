{"id": "2507.09140", "categories": ["cs.GR"], "pdf": "https://arxiv.org/pdf/2507.09140", "abs": "https://arxiv.org/abs/2507.09140", "authors": ["Chuang Chen", "Xiaoxuan Xie", "Yongming Zhang", "Tianyu Zhang", "Haoran Xie"], "title": "Interactive Drawing Guidance for Anime Illustrations with Diffusion Model", "comment": "9 pages, 7 figures. In proceedings of NICOGRAPH International 2025", "summary": "Creating high-quality anime illustrations presents notable challenges,\nparticularly for beginners, due to the intricate styles and fine details\ninherent in anime art. We present an interactive drawing guidance system\nspecifically designed for anime illustrations to address this issue. It offers\nreal-time guidance to help users refine their work and streamline the creative\nprocess. Our system is built upon the StreamDiffusion pipeline to deliver\nreal-time drawing assistance. We fine-tune Stable Diffusion with LoRA to\nsynthesize anime style RGB images from user-provided hand-drawn sketches and\nprompts. Leveraging the Informative Drawings model, we transform these RGB\nimages into rough sketches, which are further refined into structured guidance\nsketches using a custom-designed optimizer. The proposed system offers precise,\nreal-time guidance aligned with the creative intent of the user, significantly\nenhancing both the efficiency and accuracy of the drawing process. To assess\nthe effectiveness of our approach, we conducted a user study, gathering\nempirical feedback on both system performance and interface usability."}
{"id": "2507.09146", "categories": ["cs.GR"], "pdf": "https://arxiv.org/pdf/2507.09146", "abs": "https://arxiv.org/abs/2507.09146", "authors": ["Ryuichi Miyauchi", "Hengyuan Chang", "Tsukasa Fukusato", "Kazunori Miyata", "Haoran Xie"], "title": "Physics-Aware Fluid Field Generation from User Sketches Using Helmholtz-Hodge Decomposition", "comment": "8 pages, 12 figures. In proceedings of NICOGRAPH International 2025", "summary": "Fluid simulation techniques are widely used in various fields such as film\nproduction, but controlling complex fluid behaviors remains challenging. While\nrecent generative models enable intuitive generation of vector fields from user\nsketches, they struggle to maintain physical properties such as\nincompressibility. To address these issues, this paper proposes a method for\ninteractively designing 2D vector fields. Conventional generative models can\nintuitively generate vector fields from user sketches, but remain difficult to\nconsider physical properties. Therefore, we add a simple editing process after\ngenerating the vector field. In the first stage, we use a latent diffusion\nmodel~(LDM) to automatically generate initial 2D vector fields from user\nsketches. In the second stage, we apply the Helmholtz-Hodge decomposition to\nlocally extract physical properties such as incompressibility from the results\ngenerated by LDM and recompose them according to user intentions. Through\nmultiple experiments, we demonstrate the effectiveness of our proposed method."}
{"id": "2507.09441", "categories": ["cs.GR", "cs.CV"], "pdf": "https://arxiv.org/pdf/2507.09441", "abs": "https://arxiv.org/abs/2507.09441", "authors": ["Ankit Sanjyal"], "title": "RectifiedHR: High-Resolution Diffusion via Energy Profiling and Adaptive Guidance Scheduling", "comment": "8 Pages, 10 Figures, Pre-Print Version, Code Available at:\n  https://github.com/ANKITSANJYAL/RectifiedHR", "summary": "High-resolution image synthesis with diffusion models often suffers from\nenergy instabilities and guidance artifacts that degrade visual quality. We\nanalyze the latent energy landscape during sampling and propose adaptive\nclassifier-free guidance (CFG) schedules that maintain stable energy\ntrajectories. Our approach introduces energy-aware scheduling strategies that\nmodulate guidance strength over time, achieving superior stability scores\n(0.9998) and consistency metrics (0.9873) compared to fixed-guidance\napproaches. We demonstrate that DPM++ 2M with linear-decreasing CFG scheduling\nyields optimal performance, providing sharper, more faithful images while\nreducing artifacts. Our energy profiling framework serves as a powerful\ndiagnostic tool for understanding and improving diffusion model behavior."}
{"id": "2507.08846", "categories": ["cs.GT", "cs.DC", "cs.DS"], "pdf": "https://arxiv.org/pdf/2507.08846", "abs": "https://arxiv.org/abs/2507.08846", "authors": ["Serdar Metin"], "title": "Precomputed Dominant Resource Fairness", "comment": "9 pages", "summary": "Although resource allocation is a well studied problem in computer science,\nuntil the prevalence of distributed systems, such as computing clouds and data\ncentres, the question had been addressed predominantly for single resource type\nscenarios. At the beginning of the last decade, with the introuction of\nDominant Resource Fairness, the studies of the resource allocation problem has\nfinally extended to the multiple resource type scenarios. Dominant Resource\nFairness is a solution, addressing the problem of fair allocation of multiple\nresource types, among users with heterogeneous demands. Based on Max-min\nFairness, which is a well established algorithm in the literature for\nallocating resources in the single resource type scenarios, Dominant Resource\nFairness generalises the scheme to the multiple resource case. It has a number\nof desirable properties that makes it preferable over alternatives, such as\nSharing Incentive, Envy-Freeness, Pareto Efficiency, and Strategy Proofness,\nand as such, it is widely adopted in distributed systems. In the present study,\nwe revisit the original study, and analyse the structure of the algorithm in\ncloser view, to come up with an alternative algorithm, which approximates the\nDominant Resource Fairness allocation in fewer steps. We name the new algorithm\nPrecomputed Dominant Resource Fairness, after its main working principle."}
{"id": "2507.09704", "categories": ["cs.GR"], "pdf": "https://arxiv.org/pdf/2507.09704", "abs": "https://arxiv.org/abs/2507.09704", "authors": ["Xiaotang Zhang", "Ziyi Chang", "Qianhui Men", "Hubert Shum"], "title": "Real-time and Controllable Reactive Motion Synthesis via Intention Guidance", "comment": null, "summary": "We propose a real-time method for reactive motion synthesis based on the\nknown trajectory of input character, predicting instant reactions using only\nhistorical, user-controlled motions. Our method handles the uncertainty of\nfuture movements by introducing an intention predictor, which forecasts key\njoint intentions to make pose prediction more deterministic from the historical\ninteraction. The intention is later encoded into the latent space of its\nreactive motion, matched with a codebook which represents mappings between\ninput and output. It samples a categorical distribution for pose generation and\nstrengthens model robustness through adversarial training. Unlike previous\noffline approaches, the system can recursively generate intentions and reactive\nmotions using feedback from earlier steps, enabling real-time, long-term\nrealistic interactive synthesis. Both quantitative and qualitative experiments\nshow our approach outperforms other matching-based motion synthesis approaches,\ndelivering superior stability and generalizability. In our method, user can\nalso actively influence the outcome by controlling the moving directions,\ncreating a personalized interaction path that deviates from predefined\ntrajectories."}
{"id": "2507.08868", "categories": ["cs.GT", "cs.DC", "91B26", "J.1; J.4"], "pdf": "https://arxiv.org/pdf/2507.08868", "abs": "https://arxiv.org/abs/2507.08868", "authors": ["Benedikt Pittl", "Werner Mach", "Erich Schikuta"], "title": "A Survey on Bilateral Multi-Round Cloud-SLA Negotiation Strategies", "comment": "Preprint", "summary": "Today, static cloud markets where consumers purchase services directly from\nproviders are dominating. Thus, consumers neither negotiate the price nor the\ncharacteristics of the service. In recent years, providers have adopted more\ndynamic trading mechanisms, as e.g. Amazon's EC2 platform shows: In addition to\nthe reservation marketspace and the on-demand marketspace, Amazon offers a spot\nmarketspace where consumers can bid for virtual machines. This spot marketspace\nwas extended with spot blocks, and recently Amazon reworked the bidding\noptions. In addition, other cloud providers, such as Virtustream, adopt dynamic\ntrading mechanisms. The scientific community envisions autonomous multi-round\nnegotiations for realizing future cloud marketspaces. Consequently, consumers\nand providers exchange offers and counteroffers to reach an agreement. This\nhelps providers increase the utilization of their datacenters, while consumers\ncan purchase highly customized cloud services.\n  In the paper at hand, we present a survey on multi-round bilateral\nnegotiation strategies for trading cloud resources. Thus, we analyzed\npeer-reviewed articles in order to identify trends, gaps, similarities, and the\nscope of such negotiation strategies. In addition, we surveyed the formalism\nthat the scientific community uses to describe such strategies. Based on these\nfindings, we derived recommendations for creating and documenting bilateral\nmulti-round negotiation strategies to foster their implementation in the\nindustry."}
{"id": "2507.09539", "categories": ["cs.PL"], "pdf": "https://arxiv.org/pdf/2507.09539", "abs": "https://arxiv.org/abs/2507.09539", "authors": ["Anna Bolotina", "Christoph M. Kirsch", "Stefanie Muroya Lei", "Matthias Pleschinger"], "title": "Bounded Model Checking of RISC-V Machine Code with Context-Free-Language Ordered Binary Decision Diagrams", "comment": null, "summary": "Symbolic execution is a powerful technique for analyzing the behavior of\nsoftware yet scalability remains a challenge due to state explosion in control\nand data flow. Existing tools typically aim at managing control flow\ninternally, often at the expense of completeness, while offloading reasoning\nover data flow to SMT solvers. Moreover, reasoning typically happens on source\ncode or intermediate representation level to leverage structural information,\nmaking machine code generation part of the trust base. We are interested in\nchanging the equation in two non-trivial ways: pushing reasoning down to\nmachine code level, and then offloading reasoning entirely into SMT solvers and\nother, possibly more efficient solver technology. In more abstract terms, we\nare asking if bit-precise reasoning technology can be made scalable on\nsoftware, and not just hardware. For this purpose, we developed two tools\ncalled rotor and bitme for model generation and bounded model checking,\nrespectively. We chose RISC-V restricted to integer arithmetic as modeling\ntarget for rotor since RISC-V integer semantics is essentially equivalent to\nestablished SMT semantics over bitvectors and arrays of bitvectors. While\nstate-of-the-art SMT solvers struggle in our experiments, we have evidence that\nthere is potential for improvement. To show the potential, we have slightly\ngeneralized and then implemented in bitme two types of binary decision diagrams\n(BDDs): algebraic decision diagrams (ADDs) and context-free-language ordered\nbinary decision diagrams (CFLOBDDs). Bitme uses BDDs to propagate program input\nthrough models, essentially generalizing constant propagation to domain\npropagation. SMT solvers only get involved when model input cannot be\npropagated, significanly speeding up SMT solving. We then study the impact on\nstate explosion of CFLOBDDs, which are potentially more scalable than ADDs."}
{"id": "2507.09792", "categories": ["cs.GR", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2507.09792", "abs": "https://arxiv.org/abs/2507.09792", "authors": ["Prashant Govindarajan", "Davide Baldelli", "Jay Pathak", "Quentin Fournier", "Sarath Chandar"], "title": "CADmium: Fine-Tuning Code Language Models for Text-Driven Sequential CAD Design", "comment": null, "summary": "Computer-aided design (CAD) is the digital construction of 2D and 3D objects,\nand is central to a wide range of engineering and manufacturing applications\nlike automobile and aviation. Despite its importance, CAD modeling remains\nlargely a time-intensive, manual task. Recent works have attempted to automate\nthis process with small transformer-based models and handcrafted CAD sequence\nrepresentations. However, there has been little effort to leverage the\npotential of large language models (LLMs) for sequential CAD design. In this\nwork, we introduce a new large-scale dataset of more than 170k CAD models\nannotated with high-quality, human-like descriptions generated with our\npipeline based on GPT-4.1. Using this dataset, we fine-tune powerful code-LLMs\nto generate CAD sequences represented in a JSON-based format from natural\nlanguage descriptions, demonstrating the viability and effectiveness of this\napproach for text-conditioned CAD generation. Because simple metrics often fail\nto reflect the quality of generated objects, we introduce geometric and\ntopological metrics based on sphericity, mean curvature, and Euler\ncharacteristic to provide richer structural insights. Our experiments and\nablation studies on both synthetic and human-annotated data demonstrate that\nCADmium is able to automate CAD design, drastically speeding up the design of\nnew objects. The dataset, code, and fine-tuned models are available online."}
{"id": "2507.09083", "categories": ["cs.GT", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.09083", "abs": "https://arxiv.org/abs/2507.09083", "authors": ["Anand Shah", "Kehang Zhu", "Yanchen Jiang", "Jeffrey G. Wang", "Arif K. Dayi", "John J. Horton", "David C. Parkes"], "title": "Learning from Synthetic Labs: Language Models as Auction Participants", "comment": null, "summary": "This paper investigates the behavior of simulated AI agents (large language\nmodels, or LLMs) in auctions, introducing a novel synthetic data-generating\nprocess to help facilitate the study and design of auctions. We find that LLMs\n-- when endowed with chain of thought reasoning capacity -- agree with the\nexperimental literature in auctions across a variety of classic auction\nformats. In particular, we find that LLM bidders produce results consistent\nwith risk-averse human bidders; that they perform closer to theoretical\npredictions in obviously strategy-proof auctions; and, that they succumb to the\nwinner's curse in common value settings. On prompting, we find that LLMs are\nnot very sensitive to naive changes in prompts (e.g., language, currency) but\ncan improve dramatically towards theoretical predictions with the right mental\nmodel (i.e., the language of Nash deviations). We run 1,000$+$ auctions for\nless than $\\$$400 with GPT-4 models (three orders of magnitude cheaper than\nmodern auction experiments) and develop a framework flexible enough to run\nauction experiments with any LLM model and a wide range of auction design\nspecifications, facilitating further experimental study by decreasing costs and\nserving as a proof-of-concept for the use of LLM proxies."}
{"id": "2507.09883", "categories": ["cs.PL"], "pdf": "https://arxiv.org/pdf/2507.09883", "abs": "https://arxiv.org/abs/2507.09883", "authors": ["Swarn Priya", "Frédéric Besson", "Connor Sughrue", "Tim Steenvoorden", "Jamie Fulford", "Freek Verbeek", "Binoy Ravindran"], "title": "BeePL: Correct-by-compilation kernel extensions", "comment": "45 pages, 18 figures", "summary": "eBPF is a technology that allows developers to safely extend kernel\nfunctionality without modifying kernel source code or developing loadable\nkernel modules. Since the kernel governs critical system operations and\nenforces isolation boundaries between user space and privileged data, any\nmechanism that modifies its behavior must meet the highest standards of safety\nand correctness. To this end, the eBPF toolchain includes a verifier, which\nstatically checks safety properties such as memory access validity, bounded\nloops, and type correctness before loading the program into the kernel.\nHowever, the existing verifier is both overly conservative in some\ncases-rejecting valid programs-and unsound in others, permitting unsafe\nbehavior that violates the intended semantics of the kernel interface.\n  To address these challenges, we introduce BeePL, a domain-specific language\nfor eBPF with a formally verified type system. The BeePL type system, along\nwith the language design, statically enforces key safety properties such as\ntype-correct memory access, safe pointer usage, absence of unbounded loops, and\nstructured control flow. These guarantees are backed by formal type soundness\nproofs, ensuring that well-typed programs satisfy the safety invariants\nrequired by the eBPF execution environment. BeePL also proves that well-typed\nsource programs meet critical eBPF-specific properties related to memory\nsafety, termination, and control flow, enabling high-level reasoning prior to\ncompilation. For properties not fully enforceable statically-such as dynamic\nbounds and undefined behavior-BeePL inserts semantics-preserving runtime checks\nduring compilation. We develop a verified compilation strategy that extends\nCompCert to generate BPF bytecode from BeePL programs, establishing a\nprincipled foundation for an end-to-end verifiable toolchain for safe kernel\nextensions."}
{"id": "2507.10542", "categories": ["cs.GR", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2507.10542", "abs": "https://arxiv.org/abs/2507.10542", "authors": ["Shivangi Aneja", "Sebastian Weiss", "Irene Baeza", "Prashanth Chandran", "Gaspard Zoss", "Matthias Nießner", "Derek Bradley"], "title": "ScaffoldAvatar: High-Fidelity Gaussian Avatars with Patch Expressions", "comment": "(SIGGRAPH 2025) Paper Video: https://youtu.be/VyWkgsGdbkk Project\n  Page: https://shivangi-aneja.github.io/projects/scaffoldavatar/", "summary": "Generating high-fidelity real-time animated sequences of photorealistic 3D\nhead avatars is important for many graphics applications, including immersive\ntelepresence and movies. This is a challenging problem particularly when\nrendering digital avatar close-ups for showing character's facial microfeatures\nand expressions. To capture the expressive, detailed nature of human heads,\nincluding skin furrowing and finer-scale facial movements, we propose to couple\nlocally-defined facial expressions with 3D Gaussian splatting to enable\ncreating ultra-high fidelity, expressive and photorealistic 3D head avatars. In\ncontrast to previous works that operate on a global expression space, we\ncondition our avatar's dynamics on patch-based local expression features and\nsynthesize 3D Gaussians at a patch level. In particular, we leverage a\npatch-based geometric 3D face model to extract patch expressions and learn how\nto translate these into local dynamic skin appearance and motion by coupling\nthe patches with anchor points of Scaffold-GS, a recent hierarchical scene\nrepresentation. These anchors are then used to synthesize 3D Gaussians\non-the-fly, conditioned by patch-expressions and viewing direction. We employ\ncolor-based densification and progressive training to obtain high-quality\nresults and faster convergence for high resolution 3K training images. By\nleveraging patch-level expressions, ScaffoldAvatar consistently achieves\nstate-of-the-art performance with visually natural motion, while encompassing\ndiverse facial expressions and styles in real time."}
{"id": "2507.09422", "categories": ["cs.GT", "math.NT", "91A06, 91A10"], "pdf": "https://arxiv.org/pdf/2507.09422", "abs": "https://arxiv.org/abs/2507.09422", "authors": ["Edan Orzech", "Martin Rinard"], "title": "Nash Equilibria with Irradical Probabilities", "comment": null, "summary": "We present for every $n\\ge4$ an $n$-player game in normal form with payoffs\nin $\\{0,1,2\\}$ that has a unique, fully mixed, Nash equilibrium in which all\nthe probability weights are irradical (i.e., algebraic but not closed form\nexpressible even with $m$-th roots for any integer $m$)."}
{"id": "2507.10301", "categories": ["cs.PL"], "pdf": "https://arxiv.org/pdf/2507.10301", "abs": "https://arxiv.org/abs/2507.10301", "authors": ["Wenhao Tang", "Sam Lindley"], "title": "Rows and Capabilities as Modal Effects", "comment": null, "summary": "Effect handlers allow programmers to model and compose computational effects\nmodularly. Effect systems statically guarantee that all effects are handled.\nSeveral recent practical effect systems are based on either row polymorphism or\ncapabilities. However, there remains a gap in understanding the precise\nrelationship between effect systems with such disparate foundations. The main\ndifficulty is that in both row-based and capability-based systems, effect\ntracking is typically entangled with other features such as functions.\n  We propose a uniform framework for encoding, analysing, and comparing effect\nsystems. Our framework exploits and generalises modal effect types, a recent\nnovel effect system which decouples effect tracking from functions via\nmodalities. Modalities offer fine-grained control over when and how effects are\ntracked, enabling us to express different strategies for effect tracking. We\ngive encodings as macro translations from existing row-based and\ncapability-based effect systems into our framework and show that these\nencodings preserve types and semantics. Our encodings reveal the essence of\neffect tracking mechanisms in different effect systems, enable a direct\nanalysis on their differences, and provide valuable insights on language\ndesign."}
{"id": "2507.09473", "categories": ["cs.GT", "cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2507.09473", "abs": "https://arxiv.org/abs/2507.09473", "authors": ["Yan Dai", "Negin Golrezaei", "Patrick Jaillet"], "title": "Incentive-Aware Dynamic Resource Allocation under Long-Term Cost Constraints", "comment": null, "summary": "Motivated by applications such as cloud platforms allocating GPUs to users or\ngovernments deploying mobile health units across competing regions, we study\nthe dynamic allocation of a reusable resource to strategic agents with private\nvaluations. Our objective is to simultaneously (i) maximize social welfare,\n(ii) satisfy multi-dimensional long-term cost constraints, and (iii)\nincentivize truthful reporting. We begin by numerically evaluating primal-dual\nmethods widely used in constrained online optimization and find them to be\nhighly fragile in strategic settings -- agents can easily manipulate their\nreports to distort future dual updates for future gain.\n  To address this vulnerability, we develop an incentive-aware framework that\nmakes primal-dual methods robust to strategic behavior. Our design combines\nepoch-based lazy updates -- where dual variables remain fixed within each epoch\n-- with randomized exploration rounds that extract approximately truthful\nsignals for learning. Leveraging carefully designed online learning subroutines\nthat can be of independent interest for dual updates, our mechanism achieves\n$\\tilde{\\mathcal{O}}(\\sqrt{T})$ social welfare regret, satisfies all cost\nconstraints, and ensures incentive alignment. This matches the performance of\nnon-strategic allocation approaches while being robust to strategic agents."}
{"id": "2507.10482", "categories": ["cs.PL", "cs.LO"], "pdf": "https://arxiv.org/pdf/2507.10482", "abs": "https://arxiv.org/abs/2507.10482", "authors": ["Simon Guilloud", "Viktor Kunčak"], "title": "Orthologic Type Systems", "comment": null, "summary": "We propose to use orthologic as the basis for designing type systems\nsupporting intersection, union, and negation types in the presence of subtyping\nassumptions. We show how to extend orthologic to support monotonic and\nantimonotonic functions, supporting the use of type constructors in such type\nsystems. We present a proof system for orthologic with function symbols,\nshowing that it admits partial cut elimination. Using these insights, we\npresent an $\\mathcal O(n^2(1+m))$ algorithm for deciding the subtyping relation\nunder $m$ assumptions. We also show $O(n^2)$ polynomial-time normalization\nalgorithm, allowing simplification of types to their minimal canonical form."}
{"id": "2507.09544", "categories": ["cs.GT"], "pdf": "https://arxiv.org/pdf/2507.09544", "abs": "https://arxiv.org/abs/2507.09544", "authors": ["Ryoga Mahara"], "title": "Existence of Fair and Efficient Allocation of Indivisible Chores", "comment": "33pages, 2 figures", "summary": "We study the problem of allocating indivisible chores among agents with\nadditive cost functions in a fair and efficient manner. A major open question\nin this area is whether there always exists an allocation that is envy-free up\nto one chore (EF1) and Pareto optimal (PO). Our main contribution is to provide\na positive answer to this question by proving the existence of such an\nallocation for indivisible chores under additive cost functions. This is\nachieved by a novel combination of a fixed point argument and a discrete\nalgorithm, providing a significant methodological advance in this area.\n  Our additional key contributions are as follows. We show that there always\nexists an allocation that is EF1 and fractional Pareto optimal (fPO), where fPO\nis a stronger efficiency concept than PO. We also show that an EF1 and PO\nallocation can be computed in polynomial time when the number of agents is\nconstant. Finally, we extend all of these results to the more general setting\nof weighted EF1 (wEF1), which accounts for the entitlements of agents."}
{"id": "2507.09902", "categories": ["cs.GT"], "pdf": "https://arxiv.org/pdf/2507.09902", "abs": "https://arxiv.org/abs/2507.09902", "authors": ["Yuanhao Wang"], "title": "Tie-breaking Agnostic Lower Bound for Fictitious Play", "comment": null, "summary": "Fictitious play (FP) is a natural learning dynamic in two-player zero-sum\ngames. Samuel Karlin conjectured in 1959 that FP converges at a rate of\n$O(t^{-1/2})$ to Nash equilibrium, where $t$ is the number of steps played.\nHowever, Daskalakis and Pan disproved the stronger form of this conjecture in\n2014, where \\emph{adversarial} tie-breaking is allowed.\n  This paper disproves Karlin's conjecture in its weaker form. In particular,\nthere exists a 10-by-10 zero-sum matrix game, in which FP converges at a rate\nof $\\Omega(t^{-1/3})$, and no ties occur except for the first step."}
{"id": "2507.09928", "categories": ["cs.GT", "math.OC"], "pdf": "https://arxiv.org/pdf/2507.09928", "abs": "https://arxiv.org/abs/2507.09928", "authors": ["Apurv Shukla", "Vijay Subramanian", "Andy Zhao", "Rahul Jain"], "title": "Generalized Quantal Response Equilibrium: Existence and Efficient Learning", "comment": null, "summary": "We introduce a new solution concept for bounded rational agents in finite\nnormal-form general-sum games called Generalized Quantal Response Equilibrium\n(GQRE) which generalizes Quantal Response\nEquilibrium~\\citep{mckelvey1995quantal}. In our setup, each player maximizes a\nsmooth, regularized expected utility of the mixed profiles used, reflecting\nbounded rationality that subsumes stochastic choice. After establishing\nexistence under mild conditions, we present computationally efficient no-regret\nindependent learning via smoothened versions of the Frank-Wolfe algorithm. Our\nalgorithm uses noisy but correlated gradient estimates generated via a\nsimulation oracle that reports on repeated plays of the game. We analyze\nconvergence properties of our algorithm under assumptions that ensure\nuniqueness of equilibrium, using a class of gap functions that generalize the\nNash gap. We end by demonstrating the effectiveness of our method on a set of\ncomplex general-sum games such as high-rank two-player games, large action\ntwo-player games, and known examples of difficult multi-player games."}
{"id": "2507.09972", "categories": ["cs.GT", "cs.CY", "econ.GN", "q-fin.EC"], "pdf": "https://arxiv.org/pdf/2507.09972", "abs": "https://arxiv.org/abs/2507.09972", "authors": ["Lucas Barbosa", "Sam Kirshner", "Rob Kopel", "Eric Tze Kuan Lim", "Tom Pagram"], "title": "A New Incentive Model For Content Trust", "comment": "20 pages, 6 figures and 2 tables", "summary": "This paper outlines an incentive-driven and decentralized approach to\nverifying the veracity of digital content at scale. Widespread misinformation,\nan explosion in AI-generated content and reduced reliance on traditional news\nsources demands a new approach for content authenticity and truth-seeking that\nis fit for a modern, digital world. By using smart contracts and digital\nidentity to incorporate 'trust' into the reward function for published content,\nnot just engagement, we believe that it could be possible to foster a\nself-propelling paradigm shift to combat misinformation through a\ncommunity-based governance model. The approach described in this paper requires\nthat content creators stake financial collateral on factual claims for an\nimpartial jury to vet with a financial reward for contribution. We hypothesize\nthat with the right financial and social incentive model users will be\nmotivated to participate in crowdsourced fact-checking and content creators\nwill place more care in their attestations. This is an exploratory paper and\nthere are a number of open issues and questions that warrant further analysis\nand exploration."}
{"id": "2507.10149", "categories": ["cs.GT", "cs.CE", "q-fin.TR"], "pdf": "https://arxiv.org/pdf/2507.10149", "abs": "https://arxiv.org/abs/2507.10149", "authors": ["Abhimanyu Nag", "Madhur Prabhakar", "Tanuj Behl"], "title": "A Coincidence of Wants Mechanism for Swap Trade Execution in Decentralized Exchanges", "comment": null, "summary": "We propose a mathematically rigorous framework for identifying and completing\nCoincidence of Wants (CoW) cycles in decentralized exchange (DEX) aggregators.\nUnlike existing auction based systems such as CoWSwap, our approach introduces\nan asset matrix formulation that not only verifies feasibility using oracle\nprices and formal conservation laws but also completes partial CoW cycles of\nswap orders that are discovered using graph traversal and are settled using\nimbalance correction. We define bridging orders and show that the resulting\nexecution is slippage free and capital preserving for LPs. Applied to real\nworld Arbitrum swap data, our algorithm demonstrates efficient discovery of CoW\ncycles and supports the insertion of synthetic orders for atomic cycle closure.\nThis work can be thought of as the detailing of a potential delta-neutral\nstrategy by liquidity providing market makers: a structured CoW cycle\nexecution."}
{"id": "2507.10550", "categories": ["cs.GT"], "pdf": "https://arxiv.org/pdf/2507.10550", "abs": "https://arxiv.org/abs/2507.10550", "authors": ["Quentin Guilmant", "Joël Ouaknine", "Isa Vialard"], "title": "The Value Problem for Weighted Timed Games with Two Clocks is Undecidable", "comment": null, "summary": "The Value Problem for weighted timed games (WTGs) consists in determining,\ngiven a two-player weighted timed game with a reachability objective and a\nrational threshold, whether or not the value of the game exceeds the threshold.\nThis problem was shown to be undecidable some ten years ago for WTGs making use\nof at least three clocks, and is known to be decidable for single-clock WTGs.\nIn this paper, we establish undecidability for two-clock WTGs making use of\nnon-negative weights, even in a time-bounded setting, closing the last\nremaining major gap in our algorithmic understanding of WTGs."}
