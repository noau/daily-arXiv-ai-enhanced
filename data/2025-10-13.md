<div id=toc></div>

# Table of Contents

- [cs.GR](#cs.GR) [Total: 5]
- [cs.PL](#cs.PL) [Total: 5]
- [cs.GT](#cs.GT) [Total: 4]


<div id='cs.GR'></div>

# cs.GR [[Back]](#toc)

### [1] [Generating Sizing Fields for Mesh Generation via GCN-based Simplification of Adaptive Background Grids](https://arxiv.org/abs/2510.08645)
*Xunyang Zhu,Hongfei Ye,Yifei Wang,Taoran Liu,Jianjun Chen*

Main category: cs.GR

TL;DR: 论文提出了一种基于图卷积网络（GCN）的自适应背景网格简化（ABGS）框架，用于优化非结构化网格生成的背景网格质量与效率。


<details>
  <summary>Details</summary>
Motivation: 背景网格的质量和效率对非结构化网格生成至关重要，但现有方法难以同时满足几何一致性、计算轻量化和无伪影（如带状伪影）的要求。

Method: 作者将网格简化任务重新定义为边得分回归问题，并使用GCN模型高效预测最优边折叠候选。通过定制损失函数综合考虑几何保真度和尺寸场准确性，替代了昂贵的程序化评估。

Result: 实验结果显示，该方法在多种复杂工程模型中均表现出色，简化后的背景网格元素减少了74%-94%，尺寸场查询时间减少了35%-88%。

Conclusion: 该方法显著提升了背景网格的简化效率和生成质量，为非结构化网格生成领域提供了高效的数据驱动解决方案。

Abstract: The sizing field defined on a triangular background grid is pivotal for
controlling the quality and efficiency of unstructured mesh generation.
However, creating an optimal background grid that is geometrically conforming,
computationally lightweight, and free from artifacts like banding is a
significant challenge. This paper introduces a novel, adaptive background grid
simplification (ABGS) framework based on a Graph Convolutional Network (GCN).
We reformulate the grid simplification task as an edge score regression problem
and train a GCN model to efficiently predict optimal edge collapse candidates.
The model is guided by a custom loss function that holistically considers both
geometric fidelity and sizing field accuracy. This data-driven approach
replaces a costly procedural evaluation, accelerating the simplification
process. Experimental results demonstrate the effectiveness of our framework
across diverse and complex engineering models. Compared to the initial dense
grids, our simplified background grids achieve an element reduction of 74%-94%,
leading to a 35%-88% decrease in sizing field query times.

</details>


### [2] [A 3D Generation Framework from Cross Modality to Parameterized Primitive](https://arxiv.org/abs/2510.08656)
*Yiming Liang,Huan Yu,Zili Wang,Shuyou Zhang,Guodong Yi,Jin Wang,Jianrong Tan*

Main category: cs.GR

TL;DR: 本文提出了一种基于参数化基元的多阶段框架，用于生成由文本和图像输入引导的3D模型，解决了表面平滑和存储开销的问题。


<details>
  <summary>Details</summary>
Motivation: 当前AI驱动的3D模型生成在多模态方面取得进展，但仍面临生成平滑表面和减少存储开销的挑战。

Method: 提出一种基于参数化基元的模型生成算法，识别模型组成元素的形状特征，并用高质量表面的参数化基元替换。同时，提出一种保留表面质量并仅存储基元参数的存储方法。

Result: 在虚拟和真实场景数据集上的实验显示，该方法取得了Chamfer距离0.003092、VIoU 0.545、F1分数0.9139和NC 0.8369的结果，基元参数文件大小约为6KB。

Conclusion: 该方法特别适用于简单模型的快速原型设计。

Abstract: Recent advancements in AI-driven 3D model generation have leveraged cross
modality, yet generating models with smooth surfaces and minimizing storage
overhead remain challenges. This paper introduces a novel multi-stage framework
for generating 3D models composed of parameterized primitives, guided by
textual and image inputs. In the framework, A model generation algorithm based
on parameterized primitives, is proposed, which can identifies the shape
features of the model constituent elements, and replace the elements with
parameterized primitives with high quality surface. In addition, a
corresponding model storage method is proposed, it can ensure the original
surface quality of the model, while retaining only the parameters of
parameterized primitives. Experiments on virtual scene dataset and real scene
dataset demonstrate the effectiveness of our method, achieving a Chamfer
Distance of 0.003092, a VIoU of 0.545, a F1-Score of 0.9139 and a NC of 0.8369,
with primitive parameter files approximately 6KB in size. Our approach is
particularly suitable for rapid prototyping of simple models.

</details>


### [3] [MCMC: Bridging Rendering, Optimization and Generative AI](https://arxiv.org/abs/2510.09078)
*Gurprit Singh,Wenzel Jakob*

Main category: cs.GR

TL;DR: 生成式人工智能（AI）在视觉语言模型中取得了前所未有的进展，MCMC方法在高维分布采样中发挥重要作用，连接了梯度优化和物理渲染的研究领域。


<details>
  <summary>Details</summary>
Motivation: 目前缺乏一个统一框架连接扩散生成模型的物理真实性与MCMC方法，本文旨在填补这一空白。

Method: 结合梯度优化和MCMC采样方法，探索高维参数空间的样本生成，并应用于物理渲染技术。

Result: MCMC方法在高维采样和物理渲染中展现出重要作用，能够有效生成具有物理真实性的样本。

Conclusion: 本文为研究人员提供了理论和实践工具，旨在推动生成式物理渲染的共同目标。

Abstract: Generative artificial intelligence (AI) has made unprecedented advances in
vision language models over the past two years. During the generative process,
new samples (images) are generated from an unknown high-dimensional
distribution. Markov Chain Monte Carlo (MCMC) methods are particularly
effective in drawing samples from such complex, high-dimensional distributions.
This makes MCMC methods an integral component for models like EBMs, ensuring
accurate sample generation.
  Gradient-based optimization is at the core of modern generative models. The
update step during the optimization forms a Markov chain where the new update
depends only on the current state. This allows exploration of the parameter
space in a memoryless manner, thus combining the benefits of gradient-based
optimization and MCMC sampling. MCMC methods have shown an equally important
role in physically based rendering where complex light paths are otherwise
quite challenging to sample from simple importance sampling techniques.
  A lot of research is dedicated towards bringing physical realism to samples
(images) generated from diffusion-based generative models in a data-driven
manner, however, a unified framework connecting these techniques is still
missing. In this course, we take the first steps toward understanding each of
these components and exploring how MCMC could potentially serve as a bridge,
linking these closely related areas of research. Our course aims to provide
necessary theoretical and practical tools to guide students, researchers and
practitioners towards the common goal of generative physically based rendering.
All Jupyter notebooks with demonstrations associated to this tutorial can be
found on the project webpage: https://sinbag.github.io/mcmc/

</details>


### [4] [Real-Time Rendering of Dynamic Line Sets using Voxel Ray Tracing](https://arxiv.org/abs/2510.09081)
*Bram Kraaijeveld,Andrei C. Jalba,Anna Vilanova,Maxime Chamberland*

Main category: cs.GR

TL;DR: 论文提出了一种高效的基于体素的射线追踪框架，用于实时渲染大规模动态线集，支持环境遮挡和真实透明度。


<details>
  <summary>Details</summary>
Motivation: 动态线集的实时渲染在许多可视化任务中具有重要意义，但高质量的全局光照和透明度在交互速率下难以实现。

Method: 框架采用了一种体素化算法，支持高效构建加速结构，并结合基于体素的剔除方法减少预处理成本。

Result: 结果表明，该方法在处理（半）透明动态线集时，质量和性能优于现有技术。

Conclusion: 该研究实现了大规模动态线集的高质量实时渲染，为相关应用提供了高效解决方案。

Abstract: Real-time rendering of dynamic line sets is relevant in many visualization
tasks, including unsteady flow visualization and interactive white matter
reconstruction from Magnetic Resonance Imaging. High-quality global
illumination and transparency are important for conveying the spatial structure
of dense line sets, yet remain difficult to achieve at interactive rates. We
propose an efficient voxel-based ray-tracing framework for rendering large
dynamic line sets with ambient occlusion and ground-truth transparency. The
framework introduces a voxelization algorithm that supports efficient
construction of acceleration structures for both voxel cone tracing and ray
tracing. To further reduce per-frame preprocessing cost, we developed a
voxel-based culling method that restricts acceleration structure construction
to camera-visible voxels. Together, these contributions enable high-quality,
real-time rendering of large-scale dynamic line sets with physically accurate
transparency. The results show that our method outperforms the state of the art
in quality and performance when rendering (semi-)opaque dynamic line sets.

</details>


### [5] [Two-Stage Gaussian Splatting Optimization for Outdoor Scene Reconstruction](https://arxiv.org/abs/2510.09489)
*Deborah Pintani,Ariel Caputo,Noah Lewis,Marc Stamminger,Fabio Pellacini,Andrea Giachetti*

Main category: cs.GR

TL;DR: 提出了一种两阶段的高斯泼溅框架，通过显式分离和优化前景与背景区域，提升了户外场景重建的保真度和新视角合成效果。


<details>
  <summary>Details</summary>
Motivation: 户外场景重建存在挑战，主要是由于前景（细节丰富）与背景（低细节、光照不均）之间的强烈对比，导致视觉效果不佳。

Method: 采用两阶段高斯泼溅框架：第一阶段初始化并优化背景高斯元，结合几何约束；第二阶段初始化并优化前景高斯元，同时固定背景。

Result: 实验表明，该方法减少了背景伪影，提升了感知质量，并支持自动环境光估计，拓展了户外渲染和混合现实应用的可能性。

Conclusion: 该框架通过显式分离与优化背景和前景，显著提升了户外场景重建的效果，为新应用场景提供了可能。

Abstract: Outdoor scene reconstruction remains challenging due to the stark contrast
between well-textured, nearby regions and distant backgrounds dominated by low
detail, uneven illumination, and sky effects. We introduce a two-stage Gaussian
Splatting framework that explicitly separates and optimizes these regions,
yielding higher-fidelity novel view synthesis. In stage one, background
primitives are initialized within a spherical shell and optimized using a loss
that combines a background-only photometric term with two geometric
regularizers: one constraining Gaussians to remain inside the shell, and
another aligning them with local tangential planes. In stage two, foreground
Gaussians are initialized from a Structure-from-Motion reconstruction, added
and refined using the standard rendering loss, while the background set remains
fixed but contributes to the final image formation. Experiments on diverse
outdoor datasets show that our method reduces background artifacts and improves
perceptual quality compared to state-of-the-art baselines. Moreover, the
explicit background separation enables automatic, object-free environment map
estimation, opening new possibilities for photorealistic outdoor rendering and
mixed-reality applications.

</details>


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [6] [Neptune: Advanced ML Operator Fusion for Locality and Parallelism on GPUs](https://arxiv.org/abs/2510.08726)
*Yifan Zhao,Egan Johnson,Prasanth Chatarasi,Vikram Adve,Sasa Misailovic*

Main category: cs.PL

TL;DR: Neptune是一个针对深度学习中的复杂降维计算优化的张量编译器，通过创新的算子融合方法，提高了计算效率。


<details>
  <summary>Details</summary>
Motivation: 现有张量编译器在处理包含循环依赖的复杂降维计算（如注意力机制）时表现不佳。

Method: Neptune引入了一种先进的算子融合技术，通过打破某些现有依赖并构建代数校正表达式来确保结果的准确性。

Result: 在十个基于注意力的基准测试中，Neptune生成的核在多种GPU架构上平均提速1.35倍，优于Triton、TVM等现有编译器。

Conclusion: Neptune显著提升了深度学习工作负载的计算效率，特别是在复杂降维计算场景中。

Abstract: Operator fusion has become a key optimization for deep learning, which
combines multiple deep learning operators to improve data reuse and reduce
global memory transfers. However, existing tensor compilers struggle to fuse
complex reduction computations involving loop-carried dependencies, such as
attention mechanisms.
  The paper introduces Neptune, a tensor compiler for advanced operator fusion
for sequences of reduction operators. Neptune presents a new approach for
advanced operator fusion, which intentionally breaks some existing dependencies
and compensates by constructing algebraic correction expressions that allow the
kernel to produce the correct result.
  On ten attention-based benchmarks, Neptune, starting from simple attention
code and a high-level scheduling template, outperforms existing compilers like
Triton, TVM, and FlexAttention, including Triton-based implementations of
FlashAttention. Across four different GPU architectures from NVIDIA and AMD,
Neptune-generated kernels have average speedup of $1.35\times$ over the next
best alternative, demonstrating its effectiveness for deep learning workloads.

</details>


### [7] [Typestate via Revocable Capabilities](https://arxiv.org/abs/2510.08889)
*Songlin Jia,Craig Liu,Siyuan He,Haotian Deng,Yuyan Bao,Tiark Rompf*

Main category: cs.PL

TL;DR: 提出了一种统一流不敏感能力机制和流敏感类型状态跟踪的新方法，解决了状态资源管理的安全性和表达性问题。


<details>
  <summary>Details</summary>
Motivation: 为应对状态资源管理的挑战，尤其是在存在别名的情况下，结合了范围构造的易用性和流敏感管理的细粒度控制的优势。

Method: 扩展流不敏感能力机制至流敏感类型状态跟踪，解耦能力的生命周期与词法范围，基于Scala 3编译器实现。

Result: 原型支持广泛的状态模式，如文件操作、高级锁定协议等，表明只需对现有能力语言进行最小扩展即可实现安全的类型状态管理。

Conclusion: 研究表明，结合范围构造和流敏感机制的能力语言可以实现既安全又表达性强的类型状态管理。

Abstract: Managing stateful resources safely and expressively is a longstanding
challenge in programming languages, especially in the presence of aliasing.
While scope-based constructs such as Java's synchronized blocks offer ease of
reasoning, they restrict expressiveness and parallelism. Conversely,
imperative, flow-sensitive management enables fine-grained control but demands
sophisticated typestate analyses and often burdens programmers with explicit
state tracking.
  In this work, we present a novel approach that unifies the strengths of both
paradigms by extending flow-insensitive capability mechanisms into
flow-sensitive typestate tracking. Our system decouples capability lifetimes
from lexical scopes, allowing functions to provide, revoke, and return
capabilities in a flow-sensitive manner, based on the existing mechanisms
explored for the safety and ergonomics of scoped capability programming.
  We implement our approach as an extension to the Scala 3 compiler, leveraging
path-dependent types and implicit resolution to enable concise, statically
safe, and expressive typestate programming. Our prototype generically supports
a wide range of stateful patterns, including file operations, advanced locking
protocols, DOM construction, and session types. This work demonstrates that
expressive and safe typestate management can be achieved with minimal
extensions to existing capability-based languages, paving the way for more
robust and ergonomic stateful programming.

</details>


### [8] [Free to Move: Reachability Types with Flow-Sensitive Effects for Safe Deallocation and Ownership Transfer](https://arxiv.org/abs/2510.08939)
*Haotian Deng,Siyuan He,Songlin Jia,Yuyan Bao,Tiark Rompf*

Main category: cs.PL

TL;DR: 本文提出了一种流敏感的效应系统，支持高阶不纯函数式语言中的显式内存管理，包括 Rust 风格的移动语义。


<details>
  <summary>Details</summary>
Motivation: 研究动机是将基于可达性的推理与显式资源控制结合，推进高阶函数式语言中安全手动内存管理的技术。

Method: 方法是通过引入多态的\emph{use}和\emph{kill}效应，细化现有的可达性限定符，记录引用的读取、写入、转移和释放操作。

Result: 结果显示系统能够表达所有权转移、上下文新鲜度和破坏性更新，并验证了释放后的使用安全性。

Conclusion: 结论是该系统整合了可达性推理与显式资源控制，为高阶函数式语言的安全内存管理提供了新方法。

Abstract: We present a flow-sensitive effect system for reachability types that
supports explicit memory management, including Rust-style move semantics, in
higher-order impure functional languages. Our system refines the existing
reachability qualifier with polymorphic \emph{use} and \emph{kill} effects that
record how references are read, written, transferred, and deallocated. The
effect discipline tracks operations performed on each resource using
qualifiers, enabling the type system to express ownership transfer, contextual
freshness, and destructive updates without regions or linearity. We formalize
the calculus, its typing and effect rules, and a compositional operational
semantics that validates use-after-free safety. All metatheoretic results,
including preservation, progress, and effect soundness, are mechanized. The
system models idioms such as reference deallocation, move semantics, reference
swapping, while exposing precise safety guarantee. Together, these
contributions integrate reachability-based reasoning with explicit resource
control, advancing the state of the art in safe manual memory management for
higher-order functional languages.

</details>


### [9] [Concept-Based Generic Programming in C++](https://arxiv.org/abs/2510.08969)
*Bjarne Stroustrup*

Main category: cs.PL

TL;DR: 论文通过C++的概念（concepts）展示了泛型编程的技术和原则，旨在消除窄化转换并提供范围检查，同时避免不必要的符号或运行时开销。


<details>
  <summary>Details</summary>
Motivation: C++的泛型编程是其核心部分，而非独立的子语言。论文旨在展示概念（concepts）的实用性及其背后的基本思想，以扩展类型系统。

Method: 论文使用C++的概念来约束泛型代码，并通过用户定义的扩展展示其功能。此外，还讨论了关键设计部分的合理性和起源。

Result: 论文展示了一个简单的类型系统，成功消除了窄化转换并提供了范围检查，同时保持了效率。

Conclusion: 概念是C++泛型编程的强大工具，能够有效扩展类型系统并支持通用编程。论文为概念的设计和使用提供了理论和实践基础。

Abstract: We present programming techniques to illustrate the facilities and principles
of C++ generic programming using concepts. Concepts are C++'s way to express
constraints on generic code. As an initial example, we provide a simple type
system that eliminates narrowing conversions and provides range checking
without unnecessary notational or run-time overheads. Concepts are used
throughout to provide user-defined extensions to the type system. The aim is to
show their utility and the fundamental ideas behind them, rather than to
provide a detailed or complete explanation of C++'s language support for
generic programming or the extensive support provided by the standard library.
Generic programming is an integral part of C++, rather than an isolated
sub-language. In particular, key facilities support general programming as well
as generic programming (e.g., uniform notation for types, lambdas, variadic
templates, and C++26 static reflection). Finally, we give design rationales and
origins for key parts of the concept design, including use patterns, the
relationship to Object-Oriented Programming, value arguments, notation, concept
type-matching, and definition checking.

</details>


### [10] [A Multilingual Python Programming Language](https://arxiv.org/abs/2510.09591)
*Saad Ahmed Bazaz,Mirza Omer Beg*

Main category: cs.PL

TL;DR: UniversalPython是一个基于Python的语言转译器，允许用户用自己的母语编写Python代码，解决了英语语言对编程初学者的障碍。


<details>
  <summary>Details</summary>
Motivation: 广泛使用的编程语言基于英语，这对非英语母语的学习者构成了障碍。研究表明，人们用母语学习效果更好，因此开发了UniversalPython。

Method: UniversalPython是一个基于Python的语言转译器，支持用户用自己的母语编写代码，例如实现了"Urdu Python"。

Result: 成功开发了一个开源的语言转译器UniversalPython，并展示了其支持乌尔都语Python的能力。

Conclusion: UniversalPython为编程语言的非英语母语学习者提供了便利，未来计划扩展支持更多语言以提高编程的可及性。

Abstract: All widely used and useful programming languages have a common problem. They
restrict entry on the basis of knowledge of the English language. The lack of
knowledge of English poses a major hurdle to many newcomers who do not have the
resources, in terms of time and money, to learn the English language. Studies
show that people learn better in their own language. Therefore, we propose a
language transpiler built on top of the Python programming language, called
UniversalPython, which allows one to write Python in their own human language.
We demonstrate the ability to create an "Urdu Python" with this transpiler. In
the future, we aim to scale the language to encapsulate more human languages to
increase the availability of programming. The source code for this transpiler
is open-source, and available at
https://github.com/universalpython/universalpython

</details>


<div id='cs.GT'></div>

# cs.GT [[Back]](#toc)

### [11] [Unending Sequential Auctions](https://arxiv.org/abs/2510.08742)
*Amir Ban*

Main category: cs.GT

TL;DR: 该论文研究了单元需求的私人价值买家参与的无限期同质物品序贯拍卖，引入买家退出概率的不确定性，证明了稳态的存在性和唯一性。无不确定性时，稳态类似定价机制；不确定性导致阈值模糊，低高价值买家均受益，但拍卖者收益可能减少。


<details>
  <summary>Details</summary>
Motivation: 序贯拍卖在实践中常见且周期性无限进行，买家存在不确定性退出。研究旨在分析不确定性对拍卖动态、均衡价格以及各方效用的影响。

Method: 将序贯拍卖建模为马尔可夫过程，引入买家退出概率的不确定性，分析稳态的存在性和特性，比较有无不确定性时的阈值行为和均衡结果。

Result: 无不确定性时，稳态表现为类似定价机制；不确定性使阈值模糊，低价值买家获得显著获胜机会，高价值买家在某些条件下效用增加，但拍卖者收益可能减少。

Conclusion: 不确定性在序贯拍卖中导致阈值模糊化，对买卖双方产生复杂影响：低高价值买家可能受益，但拍卖者效用常受损害。

Abstract: Sequential auctions for identical items with unit-demand, private-value
buyers are common and often occur periodically without end, as new bidders
replace departing ones. We model bidder uncertainty by introducing a
probability that a bidder must exit the auction in each period. Treating the
sequential auction as a Markov process, we demonstrate the existence of a
unique steady state.
  In the absence of uncertainty, the steady state resembles a posted-price
mechanism: bidders with values above a threshold almost surely win items by
repeatedly bidding the threshold price, while those below the threshold almost
surely do not. The equilibrium price corresponds to the threshold value that
balances supply (bidders with values above the threshold) and demand (auction
winners).
  When uncertainty is introduced, the threshold value persists but becomes less
precise, growing "fuzzier" as uncertainty increases. This uncertainty benefits
low-value bidders, those below the threshold, by giving them a significant
chance of winning. Surprisingly, high-value bidders also benefit from
uncertainty, up to a certain value limit, as it lowers equilibrium bids and
increases their expected utility. On the other hand, this bidder uncertainty
often reduces the auctioneer's utility.

</details>


### [12] [Robust autobidding for noisy conversion prediction models](https://arxiv.org/abs/2510.08788)
*Andrey Pudovikov,Alexandra Khirianova,Ekaterina Solodneva,Gleb Molodtsov,Aleksandr Katrutsa,Yuriy Dorn,Egor Samosvat*

Main category: cs.GT

TL;DR: RobustBid是一种高效的自动竞价方法，通过考虑CTR和CVR预测的不确定性，利用鲁棒优化技术防止竞价错误，提升广告收益。


<details>
  <summary>Details</summary>
Motivation: 现代广告拍卖系统依赖CTR和CVR的预测值进行自动竞价，但这些预测的不确定性会影响广告商的收益和竞价策略，因此需要一种鲁棒的自动竞价方法。

Method: RobustBid采用先进的鲁棒优化技术，生成对CTR/CVR预测扰动的稳健竞价策略，并通过解析解实现高效运行。

Result: 实验表明，在CTR/CVR预测存在较大扰动时，RobustBid在总转化量（TCV）和平均每次点击成本（CPC_{avg}）上优于非鲁棒基线和RiskBid算法。

Conclusion: RobustBid通过鲁棒优化技术在不确定性环境下提供更优的竞价策略，显著提升了广告效果和成本效率。

Abstract: Managing millions of digital auctions is an essential task for modern
advertising auction systems. The main approach to managing digital auctions is
an autobidding approach, which depends on the Click-Through Rate and Conversion
Rate values. While these quantities are estimated with ML models, their
prediction uncertainty directly impacts advertisers' revenue and bidding
strategies. To address this issue, we propose RobustBid, an efficient method
for robust autobidding taking into account uncertainty in CTR and CVR
predictions. Our approach leverages advanced, robust optimization techniques to
prevent large errors in bids if the estimates of CTR/CVR are perturbed. We
derive the analytical solution of the stated robust optimization problem, which
leads to the runtime efficiency of the RobustBid method. The synthetic,
iPinYou, and BAT benchmarks are used in our experimental evaluation of
RobustBid. We compare our method with the non-robust baseline and the RiskBid
algorithm in terms of total conversion volume (TCV) and average cost-per-click
($CPC_{avg}$) performance metrics. The experiments demonstrate that RobustBid
provides bids that yield larger TCV and smaller $CPC_{avg}$ than competitors in
the case of large perturbations in CTR/CVR predictions.

</details>


### [13] [Measuring the Hidden Cost of Data Valuation through Collective Disclosure](https://arxiv.org/abs/2510.08869)
*Patrick Mesana,Gilles Caporossi,Sebastien Gambs*

Main category: cs.GT

TL;DR: 数据估值方法为每个数据点分配贡献边际效用，但直接作为支付机制会导致隐藏成本，即边际价值接近零的贡献者得不到回报。论文引入信息披露博弈模型，通过差分隐私机制和数据Shapley值模拟，揭示数据估值存在显式获取成本，且数据联盟的披露策略影响成本分布。


<details>
  <summary>Details</summary>
Motivation: 现有数据估值方法在直接作为支付机制时，会导致边际价值低的贡献者被忽视，尽管他们的数据仍需收集和评估。论文旨在通过博弈论模型和模拟解决这一隐藏成本问题。

Method: 论文提出信息披露博弈模型，涉及数据联盟和数据消费者。数据联盟采用差分隐私机制逐步释放信息，并通过数据Shapley值和多臂老虎机策略进行模拟。

Result: 在Yelp评论有用性预测任务中，模拟结果显示数据估值存在显式获取成本，且数据联盟的披露策略显著影响成本在不同成员间的分布。

Conclusion: 论文揭示了数据估值中的隐藏成本问题，并通过博弈模型和模拟展示了如何通过信息披露策略优化成本分配。

Abstract: Data valuation methods assign marginal utility to each data point that has
contributed to the training of a machine learning model. If used directly as a
payout mechanism, this creates a hidden cost of valuation, in which
contributors with near-zero marginal value would receive nothing, even though
their data had to be collected and assessed. To better formalize this cost, we
introduce a conceptual and game-theoretic model, the Information Disclosure
Game, between a Data Union (sometimes also called a data trust), a member-run
agent representing contributors, and a Data Consumer (e.g., a platform). After
first aggregating members' data, the DU releases information progressively by
adding Laplacian noise under a differentially-private mechanism. Through
simulations with strategies guided by data Shapley values and multi-armed
bandit exploration, we demonstrate on a Yelp review helpfulness prediction task
that data valuation inherently incurs an explicit acquisition cost and that the
DU's collective disclosure policy changes how this cost is distributed across
members.

</details>


### [14] [Approximately Bisubmodular Regret Minimization in Billboard and Social Media Advertising](https://arxiv.org/abs/2510.09084)
*Dildar Ali,Suman Benerjee,Yamuna Prasad*

Main category: cs.GT

TL;DR: 本研究探讨了广告牌分配问题，提出了两种最小化遗憾的解决方案：预算有效贪婪方法和随机化的预算有效贪婪方法。


<details>
  <summary>Details</summary>
Motivation: 广告提供商需要在满足广告商需求的同时最小化遗憾，而现有方法在此问题上的表现有限。

Method: 提出了两种方法：1）增量式的预算有效贪婪方法；2）引入随机性的贪婪采样方法，以减少计算复杂度。

Result: 实验表明，随机化的预算有效贪婪方法在合理计算时间内有效最小化了遗憾。

Conclusion: 随机化的贪婪方法为广告牌分配问题提供了高效且实用的解决方案。

Abstract: In a typical \emph{billboard advertisement} technique, a number of digital
billboards are owned by an \emph{influence provider}, and several commercial
houses approach the influence provider for a specific number of views of their
advertisement content on a payment basis. If the influence provider provides
the demanded or more influence, then he will receive the full payment else a
partial payment. In the context of an influence provider, if he provides more
or less than the advertisers demanded influence, it is a loss for him. This is
formalized as 'Regret', and naturally, in the context of the influence
provider, the goal will be to allocate the billboard slots among the
advertisers such that the total regret is minimized. In this paper, we study
this problem as a discrete optimization problem and propose two solution
approaches. The first one selects the billboard slots from the available ones
in an incremental greedy manner, and we call this method the Budget Effective
Greedy approach. In the second one, we introduce randomness in the first one,
where we do it for a sample of slots instead of calculating the marginal gains
of all the billboard slots. We analyze both algorithms to understand their time
and space complexity. We implement them with real-life datasets and conduct a
number of experiments. We observe that the randomized budget effective greedy
approach takes reasonable computational time while minimizing the regret.

</details>
