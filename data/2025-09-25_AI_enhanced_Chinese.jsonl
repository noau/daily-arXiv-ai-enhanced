{"id": "2509.19607", "categories": ["cs.PL"], "pdf": "https://arxiv.org/pdf/2509.19607", "abs": "https://arxiv.org/abs/2509.19607", "authors": ["William J. Bowman"], "title": "Macro-embedding Compiler Intermediate Languages in Racket", "comment": null, "summary": "We present the design and implementation of a macro-embedding of a family of\ncompiler intermediate languages, from a Scheme-like language to x86-64, into\nRacket. This embedding is used as part of a testing framework for a compilers\ncourse to derive interpreters for all the intermediate languages. The embedding\nimplements features including safe, functional abstractions as well as unsafe\nassembly features, and the interactions between the two at various intermediate\nstages.\n  This paper aims to demonstrate language-oriented techniques and abstractions\nfor implementing (1) a large family of languages and (2) interoperability\nbetween low- and high-level languages. The primary strength of this approach is\nthe high degree of code reuse and interoperability compared to implementing\neach interpreter separately. The design emphasizes modularity and\ncompositionality of an open set of language features by local macro expansion\ninto a single host language, rather than implementing a language pre-defined by\na closed set of features. This enables reuse from both the host language\n(Racket) and between intermediate languages, and enables interoperability\nbetween high- and low-level features, simplifying development of the\nintermediate language semantics. It also facilitates extending or redefining\nindividual language features in intermediate languages, and exposing multiple\ninterfaces to the embedded languages.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86\u4e00\u79cd\u5728Racket\u4e2d\u5d4c\u5165\u4ece\u7c7bScheme\u8bed\u8a00\u5230x86-64\u7684\u7f16\u8bd1\u5668\u4e2d\u95f4\u8bed\u8a00\u5bb6\u65cf\u7684\u8bbe\u8ba1\u4e0e\u5b9e\u73b0\uff0c\u7528\u4e8e\u7f16\u8bd1\u5668\u8bfe\u7a0b\u7684\u6d4b\u8bd5\u6846\u67b6\u3002", "motivation": "\u63a2\u8ba8\u8bed\u8a00\u5bfc\u5411\u6280\u672f\u548c\u62bd\u8c61\uff0c\u4ee5\u5b9e\u73b0\uff081\uff09\u5927\u89c4\u6a21\u8bed\u8a00\u5bb6\u65cf\u548c\uff082\uff09\u9ad8\u4f4e\u7ea7\u8bed\u8a00\u95f4\u7684\u4e92\u64cd\u4f5c\u6027\u3002", "method": "\u901a\u8fc7\u5b8f\u6269\u5c55\u5c06\u5f00\u653e\u8bed\u8a00\u7279\u5f81\u6a21\u5757\u5316\u548c\u7ec4\u5408\u5316\u5d4c\u5165\u5355\u4e00\u5bbf\u4e3b\u8bed\u8a00\uff08Racket\uff09\uff0c\u652f\u6301\u529f\u80fd\u6027\u548c\u4e0d\u5b89\u5168\u7279\u5f81\u7684\u4ea4\u4e92\u3002", "result": "\u5b9e\u73b0\u4e86\u9ad8\u4ee3\u7801\u590d\u7528\u548c\u4e92\u64cd\u4f5c\u6027\uff0c\u7b80\u5316\u4e86\u4e2d\u95f4\u8bed\u8a00\u8bed\u4e49\u5f00\u53d1\uff0c\u5e76\u652f\u6301\u7279\u5f81\u6269\u5c55\u548c\u591a\u63a5\u53e3\u66b4\u9732\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u901a\u8fc7\u5bbf\u4e3b\u8bed\u8a00\u548c\u4e2d\u95f4\u8bed\u8a00\u7684\u590d\u7528\u4e0e\u4e92\u64cd\u4f5c\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5f00\u53d1\u6548\u7387\u548c\u7075\u6d3b\u6027\u3002"}}
{"id": "2509.19613", "categories": ["cs.PL"], "pdf": "https://arxiv.org/pdf/2509.19613", "abs": "https://arxiv.org/abs/2509.19613", "authors": ["William J. Bowman"], "title": "Compilation as Multi-Language Semantics", "comment": null, "summary": "Modeling interoperability between programs in different languages is a key\nproblem when modeling verified and secure compilation, which has been\nsuccessfully addressed using multi-language semantics. Unfortunately, existing\nmodels of compilation using multi-language semantics define two variants of\neach compiler pass: a syntactic translation on open terms to model compilation,\nand a run-time translation of closed terms at multi-language boundaries to\nmodel interoperability.\n  In this talk, I discuss work-in-progress approach to uniformly model a\ncompiler entirely as a reduction system on open term in a multi-language\nsemantics, rather than as a syntactic translation. This simultaneously defines\nthe compiler and the interoperability semantics, reducing duplication. It also\nprovides interesting semantic insights. Normalization of the cross-language\nredexes performs ahead-of-time (AOT) compilation. Evaluation in the\nmulti-language models just-in-time (JIT) compilation. Confluence of\nmulti-language reduction implies compiler correctness, and part of the secure\ncompilation proof (full abstraction), enabling focus on the difficult part of\nthe proof. Subject reduction of the multi-language reduction implies\ntype-preservation of the compiler.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7edf\u4e00\u5efa\u6a21\u7f16\u8bd1\u5668\u7684\u65b9\u6cd5\uff0c\u5c06\u5176\u89c6\u4e3a\u591a\u8bed\u8a00\u8bed\u4e49\u4e2d\u7684\u5f00\u653e\u9879\u5f52\u7ea6\u7cfb\u7edf\uff0c\u51cf\u5c11\u4e86\u4f20\u7edf\u65b9\u6cd5\u4e2d\u7684\u91cd\u590d\u5b9a\u4e49\uff0c\u5e76\u4e3a\u7f16\u8bd1\u548c\u4e92\u64cd\u4f5c\u6027\u8bed\u4e49\u63d0\u4f9b\u4e86\u6709\u8da3\u7684\u89c1\u89e3\u3002", "motivation": "\u4f20\u7edf\u591a\u8bed\u8a00\u8bed\u4e49\u4e2d\uff0c\u7f16\u8bd1\u5668\u5efa\u6a21\u9700\u8981\u4e3a\u6bcf\u4e2a\u7f16\u8bd1\u901a\u9053\u5b9a\u4e49\u4e24\u4e2a\u53d8\u4f53\uff1a\u7f16\u8bd1\u7684\u8bed\u6cd5\u7ffb\u8bd1\u548c\u8fd0\u884c\u65f6\u7ffb\u8bd1\u3002\u8fd9\u79cd\u65b9\u6cd5\u5b58\u5728\u91cd\u590d\u5b9a\u4e49\u7684\u7f3a\u70b9\u3002", "method": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u65b9\u6cd5\uff0c\u5c06\u7f16\u8bd1\u5668\u7edf\u4e00\u5efa\u6a21\u4e3a\u591a\u8bed\u8a00\u8bed\u4e49\u4e2d\u7684\u5f00\u653e\u9879\u5f52\u7ea6\u7cfb\u7edf\uff0c\u800c\u975e\u8bed\u6cd5\u7ffb\u8bd1\u3002\u8fd9\u4e00\u65b9\u6cd5\u901a\u8fc7\u5f52\u4e00\u5316\u8de8\u8bed\u8a00\u5f52\u7ea6\u6765\u8868\u793aAOT\u7f16\u8bd1\uff0c\u901a\u8fc7\u591a\u8bed\u8a00\u8bc4\u4f30\u6765\u8868\u793aJIT\u7f16\u8bd1\u3002", "result": "\u8be5\u65b9\u6cd5\u51cf\u5c11\u4e86\u4f20\u7edf\u65b9\u6cd5\u4e2d\u7684\u91cd\u590d\u5b9a\u4e49\uff0c\u540c\u65f6\u63d0\u4f9b\u4e86\u7f16\u8bd1\u5668\u6b63\u786e\u6027\u548c\u5b89\u5168\u6027\u8bc1\u660e\u7684\u601d\u8def\uff0c\u5e76\u901a\u8fc7\u4e3b\u9898\u5f52\u7ea6\u786e\u4fdd\u7f16\u8bd1\u5668\u4fdd\u6301\u7c7b\u578b\u4e0d\u53d8\u6027\u3002", "conclusion": "\u7edf\u4e00\u5efa\u6a21\u65b9\u6cd5\u4e0d\u4ec5\u7b80\u5316\u4e86\u7f16\u8bd1\u5668\u7684\u5b9a\u4e49\uff0c\u8fd8\u4e3a\u7f16\u8bd1\u5668\u7684\u6b63\u786e\u6027\u548c\u5b89\u5168\u6027\u63d0\u4f9b\u4e86\u65b0\u7684\u8bc1\u660e\u8def\u5f84\u3002"}}
{"id": "2509.20020", "categories": ["cs.PL", "cs.LG", "cs.MS", "cs.SC", "F.2.2; I.1.2; I.1.3"], "pdf": "https://arxiv.org/pdf/2509.20020", "abs": "https://arxiv.org/abs/2509.20020", "authors": ["Maurice Wenig", "Paul G. Rump", "Mark Blacher", "Joachim Giesen"], "title": "The Syntax and Semantics of einsum", "comment": "21 pages, 1 figure. Includes formal definitions, proofs of algebraic\n  properties, and nesting/denesting rules for the einsum notation", "summary": "In 2011, einsum was introduced to NumPy as a practical and convenient\nnotation for tensor expressions in machine learning, quantum circuit\nsimulation, and other fields. It has since been implemented in additional\nPython frameworks such as PyTorch and TensorFlow, as well as in other\nprogramming languages such as Julia. Despite its practical success, the einsum\nnotation still lacks a solid theoretical basis, and is not unified across the\ndifferent frameworks, limiting opportunities for formal reasoning and\nsystematic optimization. In this work, we discuss the terminology of tensor\nexpressions and provide a formal definition of the einsum language. Based on\nthis definition, we formalize and prove important equivalence rules for tensor\nexpressions and highlight their relevance in practical applications.", "AI": {"tldr": "\u672c\u6587\u63a2\u8ba8\u4e86einsum\u7b26\u53f7\u7684\u7406\u8bba\u57fa\u7840\uff0c\u63d0\u51fa\u4e86\u4e00\u4e2a\u6b63\u5f0f\u7684einsum\u8bed\u8a00\u5b9a\u4e49\uff0c\u5e76\u8bc1\u660e\u4e86\u5f20\u91cf\u8868\u8fbe\u5f0f\u7684\u91cd\u8981\u7b49\u4ef7\u89c4\u5219\u53ca\u5176\u5b9e\u9645\u5e94\u7528\u4ef7\u503c\u3002", "motivation": "\u5c3d\u7ba1einsum\u7b26\u53f7\u5728\u5b9e\u8df5\u4e2d\u5e7f\u6cdb\u5e94\u7528\u4e8e\u673a\u5668\u5b66\u4e60\u3001\u91cf\u5b50\u7535\u8def\u6a21\u62df\u7b49\u9886\u57df\uff0c\u4f46\u5176\u7f3a\u4e4f\u575a\u5b9e\u7684\u7406\u8bba\u57fa\u7840\uff0c\u4e14\u5728\u4e0d\u540c\u6846\u67b6\u4e2d\u672a\u7edf\u4e00\uff0c\u9650\u5236\u4e86\u5f62\u5f0f\u5316\u63a8\u7406\u548c\u7cfb\u7edf\u4f18\u5316\u7684\u673a\u4f1a\u3002", "method": "\u672c\u6587\u8ba8\u8bba\u4e86\u5f20\u91cf\u8868\u8fbe\u5f0f\u7684\u672f\u8bed\u5b66\uff0c\u5e76\u63d0\u51fa\u4e86einsum\u8bed\u8a00\u7684\u6b63\u5f0f\u5b9a\u4e49\u3002\u5728\u6b64\u57fa\u7840\u4e0a\uff0c\u5f62\u5f0f\u5316\u5e76\u8bc1\u660e\u4e86\u5f20\u91cf\u8868\u8fbe\u5f0f\u7684\u91cd\u8981\u7b49\u4ef7\u89c4\u5219\u3002", "result": "\u901a\u8fc7\u5f62\u5f0f\u5316einsum\u8bed\u8a00\uff0c\u8bc1\u660e\u4e86\u5f20\u91cf\u8868\u8fbe\u5f0f\u7684\u91cd\u8981\u7b49\u4ef7\u89c4\u5219\uff0c\u5e76\u63ed\u793a\u4e86\u8fd9\u4e9b\u89c4\u5219\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u7684\u4ef7\u503c\u3002", "conclusion": "\u672c\u6587\u586b\u8865\u4e86einsum\u7b26\u53f7\u7684\u7406\u8bba\u7a7a\u767d\uff0c\u4e3a\u5176\u5f62\u5f0f\u5316\u63a8\u7406\u548c\u4f18\u5316\u63d0\u4f9b\u4e86\u57fa\u7840\uff0c\u5bf9\u5b9e\u9645\u5e94\u7528\u5177\u6709\u91cd\u8981\u610f\u4e49\u3002"}}
{"id": "2509.20147", "categories": ["cs.GT", "cs.LG", "cs.MA", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2509.20147", "abs": "https://arxiv.org/abs/2509.20147", "authors": ["Siddharth Chandak", "Ilai Bistritz", "Nicholas Bambos"], "title": "Choose Your Battles: Distributed Learning Over Multiple Tug of War Games", "comment": "Submitted to IEEE TAC", "summary": "Consider N players and K games taking place simultaneously. Each of these\ngames is modeled as a Tug-of-War (ToW) game where increasing the action of one\nplayer decreases the reward for all other players. Each player participates in\nonly one game at any given time. At each time step, a player decides the game\nin which they wish to participate in and the action they take in that game.\nTheir reward depends on the actions of all players that are in the same game.\nThis system of K games is termed `Meta Tug-of-War' (Meta-ToW) game. These games\ncan model scenarios such as power control, distributed task allocation, and\nactivation in sensor networks. We propose the Meta Tug-of-Peace algorithm, a\ndistributed algorithm where the action updates are done using a simple\nstochastic approximation algorithm, and the decision to switch games is made\nusing an infrequent 1-bit communication between the players. We prove that in\nMeta-ToW games, our algorithm converges to an equilibrium that satisfies a\ntarget Quality of Service reward vector for the players. We then demonstrate\nthe efficacy of our algorithm through simulations for the scenarios mentioned\nabove.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u79f0\u4e3a\u201cMeta Tug-of-Peace\u201d\u7684\u5206\u5e03\u5f0f\u7b97\u6cd5\uff0c\u7528\u4e8e\u89e3\u51b3\u591a\u73a9\u5bb6\u53c2\u4e0e\u7684\u591a\u4e2a\u6e38\u620f\u4e2d\u7684\u52a8\u6001\u51b3\u7b56\u95ee\u9898\uff0c\u786e\u4fdd\u73a9\u5bb6\u8fbe\u5230\u76ee\u6807\u670d\u52a1\u8d28\u91cf\u5956\u52b1\u3002", "motivation": "\u7814\u7a76\u52a8\u673a\u662f\u4e3a\u591a\u73a9\u5bb6\u540c\u65f6\u53c2\u4e0e\u591a\u4e2a\u7ade\u4e89\u6027\u6e38\u620f\uff08\u5982\u7535\u529b\u63a7\u5236\u3001\u4f20\u611f\u5668\u7f51\u7edc\u4e2d\u7684\u4efb\u52a1\u5206\u914d\u7b49\uff09\u63d0\u4f9b\u4e00\u79cd\u5206\u5e03\u5f0f\u89e3\u51b3\u65b9\u6848\uff0c\u786e\u4fdd\u73a9\u5bb6\u5728\u52a8\u6001\u73af\u5883\u4e2d\u8fbe\u5230\u5747\u8861\u72b6\u6001\u3002", "method": "\u65b9\u6cd5\u662f\u901a\u8fc7\u4e00\u79cd\u7b80\u5355\u7684\u968f\u673a\u903c\u8fd1\u7b97\u6cd5\u66f4\u65b0\u73a9\u5bb6\u52a8\u4f5c\uff0c\u5e76\u91c7\u7528\u4f4e\u98911\u4f4d\u901a\u4fe1\u673a\u5236\u51b3\u5b9a\u6e38\u620f\u5207\u6362\uff0c\u8bbe\u8ba1\u4e86Meta Tug-of-Peace\u7b97\u6cd5\u3002", "result": "\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u7b97\u6cd5\u80fd\u591f\u5728Meta-ToW\u6e38\u620f\u4e2d\u6536\u655b\u5230\u4e00\u4e2a\u6ee1\u8db3\u76ee\u6807\u670d\u52a1\u8d28\u91cf\u5956\u52b1\u5411\u91cf\u7684\u5747\u8861\u72b6\u6001\uff0c\u5e76\u901a\u8fc7\u4eff\u771f\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002", "conclusion": "\u7ed3\u8bba\u662fMeta Tug-of-Peace\u7b97\u6cd5\u5728\u591a\u73a9\u5bb6\u52a8\u6001\u7ade\u4e89\u6e38\u620f\u4e2d\u5177\u6709\u9ad8\u6548\u6027\u548c\u5b9e\u7528\u6027\uff0c\u9002\u7528\u4e8e\u591a\u79cd\u73b0\u5b9e\u573a\u666f\u3002"}}
{"id": "2509.19412", "categories": ["cs.GR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.19412", "abs": "https://arxiv.org/abs/2509.19412", "authors": ["Emmanouil Karystinaios", "Francesco Foscarin", "Gerhard Widmer"], "title": "EngravingGNN: A Hybrid Graph Neural Network for End-to-End Piano Score Engraving", "comment": "Accepted at the International Conference on Technologies for Music\n  Notation and Representation (TENOR) 2025", "summary": "This paper focuses on automatic music engraving, i.e., the creation of a\nhumanly-readable musical score from musical content. This step is fundamental\nfor all applications that include a human player, but it remains a mostly\nunexplored topic in symbolic music processing. In this work, we formalize the\nproblem as a collection of interdependent subtasks, and propose a unified graph\nneural network (GNN) framework that targets the case of piano music and\nquantized symbolic input. Our method employs a multi-task GNN to jointly\npredict voice connections, staff assignments, pitch spelling, key signature,\nstem direction, octave shifts, and clef signs. A dedicated postprocessing\npipeline generates print-ready MusicXML/MEI outputs. Comprehensive evaluation\non two diverse piano corpora (J-Pop and DCML Romantic) demonstrates that our\nunified model achieves good accuracy across all subtasks, compared to existing\nsystems that only specialize in specific subtasks. These results indicate that\na shared GNN encoder with lightweight task-specific decoders in a multi-task\nsetting offers a scalable and effective solution for automatic music engraving.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u56fe\u795e\u7ecf\u7f51\u7edc\uff08GNN\uff09\u7684\u7edf\u4e00\u6846\u67b6\uff0c\u7528\u4e8e\u81ea\u52a8\u97f3\u4e50\u96d5\u523b\uff0c\u80fd\u591f\u540c\u65f6\u5904\u7406\u591a\u4e2a\u5b50\u4efb\u52a1\uff0c\u5e76\u5728\u94a2\u7434\u97f3\u4e50\u548c\u91cf\u5316\u7b26\u53f7\u8f93\u5165\u4e0a\u8868\u73b0\u51fa\u8272\u3002", "motivation": "\u97f3\u4e50\u96d5\u523b\u662f\u4ece\u97f3\u4e50\u5185\u5bb9\u751f\u6210\u4eba\u7c7b\u53ef\u8bfb\u4e50\u8c31\u7684\u6b65\u9aa4\uff0c\u867d\u5bf9\u6d89\u53ca\u4eba\u7c7b\u6f14\u594f\u8005\u7684\u5e94\u7528\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u5728\u7b26\u53f7\u97f3\u4e50\u5904\u7406\u9886\u57df\u4ecd\u9c9c\u6709\u7814\u7a76\u3002\u672c\u6587\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\u3002", "method": "\u8be5\u65b9\u6cd5\u91c7\u7528\u591a\u4efb\u52a1\u56fe\u795e\u7ecf\u7f51\u7edc\uff08GNN\uff09\uff0c\u8054\u5408\u9884\u6d4b\u58f0\u90e8\u8fde\u63a5\u3001\u4e94\u7ebf\u8c31\u5206\u914d\u3001\u97f3\u9ad8\u62fc\u5199\u3001\u8c03\u53f7\u3001\u7b26\u5e72\u65b9\u5411\u3001\u516b\u5ea6\u79fb\u4f4d\u548c\u8c31\u53f7\u6807\u5fd7\uff0c\u5e76\u901a\u8fc7\u540e\u5904\u7406\u6d41\u7a0b\u751f\u6210\u53ef\u6253\u5370\u7684MusicXML/MEI\u8f93\u51fa\u3002", "result": "\u5728J-Pop\u548cDCML Romantic\u4e24\u79cd\u94a2\u7434\u6570\u636e\u96c6\u4e0a\u7684\u7efc\u5408\u8bc4\u4f30\u8868\u660e\uff0c\u7edf\u4e00\u6a21\u578b\u5728\u6240\u6709\u5b50\u4efb\u52a1\u4e0a\u5747\u8868\u73b0\u51fa\u8f83\u9ad8\u7684\u51c6\u786e\u7387\uff0c\u4f18\u4e8e\u4ec5\u4e13\u6ce8\u4e8e\u7279\u5b9a\u5b50\u4efb\u52a1\u7684\u73b0\u6709\u7cfb\u7edf\u3002", "conclusion": "\u591a\u4efb\u52a1GNN\u6846\u67b6\u4e3a\u81ea\u52a8\u97f3\u4e50\u96d5\u523b\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u4e14\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u8868\u660e\u5171\u4eab\u7f16\u7801\u5668\u4e0e\u8f7b\u91cf\u7ea7\u4efb\u52a1\u7279\u5b9a\u89e3\u7801\u5668\u7684\u7ec4\u5408\u5177\u6709\u663e\u8457\u4f18\u52bf\u3002"}}
{"id": "2509.20329", "categories": ["cs.GT"], "pdf": "https://arxiv.org/pdf/2509.20329", "abs": "https://arxiv.org/abs/2509.20329", "authors": ["Brendan Gould", "Kyriakos Vamvoudakis"], "title": "A Novel Framework for Honey-X Deception in Zero-Sum Games", "comment": "8 pages, 4 figures", "summary": "In this paper, we present a novel, game-theoretic model of deception in\ntwo-player, zero-sum games. Our framework leverages an information asymmetry:\none player (the deceiver) has access to accurate payoff information, while the\nother (the victim) observes a modified version of these payoffs due to the\ndeception strategy employed. The deceiver's objective is to choose a\ndeception-action pair that optimally exploits the victim's best response to the\naltered payoffs, subject to a constraint on the deception's magnitude. We\ncharacterize the optimal deceptive strategy as the solution to a bi-level\noptimization problem, and we provide both an exact solution and an efficient\nmethod for computing a high-quality feasible point. Finally, we demonstrate the\neffectiveness of our approach on numerical examples inspired by honeypot\ndeception.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u535a\u5f08\u8bba\u6a21\u578b\uff0c\u7528\u4e8e\u7814\u7a76\u53cc\u4eba\u96f6\u548c\u535a\u5f08\u4e2d\u7684\u6b3a\u9a97\u884c\u4e3a\u3002", "motivation": "\u7814\u7a76\u52a8\u673a\u5728\u4e8e\u63a2\u8ba8\u4fe1\u606f\u4e0d\u5bf9\u79f0\u60c5\u51b5\u4e0b\uff0c\u6b3a\u9a97\u8005\u5982\u4f55\u901a\u8fc7\u7b56\u7565\u6027\u5730\u4fee\u6539\u6536\u76ca\u4fe1\u606f\u6765\u6700\u5927\u5316\u81ea\u8eab\u5229\u76ca\uff0c\u540c\u65f6\u7ea6\u675f\u6b3a\u9a97\u7684\u5e45\u5ea6\u3002", "method": "\u65b9\u6cd5\u5305\u62ec\u5c06\u6700\u4f18\u6b3a\u9a97\u7b56\u7565\u5efa\u6a21\u4e3a\u4e00\u4e2a\u53cc\u5c42\u4f18\u5316\u95ee\u9898\uff0c\u5e76\u63d0\u4f9b\u7cbe\u786e\u89e3\u548c\u9ad8\u6548\u8ba1\u7b97\u9ad8\u8d28\u91cf\u53ef\u884c\u89e3\u7684\u65b9\u6cd5\u3002", "result": "\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u53d7\u871c\u7f50\u6b3a\u9a97\u542f\u53d1\u7684\u6570\u503c\u793a\u4f8b\u4e2d\u8868\u73b0\u51fa\u9ad8\u6548\u6027\u3002", "conclusion": "\u7ed3\u8bba\u6307\u51fa\u8be5\u6846\u67b6\u4e3a\u535a\u5f08\u8bba\u4e2d\u7684\u6b3a\u9a97\u884c\u4e3a\u63d0\u4f9b\u4e86\u65b0\u7684\u7406\u8bba\u5206\u6790\u548c\u5b9e\u7528\u5de5\u5177\u3002"}}
{"id": "2509.19939", "categories": ["cs.GR", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2509.19939", "abs": "https://arxiv.org/abs/2509.19939", "authors": ["Hyunjin Cho", "Giyun Choi", "Jongwon Choi"], "title": "AJAHR: Amputated Joint Aware 3D Human Mesh Recovery", "comment": "8pages, Project Page: https://chojinie.github.io/project_AJAHR/", "summary": "Existing human mesh recovery methods assume a standard human body structure,\noverlooking diverse anatomical conditions such as limb loss. This assumption\nintroduces bias when applied to individuals with amputations - a limitation\nfurther exacerbated by the scarcity of suitable datasets. To address this gap,\nwe propose Amputated Joint Aware 3D Human Mesh Recovery (AJAHR), which is an\nadaptive pose estimation framework that improves mesh reconstruction for\nindividuals with limb loss. Our model integrates a body-part amputation\nclassifier, jointly trained with the mesh recovery network, to detect potential\namputations. We also introduce Amputee 3D (A3D), which is a synthetic dataset\noffering a wide range of amputee poses for robust training. While maintaining\ncompetitive performance on non-amputees, our approach achieves state-of-the-art\nresults for amputated individuals. Additional materials can be found at the\nproject webpage.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aAJAHR\u7684\u81ea\u9002\u5e94\u59ff\u52bf\u4f30\u8ba1\u6846\u67b6\uff0c\u4e13\u6ce8\u4e8e\u6539\u8fdb\u622a\u80a2\u8005\u7684\u4e09\u7ef4\u4eba\u4f53\u7f51\u683c\u91cd\u5efa\uff0c\u5e76\u5f15\u5165\u4e86\u5408\u6210\u6570\u636e\u96c6A3D\u4ee5\u652f\u6301\u8bad\u7ec3\u3002", "motivation": "\u73b0\u6709\u7684\u4eba\u4f53\u7f51\u683c\u6062\u590d\u65b9\u6cd5\u5047\u8bbe\u6807\u51c6\u4eba\u4f53\u7ed3\u6784\uff0c\u5ffd\u89c6\u4e86\u622a\u80a2\u7b49\u591a\u6837\u6027\u89e3\u5256\u6761\u4ef6\uff0c\u5bfc\u81f4\u5bf9\u622a\u80a2\u8005\u7684\u5e94\u7528\u5b58\u5728\u504f\u5dee\u3002\u6570\u636e\u96c6\u7684\u7a00\u7f3a\u6027\u8fdb\u4e00\u6b65\u52a0\u5267\u4e86\u8fd9\u4e00\u95ee\u9898\u3002", "method": "AJAHR\u6846\u67b6\u7ed3\u5408\u4e86\u8eab\u4f53\u90e8\u4f4d\u622a\u80a2\u5206\u7c7b\u5668\uff0c\u4e0e\u7f51\u683c\u6062\u590d\u7f51\u7edc\u8054\u5408\u8bad\u7ec3\u4ee5\u68c0\u6d4b\u6f5c\u5728\u622a\u80a2\u60c5\u51b5\u3002\u540c\u65f6\u5f15\u5165\u4e86\u5408\u6210\u6570\u636e\u96c6A3D\u3002", "result": "\u5728\u975e\u622a\u80a2\u8005\u4e0a\u4fdd\u6301\u7ade\u4e89\u529b\u7684\u540c\u65f6\uff0c\u8be5\u65b9\u6cd5\u5728\u622a\u80a2\u8005\u7684\u7f51\u683c\u91cd\u5efa\u4e0a\u8fbe\u5230\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "conclusion": "AJAHR\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e86\u622a\u80a2\u8005\u7684\u4e09\u7ef4\u4eba\u4f53\u7f51\u683c\u6062\u590d\u95ee\u9898\uff0c\u5e76\u4e3a\u76f8\u5173\u9886\u57df\u63d0\u4f9b\u4e86\u5b9e\u7528\u7684\u6570\u636e\u96c6\u548c\u65b9\u6cd5\u3002"}}
{"id": "2509.19995", "categories": ["cs.GR", "cs.CG", "cs.CV"], "pdf": "https://arxiv.org/pdf/2509.19995", "abs": "https://arxiv.org/abs/2509.19995", "authors": ["Rui Xu", "Tianyang Xue", "Qiujie Dong", "Le Wan", "Zhe Zhu", "Peng Li", "Zhiyang Dou", "Cheng Lin", "Shiqing Xin", "Yuan Liu", "Wenping Wang", "Taku Komura"], "title": "MeshMosaic: Scaling Artist Mesh Generation via Local-to-Global Assembly", "comment": "Project is available at:\n  https://xrvitd.github.io/MeshMosaic/index.html", "summary": "Scaling artist-designed meshes to high triangle numbers remains challenging\nfor autoregressive generative models. Existing transformer-based methods suffer\nfrom long-sequence bottlenecks and limited quantization resolution, primarily\ndue to the large number of tokens required and constrained quantization\ngranularity. These issues prevent faithful reproduction of fine geometric\ndetails and structured density patterns. We introduce MeshMosaic, a novel\nlocal-to-global framework for artist mesh generation that scales to over 100K\ntriangles--substantially surpassing prior methods, which typically handle only\naround 8K faces. MeshMosaic first segments shapes into patches, generating each\npatch autoregressively and leveraging shared boundary conditions to promote\ncoherence, symmetry, and seamless connectivity between neighboring regions.\nThis strategy enhances scalability to high-resolution meshes by quantizing\npatches individually, resulting in more symmetrical and organized mesh density\nand structure. Extensive experiments across multiple public datasets\ndemonstrate that MeshMosaic significantly outperforms state-of-the-art methods\nin both geometric fidelity and user preference, supporting superior detail\nrepresentation and practical mesh generation for real-world applications.", "AI": {"tldr": "MeshMosaic\u662f\u4e00\u79cd\u65b0\u9896\u7684\u5c40\u90e8\u5230\u5168\u5c40\u6846\u67b6\uff0c\u7528\u4e8e\u827a\u672f\u5bb6\u8bbe\u8ba1\u7684\u7f51\u683c\u751f\u6210\uff0c\u80fd\u591f\u6269\u5c55\u5230\u8d85\u8fc710\u4e07\u4e2a\u4e09\u89d2\u5f62\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u7684\u81ea\u56de\u5f52\u751f\u6210\u6a21\u578b\u5728\u9ad8\u4e09\u89d2\u5f62\u6570\u91cf\u7684\u827a\u672f\u5bb6\u8bbe\u8ba1\u7f51\u683c\u751f\u6210\u4e2d\u5b58\u5728\u957f\u5e8f\u5217\u74f6\u9888\u548c\u91cf\u5316\u5206\u8fa8\u7387\u9650\u5236\u7684\u95ee\u9898\uff0c\u65e0\u6cd5\u5fe0\u5b9e\u518d\u73b0\u7cbe\u7ec6\u51e0\u4f55\u7ec6\u8282\u548c\u7ed3\u6784\u5316\u5bc6\u5ea6\u6a21\u5f0f\u3002", "method": "MeshMosaic\u9996\u5148\u5c06\u5f62\u72b6\u5206\u5272\u4e3a\u8865\u4e01\uff0c\u81ea\u56de\u5f52\u751f\u6210\u6bcf\u4e2a\u8865\u4e01\uff0c\u5e76\u901a\u8fc7\u5171\u4eab\u8fb9\u754c\u6761\u4ef6\u4fc3\u8fdb\u90bb\u8fd1\u533a\u57df\u7684\u8fde\u8d2f\u6027\u3001\u5bf9\u79f0\u6027\u548c\u65e0\u7f1d\u8fde\u63a5\u3002", "result": "\u5728\u591a\u4e2a\u516c\u5f00\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cMeshMosaic\u5728\u51e0\u4f55\u4fdd\u771f\u5ea6\u548c\u7528\u6237\u504f\u597d\u65b9\u9762\u663e\u8457\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u65b9\u6cd5\u3002", "conclusion": "MeshMosaic\u652f\u6301\u5353\u8d8a\u7684\u7ec6\u8282\u8868\u793a\u548c\u5b9e\u9645\u5e94\u7528\u4e2d\u7684\u7f51\u683c\u751f\u6210\uff0c\u4e3a\u9ad8\u5206\u8fa8\u7387\u7f51\u683c\u7684\u53ef\u6269\u5c55\u6027\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.20128", "categories": ["cs.GR", "cs.AI", "cs.CV", "cs.MM"], "pdf": "https://arxiv.org/pdf/2509.20128", "abs": "https://arxiv.org/abs/2509.20128", "authors": ["Tianle Lyu", "Junchuan Zhao", "Ye Wang"], "title": "KSDiff: Keyframe-Augmented Speech-Aware Dual-Path Diffusion for Facial Animation", "comment": "5 pages, 3 figures, 3 tables", "summary": "Audio-driven facial animation has made significant progress in multimedia\napplications, with diffusion models showing strong potential for talking-face\nsynthesis. However, most existing works treat speech features as a monolithic\nrepresentation and fail to capture their fine-grained roles in driving\ndifferent facial motions, while also overlooking the importance of modeling\nkeyframes with intense dynamics. To address these limitations, we propose\nKSDiff, a Keyframe-Augmented Speech-Aware Dual-Path Diffusion framework.\nSpecifically, the raw audio and transcript are processed by a Dual-Path Speech\nEncoder (DPSE) to disentangle expression-related and head-pose-related\nfeatures, while an autoregressive Keyframe Establishment Learning (KEL) module\npredicts the most salient motion frames. These components are integrated into a\nDual-path Motion generator to synthesize coherent and realistic facial motions.\nExtensive experiments on HDTF and VoxCeleb demonstrate that KSDiff achieves\nstate-of-the-art performance, with improvements in both lip synchronization\naccuracy and head-pose naturalness. Our results highlight the effectiveness of\ncombining speech disentanglement with keyframe-aware diffusion for talking-head\ngeneration.", "AI": {"tldr": "KSDiff\u662f\u4e00\u79cd\u57fa\u4e8e\u5173\u952e\u5e27\u589e\u5f3a\u548c\u8bed\u97f3\u611f\u77e5\u7684\u53cc\u8def\u5f84\u6269\u6563\u6846\u67b6\uff0c\u901a\u8fc7\u5206\u79bb\u8bed\u97f3\u7279\u5f81\u4e2d\u7684\u8868\u60c5\u548c\u5934\u90e8\u59ff\u6001\u4fe1\u606f\uff0c\u5e76\u7ed3\u5408\u5173\u952e\u5e27\u9884\u6d4b\uff0c\u663e\u8457\u63d0\u5347\u4e86\u8bf4\u8bdd\u5934\u90e8\u52a8\u753b\u7684\u540c\u6b65\u6027\u548c\u81ea\u7136\u6027\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u901a\u5e38\u5c06\u8bed\u97f3\u7279\u5f81\u4f5c\u4e3a\u4e00\u4e2a\u6574\u4f53\u5904\u7406\uff0c\u672a\u80fd\u6355\u6349\u5230\u7ec6\u7c92\u5ea6\u9a71\u52a8\u7684\u9762\u90e8\u8fd0\u52a8\u5dee\u5f02\uff0c\u540c\u65f6\u5ffd\u7565\u4e86\u52a8\u6001\u5173\u952e\u5e27\u7684\u91cd\u8981\u6027\u3002", "method": "KSDiff\u6846\u67b6\u5305\u542b\u53cc\u8def\u5f84\u8bed\u97f3\u7f16\u7801\u5668\uff08DPSE\uff09\u548c\u81ea\u56de\u5f52\u5173\u952e\u5e27\u5b66\u4e60\u6a21\u5757\uff08KEL\uff09\uff0c\u5206\u522b\u7528\u4e8e\u5206\u79bb\u8bed\u97f3\u7279\u5f81\u548c\u9884\u6d4b\u5173\u952e\u5e27\uff0c\u5e76\u901a\u8fc7\u53cc\u8def\u5f84\u8fd0\u52a8\u751f\u6210\u5668\u5408\u6210\u9762\u90e8\u52a8\u753b\u3002", "result": "\u5728HDTF\u548cVoxCeleb\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cKSDiff\u5728\u5507\u90e8\u540c\u6b65\u51c6\u786e\u6027\u548c\u5934\u90e8\u59ff\u6001\u81ea\u7136\u6027\u65b9\u9762\u5747\u8fbe\u5230\u6700\u4f18\u6027\u80fd\u3002", "conclusion": "KSDiff\u901a\u8fc7\u7ed3\u5408\u8bed\u97f3\u7279\u5f81\u5206\u79bb\u548c\u5173\u952e\u5e27\u611f\u77e5\u6269\u6563\u6a21\u578b\uff0c\u663e\u8457\u63d0\u5347\u4e86\u8bf4\u8bdd\u5934\u90e8\u52a8\u753b\u7684\u751f\u6210\u8d28\u91cf\u3002"}}
{"id": "2509.20198", "categories": ["cs.GR"], "pdf": "https://arxiv.org/pdf/2509.20198", "abs": "https://arxiv.org/abs/2509.20198", "authors": ["Philipp Erler", "Lukas Herzberger", "Michael Wimmer", "Markus Sch\u00fctz"], "title": "LidarScout: Direct Out-of-Core Rendering of Massive Point Clouds", "comment": "Published at High-Performance Graphics 2025", "summary": "Large-scale terrain scans are the basis for many important tasks, such as\ntopographic mapping, forestry, agriculture, and infrastructure planning. The\nresulting point cloud data sets are so massive in size that even basic tasks\nlike viewing take hours to days of pre-processing in order to create\nlevel-of-detail structures that allow inspecting the data set in their entirety\nin real time. In this paper, we propose a method that is capable of instantly\nvisualizing massive country-sized scans with hundreds of billions of points.\nUpon opening the data set, we first load a sparse subsample of points and\ninitialize an overview of the entire point cloud, immediately followed by a\nsurface reconstruction process to generate higher-quality, hole-free\nheightmaps. As users start navigating towards a region of interest, we continue\nto prioritize the heightmap construction process to the user's viewpoint. Once\na user zooms in closely, we load the full-resolution point cloud data for that\nregion and update the corresponding height map textures with the\nfull-resolution data. As users navigate elsewhere, full-resolution point data\nthat is no longer needed is unloaded, but the updated heightmap textures are\nretained as a form of medium level of detail. Overall, our method constitutes a\nform of direct out-of-core rendering for massive point cloud data sets\n(terabytes, compressed) that requires no preprocessing and no additional disk\nspace. Source code, executable, pre-trained model, and dataset are available\nat: https://github.com/cg-tuwien/lidarscout", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65e0\u9700\u9884\u5904\u7406\u7684\u65b9\u6cd5\uff0c\u80fd\u591f\u5373\u65f6\u53ef\u89c6\u5316\u5305\u542b\u6570\u767e\u4ebf\u70b9\u7684\u5927\u89c4\u6a21\u5730\u5f62\u626b\u63cf\u6570\u636e\uff0c\u901a\u8fc7\u52a0\u8f7d\u7a00\u758f\u5b50\u6837\u672c\u3001\u8868\u9762\u91cd\u5efa\u548c\u52a8\u6001\u52a0\u8f7d\u5168\u5206\u8fa8\u7387\u6570\u636e\u5b9e\u73b0\u5b9e\u65f6\u4ea4\u4e92\u3002", "motivation": "\u5927\u89c4\u6a21\u5730\u5f62\u626b\u63cf\u6570\u636e\u96c6\u7684\u70b9\u4e91\u6570\u636e\u4f53\u79ef\u5e9e\u5927\uff0c\u5373\u4f7f\u662f\u57fa\u672c\u7684\u67e5\u770b\u4efb\u52a1\u4e5f\u9700\u8981\u6570\u5c0f\u65f6\u5230\u6570\u5929\u7684\u9884\u5904\u7406\uff0c\u5bfc\u81f4\u65e0\u6cd5\u5b9e\u65f6\u4ea4\u4e92\u3002\u8bba\u6587\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u74f6\u9888\u3002", "method": "\u8bba\u6587\u7684\u65b9\u6cd5\u5305\u62ec\u521d\u59cb\u52a0\u8f7d\u7a00\u758f\u5b50\u6837\u672c\u70b9\uff0c\u751f\u6210\u6982\u89c8\uff1b\u968f\u540e\u8fdb\u884c\u8868\u9762\u91cd\u5efa\u4ee5\u751f\u6210\u9ad8\u8d28\u91cf\u65e0\u7a7a\u6d1e\u7684\u9ad8\u5ea6\u56fe\uff1b\u6839\u636e\u7528\u6237\u89c6\u89d2\u52a8\u6001\u52a0\u8f7d\u548c\u5378\u8f7d\u5168\u5206\u8fa8\u7387\u70b9\u4e91\u6570\u636e\u3002", "result": "\u8be5\u65b9\u6cd5\u5b9e\u73b0\u4e86\u5bf9\u6d77\u91cf\u70b9\u4e91\u6570\u636e\u96c6\u7684\u76f4\u63a5\u5916\u6838\u6e32\u67d3\uff0c\u65e0\u9700\u9884\u5904\u7406\u6216\u989d\u5916\u78c1\u76d8\u7a7a\u95f4\uff0c\u652f\u6301\u5b9e\u65f6\u4ea4\u4e92\u548c\u9ad8\u8d28\u91cf\u53ef\u89c6\u5316\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u5927\u89c4\u6a21\u70b9\u4e91\u6570\u636e\u7684\u5b9e\u65f6\u53ef\u89c6\u5316\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\uff0c\u514b\u670d\u4e86\u4f20\u7edf\u9884\u5904\u7406\u65b9\u6cd5\u7684\u74f6\u9888\uff0c\u9002\u7528\u4e8e\u591a\u79cd\u5b9e\u9645\u5e94\u7528\u573a\u666f\u3002"}}
