{"id": "2512.18134", "categories": ["cs.PL", "cs.AR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.18134", "abs": "https://arxiv.org/abs/2512.18134", "authors": ["Rupanshu Soi", "Rohan Yadav", "Fredrik Kjolstad", "Alex Aiken", "Maryam Mehri Dehnavi", "Michael Garland", "Michael Bauer"], "title": "Optimal Software Pipelining and Warp Specialization for Tensor Core GPUs", "comment": null, "summary": "GPU architectures have continued to grow in complexity, with recent incarnations introducing increasingly powerful fixed-function units for matrix multiplication and data movement to accompany highly parallel general-purpose cores. To fully leverage these machines, software must use sophisticated schedules that maximally utilize all hardware resources. Since realizing such schedules is complex, both programmers and compilers routinely employ program transformations, such as software pipelining (SWP) and warp specialization (WS), to do so in practice. However, determining how best to use SWP and WS in combination is a challenging problem that is currently handled through a mix of brittle compilation heuristics and fallible human intuition, with little insight into the space of solutions. To remedy this situation, we introduce a novel formulation of SWP and WS as a joint optimization problem that can be solved holistically by off-the-shelf constraint solvers. We reify our approach in Twill, the first system that automatically derives optimal SWP and WS schedules for a large class of iterative programs. Twill is heuristic-free, easily extensible to new GPU architectures, and guaranteed to produce optimal schedules. We show that Twill can rediscover, and thereby prove optimal, the SWP and WS schedules manually developed by experts for Flash Attention on both the NVIDIA Hopper and Blackwell GPU architectures."}
{"id": "2512.18842", "categories": ["cs.PL"], "pdf": "https://arxiv.org/pdf/2512.18842", "abs": "https://arxiv.org/abs/2512.18842", "authors": ["Aleksandr Fedchin", "Antero Mejr", "Hari Sundar", "Jeffrey S. Foster"], "title": "DafnyMPI: A Dafny Library for Verifying Message-Passing Concurrent Programs", "comment": "To appear in Proceedings of the ACM on Programming Languages (POPL)", "summary": "The Message Passing Interface (MPI) is widely used in parallel, high-performance programming, yet writing bug-free software that uses MPI remains difficult. We introduce DafnyMPI, a novel, scalable approach to formally verifying MPI software. DafnyMPI allows proving deadlock freedom, termination, and functional equivalence with simpler sequential implementations. In contrast to existing specialized frameworks, DafnyMPI avoids custom concurrency logics and instead relies on Dafny, a verification-ready programming language used for sequential programs, extending it with concurrent reasoning abilities. DafnyMPI is implemented as a library that enables safe MPI programming by requiring users to specify the communication topology upfront and to verify that calls to communication primitives such as MPI_ISEND and MPI_WAIT meet their preconditions. We formalize DafnyMPI using a core calculus and prove that the preconditions suffice to guarantee deadlock freedom. Functional equivalence is proved via rely-guarantee reasoning over message payloads and a system that guarantees safe use of read and write buffers. Termination and the absence of runtime errors are proved using standard Dafny techniques. To further demonstrate the applicability of DafnyMPI, we verify numerical solutions to three canonical partial differential equations. We believe DafnyMPI demonstrates how to make formal verification viable for a broader class of programs and provides proof engineers with additional tools for software verification of parallel and concurrent systems."}
{"id": "2512.17950", "categories": ["cs.GT", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.17950", "abs": "https://arxiv.org/abs/2512.17950", "authors": ["Zhao Song", "Song Yue", "Jiahao Zhang"], "title": "Which Coauthor Should I Nominate in My 99 ICLR Submissions? A Mathematical Analysis of the ICLR 2026 Reciprocal Reviewer Nomination Policy", "comment": null, "summary": "The rapid growth of AI conference submissions has created an overwhelming reviewing burden. To alleviate this, recent venues such as ICLR 2026 introduced a reviewer nomination policy: each submission must nominate one of its authors as a reviewer, and any paper nominating an irresponsible reviewer is desk-rejected. We study this new policy from the perspective of author welfare. Assuming each author carries a probability of being irresponsible, we ask: how can authors (or automated systems) nominate reviewers to minimize the risk of desk rejections? We formalize and analyze three variants of the desk-rejection risk minimization problem. The basic problem, which minimizes expected desk rejections, is solved optimally by a simple greedy algorithm. We then introduce hard and soft nomination limit variants that constrain how many papers may nominate the same author, preventing widespread failures if one author is irresponsible. These formulations connect to classical optimization frameworks, including minimum-cost flow and linear programming, allowing us to design efficient, principled nomination strategies. Our results provide the first theoretical study for reviewer nomination policies, offering both conceptual insights and practical directions for authors to wisely choose which co-author should serve as the nominated reciprocal reviewer."}
{"id": "2512.17952", "categories": ["cs.GT", "cs.AI", "econ.TH"], "pdf": "https://arxiv.org/pdf/2512.17952", "abs": "https://arxiv.org/abs/2512.17952", "authors": ["Hanyu Li", "Xiaotie Deng"], "title": "Will AI Trade? A Computational Inversion of the No-Trade Theorem", "comment": "Accepted in WINE 2025", "summary": "Classic no-trade theorems attribute trade to heterogeneous beliefs. We re-examine this conclusion for AI agents, asking if trade can arise from computational limitations, under common beliefs. We model agents' bounded computational rationality within an unfolding game framework, where computational power determines the complexity of its strategy. Our central finding inverts the classic paradigm: a stable no-trade outcome (Nash equilibrium) is reached only when \"almost rational\" agents have slightly different computational power. Paradoxically, when agents possess identical power, they may fail to converge to equilibrium, resulting in persistent strategic adjustments that constitute a form of trade. This instability is exacerbated if agents can strategically under-utilize their computational resources, which eliminates any chance of equilibrium in Matching Pennies scenarios. Our results suggest that the inherent computational limitations of AI agents can lead to situations where equilibrium is not reached, creating a more lively and unpredictable trade environment than traditional models would predict."}
{"id": "2512.17979", "categories": ["cs.GT", "cs.AI", "cs.MA", "stat.AP"], "pdf": "https://arxiv.org/pdf/2512.17979", "abs": "https://arxiv.org/abs/2512.17979", "authors": ["Matthieu Mastio", "Paul Saves", "Benoit Gaudou", "Nicolas Verstaevel"], "title": "Adaptive Agents in Spatial Double-Auction Markets: Modeling the Emergence of Industrial Symbiosis", "comment": "AAMAS CC-BY 4.0 licence. Adaptive Agents in Spatial Double-Auction Markets: Modeling the Emergence of Industrial Symbiosis. Full paper. In Proc. of the 25th International Conference on Autonomous Agents and Multiagent Systems (AAMAS 2026), Paphos, Cyprus, May 25 - 29, 2026, IFAAMAS, 10 pages", "summary": "Industrial symbiosis fosters circularity by enabling firms to repurpose residual resources, yet its emergence is constrained by socio-spatial frictions that shape costs, matching opportunities, and market efficiency. Existing models often overlook the interaction between spatial structure, market design, and adaptive firm behavior, limiting our understanding of where and how symbiosis arises. We develop an agent-based model where heterogeneous firms trade byproducts through a spatially embedded double-auction market, with prices and quantities emerging endogenously from local interactions. Leveraging reinforcement learning, firms adapt their bidding strategies to maximize profit while accounting for transport costs, disposal penalties, and resource scarcity. Simulation experiments reveal the economic and spatial conditions under which decentralized exchanges converge toward stable and efficient outcomes. Counterfactual regret analysis shows that sellers' strategies approach a near Nash equilibrium, while sensitivity analysis highlights how spatial structures and market parameters jointly govern circularity. Our model provides a basis for exploring policy interventions that seek to align firm incentives with sustainability goals, and more broadly demonstrates how decentralized coordination can emerge from adaptive agents in spatially constrained markets."}
{"id": "2512.18296", "categories": ["cs.GT", "stat.AP"], "pdf": "https://arxiv.org/pdf/2512.18296", "abs": "https://arxiv.org/abs/2512.18296", "authors": ["Lijun Bo", "Weiqiang Chang"], "title": "Privacy Data Pricing: A Stackelberg Game Approach", "comment": "21 pages", "summary": "Data markets are emerging as key mechanisms for trading personal and organizational data. Traditional data pricing studies -- such as query-based or arbitrage-free pricing models -- mainly emphasize price consistency and profit maximization but often neglect privacy constraints and strategic interactions. The widespread adoption of differential privacy (DP) introduces a fundamental privacy-utility trade-off: noise protects individuals' privacy but reduces data accuracy and market value. This paper develops a Stackelberg game framework for pricing DP data, where the market maker (leader) sets the price function and the data buyer (follower) selects the optimal query precision under DP constraints. We derive the equilibrium strategies for both parties under a balanced pricing function where the pricing decision variable enters linearly into the original pricing model. We obtain closed-form solutions for the optimal variance and pricing level, and determine the boundary conditions for market participation. Furthermore, we extend the analysis to Stackelberg games involving nonlinear power pricing functions. The model bridges DP and economic mechanism design, offering a unified foundation for incentive-compatible and privacy-conscious data pricing in data markets."}
{"id": "2512.18444", "categories": ["cs.GT", "cs.AI", "cs.DC", "cs.MA"], "pdf": "https://arxiv.org/pdf/2512.18444", "abs": "https://arxiv.org/abs/2512.18444", "authors": ["Grammateia Kotsialou"], "title": "Snowveil: A Framework for Decentralised Preference Discovery", "comment": null, "summary": "Aggregating subjective preferences of a large group is a fundamental challenge in computational social choice, traditionally reliant on central authorities. To address the limitations of this model, this paper introduces Decentralised Preference Discovery (DPD), the problem of determining the collective will of an electorate under constraints of censorship resistance, partial information, and asynchronous communication. We propose Snowveil, a novel framework for this task. Snowveil uses an iterative, gossip-based protocol where voters repeatedly sample the preferences of a small, random subset of the electorate to progressively converge on a collective outcome. We demonstrate the framework's modularity by designing the Constrained Hybrid Borda (CHB), a novel aggregation rule engineered to balance broad consensus with strong plurality support, and provide a rigorous axiomatic analysis of its properties. By applying a potential function and submartingale theory, we develop a multi-level analytical method to show that the system almost surely converges to a stable, single-winner in finite time, a process that can then be iterated to construct a set of winning candidates for multi-winner scenarios. This technique is largely agnostic to the specific aggregation rule, requiring only that it satisfies core social choice axioms like Positive Responsiveness, thus offering a formal toolkit for a wider class of DPD protocols. Furthermore, we present a comprehensive empirical analysis through extensive simulation, validating Snowveil's $O(n)$ scalability. Overall, this work advances the understanding of how a stable consensus can emerge from subjective, complex, and diverse preferences in decentralised systems for large electorates."}
{"id": "2512.18620", "categories": ["cs.GT"], "pdf": "https://arxiv.org/pdf/2512.18620", "abs": "https://arxiv.org/abs/2512.18620", "authors": ["Hau Chan", "Jianan Lin", "Chenhao Wang"], "title": "Obnoxious Facility Location Problems: Strategyproof Mechanisms Optimizing $L_p$-Aggregated Utilities and Costs", "comment": "To appear in AAMAS 2026", "summary": "We study the problem of locating a single obnoxious facility on the normalized line segment $[0,1]$ with strategic agents from a mechanism design perspective. Each agent has a preference for the undesirable location of the facility and would prefer the facility to be far away from their location. We consider the utility of the agent, defined as the distance between the agent's location and the facility location, and the cost of each agent, equal to one minus the utility. Given this standard setting of obnoxious facility location problems, our goal is to design (group) strategyproof mechanisms to elicit agent locations truthfully and determine facility location approximately optimizing the $L_p$-aggregated utility and cost objectives, which generalizes the $L_p$-norm ($p\\ge 1$) of the agents' utilities and agents' costs to any $p \\in [-\\infty, \\infty]$, respectively. We establish upper and lower bounds on the approximation ratios of deterministic and randomized (group) strategyproof mechanisms for maximizing the $L_p$-aggregated utilities or minimizing the $L_p$-aggregated costs across the range of \\(p\\)-values. While there are gaps between upper and lower bounds for randomized mechanisms, our bounds for deterministic mechanisms are tight."}
{"id": "2512.18858", "categories": ["cs.GT", "stat.AP", "stat.CO"], "pdf": "https://arxiv.org/pdf/2512.18858", "abs": "https://arxiv.org/abs/2512.18858", "authors": ["Avirup Chakraborty", "Shirsa Maitra", "Tathagata Banerjee", "Diganta Mukherjee", "Tridib Mukherjee"], "title": "Adapting Skill Ratings to Luck-Based Hidden-Information Games", "comment": "13 pages, 4 figures", "summary": "Rating systems play a crucial role in evaluating player skill across competitive environments. The Elo rating system, originally designed for deterministic and information-complete games such as chess, has been widely adopted and modified in various domains. However, the traditional Elo rating system only considers game outcomes for rating calculation and assumes uniform initial states across players. This raises important methodological challenges in skill modelling for popular partially randomized incomplete-information games such as Rummy. In this paper, we examine the limitations of conventional Elo ratings when applied to luck-driven environments and propose a modified Elo framework specifically tailored for Rummy. Our approach incorporates score-based performance metrics and explicitly models the influence of initial hand quality to disentangle skill from luck. Through extensive simulations involving 270,000 games across six strategies of varying sophistication, we demonstrate that our proposed system achieves stable convergence, superior discriminative power, and enhanced predictive accuracy compared to traditional Elo formulations. The framework maintains computational simplicity while effectively capturing the interplay of skill, strategy, and randomness, with broad applicability to other stochastic competitive environments."}
{"id": "2512.18989", "categories": ["cs.GT"], "pdf": "https://arxiv.org/pdf/2512.18989", "abs": "https://arxiv.org/abs/2512.18989", "authors": ["Youzhi Zhang"], "title": "Considering the Difference in Utility Functions of Team Players in Adversarial Team Games", "comment": null, "summary": "The United Nations' 2030 Agenda for Sustainable Development requires that all countries collaborate to fight adversarial factors to achieve peace and prosperity for humans and the planet. This scenario can be formulated as an adversarial team game in AI literature, where a team of players play against an adversary. However, previous solution concepts for this game assume that team players have the same utility functions, which cannot cover the real-world case that countries do not always have the same utility function. This paper argues that studying adversarial team games should not ignore the difference in utility functions of team players. We show that ignoring the difference in utility functions of team players could cause the computed equilibrium to be unstable. To show the benefit of considering the difference in utility functions of team players, we introduce a novel solution concept called Co-opetition Equilibrium (CoE) for the adversarial team game. In this game, team players with different utility functions (i.e., cooperation between team players) correlate their actions to play against the adversary (i.e., competition between the team and the adversary). We further introduce the team-maximizing CoE, which is a CoE but maximizes the team's utility among all CoEs. Both equilibria can overcome the issue caused by ignoring the difference in utility functions of team players. We further show the opportunities for theoretical and algorithmic contributions based on our position of considering the difference in utility functions of team players."}
{"id": "2512.19113", "categories": ["cs.GT"], "pdf": "https://arxiv.org/pdf/2512.19113", "abs": "https://arxiv.org/abs/2512.19113", "authors": ["Luca Pennella", "Pietro Saggese", "Fabio Pinelli", "Letterio Galletta"], "title": "A Unified Framework and Comparative Study of Decentralized Finance Derivatives Protocols", "comment": "Submitted; under review", "summary": "Decentralized Finance (DeFi) applications introduce novel financial instruments replicating and extending traditional ones through blockchain-based smart contracts. Among these, derivatives protocols enable the decentralized trading of cryptoassets that are the counterpart of derivative products available in traditional finance. Despite their growing significance, DeFi derivatives protocols remain relatively understudied compared to other DeFi instruments, such as lending protocols and decentralized exchanges with automated market makers. This paper systematically analyzes DeFi derivatives protocols - categorized into perpetual, options, and synthetics - in the field, highlighting similarities, differences, dynamics, and actors. As a result of our study, we provide a formal characterization of decentralized derivative products and introduce a unifying conceptual framework that captures the design principles and core architecture of such protocols. We complement our theoretical analysis with numerical simulations: we evaluate protocol dynamics under various economic conditions, including changes in underlying asset prices, volatility, protocol-specific fees, leverage, and their impact on liquidation and profitability."}
{"id": "2512.19292", "categories": ["cs.GT"], "pdf": "https://arxiv.org/pdf/2512.19292", "abs": "https://arxiv.org/abs/2512.19292", "authors": ["Ruijun Ma", "Xin Chen", "Xiaoqing Wen", "Hui Xu", "Shengnan Ye", "Chuanjian Zhang", "Senling Wang"], "title": "LOCO: A Low-Cost SNU-Self-Resilient Latch Using an Output-Split C-Element", "comment": null, "summary": "As the CMOS technology enters nanometer scales, integrated circuits (ICs) become increasingly sensitive to radiation-induced soft errors, which can corrupt the state of storage elements and cause severe reliability issues. Many hardened designs have been proposed to mitigate soft errors by using filtering elements. However, existing filtering elements only protect their inputs against soft errors and leave their outputs unprotected. Therefore, additional filtering elements must be added to protect outputs, resulting in extra overhead. In this paper, we first propose a novel Output-Split C-element (OSC) to protect both its input and output nodes, and then a novel LOw-COst single-node-upset (SNU) self-resilient latch (LOCO) to use OSCs to achieve both soft error resilience and low overhead. The usage of OSCs effectively reduce the short-circuit current of the LOCO latch during switching activities. Furthermore, the usage of clock gating and high-speed path reduces power consumption and delay, respectively. Compared with state-of-the-art SNU-resilient hardened designs, the LOCO latch achieves 19% fewer transistors, 63.58% lower power, 74% less delay, and 92% lower power-delay-product (PDP) on average. In addition, the LOCO latch exhibits better stability under variations in PVT (Process, Voltage, and Temperature)."}
{"id": "2512.19328", "categories": ["cs.GT"], "pdf": "https://arxiv.org/pdf/2512.19328", "abs": "https://arxiv.org/abs/2512.19328", "authors": ["Bingqing Liu", "David Watling", "Joseph Y. J. Chow"], "title": "Stochastic assignment games for Mobility-as-a-Service markets", "comment": null, "summary": "We study the stochastic assignment game and extend it to model multimodal mobility markets with a regulator or a Mobility-as-a-Service (MaaS) platform. We start by presenting general forms of one-to-one and many-to-many stochastic assignment games. Optimality conditions are discussed. The core of stochastic assignment games is defined, with expected payoffs of sellers and buyers in stochastic assignment games as payoffs from a hypothetical \"ideal matching\" that represent sellers' and buyers' expectations under imperfect information. To apply stochastic assignment games to the urban mobility markets, we extend the general stochastic many-to-many assignment game into a stochastic Stackelberg game to model MaaS systems, where the platform is the leader, and users and operators are the followers. The platform sets fares to maximize revenue. Users and operator react to the fare settings to form a stochastic many-to-many assignment game considering both fixed-route services and Mobility-on-Demand (MOD). The Stackelberg game is formulated as a bilevel problem. The lower level is the stochastic many-to-many assignment game between users and operators, shown to yield a coalitional logit model. The upper-level problem is a fare adjustment problem maximizing revenue. An iterative balancing algorithm is proposed to solve the lower-level problem exactly. The bilevel problem is solved through an iterative fare adjusting heuristic, whose solution is shown to be equivalent to the bilevel problem with an additional condition when it converges. Two case studies are conducted. The model can be applied to design MaaS fares maximizing income of the platform while anticipating the selfish behavior and heterogeneity of users and operators. Public agencies can also use the model to manage multimodal transportation systems."}
{"id": "2512.19388", "categories": ["cs.GT"], "pdf": "https://arxiv.org/pdf/2512.19388", "abs": "https://arxiv.org/abs/2512.19388", "authors": ["Matteo Castiglioni", "Junjie Chen", "Yingkai Li"], "title": "Fair Team Contracts", "comment": null, "summary": "A principal selects a team of agents for collaborating on a joint project. The principal aims to design a revenue-optimal contract that incentivize the team of agents to exert costly effort while satisfying fairness constraints. We show that the optimal fair contract ensures that there is a minimum share, and every agent receives a linear contract weakly higher than the minimum share that is sufficient to incentivize them to exert costly effort. We utilize this structure to design an FPTAS for additive success functions and a constant approximation algorithm for submodular success functions. Moreover, we show that adopting optimal fair contracts can lead to a 25% revenue increase compared to the optimal non-discriminatory contracts even for additive success functions."}
{"id": "2512.19405", "categories": ["cs.GT", "econ.TH"], "pdf": "https://arxiv.org/pdf/2512.19405", "abs": "https://arxiv.org/abs/2512.19405", "authors": ["Jussi Keppo", "Yingkai Li"], "title": "Three Tiers and Thresholds: Incentives in Private Market Investing", "comment": null, "summary": "This paper studies optimal contract design in private market investing, focusing on internal decision making in venture capital and private equity firms. A principal relies on an agent who privately exerts costly due diligence effort and then recommends whether to invest. Outcomes are observable ex post even when an opportunity is declined, allowing compensation to reward both successful investments and prudent decisions to pass. We characterize profit maximizing contracts that induce information acquisition and truthful reporting. We show that three tier contracts are sufficient, with payments contingent on the agent's recommendation and the realized return. In symmetric environments satisfying the monotone likelihood ratio property, the optimal contract further simplifies to a threshold contract that pays only when the recommendation is aligned with an extreme realized return. These results provide guidance for performance based compensation that promotes diligent screening while limiting excessive risk taking."}
