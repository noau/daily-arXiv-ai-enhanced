{"id": "2510.03446", "categories": ["cs.GT", "cs.MA", "econ.GN", "q-fin.EC", "q-fin.RM"], "pdf": "https://arxiv.org/pdf/2510.03446", "abs": "https://arxiv.org/abs/2510.03446", "authors": ["Oliver Slumbers", "Benjamin Patrick Evans", "Sumitra Ganesh", "Leo Ardon"], "title": "Downside Risk-Aware Equilibria for Strategic Decision-Making", "comment": "Accepted at ECAI 2024 Workshop on AI In Finance", "summary": "Game theory has traditionally had a relatively limited view of risk based on\nhow a player's expected reward is impacted by the uncertainty of the actions of\nother players. Recently, a new game-theoretic approach provides a more holistic\nview of risk also considering the reward-variance. However, these\nvariance-based approaches measure variance of the reward on both the upside and\ndownside. In many domains, such as finance, downside risk only is of key\nimportance, as this represents the potential losses associated with a decision.\nIn contrast, large upside \"risk\" (e.g. profits) are not an issue. To address\nthis restrictive view of risk, we propose a novel solution concept, downside\nrisk aware equilibria (DRAE) based on lower partial moments. DRAE restricts\ndownside risk, while placing no restrictions on upside risk, and additionally,\nmodels higher-order risk preferences. We demonstrate the applicability of DRAE\non several games, successfully finding equilibria which balance downside risk\nwith expected reward, and prove the existence and optimality of this\nequilibria."}
{"id": "2510.03855", "categories": ["cs.GT"], "pdf": "https://arxiv.org/pdf/2510.03855", "abs": "https://arxiv.org/abs/2510.03855", "authors": ["Tianlong Nan", "Shuvomoy Das Gupta", "Garud Iyengar", "Christian Kroer"], "title": "On the $O(1/T)$ Convergence of Alternating Gradient Descent-Ascent in Bilinear Games", "comment": "34 pages, 56 figures", "summary": "We study the alternating gradient descent-ascent (AltGDA) algorithm in\ntwo-player zero-sum games. Alternating methods, where players take turns to\nupdate their strategies, have long been recognized as simple and practical\napproaches for learning in games, exhibiting much better numerical performance\nthan their simultaneous counterparts. However, our theoretical understanding of\nalternating algorithms remains limited, and results are mostly restricted to\nthe unconstrained setting. We show that for two-player zero-sum games that\nadmit an interior Nash equilibrium, AltGDA converges at an $O(1/T)$ ergodic\nconvergence rate when employing a small constant stepsize. This is the first\nresult showing that alternation improves over the simultaneous counterpart of\nGDA in the constrained setting. For games without an interior equilibrium, we\nshow an $O(1/T)$ local convergence rate with a constant stepsize that is\nindependent of any game-specific constants. In a more general setting, we\ndevelop a performance estimation programming (PEP) framework to jointly\noptimize the AltGDA stepsize along with its worst-case convergence rate. The\nPEP results indicate that AltGDA may achieve an $O(1/T)$ convergence rate for a\nfinite horizon $T$, whereas its simultaneous counterpart appears limited to an\n$O(1/\\sqrt{T})$ rate."}
{"id": "2510.04343", "categories": ["cs.GT", "cs.DS", "math.OC"], "pdf": "https://arxiv.org/pdf/2510.04343", "abs": "https://arxiv.org/abs/2510.04343", "authors": ["Tim S. G. van Eck", "Pieter Kleer", "Johan S. H. van Leeuwaarden"], "title": "Robust Optimality of Bundling Goods Beyond Finite Variance", "comment": null, "summary": "When selling many goods with independent valuations, we develop a\ndistributionally robust framework, consisting of a two-player game between\nseller and nature. The seller has only limited knowledge about the value\ndistribution. The seller selects a revenue-maximizing mechanism, after which\nnature chooses a revenue-minimizing distribution from all distributions that\ncomply with the limited knowledge. When the seller knows the mean and variance\nof valuations, bundling is known to be an asymptotically optimal deterministic\nmechanism, achieving a normalized revenue close to the mean. Moving beyond this\nvariance assumption, we assume knowledge of the mean absolute deviation (MAD),\naccommodating more dispersion and heavy-tailed valuations with infinite\nvariance. We show for a large range of MAD values that bundling remains\noptimal, but the seller can only guarantee a revenue strictly smaller than the\nmean. Another noteworthy finding is indifference to the order of play, as both\nthe max-min and min-max versions of the problem yield identical values. This\ncontrasts with deterministic mechanisms and the separate sale of goods, where\nthe order of play significantly impacts outcomes. We further underscore the\nuniversality of the optimal bundling price by demonstrating its efficacy in\noptimizing not only absolute revenue but also the absolute regret and ratio\nobjective among all bundling prices"}
{"id": "2510.04407", "categories": ["cs.GT", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.04407", "abs": "https://arxiv.org/abs/2510.04407", "authors": ["Brian Hu Zhang", "Ioannis Anagnostides", "Tuomas Sandholm"], "title": "Scale-Invariant Regret Matching and Online Learning with Optimal Convergence: Bridging Theory and Practice in Zero-Sum Games", "comment": null, "summary": "A considerable chasm has been looming for decades between theory and practice\nin zero-sum game solving through first-order methods. Although a convergence\nrate of $T^{-1}$ has long been established since Nemirovski's mirror-prox\nalgorithm and Nesterov's excessive gap technique in the early 2000s, the most\neffective paradigm in practice is *counterfactual regret minimization*, which\nis based on *regret matching* and its modern variants. In particular, the state\nof the art across most benchmarks is *predictive* regret matching$^+$\n(PRM$^+$), in conjunction with non-uniform averaging. Yet, such algorithms can\nexhibit slower $\\Omega(T^{-1/2})$ convergence even in self-play.\n  In this paper, we close the gap between theory and practice. We propose a new\nscale-invariant and parameter-free variant of PRM$^+$, which we call\nIREG-PRM$^+$. We show that it achieves $T^{-1/2}$ best-iterate and $T^{-1}$\n(i.e., optimal) average-iterate convergence guarantees, while also being on par\nwith PRM$^+$ on benchmark games. From a technical standpoint, we draw an\nanalogy between IREG-PRM$^+$ and optimistic gradient descent with *adaptive*\nlearning rate. The basic flaw of PRM$^+$ is that the ($\\ell_2$-)norm of the\nregret vector -- which can be thought of as the inverse of the learning rate --\ncan decrease. By contrast, we design IREG-PRM$^+$ so as to maintain the\ninvariance that the norm of the regret vector is nondecreasing. This enables us\nto derive an RVU-type bound for IREG-PRM$^+$, the first such property that does\nnot rely on introducing additional hyperparameters to enforce smoothness.\n  Furthermore, we find that IREG-PRM$^+$ performs on par with an adaptive\nversion of optimistic gradient descent that we introduce whose learning rate\ndepends on the misprediction error, demystifying the effectiveness of the\nregret matching family *vis-a-vis* more standard optimization techniques."}
{"id": "2510.03415", "categories": ["cs.PL", "cs.AI", "cs.CL", "cs.SE"], "pdf": "https://arxiv.org/pdf/2510.03415", "abs": "https://arxiv.org/abs/2510.03415", "authors": ["Aditya Thimmaiah", "Jiyang Zhang", "Jayanth Srinivasa", "Junyi Jessy Li", "Milos Gligoric"], "title": "PLSEMANTICSBENCH: Large Language Models As Programming Language Interpreters", "comment": null, "summary": "As large language models (LLMs) excel at code reasoning, a natural question\narises: can an LLM execute programs (i.e., act as an interpreter) purely based\non a programming language's formal semantics? If so, it will enable rapid\nprototyping of new programming languages and language features. We study this\nquestion using the imperative language IMP (a subset of C), formalized via\nsmall-step operational semantics (SOS) and rewriting-based operational\nsemantics (K-semantics). We introduce three evaluation sets-Human-Written,\nLLM-Translated, and Fuzzer- Generated-whose difficulty is controlled by\ncode-complexity metrics spanning the size, control-flow, and data-flow axes.\nGiven a program and its semantics formalized with SOS/K-semantics, models are\nevaluated on three tasks ranging from coarse to fine: (1) final-state\nprediction, (2) semantic rule prediction, and (3) execution trace prediction.\nTo distinguish pretraining memorization from semantic competence, we define two\nnonstandard semantics obtained through systematic mutations of the standard\nrules. Across strong code/reasoning LLMs, performance drops under nonstandard\nsemantics despite high performance under the standard one. We further find that\n(i) there are patterns to different model failures, (ii) most reasoning models\nperform exceptionally well on coarse grained tasks involving reasoning about\nhighly complex programs often containing nested loop depths beyond five, and\nsurprisingly, (iii) providing formal semantics helps on simple programs but\noften hurts on more complex ones. Overall, the results show a promise that LLMs\ncould serve as programming language interpreters, but points to the lack of\ntheir robust semantics understanding. We release the benchmark and the\nsupporting code at https://github.com/EngineeringSoftware/PLSemanticsBench."}
{"id": "2510.03308", "categories": ["cs.GR", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2510.03308", "abs": "https://arxiv.org/abs/2510.03308", "authors": ["Jiong Lin", "Jialong Ning", "Judah Goldfeder", "Hod Lipson"], "title": "Creative synthesis of kinematic mechanisms", "comment": "6pages, 6 figures", "summary": "In this paper, we formulate the problem of kinematic synthesis for planar\nlinkages as a cross-domain image generation task. We develop a planar linkages\ndataset using RGB image representations, covering a range of mechanisms: from\nsimple types such as crank-rocker and crank-slider to more complex eight-bar\nlinkages like Jansen's mechanism. A shared-latent variational autoencoder (VAE)\nis employed to explore the potential of image generative models for\nsynthesizing unseen motion curves and simulating novel kinematics. By encoding\nthe drawing speed of trajectory points as color gradients, the same\narchitecture also supports kinematic synthesis conditioned on both trajectory\nshape and velocity profiles. We validate our method on three datasets of\nincreasing complexity: a standard four-bar linkage set, a mixed set of four-bar\nand crank-slider mechanisms, and a complex set including multi-loop mechanisms.\nPreliminary results demonstrate the effectiveness of image-based\nrepresentations for generative mechanical design, showing that mechanisms with\nrevolute and prismatic joints, and potentially cams and gears, can be\nrepresented and synthesized within a unified image generation framework."}
{"id": "2510.04425", "categories": ["cs.GT"], "pdf": "https://arxiv.org/pdf/2510.04425", "abs": "https://arxiv.org/abs/2510.04425", "authors": ["Bo Li", "Ankang Sun", "Zunyu Wang", "Yu Zhou"], "title": "Bin Packing and Covering: Pushing the Frontier on the Maximin Share Fairness", "comment": "Appears in the 21st Conference on Web and Internet Economics (WINE),\n  2025", "summary": "We study a fundamental fair allocation problem, where the agent's value is\ndetermined by the number of bins either used to pack or cover the items\nallocated to them. Fairness is evaluated using the maximin share (MMS)\ncriterion. This problem is not only motivated by practical applications, but\nalso serves as a natural framework for studying group fairness. As MMS is not\nalways satisfiable, we consider two types of approximations: cardinal and\nordinal. For cardinal approximation, we relax the requirements of being packed\nor covered for a bin, and for ordinal approximation, we relax the number of\nbins that are packed or covered. For all models of interest, we provide\nconstant approximation algorithms."}
{"id": "2510.04049", "categories": ["cs.PL", "03B70, 68T27, 68T30"], "pdf": "https://arxiv.org/pdf/2510.04049", "abs": "https://arxiv.org/abs/2510.04049", "authors": ["Xiangyu Guo", "Ajay Bansal"], "title": "Encoding Numeric Computations and Infusing Heuristic Knowledge Using Integrity Constraints in stableKanren", "comment": "12 pages, 2 figures, ICFP '25 The miniKanren and Relational\n  Programming Workshop", "summary": "This paper presents examples of using integrity constraints in stableKanren\nto encode numeric computations for problem solving. Then, we use one of the\nexamples to introduce multiple ways to infuse heuristic knowledge and reduce\nsolving time. stableKanren is an extension of miniKanren that supports normal\nlogic programs under stable model semantics. stableKanren further supports\nnumeric computation by constructing a constraint store for integrity\nconstraints. There are three ways to extend a relational programming language\nwith numeric computations: relational number representation, grounding numbers\nto symbols, and constraint store construction. We demonstrate that the numeric\ncomputations in stableKanren have a straightforward numerical representation\ncompared to relational number representations. More importantly, stableKanren\nbalances symbolic and numeric computation in relational programming by avoiding\nthe grounding of all numbers to symbols. Lastly, it also has simpler syntax\ncompared to other constraint store construction approaches. stableKanren\nsupports combinatorial search problem solving under a declarative generate and\ntest paradigm. Such a paradigm generates all possible combinations of solutions\nto the problem, then applies a set of constraints to prune out the unwanted\nsolutions. We demonstrate that different approaches to writing programs or\nqueries affect the solver's performance in the SEND+MORE=MONEY puzzle. The\nperformance gradually improves as more heuristic knowledge is infused through\nthe programs or queries. Additionally, we show how to use an external function\nto achieve a hybrid solution."}
{"id": "2510.03312", "categories": ["cs.GR", "cs.CV", "eess.IV"], "pdf": "https://arxiv.org/pdf/2510.03312", "abs": "https://arxiv.org/abs/2510.03312", "authors": ["Rong Liu", "Zhongpai Gao", "Benjamin Planche", "Meida Chen", "Van Nguyen Nguyen", "Meng Zheng", "Anwesa Choudhuri", "Terrence Chen", "Yue Wang", "Andrew Feng", "Ziyan Wu"], "title": "Universal Beta Splatting", "comment": null, "summary": "We introduce Universal Beta Splatting (UBS), a unified framework that\ngeneralizes 3D Gaussian Splatting to N-dimensional anisotropic Beta kernels for\nexplicit radiance field rendering. Unlike fixed Gaussian primitives, Beta\nkernels enable controllable dependency modeling across spatial, angular, and\ntemporal dimensions within a single representation. Our unified approach\ncaptures complex light transport effects, handles anisotropic view-dependent\nappearance, and models scene dynamics without requiring auxiliary networks or\nspecific color encodings. UBS maintains backward compatibility by approximating\nto Gaussian Splatting as a special case, guaranteeing plug-in usability and\nlower performance bounds. The learned Beta parameters naturally decompose scene\nproperties into interpretable without explicit supervision: spatial (surface\nvs. texture), angular (diffuse vs. specular), and temporal (static vs.\ndynamic). Our CUDA-accelerated implementation achieves real-time rendering\nwhile consistently outperforming existing methods across static,\nview-dependent, and dynamic benchmarks, establishing Beta kernels as a scalable\nuniversal primitive for radiance field rendering. Our project website is\navailable at https://rongliu-leo.github.io/universal-beta-splatting/."}
{"id": "2510.04624", "categories": ["cs.GT", "cs.AI", "cs.LG", "cs.MA", "econ.TH"], "pdf": "https://arxiv.org/pdf/2510.04624", "abs": "https://arxiv.org/abs/2510.04624", "authors": ["Eugene Lim", "Tzeh Yuan Neoh", "Nicholas Teh"], "title": "Fairness in Repeated Matching: A Maximin Perspective", "comment": null, "summary": "We study a sequential decision-making model where a set of items is\nrepeatedly matched to the same set of agents over multiple rounds. The\nobjective is to determine a sequence of matchings that either maximizes the\nutility of the least advantaged agent at the end of all rounds (optimal) or at\nthe end of every individual round (anytime optimal). We investigate the\ncomputational challenges associated with finding (anytime) optimal outcomes and\ndemonstrate that these problems are generally computationally intractable.\nHowever, we provide approximation algorithms, fixed-parameter tractable\nalgorithms, and identify several special cases whereby the problem(s) can be\nsolved efficiently. Along the way, we also establish characterizations of\nPareto-optimal/maximum matchings, which may be of independent interest to works\nin matching theory and house allocation."}
{"id": "2510.04890", "categories": ["cs.PL", "cs.AR", "cs.SE"], "pdf": "https://arxiv.org/pdf/2510.04890", "abs": "https://arxiv.org/abs/2510.04890", "authors": ["Shihan Fang", "Wenxin Zheng"], "title": "Retrofitting Control Flow Graphs in LLVM IR for Auto Vectorization", "comment": null, "summary": "Modern processors increasingly rely on SIMD instruction sets, such as AVX and\nRVV, to significantly enhance parallelism and computational performance.\nHowever, production-ready compilers like LLVM and GCC often fail to fully\nexploit available vectorization opportunities due to disjoint vectorization\npasses and limited extensibility. Although recent attempts in heuristics and\nintermediate representation (IR) designs have attempted to address these\nproblems, efficiently simplifying control flow analysis and accurately\nidentifying vectorization opportunities remain challenging tasks.\n  To address these issues, we introduce a novel vectorization pipeline\nfeaturing two specialized IR extensions: SIR, which encodes high-level\nstructural information, and VIR, which explicitly represents instruction\ndependencies through data dependency analysis. Leveraging the detailed\ndependency information provided by VIR, we develop a flexible and extensible\nvectorization framework. This approach substantially improves interoperability\nacross vectorization passes and expands the search space for identifying\nisomorphic instructions, ultimately enhancing both the scope and efficiency of\nautomatic vectorization. Experimental evaluations demonstrate that our proposed\nvectorization pipeline achieves significant performance improvements,\ndelivering speedups of up to 53% and 58% compared to LLVM and GCC,\nrespectively."}
{"id": "2510.03433", "categories": ["cs.GR"], "pdf": "https://arxiv.org/pdf/2510.03433", "abs": "https://arxiv.org/abs/2510.03433", "authors": ["Áron Samuel Kovács", "Pedro Hermosilla", "Renata G. Raidou"], "title": "Style Brush: Guided Style Transfer for 3D Objects", "comment": null, "summary": "We introduce Style Brush, a novel style transfer method for textured meshes\ndesigned to empower artists with fine-grained control over the stylization\nprocess. Our approach extends traditional 3D style transfer methods by\nintroducing a novel loss function that captures style directionality, supports\nmultiple style images or portions thereof, and enables smooth transitions\nbetween styles in the synthesized texture. The use of easily generated guiding\ntextures streamlines user interaction, making our approach accessible to a\nbroad audience. Extensive evaluations with various meshes, style images, and\ncontour shapes demonstrate the flexibility of our method and showcase the\nvisual appeal of the generated textures."}
{"id": "2510.04915", "categories": ["cs.GT", "cs.MA", "cs.SY", "eess.SY", "math.OC"], "pdf": "https://arxiv.org/pdf/2510.04915", "abs": "https://arxiv.org/abs/2510.04915", "authors": ["S. Rasoul Etesami"], "title": "A Fixed Point Framework for the Existence of EFX Allocations", "comment": null, "summary": "We consider the problem of the existence of an envy-free allocation up to any\ngood (EFX) for linear valuations and establish new results by connecting this\nproblem to a fixed point framework. Specifically, we first use randomized\nrounding to extend the discrete EFX constraints into a continuous space and\nshow that an EFX allocation exists if and only if the optimal value of the\ncontinuously extended objective function is nonpositive. In particular, we\ndemonstrate that this optimization problem can be formulated as an\nunconstrained difference of convex (DC) program, which can be further\nsimplified to the minimization of a piecewise linear concave function over a\npolytope. Leveraging this connection, we show that the proposed DC program has\na nonpositive optimal objective value if and only if a well-defined continuous\nvector map admits a fixed point. Crucially, we prove that the reformulated\nfixed point problem satisfies all the conditions of Brouwer's fixed point\ntheorem, except that self-containedness is violated by an arbitrarily small\npositive constant. To address this, we propose a slightly perturbed continuous\nmap that always admits a fixed point. This fixed point serves as a proxy for\nthe fixed point (if it exists) of the original map, and hence for an EFX\nallocation through an appropriate transformation. Our results offer a new\napproach to establishing the existence of EFX allocations through fixed point\ntheorems. Moreover, the equivalence with DC programming enables a more\nefficient and systematic method for computing such allocations (if one exists)\nusing tools from nonlinear optimization. Our findings bridge the discrete\nproblem of finding an EFX allocation with two continuous frameworks: solving an\nunconstrained DC program and identifying a fixed point of a continuous vector\nmap."}
{"id": "2510.04994", "categories": ["cs.PL"], "pdf": "https://arxiv.org/pdf/2510.04994", "abs": "https://arxiv.org/abs/2510.04994", "authors": ["Sjoerd Dost"], "title": "concurrentKanren: miniKanren for parallel execution", "comment": "13 pages, 1 figure, for associated repo see\n  https://github.com/deosjr/concurrentKanren", "summary": "Concurrent logic programming predates miniKanren, but concurrent\nimplementations of miniKanren have remained largely unexplored. In this work we\npresent a parallel implementation of miniKanren in Go, demonstrating its\nfeasibility and potential for performance improvements. Our approach leverages\nimplicit parallelism allowing legacy programs to benefit from parallel\nexecution. We discuss implementation strategies and evaluate the impact of\nparallelism, laying groundwork for future language-agnostic models."}
{"id": "2510.03434", "categories": ["cs.GR", "cs.DC", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.03434", "abs": "https://arxiv.org/abs/2510.03434", "authors": ["Zhiying Jiang", "Raihan Seraj", "Marcos Villagra", "Bidhan Roy"], "title": "Paris: A Decentralized Trained Open-Weight Diffusion Model", "comment": null, "summary": "We present Paris, the first publicly released diffusion model pre-trained\nentirely through decentralized computation. Paris demonstrates that\nhigh-quality text-to-image generation can be achieved without centrally\ncoordinated infrastructure. Paris is open for research and commercial use.\nParis required implementing our Distributed Diffusion Training framework from\nscratch. The model consists of 8 expert diffusion models (129M-605M parameters\neach) trained in complete isolation with no gradient, parameter, or\nintermediate activation synchronization. Rather than requiring synchronized\ngradient updates across thousands of GPUs, we partition data into semantically\ncoherent clusters where each expert independently optimizes its subset while\ncollectively approximating the full distribution. A lightweight transformer\nrouter dynamically selects appropriate experts at inference, achieving\ngeneration quality comparable to centrally coordinated baselines. Eliminating\nsynchronization enables training on heterogeneous hardware without specialized\ninterconnects. Empirical validation confirms that Paris's decentralized\ntraining maintains generation quality while removing the dedicated GPU cluster\nrequirement for large-scale diffusion models. Paris achieves this using\n14$\\times$ less training data and 16$\\times$ less compute than the prior\ndecentralized baseline."}
{"id": "2510.03597", "categories": ["cs.GR", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.03597", "abs": "https://arxiv.org/abs/2510.03597", "authors": ["Sina Alemohammad", "Zhangyang Wang", "Richard G. Baraniuk"], "title": "Neon: Negative Extrapolation From Self-Training Improves Image Generation", "comment": null, "summary": "Scaling generative AI models is bottlenecked by the scarcity of high-quality\ntraining data. The ease of synthesizing from a generative model suggests using\n(unverified) synthetic data to augment a limited corpus of real data for the\npurpose of fine-tuning in the hope of improving performance. Unfortunately,\nhowever, the resulting positive feedback loop leads to model autophagy disorder\n(MAD, aka model collapse) that results in a rapid degradation in sample quality\nand/or diversity. In this paper, we introduce Neon (for Negative Extrapolation\nfrOm self-traiNing), a new learning method that turns the degradation from\nself-training into a powerful signal for self-improvement. Given a base model,\nNeon first fine-tunes it on its own self-synthesized data but then,\ncounterintuitively, reverses its gradient updates to extrapolate away from the\ndegraded weights. We prove that Neon works because typical inference samplers\nthat favor high-probability regions create a predictable anti-alignment between\nthe synthetic and real data population gradients, which negative extrapolation\ncorrects to better align the model with the true data distribution. Neon is\nremarkably easy to implement via a simple post-hoc merge that requires no new\nreal data, works effectively with as few as 1k synthetic samples, and typically\nuses less than 1% additional training compute. We demonstrate Neon's\nuniversality across a range of architectures (diffusion, flow matching,\nautoregressive, and inductive moment matching models) and datasets (ImageNet,\nCIFAR-10, and FFHQ). In particular, on ImageNet 256x256, Neon elevates the\nxAR-L model to a new state-of-the-art FID of 1.02 with only 0.36% additional\ntraining compute. Code is available at https://github.com/SinaAlemohammad/Neon"}
{"id": "2510.03813", "categories": ["cs.GR", "cs.AI", "cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.03813", "abs": "https://arxiv.org/abs/2510.03813", "authors": ["Byungjun Kim", "Soobin Um", "Jong Chul Ye"], "title": "Diverse Text-to-Image Generation via Contrastive Noise Optimization", "comment": null, "summary": "Text-to-image (T2I) diffusion models have demonstrated impressive performance\nin generating high-fidelity images, largely enabled by text-guided inference.\nHowever, this advantage often comes with a critical drawback: limited\ndiversity, as outputs tend to collapse into similar modes under strong text\nguidance. Existing approaches typically optimize intermediate latents or text\nconditions during inference, but these methods deliver only modest gains or\nremain sensitive to hyperparameter tuning. In this work, we introduce\nContrastive Noise Optimization, a simple yet effective method that addresses\nthe diversity issue from a distinct perspective. Unlike prior techniques that\nadapt intermediate latents, our approach shapes the initial noise to promote\ndiverse outputs. Specifically, we develop a contrastive loss defined in the\nTweedie data space and optimize a batch of noise latents. Our contrastive\noptimization repels instances within the batch to maximize diversity while\nkeeping them anchored to a reference sample to preserve fidelity. We further\nprovide theoretical insights into the mechanism of this preprocessing to\nsubstantiate its effectiveness. Extensive experiments across multiple T2I\nbackbones demonstrate that our approach achieves a superior quality-diversity\nPareto frontier while remaining robust to hyperparameter choices."}
{"id": "2510.03837", "categories": ["cs.GR", "cs.CV"], "pdf": "https://arxiv.org/pdf/2510.03837", "abs": "https://arxiv.org/abs/2510.03837", "authors": ["Shen Fan", "Przemyslaw Musialski"], "title": "Joint Neural SDF Reconstruction and Semantic Segmentation for CAD Models", "comment": null, "summary": "We propose a simple, data-efficient pipeline that augments an implicit\nreconstruction network based on neural SDF-based CAD parts with a\npart-segmentation head trained under PartField-generated supervision. Unlike\nmethods tied to fixed taxonomies, our model accepts meshes with any number of\nparts and produces coherent, geometry-aligned labels in a single pass. We\nevaluate on randomly sampled CAD meshes from the ABC dataset with intentionally\nvaried part cardinalities, including over-segmented shapes, and report strong\nperformance across reconstruction (CDL1/CDL2, F1-micro, NC) and segmentation\n(mIoU, Accuracy), together with a new Segmentation Consistency metric that\ncaptures local label smoothness. We attach a lightweight segmentation head to\nthe Flat-CAD SDF trunk; on a paired evaluation it does not alter reconstruction\nwhile providing accurate part labels for meshes with any number of parts. Even\nunder degraded reconstructions on thin or intricate geometries, segmentation\nremains accurate and label-coherent, often preserving the correct part count.\nOur approach therefore offers a practical route to semantically structured CAD\nmeshes without requiring curated taxonomies or exact palette matches. We\ndiscuss limitations in boundary precision, partly due to per-face supervision,\nand outline paths toward boundary-aware training and higher resolution labels."}
{"id": "2510.03964", "categories": ["cs.GR"], "pdf": "https://arxiv.org/pdf/2510.03964", "abs": "https://arxiv.org/abs/2510.03964", "authors": ["Ville Cantory", "Darya Biparva", "Haoyu Tan", "Tongyu Nie", "John Schroeder", "Ruofei Du", "Victoria Interrante", "Piotr Didyk"], "title": "Enhancing Foveated Rendering with Weighted Reservoir Sampling", "comment": "To appear in The 18th ACM SIGGRAPH Conference on Motion, Interaction,\n  and Games (MIG '25), December 03-05, 2025, Zurich, Switzerland", "summary": "Spatiotemporal sensitivity to high frequency information declines with\nincreased peripheral eccentricity. Foveated rendering exploits this by\ndecreasing the spatial resolution of rendered images in peripheral vision,\nreducing the rendering cost by omitting high frequency details. As foveation\nlevels increase, the rendering quality is reduced, and traditional foveated\nrendering systems tend not to preserve samples that were previously rendered at\nhigh spatial resolution in previous frames. Additionally, prior research has\nshown that saccade landing positions are distributed around a target location\nrather than landing at a single point, and that even during fixations, eyes\nperform small microsaccades around a fixation point. This creates an\nopportunity for sampling from temporally neighbouring frames with differing\nfoveal locations to reduce the required rendered size of the foveal region\nwhile achieving a higher perceived image quality. We further observe that the\ntemporal presentation of pixels frame-to-frame can be viewed as a data stream,\npresenting a random sampling problem. Following this intuition, we propose a\nWeighted Reservoir Sampling technique to efficiently maintain a reservoir of\nthe perceptually relevant high quality pixel samples from previous frames and\nincorporate them into the computation of the current frame. This allows the\nrenderer to render a smaller region of foveal pixels per frame by temporally\nreusing pixel samples that are still relevant to reconstruct a higher perceived\nimage quality, while allowing for higher levels of foveation. Our method\noperates on the output of foveated rendering, and runs in under 1\\,ms at 4K\nresolution, making it highly efficient and integrable with real-time VR and AR\nfoveated rendering systems."}
{"id": "2510.04536", "categories": ["cs.GR", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2510.04536", "abs": "https://arxiv.org/abs/2510.04536", "authors": ["Shun-ichiro Hayashi", "Daichi Mukunoki", "Tetsuya Hoshino", "Satoshi Ohshima", "Takahiro Katagiri"], "title": "3Dify: a Framework for Procedural 3D-CG Generation Assisted by LLMs Using MCP and RAG", "comment": null, "summary": "This paper proposes \"3Dify,\" a procedural 3D computer graphics (3D-CG)\ngeneration framework utilizing Large Language Models (LLMs). The framework\nenables users to generate 3D-CG content solely through natural language\ninstructions. 3Dify is built upon Dify, an open-source platform for AI\napplication development, and incorporates several state-of-the-art LLM-related\ntechnologies such as the Model Context Protocol (MCP) and Retrieval-Augmented\nGeneration (RAG). For 3D-CG generation support, 3Dify automates the operation\nof various Digital Content Creation (DCC) tools via MCP. When DCC tools do not\nsupport MCP-based interaction, the framework employs the Computer-Using Agent\n(CUA) method to automate Graphical User Interface (GUI) operations. Moreover,\nto enhance image generation quality, 3Dify allows users to provide feedback by\nselecting preferred images from multiple candidates. The LLM then learns\nvariable patterns from these selections and applies them to subsequent\ngenerations. Furthermore, 3Dify supports the integration of locally deployed\nLLMs, enabling users to utilize custom-developed models and to reduce both time\nand monetary costs associated with external API calls by leveraging their own\ncomputational resources."}
{"id": "2510.04539", "categories": ["cs.GR", "cs.CV"], "pdf": "https://arxiv.org/pdf/2510.04539", "abs": "https://arxiv.org/abs/2510.04539", "authors": ["Zeng Tao", "Zheng Ding", "Zeyuan Chen", "Xiang Zhang", "Leizhi Li", "Zhuowen Tu"], "title": "C3Editor: Achieving Controllable Consistency in 2D Model for 3D Editing", "comment": null, "summary": "Existing 2D-lifting-based 3D editing methods often encounter challenges\nrelated to inconsistency, stemming from the lack of view-consistent 2D editing\nmodels and the difficulty of ensuring consistent editing across multiple views.\nTo address these issues, we propose C3Editor, a controllable and consistent\n2D-lifting-based 3D editing framework. Given an original 3D representation and\na text-based editing prompt, our method selectively establishes a\nview-consistent 2D editing model to achieve superior 3D editing results. The\nprocess begins with the controlled selection of a ground truth (GT) view and\nits corresponding edited image as the optimization target, allowing for\nuser-defined manual edits. Next, we fine-tune the 2D editing model within the\nGT view and across multiple views to align with the GT-edited image while\nensuring multi-view consistency. To meet the distinct requirements of GT view\nfitting and multi-view consistency, we introduce separate LoRA modules for\ntargeted fine-tuning. Our approach delivers more consistent and controllable 2D\nand 3D editing results than existing 2D-lifting-based methods, outperforming\nthem in both qualitative and quantitative evaluations."}
{"id": "2510.04637", "categories": ["cs.GR", "cs.CV"], "pdf": "https://arxiv.org/pdf/2510.04637", "abs": "https://arxiv.org/abs/2510.04637", "authors": ["Zeyi Zhang", "Yanju Zhou", "Heyuan Yao", "Tenglong Ao", "Xiaohang Zhan", "Libin Liu"], "title": "Social Agent: Mastering Dyadic Nonverbal Behavior Generation via Conversational LLM Agents", "comment": "SIGGRAPH ASIA 2025 (Conference Track); Project page:\n  https://pku-mocca.github.io/Social-Agent-Page/", "summary": "We present Social Agent, a novel framework for synthesizing realistic and\ncontextually appropriate co-speech nonverbal behaviors in dyadic conversations.\nIn this framework, we develop an agentic system driven by a Large Language\nModel (LLM) to direct the conversation flow and determine appropriate\ninteractive behaviors for both participants. Additionally, we propose a novel\ndual-person gesture generation model based on an auto-regressive diffusion\nmodel, which synthesizes coordinated motions from speech signals. The output of\nthe agentic system is translated into high-level guidance for the gesture\ngenerator, resulting in realistic movement at both the behavioral and motion\nlevels. Furthermore, the agentic system periodically examines the movements of\ninterlocutors and infers their intentions, forming a continuous feedback loop\nthat enables dynamic and responsive interactions between the two participants.\nUser studies and quantitative evaluations show that our model significantly\nimproves the quality of dyadic interactions, producing natural, synchronized\nnonverbal behaviors."}
{"id": "2510.04999", "categories": ["cs.GR", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2510.04999", "abs": "https://arxiv.org/abs/2510.04999", "authors": ["Nilay Kumar", "Priyansh Bhandari", "G. Maragatham"], "title": "Bridging Text and Video Generation: A Survey", "comment": null, "summary": "Text-to-video (T2V) generation technology holds potential to transform\nmultiple domains such as education, marketing, entertainment, and assistive\ntechnologies for individuals with visual or reading comprehension challenges,\nby creating coherent visual content from natural language prompts. From its\ninception, the field has advanced from adversarial models to diffusion-based\nmodels, yielding higher-fidelity, temporally consistent outputs. Yet challenges\npersist, such as alignment, long-range coherence, and computational efficiency.\nAddressing this evolving landscape, we present a comprehensive survey of\ntext-to-video generative models, tracing their development from early GANs and\nVAEs to hybrid Diffusion-Transformer (DiT) architectures, detailing how these\nmodels work, what limitations they addressed in their predecessors, and why\nshifts toward new architectural paradigms were necessary to overcome challenges\nin quality, coherence, and control. We provide a systematic account of the\ndatasets, which the surveyed text-to-video models were trained and evaluated\non, and, to support reproducibility and assess the accessibility of training\nsuch models, we detail their training configurations, including their hardware\nspecifications, GPU counts, batch sizes, learning rates, optimizers, epochs,\nand other key hyperparameters. Further, we outline the evaluation metrics\ncommonly used for evaluating such models and present their performance across\nstandard benchmarks, while also discussing the limitations of these metrics and\nthe emerging shift toward more holistic, perception-aligned evaluation\nstrategies. Finally, drawing from our analysis, we outline the current open\nchallenges and propose a few promising future directions, laying out a\nperspective for future researchers to explore and build upon in advancing T2V\nresearch and applications."}
{"id": "2510.05081", "categories": ["cs.GR", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2510.05081", "abs": "https://arxiv.org/abs/2510.05081", "authors": ["Ronen Kamenetsky", "Sara Dorfman", "Daniel Garibi", "Roni Paiss", "Or Patashnik", "Daniel Cohen-Or"], "title": "SAEdit: Token-level control for continuous image editing via Sparse AutoEncoder", "comment": "Project page at: https://ronen94.github.io/SAEdit/", "summary": "Large-scale text-to-image diffusion models have become the backbone of modern\nimage editing, yet text prompts alone do not offer adequate control over the\nediting process. Two properties are especially desirable: disentanglement,\nwhere changing one attribute does not unintentionally alter others, and\ncontinuous control, where the strength of an edit can be smoothly adjusted. We\nintroduce a method for disentangled and continuous editing through token-level\nmanipulation of text embeddings. The edits are applied by manipulating the\nembeddings along carefully chosen directions, which control the strength of the\ntarget attribute. To identify such directions, we employ a Sparse Autoencoder\n(SAE), whose sparse latent space exposes semantically isolated dimensions. Our\nmethod operates directly on text embeddings without modifying the diffusion\nprocess, making it model agnostic and broadly applicable to various image\nsynthesis backbones. Experiments show that it enables intuitive and efficient\nmanipulations with continuous control across diverse attributes and domains."}
{"id": "2510.05097", "categories": ["cs.GR", "cs.CV"], "pdf": "https://arxiv.org/pdf/2510.05097", "abs": "https://arxiv.org/abs/2510.05097", "authors": ["Robin Courant", "Xi Wang", "David Loiseaux", "Marc Christie", "Vicky Kalogeiton"], "title": "Pulp Motion: Framing-aware multimodal camera and human motion generation", "comment": "Project page:\n  https://www.lix.polytechnique.fr/vista/projects/2025_pulpmotion_courant/", "summary": "Treating human motion and camera trajectory generation separately overlooks a\ncore principle of cinematography: the tight interplay between actor performance\nand camera work in the screen space. In this paper, we are the first to cast\nthis task as a text-conditioned joint generation, aiming to maintain consistent\non-screen framing while producing two heterogeneous, yet intrinsically linked,\nmodalities: human motion and camera trajectories. We propose a simple,\nmodel-agnostic framework that enforces multimodal coherence via an auxiliary\nmodality: the on-screen framing induced by projecting human joints onto the\ncamera. This on-screen framing provides a natural and effective bridge between\nmodalities, promoting consistency and leading to more precise joint\ndistribution. We first design a joint autoencoder that learns a shared latent\nspace, together with a lightweight linear transform from the human and camera\nlatents to a framing latent. We then introduce auxiliary sampling, which\nexploits this linear transform to steer generation toward a coherent framing\nmodality. To support this task, we also introduce the PulpMotion dataset, a\nhuman-motion and camera-trajectory dataset with rich captions, and high-quality\nhuman motions. Extensive experiments across DiT- and MAR-based architectures\nshow the generality and effectiveness of our method in generating on-frame\ncoherent human-camera motions, while also achieving gains on textual alignment\nfor both modalities. Our qualitative results yield more cinematographically\nmeaningful framings setting the new state of the art for this task. Code,\nmodels and data are available in our\n\\href{https://www.lix.polytechnique.fr/vista/projects/2025_pulpmotion_courant/}{project\npage}."}
