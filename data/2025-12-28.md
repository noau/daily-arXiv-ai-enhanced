<div id=toc></div>

# Table of Contents

- [cs.GR](#cs.GR) [Total: 3]
- [cs.GT](#cs.GT) [Total: 3]


<div id='cs.GR'></div>

# cs.GR [[Back]](#toc)

### [1] [Efficient Computation of Integer-constrained Cones for Conformal Parameterizations](https://arxiv.org/abs/2512.20904)
*Wei Du,Qing Fang,Ligang Liu,Xiao-Ming Fu*

Main category: cs.GR

TL;DR: 提出一种高效方法，计算整数约束的锥奇异点集，生成旋转无缝且低变形的共形参数化。


<details>
  <summary>Details</summary>
Motivation: 传统方法在处理高亏格表面的共形参数化时效率较低，本文旨在通过优化离散变量（顶点约束位置、整数约束角度和锥数）实现高效且低变形的参数化。

Method: 交替优化三种离散变量（顶点位置、角度约束和锥数），采用显式构造算法减小优化规模，并引入新导数公式移动锥以降低变形。

Result: 在大量测试数据上生成旋转无缝且低变形的参数化，平均速度比现有方法快30倍，同时保持相似的锥数和变形水平。

Conclusion: 本文方法在效率上显著优于现有技术，适用于高亏格表面的共形参数化任务。

Abstract: We propose an efficient method to compute a small set of integer-constrained cone singularities, which induce a rotationally seamless conformal parameterization with low distortion. Since the problem only involves discrete variables, i.e., vertex-constrained positions, integer-constrained angles, and the number of cones, we alternately optimize these three types of variables to achieve tractable convergence. Central to high efficiency is an explicit construction algorithm that reduces the optimization problem scale to be slightly greater than the number of integer variables for determining the optimal angles with fixed positions and numbers, even for high-genus surfaces. In addition, we derive a new derivative formula that allows us to move the cones, effectively reducing distortion until convergence. Combined with other strategies, including repositioning and adding cones to decrease distortion, adaptively selecting a constrained number of integer variables for efficient optimization, and pairing cones to reduce the number, we quickly achieve a favorable tradeoff between the number of cones and the parameterization distortion. We demonstrate the effectiveness and practicability of our cones by using them to generate rotationally seamless and low-distortion parameterizations on a massive test data set. Our method demonstrates an order-of-magnitude speedup (30$\times$ faster on average) compared to state-of-the-art approaches while maintaining comparable cone numbers and parameterization distortion.

</details>


### [2] [AirGS: Real-Time 4D Gaussian Streaming for Free-Viewpoint Video Experiences](https://arxiv.org/abs/2512.20943)
*Zhe Wang,Jinghang Li,Yifei Zhu*

Main category: cs.GR

TL;DR: AirGS是一个优化的4D高斯泼溅框架，通过重新设计训练和交付流程，提供高质量、低延迟的自由视点视频体验，显著提升了长期序列的质量和带宽效率。


<details>
  <summary>Details</summary>
Motivation: 现有的4D高斯泼溅技术在长序列中质量下降，且带宽和存储开销大，限制了其在实时和大规模部署中的应用。

Method: AirGS将高斯视频流转换为多通道2D格式，智能识别关键帧，结合时间相干性和膨胀损失以减少训练时间和表示大小，并通过轻量级剪枝算法优化传输。

Result: 实验表明，AirGS在场景变化时PSNR质量偏差降低20%以上，帧级PSNR稳定高于30，训练速度提升6倍，每帧传输大小减少近50%。

Conclusion: AirGS通过高效的设计和优化，显著提升了4DGS的性能和适用性，为实时和大规模自由视点视频应用提供了可行解决方案。

Abstract: Free-viewpoint video (FVV) enables immersive viewing experiences by allowing users to view scenes from arbitrary perspectives. As a prominent reconstruction technique for FVV generation, 4D Gaussian Splatting (4DGS) models dynamic scenes with time-varying 3D Gaussian ellipsoids and achieves high-quality rendering via fast rasterization. However, existing 4DGS approaches suffer from quality degradation over long sequences and impose substantial bandwidth and storage overhead, limiting their applicability in real-time and wide-scale deployments. Therefore, we present AirGS, a streaming-optimized 4DGS framework that rearchitects the training and delivery pipeline to enable high-quality, low-latency FVV experiences. AirGS converts Gaussian video streams into multi-channel 2D formats and intelligently identifies keyframes to enhance frame reconstruction quality. It further combines temporal coherence with inflation loss to reduce training time and representation size. To support communication-efficient transmission, AirGS models 4DGS delivery as an integer linear programming problem and design a lightweight pruning level selection algorithm to adaptively prune the Gaussian updates to be transmitted, balancing reconstruction quality and bandwidth consumption. Extensive experiments demonstrate that AirGS reduces quality deviation in PSNR by more than 20% when scene changes, maintains frame-level PSNR consistently above 30, accelerates training by 6 times, reduces per-frame transmission size by nearly 50% compared to the SOTA 4DGS approaches.

</details>


### [3] [TexAvatars : Hybrid Texel-3D Representations for Stable Rigging of Photorealistic Gaussian Head Avatars](https://arxiv.org/abs/2512.21099)
*Jaeseong Lee,Junyeong Ahn,Taewoong Kang,Jaegul Choo*

Main category: cs.GR

TL;DR: TexAvatars是一种混合头像表示方法，结合了分析绑定和纹理空间的优点，显著提升了头像在极端姿态和表情下的表现力和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 构建逼真且可驱动的3D头部虚拟形象在AR/XR中至关重要，但现有方法在极端表情和姿态下泛化能力不足，限制了实际应用。

Method: TexAvatars结合了分析绑定的几何基础和纹理空间的空间连续性，通过CNN在UV空间中预测局部几何属性，并通过网格感知的Jacobians驱动3D变形。

Result: 该方法在极端姿态和表情下表现出色，能够捕捉细微的表情效果，如肌肉引起的皱纹和口腔几何细节，实现了先进的性能。

Conclusion: TexAvatars通过分离语义建模和几何控制，提升了虚拟形象的泛化能力、可解释性和稳定性，适用于复杂的头部重演场景。

Abstract: Constructing drivable and photorealistic 3D head avatars has become a central task in AR/XR, enabling immersive and expressive user experiences. With the emergence of high-fidelity and efficient representations such as 3D Gaussians, recent works have pushed toward ultra-detailed head avatars. Existing approaches typically fall into two categories: rule-based analytic rigging or neural network-based deformation fields. While effective in constrained settings, both approaches often fail to generalize to unseen expressions and poses, particularly in extreme reenactment scenarios. Other methods constrain Gaussians to the global texel space of 3DMMs to reduce rendering complexity. However, these texel-based avatars tend to underutilize the underlying mesh structure. They apply minimal analytic deformation and rely heavily on neural regressors and heuristic regularization in UV space, which weakens geometric consistency and limits extrapolation to complex, out-of-distribution deformations. To address these limitations, we introduce TexAvatars, a hybrid avatar representation that combines the explicit geometric grounding of analytic rigging with the spatial continuity of texel space. Our approach predicts local geometric attributes in UV space via CNNs, but drives 3D deformation through mesh-aware Jacobians, enabling smooth and semantically meaningful transitions across triangle boundaries. This hybrid design separates semantic modeling from geometric control, resulting in improved generalization, interpretability, and stability. Furthermore, TexAvatars captures fine-grained expression effects, including muscle-induced wrinkles, glabellar lines, and realistic mouth cavity geometry, with high fidelity. Our method achieves state-of-the-art performance under extreme pose and expression variations, demonstrating strong generalization in challenging head reenactment settings.

</details>


<div id='cs.GT'></div>

# cs.GT [[Back]](#toc)

### [4] [Mechanism-Based Intelligence (MBI): Differentiable Incentives for Rational Coordination and Guaranteed Alignment in Multi-Agent Systems](https://arxiv.org/abs/2512.20688)
*Stefano Grassi*

Main category: cs.GT

TL;DR: 论文提出了一种基于机制的智能范式（MBI），通过动态可微价格机制（DPM）解决多智能体系统的协调问题，确保激励兼容性并高效收敛到全局最优解。


<details>
  <summary>Details</summary>
Motivation: 多智能体系统在处理分散信息和激励对齐问题时表现脆弱，缺乏高效协调的通用方法。

Method: 采用基于机制的智能范式（MBI）和动态可微价格机制（DPM），计算损失梯度作为激励信号，保证主导策略激励兼容性（DSIC）和全局最优收敛。

Result: 该方法在规模上线性增长（O(N)），比无模型强化学习快50倍，并提供可证明的高效、可审计和通用多智能体协调方案。

Conclusion: MBI框架通过经济原则确保了多智能体系统的协调性、可信性和可扩展性，为解决复杂协调问题提供了新途径。

Abstract: Autonomous multi-agent systems are fundamentally fragile: they struggle to solve the Hayekian Information problem (eliciting dispersed private knowledge) and the Hurwiczian Incentive problem (aligning local actions with global objectives), making coordination computationally intractable. I introduce Mechanism-Based Intelligence (MBI), a paradigm that reconceptualizes intelligence as emergent from the coordination of multiple "brains", rather than a single one. At its core, the Differentiable Price Mechanism (DPM) computes the exact loss gradient $$ \mathbf{G}_i = - \frac{\partial \mathcal{L}}{\partial \mathbf{x}_i} $$ as a dynamic, VCG-equivalent incentive signal, guaranteeing Dominant Strategy Incentive Compatibility (DSIC) and convergence to the global optimum. A Bayesian extension ensures incentive compatibility under asymmetric information (BIC). The framework scales linearly ($\mathcal{O}(N)$) with the number of agents, bypassing the combinatorial complexity of Dec-POMDPs and is empirically 50x faster than Model-Free Reinforcement Learning. By structurally aligning agent self-interest with collective objectives, it provides a provably efficient, auditable and generalizable approach to coordinated, trustworthy and scalable multi-agent intelligence grounded in economic principles.

</details>


### [5] [(Im)possibility of Incentive Design for Challenge-based Blockchain Protocols](https://arxiv.org/abs/2512.20864)
*Suhyeon Lee,Dieu-Huyen Nguyen,Donghwan Lee*

Main category: cs.GT

TL;DR: 区块链提供了去中心化和安全的执行环境，但链上计算成本高。挑战协议如Truebit和乐观汇总试图降低成本，但缺乏对诚实挑战者的激励和欺诈者的惩罚研究。


<details>
  <summary>Details</summary>
Motivation: 研究区块链中挑战协议的激励和欺诈惩罚机制，以解决诚实挑战者激励不足和不诚实提议者的惩罚问题。

Method: 构建了一个包含勾结少数、异构成本和三种排序模式的模型，分析单赢家和多赢家设计条件下是否同时满足诚实非损失和欺诈威慑的目标。

Result: 单赢家设计中激励设计不可行或规模有限，而多赢家设计中可简单明确地满足诚实非损失和欺诈威慑的条件。

Conclusion: 多赢家设计在激励诚实行为和威慑欺诈方面优于单赢家设计，为区块链挑战协议的优化提供了理论支持。

Abstract: Blockchains offer a decentralized and secure execution environment strong enough to host cryptocurrencies, but the state-replication model makes on-chain computation expensive. To avoid heavy on-chain workloads, systems like Truebit and optimistic rollups use challenge-based protocols, performing computations off-chain and invoking the chain only when challenged. This keeps normal-case costs low and, if at least one honest challenger exists, can catch fraud. What has been less clear is whether honest challengers are actually incentivized and a dishonest proposer is properly damaged under the worst case environment. We build a model with a colluding minority, heterogeneous costs, and three ordering modes. We then ask whether two goals can be met together: honest non-loss and fraud deterrence. Our results are clear: in single-winner designs, the incentive design is impossible or limited in scale. By contrast, in multi-winner designs, we obtain simple, explicit conditions under which both goals hold.

</details>


### [6] [Policy-Conditioned Policies for Multi-Agent Task Solving](https://arxiv.org/abs/2512.21024)
*Yue Lin,Shuhui Zhu,Wenhao Li,Ang Li,Dan Qiao,Pascal Poupart,Hongyuan Zha,Baoxiang Wang*

Main category: cs.GT

TL;DR: 本文提出了一种将策略表示为可解释的源代码，并利用大型语言模型（LLMs）作为近似解释器的方法，以解决多智能体任务中的动态策略适应问题。


<details>
  <summary>Details</summary>
Motivation: 在多智能体任务中，动态策略适应的核心挑战在于难以直接基于对手的策略进行调整。传统的深度强化学习方法存在“表示瓶颈”：神经策略是不透明的高维参数向量，其他智能体难以理解。

Method: 通过将策略表示为人类可读的源代码，并利用LLMs作为近似解释器，本文提出了一种程序化表示方法。进一步设计了	extit{Programmatic Iterated Best Response (PIBR)}算法，利用文本梯度和结构化反馈优化策略代码。

Result: 该方法在多个标准协调矩阵游戏和合作性Level-Based Foraging环境中验证了有效性。

Conclusion: 本文通过程序化表示和LLMs的有效结合，为解决多智能体任务中的动态策略适应问题提供了一种新范式。

Abstract: In multi-agent tasks, the central challenge lies in the dynamic adaptation of strategies. However, directly conditioning on opponents' strategies is intractable in the prevalent deep reinforcement learning paradigm due to a fundamental ``representational bottleneck'': neural policies are opaque, high-dimensional parameter vectors that are incomprehensible to other agents. In this work, we propose a paradigm shift that bridges this gap by representing policies as human-interpretable source code and utilizing Large Language Models (LLMs) as approximate interpreters. This programmatic representation allows us to operationalize the game-theoretic concept of \textit{Program Equilibrium}. We reformulate the learning problem by utilizing LLMs to perform optimization directly in the space of programmatic policies. The LLM functions as a point-wise best-response operator that iteratively synthesizes and refines the ego agent's policy code to respond to the opponent's strategy. We formalize this process as \textit{Programmatic Iterated Best Response (PIBR)}, an algorithm where the policy code is optimized by textual gradients, using structured feedback derived from game utility and runtime unit tests. We demonstrate that this approach effectively solves several standard coordination matrix games and a cooperative Level-Based Foraging environment.

</details>
