<div id=toc></div>

# Table of Contents

- [cs.GR](#cs.GR) [Total: 3]
- [cs.GT](#cs.GT) [Total: 3]


<div id='cs.GR'></div>

# cs.GR [[Back]](#toc)

### [1] [Efficient Computation of Integer-constrained Cones for Conformal Parameterizations](https://arxiv.org/abs/2512.20904)
*Wei Du,Qing Fang,Ligang Liu,Xiao-Ming Fu*

Main category: cs.GR

TL;DR: 提出一种高效计算整数约束锥奇异点的方法，实现低失真且旋转无缝的共形参数化建模。


<details>
  <summary>Details</summary>
Motivation: 为了解决高维曲面参数化中的计算效率和低失真问题，特别是在处理离散变量（如顶点约束位置、整数约束角度和锥数）时的优化挑战。

Method: 通过交替优化三种离散变量（顶点位置、角度和锥数），并结合显式构造算法和新导数公式，减少优化问题规模并有效移动锥点以降低失真。

Result: 实验证明，该方法在保持锥数和参数化失真可比较的前提下，计算速度比现有方法快一个数量级（平均30倍）。

Conclusion: 该方法在高效性和实用性方面表现出色，适用于大规模数据集的旋转无缝和低失真参数化。

Abstract: We propose an efficient method to compute a small set of integer-constrained cone singularities, which induce a rotationally seamless conformal parameterization with low distortion. Since the problem only involves discrete variables, i.e., vertex-constrained positions, integer-constrained angles, and the number of cones, we alternately optimize these three types of variables to achieve tractable convergence. Central to high efficiency is an explicit construction algorithm that reduces the optimization problem scale to be slightly greater than the number of integer variables for determining the optimal angles with fixed positions and numbers, even for high-genus surfaces. In addition, we derive a new derivative formula that allows us to move the cones, effectively reducing distortion until convergence. Combined with other strategies, including repositioning and adding cones to decrease distortion, adaptively selecting a constrained number of integer variables for efficient optimization, and pairing cones to reduce the number, we quickly achieve a favorable tradeoff between the number of cones and the parameterization distortion. We demonstrate the effectiveness and practicability of our cones by using them to generate rotationally seamless and low-distortion parameterizations on a massive test data set. Our method demonstrates an order-of-magnitude speedup (30$\times$ faster on average) compared to state-of-the-art approaches while maintaining comparable cone numbers and parameterization distortion.

</details>


### [2] [AirGS: Real-Time 4D Gaussian Streaming for Free-Viewpoint Video Experiences](https://arxiv.org/abs/2512.20943)
*Zhe Wang,Jinghang Li,Yifei Zhu*

Main category: cs.GR

TL;DR: AirGS是一个优化的4D高斯喷涂框架，通过重新设计训练和交付流程，提升了自由视点视频的质量和实时性，同时显著降低了带宽和存储需求。


<details>
  <summary>Details</summary>
Motivation: 现有的4D高斯喷涂（4DGS）方法在长序列中质量下降明显，且带宽和存储开销大，限制了其在实时和大规模部署中的应用。

Method: AirGS将高斯视频流转换为多通道2D格式，智能识别关键帧以提升重建质量，并利用时间一致性和膨胀损失减少训练时间和表示大小。此外，AirGS将4DGS交付建模为带宽优化问题，并提出一种分层表示方法，大幅减少了冗余。

Result: 实验结果表明，AirGS显著减少了70%的存储和带宽占用，同时在长时间序列中保持了高质量的渲染效果。

Conclusion: AirGS通过创新的分层表示和关键帧优化，解决了4DGS在长期质量和效率上的瓶颈，为实时自由视点视频的应用提供了可行方案。

Abstract: Free-viewpoint video (FVV) enables immersive viewing experiences by allowing users to view scenes from arbitrary perspectives. As a prominent reconstruction technique for FVV generation, 4D Gaussian Splatting (4DGS) models dynamic scenes with time-varying 3D Gaussian ellipsoids and achieves high-quality rendering via fast rasterization. However, existing 4DGS approaches suffer from quality degradation over long sequences and impose substantial bandwidth and storage overhead, limiting their applicability in real-time and wide-scale deployments. Therefore, we present AirGS, a streaming-optimized 4DGS framework that rearchitects the training and delivery pipeline to enable high-quality, low-latency FVV experiences. AirGS converts Gaussian video streams into multi-channel 2D formats and intelligently identifies keyframes to enhance frame reconstruction quality. It further combines temporal coherence with inflation loss to reduce training time and representation size. To support communication-efficient transmission, AirGS models 4DGS delivery as an integer linear programming problem and design a lightweight pruning level selection algorithm to adaptively prune the Gaussian updates to be transmitted, balancing reconstruction quality and bandwidth consumption. Extensive experiments demonstrate that AirGS reduces quality deviation in PSNR by more than 20% when scene changes, maintains frame-level PSNR consistently above 30, accelerates training by 6 times, reduces per-frame transmission size by nearly 50% compared to the SOTA 4DGS approaches.

</details>


### [3] [TexAvatars : Hybrid Texel-3D Representations for Stable Rigging of Photorealistic Gaussian Head Avatars](https://arxiv.org/abs/2512.21099)
*Jaeseong Lee,Junyeong Ahn,Taewoong Kang,Jaegul Choo*

Main category: cs.GR

TL;DR: TexAvatars是一种混合3D头部虚拟形象表示方法，结合了解析绑定的几何基础与纹理空间的空间连续性，显著提升了重建的泛化性和逼真度。


<details>
  <summary>Details</summary>
Motivation: 现有的3D头部虚拟形象方法在极端姿态和表情下泛化能力不足，且几何一致性较弱。TexAvatars旨在通过混合设计解决这些问题。

Method: TexAvatars通过CNN预测UV空间的局部几何属性，并利用网格感知的雅可比矩阵驱动3D变形，实现平滑且语义明确的过渡。

Result: TexAvatars在极端姿态和表情下表现优异，能够高保真地捕捉肌肉引起的皱纹、眉间纹等细节，提升了泛化性和逼真度。

Conclusion: TexAvatars通过混合设计显著改善了3D头部虚拟形象的几何一致性和泛化能力，为AR/XR应用提供了更高质量的解决方案。

Abstract: Constructing drivable and photorealistic 3D head avatars has become a central task in AR/XR, enabling immersive and expressive user experiences. With the emergence of high-fidelity and efficient representations such as 3D Gaussians, recent works have pushed toward ultra-detailed head avatars. Existing approaches typically fall into two categories: rule-based analytic rigging or neural network-based deformation fields. While effective in constrained settings, both approaches often fail to generalize to unseen expressions and poses, particularly in extreme reenactment scenarios. Other methods constrain Gaussians to the global texel space of 3DMMs to reduce rendering complexity. However, these texel-based avatars tend to underutilize the underlying mesh structure. They apply minimal analytic deformation and rely heavily on neural regressors and heuristic regularization in UV space, which weakens geometric consistency and limits extrapolation to complex, out-of-distribution deformations. To address these limitations, we introduce TexAvatars, a hybrid avatar representation that combines the explicit geometric grounding of analytic rigging with the spatial continuity of texel space. Our approach predicts local geometric attributes in UV space via CNNs, but drives 3D deformation through mesh-aware Jacobians, enabling smooth and semantically meaningful transitions across triangle boundaries. This hybrid design separates semantic modeling from geometric control, resulting in improved generalization, interpretability, and stability. Furthermore, TexAvatars captures fine-grained expression effects, including muscle-induced wrinkles, glabellar lines, and realistic mouth cavity geometry, with high fidelity. Our method achieves state-of-the-art performance under extreme pose and expression variations, demonstrating strong generalization in challenging head reenactment settings.

</details>


<div id='cs.GT'></div>

# cs.GT [[Back]](#toc)

### [4] [Mechanism-Based Intelligence (MBI): Differentiable Incentives for Rational Coordination and Guaranteed Alignment in Multi-Agent Systems](https://arxiv.org/abs/2512.20688)
*Stefano Grassi*

Main category: cs.GT

TL;DR: 该论文提出了基于机制的智能范式（MBI），通过可微分价格机制（DPM）解决多智能体系统中的信息分散和激励对齐问题，显著提升了协调效率和计算速度。


<details>
  <summary>Details</summary>
Motivation: 多智能体系统在解决信息分散和激励对齐问题时面临计算复杂性和协调困难的挑战，需要一种新的方法来实现高效、可扩展的协调。

Method: 论文引入了可微分价格机制（DPM），计算动态的激励信号以确保激励相容性，并通过贝叶斯扩展处理不对称信息问题。

Result: 该方法在代理数量上具有线性复杂度，比无模型强化学习快50倍，并能够确保全局最优解和激励相容性。

Conclusion: MBI提供了一种高效、可审计且通用的多智能体协调方法，基于经济学原理实现了可扩展和可信赖的智能体协调。

Abstract: Autonomous multi-agent systems are fundamentally fragile: they struggle to solve the Hayekian Information problem (eliciting dispersed private knowledge) and the Hurwiczian Incentive problem (aligning local actions with global objectives), making coordination computationally intractable. I introduce Mechanism-Based Intelligence (MBI), a paradigm that reconceptualizes intelligence as emergent from the coordination of multiple "brains", rather than a single one. At its core, the Differentiable Price Mechanism (DPM) computes the exact loss gradient $$ \mathbf{G}_i = - \frac{\partial \mathcal{L}}{\partial \mathbf{x}_i} $$ as a dynamic, VCG-equivalent incentive signal, guaranteeing Dominant Strategy Incentive Compatibility (DSIC) and convergence to the global optimum. A Bayesian extension ensures incentive compatibility under asymmetric information (BIC). The framework scales linearly ($\mathcal{O}(N)$) with the number of agents, bypassing the combinatorial complexity of Dec-POMDPs and is empirically 50x faster than Model-Free Reinforcement Learning. By structurally aligning agent self-interest with collective objectives, it provides a provably efficient, auditable and generalizable approach to coordinated, trustworthy and scalable multi-agent intelligence grounded in economic principles.

</details>


### [5] [(Im)possibility of Incentive Design for Challenge-based Blockchain Protocols](https://arxiv.org/abs/2512.20864)
*Suhyeon Lee,Dieu-Huyen Nguyen,Donghwan Lee*

Main category: cs.GT

TL;DR: 论文研究了区块链系统中挑战机制的激励问题，特别是在单赢家与多赢家设计下的表现差异。


<details>
  <summary>Details</summary>
Motivation: 区块链的状态复制模型使得链上计算昂贵，现有系统通过挑战机制减轻链上负担，但缺乏对诚实挑战者激励与欺诈者惩罚的清晰分析。

Method: 构建了一个包含合谋少数派、异构成本和三种排序模式的模型，探讨能否同时实现诚实无损失和欺诈抑制的目标。

Result: 研究表明，单赢家设计中激励机制难以实现或规模受限；而多赢家设计下，能够满足简单明确的条件以实现双重目标。

Conclusion: 多赢家设计在保障诚实激励和欺诈抑制方面更具可行性，为区块链挑战机制的设计提供了新的视角。

Abstract: Blockchains offer a decentralized and secure execution environment strong enough to host cryptocurrencies, but the state-replication model makes on-chain computation expensive. To avoid heavy on-chain workloads, systems like Truebit and optimistic rollups use challenge-based protocols, performing computations off-chain and invoking the chain only when challenged. This keeps normal-case costs low and, if at least one honest challenger exists, can catch fraud. What has been less clear is whether honest challengers are actually incentivized and a dishonest proposer is properly damaged under the worst case environment. We build a model with a colluding minority, heterogeneous costs, and three ordering modes. We then ask whether two goals can be met together: honest non-loss and fraud deterrence. Our results are clear: in single-winner designs, the incentive design is impossible or limited in scale. By contrast, in multi-winner designs, we obtain simple, explicit conditions under which both goals hold.

</details>


### [6] [Policy-Conditioned Policies for Multi-Agent Task Solving](https://arxiv.org/abs/2512.21024)
*Yue Lin,Shuhui Zhu,Wenhao Li,Ang Li,Dan Qiao,Pascal Poupart,Hongyuan Zha,Baoxiang Wang*

Main category: cs.GT

TL;DR: 该论文提出一种利用大型语言模型（LLMs）将策略表示为人类可理解的源代码的方法，以解决多智能体任务中的策略动态适应问题。


<details>
  <summary>Details</summary>
Motivation: 在多智能体任务中，动态调整策略是核心挑战，但直接在深度强化学习范式下基于对手策略进行条件化是不可行的，因为神经策略是难以理解的高维参数向量。因此，需要通过人类可解释的表示来解决这一“表示瓶颈”。

Method: 论文提出将策略表示为源代码，并利用LLMs作为近似解释器，以操作博弈论中的“程序均衡”概念。通过学习LLMs直接在程序化策略空间中进行优化，LLMs作为点对点的最佳响应操作符，迭代合成和优化代理的策略代码。

Result: 该方法在多个标准协调矩阵游戏和合作性Level-Based Foraging环境中有效解决了问题。

Conclusion: 通过LLMs将策略表示为源代码并优化，能够有效解决多智能体任务中的策略适应问题，展示了程序化表示的潜力。

Abstract: In multi-agent tasks, the central challenge lies in the dynamic adaptation of strategies. However, directly conditioning on opponents' strategies is intractable in the prevalent deep reinforcement learning paradigm due to a fundamental ``representational bottleneck'': neural policies are opaque, high-dimensional parameter vectors that are incomprehensible to other agents. In this work, we propose a paradigm shift that bridges this gap by representing policies as human-interpretable source code and utilizing Large Language Models (LLMs) as approximate interpreters. This programmatic representation allows us to operationalize the game-theoretic concept of \textit{Program Equilibrium}. We reformulate the learning problem by utilizing LLMs to perform optimization directly in the space of programmatic policies. The LLM functions as a point-wise best-response operator that iteratively synthesizes and refines the ego agent's policy code to respond to the opponent's strategy. We formalize this process as \textit{Programmatic Iterated Best Response (PIBR)}, an algorithm where the policy code is optimized by textual gradients, using structured feedback derived from game utility and runtime unit tests. We demonstrate that this approach effectively solves several standard coordination matrix games and a cooperative Level-Based Foraging environment.

</details>
