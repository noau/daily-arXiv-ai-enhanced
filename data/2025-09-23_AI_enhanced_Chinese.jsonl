{"id": "2509.16450", "categories": ["cs.GT", "econ.TH"], "pdf": "https://arxiv.org/pdf/2509.16450", "abs": "https://arxiv.org/abs/2509.16450", "authors": ["Jiaxin Song", "Pooja Kulkarni", "Parnian Shahkar", "Bhaskar Ray Chaudhury"], "title": "On the Existence and Complexity of Core-Stable Data Exchanges", "comment": "28 pages, 5 figures, accepted by NeurIPS'25", "summary": "The rapid growth of data-driven technologies and the emergence of various\ndata-sharing paradigms have underscored the need for efficient and stable data\nexchange protocols. In any such exchange, agents must carefully balance the\nbenefit of acquiring valuable data against the cost of sharing their own.\nEnsuring stability in these exchanges is essential to prevent agents -- or\ngroups of agents -- from departing and conducting local (and potentially more\nfavorable) exchanges among themselves. To address this, we study a model where\nagents participate in a data exchange. Each agent has an associated payoff for\nthe data acquired from other agents and a cost incurred during sharing its own\ndata. The net utility of an agent is payoff minus the cost. We adapt the\nclassical notion of core-stability from cooperative game theory to data\nexchange. A data exchange is core-stable if no subset of agents has any\nincentive to deviate to a different exchange. We show that a core-stable data\nexchange is guaranteed to exist when agents have concave payoff functions and\nconvex cost functions -- a setting typical in domains like PAC learning and\nrandom discovery models. We show that relaxing either of the foregoing\nconditions may result in the nonexistence of core-stable data exchanges. Then,\nwe prove that finding a core-stable exchange is PPAD-hard, even when the\npotential blocking coalitions are restricted to constant size. To the best of\nour knowledge, this provides the first known PPAD-hardness result for core-like\nguarantees in data economics. Finally, we show that data exchange can be\nmodelled as a balanced $n$-person game. This immediately gives a pivoting\nalgorithm via Scarf's theorem \\cite{Scarf1967core}. We show that the pivoting\nalgorithm works well in practice through our empirical results.", "AI": {"tldr": "\u7814\u7a76\u6570\u636e\u4ea4\u6362\u4e2d\u7684\u6838\u5fc3\u7a33\u5b9a\u6027\u95ee\u9898\uff0c\u8bc1\u660e\u5728\u51f9\u6536\u76ca\u51fd\u6570\u548c\u51f8\u6210\u672c\u51fd\u6570\u6761\u4ef6\u4e0b\u5b58\u5728\u6838\u5fc3\u7a33\u5b9a\u7684\u6570\u636e\u4ea4\u6362\uff0c\u5e76\u63a2\u8ba8\u5176\u8ba1\u7b97\u590d\u6742\u6027\u3002", "motivation": "\u6570\u636e\u9a71\u52a8\u6280\u672f\u7684\u5feb\u901f\u53d1\u5c55\u4ee5\u53ca\u5404\u79cd\u6570\u636e\u5171\u4eab\u6a21\u5f0f\u7684\u51fa\u73b0\uff0c\u51f8\u663e\u4e86\u5bf9\u9ad8\u6548\u7a33\u5b9a\u6570\u636e\u4ea4\u6362\u534f\u8bae\u7684\u9700\u6c42\u3002", "method": "\u91c7\u7528\u5408\u4f5c\u535a\u5f08\u7406\u8bba\u4e2d\u7684\u6838\u5fc3\u7a33\u5b9a\u6027\u6982\u5ff5\uff0c\u5206\u6790\u6570\u636e\u4ea4\u6362\u4e2d\u7684\u6536\u76ca\u548c\u6210\u672c\u51fd\u6570\uff0c\u8bc1\u660e\u6838\u5fc3\u7a33\u5b9a\u6761\u4ef6\u7684\u5b58\u5728\u6027\u53ca\u5176\u8ba1\u7b97\u590d\u6742\u6027\u3002", "result": "\u5728\u51f9\u6536\u76ca\u51fd\u6570\u548c\u51f8\u6210\u672c\u51fd\u6570\u60c5\u51b5\u4e0b\uff0c\u6838\u5fc3\u7a33\u5b9a\u7684\u6570\u636e\u4ea4\u6362\u5b58\u5728\uff1b\u7a81\u7834\u8fd9\u4e9b\u6761\u4ef6\u5219\u53ef\u80fd\u5bfc\u81f4\u7a33\u5b9a\u6027\u7f3a\u5931\u3002\u8ba1\u7b97\u6838\u5fc3\u7a33\u5b9a\u7684\u4ea4\u6362\u662fPPAD\u96be\u7684\u3002", "conclusion": "\u6570\u636e\u4ea4\u6362\u53ef\u5efa\u6a21\u4e3a\u5e73\u8861\u591a\u4eba\u535a\u5f08\uff0cScarf\u5b9a\u7406\u63d0\u4f9b\u7684pivoting\u7b97\u6cd5\u5728\u5b9e\u8df5\u4e2d\u8868\u73b0\u826f\u597d\u3002"}}
{"id": "2509.16246", "categories": ["cs.PL", "cs.AR"], "pdf": "https://arxiv.org/pdf/2509.16246", "abs": "https://arxiv.org/abs/2509.16246", "authors": ["Juxin Niu", "Yuxin Du", "Dan Niu", "Xi Wang", "Zhe Jiang", "Nan Guan"], "title": "VerilogMonkey: Exploring Parallel Scaling for Automated Verilog Code Generation with LLMs", "comment": null, "summary": "We present VerilogMonkey, an empirical study of parallel scaling for the\nunder-explored task of automated Verilog generation. Parallel scaling improves\nLLM performance by sampling many outputs in parallel. Across multiple\nbenchmarks and mainstream LLMs, we find that scaling to hundreds of samples is\ncost-effective in both time and money and, even without any additional\nenhancements such as post-training or agentic methods, surpasses prior results\non LLM-based Verilog generation. We further dissect why parallel scaling\ndelivers these gains and show how output randomness in LLMs affects its\neffectiveness.", "AI": {"tldr": "VerilogMonkey\u7814\u7a76\u8868\u660e\uff0c\u5728\u81ea\u52a8\u751f\u6210Verilog\u7684\u4efb\u52a1\u4e2d\uff0c\u901a\u8fc7\u5e76\u884c\u6269\u5c55\u91c7\u6837\u6570\u767e\u4e2a\u8f93\u51fa\uff0c\u65e0\u9700\u989d\u5916\u589e\u5f3a\u65b9\u6cd5\uff0c\u5373\u53ef\u8d85\u8d8a\u4ee5\u5f80\u57fa\u4e8eLLM\u7684Verilog\u751f\u6210\u6210\u679c\uff0c\u540c\u65f6\u5728\u65f6\u95f4\u548c\u6210\u672c\u4e0a\u5177\u6709\u6027\u4ef7\u6bd4\u3002", "motivation": "\u63a2\u7d22\u5e76\u884c\u6269\u5c55\u5728\u81ea\u52a8\u751f\u6210Verilog\u4efb\u52a1\u4e2d\u7684\u6548\u679c\uff0c\u8fd9\u4e00\u4efb\u52a1\u76ee\u524d\u7814\u7a76\u8f83\u5c11\u3002\u901a\u8fc7\u5b9e\u8bc1\u7814\u7a76\uff0c\u9a8c\u8bc1\u5e76\u884c\u6269\u5c55\u5bf9\u63d0\u5347LLM\u6027\u80fd\u7684\u6f5c\u529b\u3002", "method": "\u91c7\u7528\u5e76\u884c\u6269\u5c55\u65b9\u6cd5\uff0c\u91c7\u6837\u5927\u91cf\u8f93\u51fa\uff0c\u5e76\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u548c\u4e3b\u6d41LLMs\u4e0a\u8fdb\u884c\u9a8c\u8bc1\u3002\u5206\u6790\u8f93\u51fa\u968f\u673a\u6027\u5bf9\u5e76\u884c\u6269\u5c55\u6548\u679c\u7684\u5f71\u54cd\u3002", "result": "\u7814\u7a76\u53d1\u73b0\uff0c\u6269\u5c55\u5230\u6570\u767e\u4e2a\u6837\u672c\u65f6\uff0c\u65e0\u9700\u989d\u5916\u589e\u5f3a\uff08\u5982\u540e\u8bad\u7ec3\u6216\u4ee3\u7406\u65b9\u6cd5\uff09\uff0c\u5373\u53ef\u5728\u65f6\u95f4\u548c\u6210\u672c\u4e0a\u9ad8\u6548\u5730\u8d85\u8d8a\u4ee5\u5f80LLM-based Verilog\u751f\u6210\u7684\u7ed3\u679c\u3002", "conclusion": "\u5e76\u884c\u6269\u5c55\u662f\u4e00\u79cd\u9ad8\u6548\u4e14\u6210\u672c\u4f4e\u5ec9\u7684\u65b9\u6cd5\uff0c\u80fd\u591f\u5728\u81ea\u52a8\u751f\u6210Verilog\u4efb\u52a1\u4e2d\u663e\u8457\u63d0\u5347LLM\u6027\u80fd\uff0c\u8f93\u51fa\u968f\u673a\u6027\u5bf9\u5176\u6548\u679c\u6709\u91cd\u8981\u5f71\u54cd\u3002"}}
{"id": "2509.16802", "categories": ["cs.GT", "cs.DM"], "pdf": "https://arxiv.org/pdf/2509.16802", "abs": "https://arxiv.org/abs/2509.16802", "authors": ["Max Dupre la Tour", "Kaito Fujii"], "title": "Discrepancy And Fair Division For Non-Additive Valuations", "comment": null, "summary": "We extend the notion of combinatorial discrepancy to \\emph{non-additive}\nfunctions. Our main result is an upper bound of $O(\\sqrt{n \\log(nk)})$ on the\nnon-additive $k$-color discrepancy when $k$ is a prime power. We demonstrate\ntwo applications of this result to problems in fair division. First, we\nestablish a bound for a consensus halving problem, where fairness is measured\nby the minimum number of items that must be transferred between the two parts\nto eliminate envy. Second, we improve the upper bound on the total subsidy\nrequired to achieve an envy-free allocation when the number of agents is a\nprime power, obtaining an $O(n \\sqrt{n \\log n})$ bound. This constitutes the\nfirst known subquadratic guarantee in this setting.", "AI": {"tldr": "\u5c06\u7ec4\u5408\u5dee\u5f02\u6269\u5c55\u81f3\u975e\u52a0\u6027\u51fd\u6570\uff0c\u63d0\u51fa\u4e00\u4e2a\u5173\u4e8e\u975e\u52a0\u6027k\u8272\u5dee\u5f02\u7684\u4e0a\u754c\uff0c\u5e76\u5e94\u7528\u4e8e\u516c\u5e73\u5206\u914d\u95ee\u9898\uff0c\u6539\u5584\u4e86\u4e24\u4e2a\u5e94\u7528\u573a\u666f\u7684\u516c\u5e73\u6027\u754c\u9650\u3002", "motivation": "\u7814\u7a76\u7ec4\u5408\u5dee\u5f02\u5728\u975e\u52a0\u6027\u51fd\u6570\u4e2d\u7684\u6269\u5c55\uff0c\u7279\u522b\u662f\u4e3a\u4e86\u89e3\u51b3\u516c\u5e73\u5206\u914d\u95ee\u9898\u4e2d\u7684\u8bbe\u8ba1\u6311\u6218\u3002", "method": "\u6269\u5c55\u7ec4\u5408\u5dee\u5f02\u4e3a\u975e\u52a0\u6027\u51fd\u6570\uff0c\u63a8\u5bfc\u51fa\u975e\u52a0\u6027k\u8272\u5dee\u5f02\u7684\u4e0a\u754c\uff0c\u5e76\u5c06\u5176\u5e94\u7528\u4e8e\u5171\u8bc6\u5206\u5272\u548c\u5ac9\u5992\u81ea\u7531\u5206\u914d\u95ee\u9898\u3002", "result": "\u5f53k\u4e3a\u7d20\u6570\u7684\u5e42\u65f6\uff0c\u975e\u52a0\u6027k\u8272\u5dee\u5f02\u4e0a\u754c\u4e3aO(\u221a(n log(nk)))\uff0c\u5e76\u5728\u516c\u5e73\u5206\u914d\u95ee\u9898\u4e2d\u53d6\u5f97\u4e86\u6539\u8fdb\u7ed3\u679c\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e0d\u4ec5\u6269\u5c55\u4e86\u7ec4\u5408\u5dee\u5f02\u7684\u7406\u8bba\u57fa\u7840\uff0c\u8fd8\u4e3a\u516c\u5e73\u5206\u914d\u95ee\u9898\u63d0\u4f9b\u4e86\u65b0\u7684\u7406\u8bba\u4fdd\u8bc1\u548c\u6539\u8fdb\u65b9\u6cd5\u3002"}}
{"id": "2509.16248", "categories": ["cs.PL", "cs.LG", "cs.SE"], "pdf": "https://arxiv.org/pdf/2509.16248", "abs": "https://arxiv.org/abs/2509.16248", "authors": ["Savini Kashmira", "Jayanaka Dantanarayana", "Thamirawaran Sathiyalogeswaran", "Yichao Yuan", "Nishil Talati", "Krisztian Flautner", "Lingjia Tang", "Jason Mars"], "title": "GraphMend: Code Transformations for Fixing Graph Breaks in PyTorch 2", "comment": null, "summary": "This paper presents GraphMend, a high-level compiler that eliminates FX graph\nbreaks in PyTorch 2 programs. Although PyTorch 2 introduced TorchDynamo and\nTorchInductor to enable just-in-time graph compilation, unresolved dynamic\ncontrol flow and unsupported Python constructs often fragment models into\nmultiple FX graphs. These fragments force frequent fallbacks to eager mode,\nincur costly CPU-to-GPU synchronizations, and reduce optimization\nopportunities. GraphMend addresses this limitation by analyzing and\ntransforming source code before execution. Built on the Jac compilation\nframework, GraphMend introduces two code transformations that remove graph\nbreaks due to dynamic control flow and Python I/O functions. This design allows\nPyTorch's compilation pipeline to capture larger, uninterrupted FX graphs\nwithout requiring manual refactoring by developers. Evaluation across eight\nHugging Face models shows that GraphMend removes all fixable graph breaks due\nto dynamic control flow and Python I/O functions, driving the break count to 0\nin 6 models and reducing it from 5 to 2 in another model. On NVIDIA RTX 3090\nand A40 GPUs, GraphMend achieves up to 75% latency reductions and up to 8%\nhigher end-to-end throughput. These results demonstrate that high-level code\ntransformation is an effective complement to PyTorch's dynamic JIT compilation\npipeline, substantially improving both usability and performance.", "AI": {"tldr": "GraphMend\u662f\u4e00\u4e2a\u9ad8\u7ea7\u7f16\u8bd1\u5668\uff0c\u901a\u8fc7\u5206\u6790\u548c\u8f6c\u6362\u6e90\u4ee3\u7801\u6765\u89e3\u51b3PyTorch 2\u7a0b\u5e8f\u4e2d\u7684FX\u56fe\u4e2d\u65ad\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u6027\u80fd\u548c\u6613\u7528\u6027\u3002", "motivation": "PyTorch 2\u7684TorchDynamo\u548cTorchInductor\u867d\u7136\u5b9e\u73b0\u4e86\u5373\u65f6\u56fe\u7f16\u8bd1\uff0c\u4f46\u52a8\u6001\u63a7\u5236\u6d41\u548c\u4e0d\u652f\u6301\u7684Python\u6784\u9020\u4ecd\u7136\u4f1a\u5bfc\u81f4\u6a21\u578b\u788e\u7247\u5316\uff0c\u9891\u7e41\u56de\u9000\u5230eager\u6a21\u5f0f\u5e76\u964d\u4f4e\u4f18\u5316\u673a\u4f1a\u3002GraphMend\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "GraphMend\u57fa\u4e8eJac\u7f16\u8bd1\u6846\u67b6\uff0c\u5f15\u5165\u4e24\u79cd\u4ee3\u7801\u8f6c\u6362\u6280\u672f\uff0c\u6d88\u9664\u52a8\u6001\u63a7\u5236\u6d41\u548cPython I/O\u51fd\u6570\u5bfc\u81f4\u7684\u56fe\u4e2d\u65ad\uff0c\u65e0\u9700\u5f00\u53d1\u8005\u624b\u52a8\u91cd\u6784\u4ee3\u7801\u3002", "result": "\u5728\u516b\u4e2aHugging Face\u6a21\u578b\u4e0a\u7684\u8bc4\u4f30\u663e\u793a\uff0cGraphMend\u6d88\u9664\u4e86\u6240\u6709\u53ef\u4fee\u590d\u7684\u56fe\u4e2d\u65ad\uff0c\u5728NVIDIA RTX 3090\u548cA40 GPU\u4e0a\u5b9e\u73b0\u4e86\u9ad8\u8fbe75%\u7684\u5ef6\u8fdf\u964d\u4f4e\u548c8%\u7684\u541e\u5410\u91cf\u63d0\u5347\u3002", "conclusion": "\u7814\u7a76\u8868\u660e\uff0c\u9ad8\u7ea7\u4ee3\u7801\u8f6c\u6362\u662fPyTorch\u52a8\u6001JIT\u7f16\u8bd1\u7ba1\u9053\u7684\u6709\u6548\u8865\u5145\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6027\u80fd\u548c\u6613\u7528\u6027\u3002"}}
{"id": "2509.17134", "categories": ["cs.GT"], "pdf": "https://arxiv.org/pdf/2509.17134", "abs": "https://arxiv.org/abs/2509.17134", "authors": ["Mohammad Ali Abam", "Davoud Kareshki", "Marzieh Nilipour", "Mohammad Hossein Paydar", "Masoud Seddighin"], "title": "Tight Bounds On the Distortion of Randomized and Deterministic Distributed Voting", "comment": "36 pages, 12 figures, submitted to NeurIPS 2025", "summary": "We study metric distortion in distributed voting, where $n$ voters are\npartitioned into $k$ groups, each selecting a local representative, and a final\nwinner is chosen from these representatives (or from the entire set of\ncandidates). This setting models systems like U.S. presidential elections,\nwhere state-level decisions determine the national outcome. We focus on four\ncost objectives from \\citep{anshelevich2022distortion}: $\\avgavg$, $\\avgmax$,\n$\\maxavg$, and $\\maxmax$. We present improved distortion bounds for both\ndeterministic and randomized mechanisms, offering a near-complete\ncharacterization of distortion in this model.\n  For deterministic mechanisms, we reduce the upper bound for $\\avgmax$ from\n$11$ to $7$, establish a tight lower bound of $5$ for $\\maxavg$ (improving on\n$2+\\sqrt{5}$), and tighten the upper bound for $\\maxmax$ from $5$ to $3$.\n  For randomized mechanisms, we consider two settings: (i) only the second\nstage is randomized, and (ii) both stages may be randomized. In case (i), we\nprove tight bounds: $5\\!-\\!2/k$ for $\\avgavg$, $3$ for $\\avgmax$ and $\\maxmax$,\nand $5$ for $\\maxavg$. In case (ii), we show tight bounds of $3$ for $\\maxavg$\nand $\\maxmax$, and nearly tight bounds for $\\avgavg$ and $\\avgmax$ within\n$[3\\!-\\!2/n,\\ 3\\!-\\!2/(kn^*)]$ and $[3\\!-\\!2/n,\\ 3]$, respectively, where $n^*$\ndenotes the largest group size.", "AI": {"tldr": "\u7814\u7a76\u4e86\u5206\u5e03\u5f0f\u6295\u7968\u4e2d\u7684\u5ea6\u91cf\u5931\u771f\u95ee\u9898\uff0c\u6539\u8fdb\u4e86\u786e\u5b9a\u6027\u548c\u968f\u673a\u673a\u5236\u7684\u5931\u771f\u754c\u9650\uff0c\u63d0\u4f9b\u4e86\u8fd1\u4e4e\u5b8c\u6574\u7684\u5931\u771f\u7279\u5f81\u63cf\u8ff0\u3002", "motivation": "\u7814\u7a76\u5206\u5e03\u5f0f\u6295\u7968\u4e2d\u7684\u5ea6\u91cf\u5931\u771f\u95ee\u9898\uff0c\u65e8\u5728\u6a21\u62df\u5982\u7f8e\u56fd\u603b\u7edf\u9009\u4e3e\u7b49\u7cfb\u7edf\u4e2d\u672c\u5730\u51b3\u7b56\u5982\u4f55\u5f71\u54cd\u5168\u5c40\u7ed3\u679c\u3002", "method": "\u91c7\u7528\u4e0d\u540c\u6210\u672c\u76ee\u6807\uff08$\\avgavg$\u3001$\\avgmax$\u3001$\\maxavg$\u3001$\\maxmax$\uff09\u5206\u6790\u786e\u5b9a\u6027\u548c\u968f\u673a\u673a\u5236\u7684\u5931\u771f\u754c\u9650\u3002", "result": "\u5728\u786e\u5b9a\u6027\u673a\u5236\u4e2d\uff0c\u6539\u8fdb\u4e86\u591a\u4e2a\u5931\u771f\u754c\u9650\uff1b\u5728\u968f\u673a\u673a\u5236\u4e2d\uff0c\u63d0\u51fa\u4e86\u4e24\u79cd\u8bbe\u7f6e\u4e0b\u7684\u7d27\u754c\u6216\u63a5\u8fd1\u7d27\u754c\u7684\u754c\u9650\u3002", "conclusion": "\u7814\u7a76\u63d0\u4f9b\u4e86\u5206\u5e03\u5f0f\u6295\u7968\u4e2d\u5ea6\u91cf\u5931\u771f\u7684\u8fd1\u4e4e\u5b8c\u6574\u7279\u5f81\u63cf\u8ff0\uff0c\u6539\u8fdb\u4e86\u73b0\u6709\u7684\u7406\u8bba\u754c\u9650\u3002"}}
{"id": "2509.17795", "categories": ["cs.PL"], "pdf": "https://arxiv.org/pdf/2509.17795", "abs": "https://arxiv.org/abs/2509.17795", "authors": ["Parosh Aziz Abdulla", "Samuel Grahn", "Bengt Jonsson", "Shankaranarayanan Krishna", "Om Swostik Mishra"], "title": "Efficient Linearizability Monitoring", "comment": null, "summary": "This paper revisits the fundamental problem of monitoring the linearizability\nof concurrent stacks, queues, sets, and multisets. Given a history of a library\nimplementing one of these abstract data types, the monitoring problem is to\nanswer whether the given history is linearizable. For stacks, queues, and\n(multi)sets, we present monitoring algorithms with complexities\n$\\mathcal{O}(n^2)$, $\\mathcal{O}(n\\; log\\, n)$, and $\\mathcal{O}{(n)}$,\nrespectively, where $n$ is the number of operations in the input history. For\nstacks and queues, our results hold under the standard assumption of {\\it\ndata-independence}, i.e., the behavior of the library is not sensitive to the\nactual values stored in the data structure. Past works to solve the same\nproblems have cubic time complexity and (more seriously) have correctness\nissues: they either (i) lack correctness proofs or (ii) the suggested\ncorrectness proofs are erroneous (we present counter-examples), or (iii) have\nincorrect algorithms. Our improved complexity results rely on substantially\ndifferent algorithms for which we provide detailed proofs of correctness. We\nhave implemented our stack and queue algorithms in LiMo (Linearizability\nMonitor). We evaluate LiMo and compare it with the state-of-the-art tool Violin\n-- whose correctness proofs we have found errors in -- which checks for\nlinearizability violations. Our experimental evaluation confirms that LiMo\noutperforms Violin regarding both efficiency and scalability.", "AI": {"tldr": "\u672c\u6587\u91cd\u65b0\u5ba1\u89c6\u4e86\u76d1\u63a7\u5e76\u53d1\u6808\u3001\u961f\u5217\u3001\u96c6\u5408\u548c\u591a\u96c6\u7684\u7ebf\u6027\u53ef\u5316\u6027\u95ee\u9898\uff0c\u63d0\u51fa\u4e86\u590d\u6742\u5ea6\u5206\u522b\u4e3aO(n\u00b2)\u3001O(n log n)\u548cO(n)\u7684\u76d1\u63a7\u7b97\u6cd5\uff0c\u5e76\u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u65b0\u5de5\u5177LiMo\u7684\u9ad8\u6548\u6027\u548c\u53ef\u6269\u5c55\u6027\u3002", "motivation": "\u5f53\u524d\u5173\u4e8e\u76d1\u63a7\u7ebf\u6027\u53ef\u5316\u6027\u7684\u7b97\u6cd5\u5b58\u5728\u65f6\u95f4\u590d\u6742\u5ea6\u8f83\u9ad8\uff08\u7acb\u65b9\u65f6\u95f4\uff09\u4ee5\u53ca\u6b63\u786e\u6027\u95ee\u9898\uff0c\u5305\u62ec\u7f3a\u4e4f\u8bc1\u660e\u3001\u8bc1\u660e\u9519\u8bef\u6216\u7b97\u6cd5\u672c\u8eab\u9519\u8bef\u3002\u672c\u6587\u65e8\u5728\u63d0\u4f9b\u66f4\u9ad8\u6548\u7684\u7b97\u6cd5\uff0c\u5e76\u901a\u8fc7\u8be6\u7ec6\u8bc1\u660e\u786e\u4fdd\u5176\u6b63\u786e\u6027\u3002", "method": "\u9488\u5bf9\u6808\u3001\u961f\u5217\u3001\uff08\u591a\uff09\u96c6\uff0c\u5206\u522b\u63d0\u51fa\u4e86\u590d\u6742\u5ea6\u4e3aO(n\u00b2)\u3001O(n log n)\u548cO(n)\u7684\u65b0\u76d1\u63a7\u7b97\u6cd5\u3002\u8fd9\u4e9b\u7b97\u6cd5\u57fa\u4e8e\u6570\u636e\u72ec\u7acb\u6027\u5047\u8bbe\uff0c\u5e76\u901a\u8fc7LiMo\u5de5\u5177\u5b9e\u73b0\u3002", "result": "\u5b9e\u9a8c\u8bc4\u4f30\u8868\u660e\uff0cLiMo\u5728\u6548\u7387\u548c\u53ef\u6269\u5c55\u6027\u4e0a\u4f18\u4e8e\u73b0\u6709\u5de5\u5177Violin\u3002\u540c\u65f6\uff0c\u65b0\u7b97\u6cd5\u7684\u590d\u6742\u5ea6\u663e\u8457\u964d\u4f4e\uff0c\u4e14\u901a\u8fc7\u8be6\u7ec6\u8bc1\u660e\u786e\u4fdd\u4e86\u6b63\u786e\u6027\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u7684\u65b0\u7b97\u6cd5\u663e\u8457\u63d0\u5347\u4e86\u76d1\u63a7\u7ebf\u6027\u53ef\u5316\u6027\u7684\u6548\u7387\uff0c\u5e76\u901a\u8fc7\u5b9e\u9a8c\u548c\u7406\u8bba\u8bc1\u660e\u9a8c\u8bc1\u4e86\u5176\u6b63\u786e\u6027\u548c\u4f18\u8d8a\u6027\u3002"}}
{"id": "2509.16336", "categories": ["cs.GR", "cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.16336", "abs": "https://arxiv.org/abs/2509.16336", "authors": ["Jan Philipp Schneider", "Pratik Singh Bisht", "Ilya Chugunov", "Andreas Kolb", "Michael Moeller", "Felix Heide"], "title": "Neural Atlas Graphs for Dynamic Scene Decomposition and Editing", "comment": null, "summary": "Learning editable high-resolution scene representations for dynamic scenes is\nan open problem with applications across the domains from autonomous driving to\ncreative editing - the most successful approaches today make a trade-off\nbetween editability and supporting scene complexity: neural atlases represent\ndynamic scenes as two deforming image layers, foreground and background, which\nare editable in 2D, but break down when multiple objects occlude and interact.\nIn contrast, scene graph models make use of annotated data such as masks and\nbounding boxes from autonomous-driving datasets to capture complex 3D spatial\nrelationships, but their implicit volumetric node representations are\nchallenging to edit view-consistently. We propose Neural Atlas Graphs (NAGs), a\nhybrid high-resolution scene representation, where every graph node is a\nview-dependent neural atlas, facilitating both 2D appearance editing and 3D\nordering and positioning of scene elements. Fit at test-time, NAGs achieve\nstate-of-the-art quantitative results on the Waymo Open Dataset - by 5 dB PSNR\nincrease compared to existing methods - and make environmental editing possible\nin high resolution and visual quality - creating counterfactual driving\nscenarios with new backgrounds and edited vehicle appearance. We find that the\nmethod also generalizes beyond driving scenes and compares favorably - by more\nthan 7 dB in PSNR - to recent matting and video editing baselines on the DAVIS\nvideo dataset with a diverse set of human and animal-centric scenes.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86Neural Atlas Graphs\uff08NAGs\uff09\uff0c\u4e00\u79cd\u6df7\u5408\u9ad8\u5206\u8fa8\u7387\u573a\u666f\u8868\u793a\u65b9\u6cd5\uff0c\u7ed3\u5408\u4e86\u795e\u7ecf\u5730\u56fe\u76842D\u7f16\u8f91\u80fd\u529b\u548c\u573a\u666f\u56fe\u76843D\u7a7a\u95f4\u5173\u7cfb\uff0c\u663e\u8457\u63d0\u5347\u4e86\u52a8\u6001\u573a\u666f\u7684\u7f16\u8f91\u6027\u548c\u590d\u6742\u6027\u652f\u6301\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u52a8\u6001\u573a\u666f\u7684\u9ad8\u5206\u8fa8\u7387\u8868\u793a\u4e2d\u9762\u4e34\u7f16\u8f91\u6027\u4e0e\u573a\u666f\u590d\u6742\u6027\u4e4b\u95f4\u7684\u6743\u8861\u95ee\u9898\uff0c\u672c\u6587\u65e8\u5728\u901a\u8fc7\u7ed3\u5408\u795e\u7ecf\u5730\u56fe\u548c\u573a\u666f\u56fe\u7684\u4f18\u52bf\uff0c\u63d0\u51fa\u4e00\u79cd\u66f4\u4f18\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "NAGs\u901a\u8fc7\u5c06\u6bcf\u4e2a\u56fe\u8282\u70b9\u8868\u793a\u4e3a\u89c6\u89d2\u4f9d\u8d56\u7684\u795e\u7ecf\u5730\u56fe\uff0c\u5b9e\u73b0\u4e862D\u5916\u89c2\u7f16\u8f91\u548c3D\u573a\u666f\u5143\u7d20\u7684\u6392\u5e8f\u4e0e\u5b9a\u4f4d\u7684\u517c\u5bb9\u3002", "result": "\u5728Waymo Open Dataset\u4e0a\uff0cNAGs\u7684PSNR\u63d0\u5347\u4e865 dB\uff0c\u5e76\u5728DAVIS\u89c6\u9891\u6570\u636e\u96c6\u4e0a\u7684\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd57 dB\u4ee5\u4e0a\u3002", "conclusion": "NAGs\u4e0d\u4ec5\u5728\u9ad8\u5206\u8fa8\u7387\u52a8\u6001\u573a\u666f\u8868\u793a\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u8fd8\u652f\u6301\u590d\u6742\u7684\u573a\u666f\u7f16\u8f91\uff0c\u9002\u7528\u4e8e\u81ea\u52a8\u9a7e\u9a76\u548c\u521b\u610f\u7f16\u8f91\u7b49\u591a\u4e2a\u9886\u57df\u3002"}}
{"id": "2509.16735", "categories": ["cs.GR"], "pdf": "https://arxiv.org/pdf/2509.16735", "abs": "https://arxiv.org/abs/2509.16735", "authors": ["Dongdong Chen", "Linlin Yao", "Mengjun Liu", "Zhenrong Shen", "Yuqi Hu", "Zhiyun Song", "Shengyu Lu", "Qian Wang", "Dinggang Shen", "Lichi Zhang"], "title": "Brain Connectivity Network Structure Learning For Brain Disorder Diagnosis", "comment": null, "summary": "Recent studies in neuroscience highlight the significant potential of brain\nconnectivity networks, which are commonly constructed from functional magnetic\nresonance imaging (fMRI) data for brain disorder diagnosis. Traditional brain\nconnectivity networks are typically obtained using predefined methods that\nincorporate manually-set thresholds to estimate inter-regional relationships.\nHowever, such approaches often introduce redundant connections or overlook\nessential interactions, compromising the value of the constructed networks.\nBesides, the insufficiency of labeled data further increases the difficulty of\nlearning generalized representations of intrinsic brain characteristics. To\nmitigate those issues, we propose a self-supervised framework to learn an\noptimal structure and representation for brain connectivity networks, focusing\non individualized generation and optimization in an unsupervised manner. We\nfirstly employ two existing whole-brain connectomes to adaptively construct\ntheir complementary brain network structure learner, and then introduce a\nmulti-state graph-based encoder with a joint iterative learning strategy to\nsimultaneously optimize both the generated network structure and its\nrepresentation. By leveraging self-supervised pretraining on large-scale\nunlabeled brain connectivity data, our framework enables the brain connectivity\nnetwork learner to generalize e ffectively to unseen disorders, while requiring\nonly minimal finetuning of the encoder for adaptation to new diagnostic tasks.\nExtensive experiments on cross-dataset brain disorder diagnosis demonstrate\nthat our method consistently outperforms state-of-the-art approaches,\nvalidating its effectiveness and generalizability. The code is publicly\navailable at https://github.com/neochen1/BCNSL.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u81ea\u76d1\u7763\u6846\u67b6\uff0c\u7528\u4e8e\u5b66\u4e60\u548c\u4f18\u5316\u8111\u529f\u80fd\u8fde\u63a5\u7f51\u7edc\u7684\u7ed3\u6784\u548c\u8868\u793a\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u65b9\u6cd5\u4e2d\u56e0\u624b\u52a8\u8bbe\u5b9a\u9608\u503c\u800c\u5f15\u5165\u7684\u5197\u4f59\u6216\u9057\u6f0f\u95ee\u9898\uff0c\u5e76\u901a\u8fc7\u5927\u89c4\u6a21\u65e0\u6807\u8bb0\u6570\u636e\u9884\u8bad\u7ec3\u63d0\u5347\u4e86\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u4f20\u7edf\u7684\u8111\u529f\u80fd\u8fde\u63a5\u7f51\u7edc\u6784\u5efa\u65b9\u6cd5\u4f9d\u8d56\u9884\u5b9a\u4e49\u7684\u624b\u52a8\u8bbe\u5b9a\u9608\u503c\uff0c\u53ef\u80fd\u5bfc\u81f4\u5197\u4f59\u6216\u9057\u6f0f\u91cd\u8981\u8fde\u63a5\uff0c\u4e14\u6807\u8bb0\u6570\u636e\u4e0d\u8db3\u9650\u5236\u4e86\u6cdb\u5316\u80fd\u529b\u3002", "method": "\u91c7\u7528\u81ea\u76d1\u7763\u6846\u67b6\uff0c\u7ed3\u5408\u4e92\u8865\u8111\u7f51\u7edc\u7ed3\u6784\u5b66\u4e60\u5668\u548c\u591a\u72b6\u6001\u56fe\u7f16\u7801\u5668\uff0c\u901a\u8fc7\u8054\u5408\u8fed\u4ee3\u5b66\u4e60\u7b56\u7565\u4f18\u5316\u7f51\u7edc\u7ed3\u6784\u548c\u8868\u793a\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u8de8\u6570\u636e\u96c6\u8111\u75be\u75c5\u8bca\u65ad\u4e2d\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u6280\u672f\uff0c\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u548c\u6cdb\u5316\u6027\u3002", "conclusion": "\u8be5\u81ea\u76d1\u7763\u6846\u67b6\u901a\u8fc7\u65e0\u76d1\u7763\u5b66\u4e60\u548c\u9884\u8bad\u7ec3\uff0c\u663e\u8457\u63d0\u5347\u4e86\u8111\u529f\u80fd\u8fde\u63a5\u7f51\u7edc\u7684\u6784\u5efa\u8d28\u91cf\u548c\u8bca\u65ad\u6027\u80fd\uff0c\u9002\u7528\u4e8e\u591a\u79cd\u8111\u75be\u75c5\u4efb\u52a1\u3002"}}
{"id": "2509.16869", "categories": ["cs.GR", "cs.AI", "cs.CV", "cs.LG", "cs.MM", "eess.IV", "Artificial intelligence, Computer vision, Machine learning, Deep\n  learning", "I.3.3; I.4.5"], "pdf": "https://arxiv.org/pdf/2509.16869", "abs": "https://arxiv.org/abs/2509.16869", "authors": ["Hrishav Bakul Barua", "Kalin Stefanov", "Ganesh Krishnasamy", "KokSheik Wong", "Abhinav Dhall"], "title": "PhysHDR: When Lighting Meets Materials and Scene Geometry in HDR Reconstruction", "comment": "Submitted to IEEE", "summary": "Low Dynamic Range (LDR) to High Dynamic Range (HDR) image translation is a\nfundamental task in many computational vision problems. Numerous data-driven\nmethods have been proposed to address this problem; however, they lack explicit\nmodeling of illumination, lighting, and scene geometry in images. This limits\nthe quality of the reconstructed HDR images. Since lighting and shadows\ninteract differently with different materials, (e.g., specular surfaces such as\nglass and metal, and lambertian or diffuse surfaces such as wood and stone),\nmodeling material-specific properties (e.g., specular and diffuse reflectance)\nhas the potential to improve the quality of HDR image reconstruction. This\npaper presents PhysHDR, a simple yet powerful latent diffusion-based generative\nmodel for HDR image reconstruction. The denoising process is conditioned on\nlighting and depth information and guided by a novel loss to incorporate\nmaterial properties of surfaces in the scene. The experimental results\nestablish the efficacy of PhysHDR in comparison to a number of recent\nstate-of-the-art methods.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86PhysHDR\uff0c\u4e00\u79cd\u57fa\u4e8e\u6f5c\u5728\u6269\u6563\u7684\u751f\u6210\u6a21\u578b\uff0c\u7528\u4e8e\u4ece\u4f4e\u52a8\u6001\u8303\u56f4\uff08LDR\uff09\u56fe\u50cf\u91cd\u5efa\u9ad8\u52a8\u6001\u8303\u56f4\uff08HDR\uff09\u56fe\u50cf\uff0c\u901a\u8fc7\u5efa\u6a21\u5149\u7167\u3001\u6df1\u5ea6\u548c\u6750\u6599\u7279\u6027\u63d0\u9ad8\u4e86\u91cd\u5efa\u8d28\u91cf\u3002", "motivation": "\u73b0\u6709\u7684LDR\u5230HDR\u56fe\u50cf\u8f6c\u6362\u65b9\u6cd5\u7f3a\u4e4f\u5bf9\u5149\u7167\u3001\u9634\u5f71\u548c\u6750\u6599\u5c5e\u6027\u7684\u663e\u5f0f\u5efa\u6a21\uff0c\u9650\u5236\u4e86HDR\u56fe\u50cf\u7684\u91cd\u5efa\u8d28\u91cf\u3002\u672c\u6587\u65e8\u5728\u901a\u8fc7\u5efa\u6a21\u6750\u6599\u7279\u6027\uff08\u5982\u955c\u9762\u53cd\u5c04\u548c\u6f2b\u53cd\u5c04\uff09\u6765\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u8bba\u6587\u63d0\u51faPhysHDR\uff0c\u4e00\u79cd\u57fa\u4e8e\u6f5c\u5728\u6269\u6563\u7684\u751f\u6210\u6a21\u578b\uff0c\u5176\u53bb\u566a\u8fc7\u7a0b\u901a\u8fc7\u5149\u7167\u548c\u6df1\u5ea6\u4fe1\u606f\u8fdb\u884c\u6761\u4ef6\u5316\uff0c\u5e76\u901a\u8fc7\u65b0\u9896\u7684\u635f\u5931\u51fd\u6570\u5f15\u5165\u573a\u666f\u4e2d\u8868\u9762\u7684\u6750\u6599\u5c5e\u6027\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cPhysHDR\u5728HDR\u56fe\u50cf\u91cd\u5efa\u8d28\u91cf\u4e0a\u4f18\u4e8e\u591a\u79cd\u6700\u65b0\u7684\u5148\u8fdb\u65b9\u6cd5\u3002", "conclusion": "PhysHDR\u901a\u8fc7\u663e\u5f0f\u5efa\u6a21\u5149\u7167\u3001\u6df1\u5ea6\u548c\u6750\u6599\u7279\u6027\uff0c\u663e\u8457\u63d0\u5347\u4e86HDR\u56fe\u50cf\u7684\u91cd\u5efa\u6548\u679c\uff0c\u5c55\u793a\u4e86\u5176\u5728\u8ba1\u7b97\u89c6\u89c9\u4efb\u52a1\u4e2d\u7684\u6f5c\u529b\u3002"}}
{"id": "2509.16960", "categories": ["cs.GR", "cs.MM"], "pdf": "https://arxiv.org/pdf/2509.16960", "abs": "https://arxiv.org/abs/2509.16960", "authors": ["Ruiyan Wang", "Zhengxue Cheng", "Zonghao Lin", "Jun Ling", "Yuzhou Liu", "Yanru An", "Rong Xie", "Li Song"], "title": "SemanticGarment: Semantic-Controlled Generation and Editing of 3D Gaussian Garments", "comment": null, "summary": "3D digital garment generation and editing play a pivotal role in fashion\ndesign, virtual try-on, and gaming. Traditional methods struggle to meet the\ngrowing demand due to technical complexity and high resource costs.\nLearning-based approaches offer faster, more diverse garment synthesis based on\nspecific requirements and reduce human efforts and time costs. However, they\nstill face challenges such as inconsistent multi-view geometry or textures and\nheavy reliance on detailed garment topology and manual rigging. We propose\nSemanticGarment, a 3D Gaussian-based method that realizes high-fidelity 3D\ngarment generation from text or image prompts and supports semantic-based\ninteractive editing for flexible user customization. To ensure multi-view\nconsistency and garment fitting, we propose to leverage structural human priors\nfor the generative model by introducing a 3D semantic clothing model, which\ninitializes the geometry structure and lays the groundwork for view-consistent\ngarment generation and editing. Without the need to regenerate or rely on\nexisting mesh templates, our approach allows for rapid and diverse\nmodifications to existing Gaussians, either globally or within a local region.\nTo address the artifacts caused by self-occlusion for garment reconstruction\nbased on single image, we develop a self-occlusion optimization strategy to\nmitigate holes and artifacts that arise when directly animating self-occluded\ngarments. Extensive experiments are conducted to demonstrate our superior\nperformance in 3D garment generation and editing.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e3D\u9ad8\u65af\u7684\u65b9\u6cd5SemanticGarment\uff0c\u5b9e\u73b0\u4e86\u4ece\u6587\u672c\u6216\u56fe\u50cf\u63d0\u793a\u751f\u6210\u9ad8\u4fdd\u771f3D\u670d\u88c5\uff0c\u5e76\u652f\u6301\u57fa\u4e8e\u8bed\u4e49\u7684\u4ea4\u4e92\u5f0f\u7f16\u8f91\u3002", "motivation": "\u4f20\u7edf\u65b9\u6cd5\u5728\u6ee1\u8db3\u65e5\u76ca\u589e\u957f\u7684\u9700\u6c42\u65f6\u56e0\u6280\u672f\u590d\u6742\u6027\u548c\u9ad8\u8d44\u6e90\u6210\u672c\u800c\u53d7\u9650\uff0c\u800c\u5b66\u4e60\u578b\u65b9\u6cd5\u867d\u7136\u63d0\u9ad8\u4e86\u901f\u5ea6\u548c\u591a\u6837\u6027\uff0c\u4f46\u4ecd\u9762\u4e34\u591a\u89c6\u89d2\u4e00\u81f4\u6027\u548c\u5bf9\u8be6\u7ec6\u62d3\u6251\u7ed3\u6784\u7684\u4f9d\u8d56\u7b49\u95ee\u9898\u3002", "method": "\u5229\u7528\u7ed3\u6784\u4eba\u4f53\u5148\u9a8c\u77e5\u8bc6\uff0c\u901a\u8fc7\u5f15\u51653D\u8bed\u4e49\u670d\u88c5\u6a21\u578b\u6765\u521d\u59cb\u5316\u51e0\u4f55\u7ed3\u6784\uff0c\u652f\u6301\u5feb\u901f\u3001\u591a\u6837\u5316\u7684\u4fee\u6539\uff0c\u5e76\u5f00\u53d1\u4e86\u81ea\u906e\u6321\u4f18\u5316\u7b56\u7565\u4ee5\u51cf\u5c11\u5355\u56fe\u50cf\u91cd\u5efa\u7684\u4f2a\u5f71\u3002", "result": "\u5927\u91cf\u5b9e\u9a8c\u8bc1\u660e\uff0c\u8be5\u65b9\u6cd5\u57283D\u670d\u88c5\u751f\u6210\u548c\u7f16\u8f91\u65b9\u9762\u8868\u73b0\u4f18\u8d8a\u3002", "conclusion": "SemanticGarment\u65b9\u6cd5\u65e0\u9700\u91cd\u65b0\u751f\u6210\u6216\u4f9d\u8d56\u73b0\u6709\u7f51\u683c\u6a21\u677f\uff0c\u5b9e\u73b0\u4e86\u9ad8\u4fdd\u771f3D\u670d\u88c5\u751f\u6210\u548c\u7075\u6d3b\u7684\u4ea4\u4e92\u5f0f\u7f16\u8f91\u3002"}}
{"id": "2509.17168", "categories": ["cs.GR", "cs.CV"], "pdf": "https://arxiv.org/pdf/2509.17168", "abs": "https://arxiv.org/abs/2509.17168", "authors": ["Chengwei Shi", "Chong Cao", "Xin Tong", "Xukun Shen"], "title": "Beat on Gaze: Learning Stylized Generation of Gaze and Head Dynamics", "comment": "arXiv submission", "summary": "Head and gaze dynamics are crucial in expressive 3D facial animation for\nconveying emotion and intention. However, existing methods frequently address\nfacial components in isolation, overlooking the intricate coordination between\ngaze, head motion, and speech. The scarcity of high-quality gaze-annotated\ndatasets hinders the development of data-driven models capable of capturing\nrealistic, personalized gaze control. To address these challenges, we propose\nStyGazeTalk, an audio-driven method that generates synchronized gaze and head\nmotion styles. We extract speaker-specific motion traits from gaze-head\nsequences with a multi-layer LSTM structure incorporating a style encoder,\nenabling the generation of diverse animation styles. We also introduce a\nhigh-precision multimodal dataset comprising eye-tracked gaze, audio, head\npose, and 3D facial parameters, providing a valuable resource for training and\nevaluating head and gaze control models. Experimental results demonstrate that\nour method generates realistic, temporally coherent, and style-aware head-gaze\nmotions, significantly advancing the state-of-the-art in audio-driven facial\nanimation.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aStyGazeTalk\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u97f3\u9891\u9a71\u52a8\u751f\u6210\u540c\u6b65\u7684\u6ce8\u89c6\u548c\u5934\u90e8\u8fd0\u52a8\u98ce\u683c\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u5728\u8868\u8fbe3D\u9762\u90e8\u52a8\u753b\u65f6\u5ffd\u7565\u5404\u7ec4\u4ef6\u534f\u8c03\u6027\u7684\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u5904\u7406\u9762\u90e8\u52a8\u753b\u65f6\uff0c\u5f80\u5f80\u5b64\u7acb\u5730\u5904\u7406\u5404\u7ec4\u4ef6\uff0c\u5ffd\u89c6\u4e86\u6ce8\u89c6\u3001\u5934\u90e8\u8fd0\u52a8\u548c\u8bed\u97f3\u4e4b\u95f4\u7684\u590d\u6742\u534f\u8c03\u6027\uff0c\u4e14\u7f3a\u4e4f\u9ad8\u8d28\u91cf\u7684\u6ce8\u89c6\u6807\u6ce8\u6570\u636e\u96c6\u3002", "method": "\u8bba\u6587\u63d0\u51faStyGazeTalk\u65b9\u6cd5\uff0c\u91c7\u7528\u591a\u5c42LSTM\u7ed3\u6784\u548c\u98ce\u683c\u7f16\u7801\u5668\uff0c\u4ece\u6ce8\u89c6-\u5934\u90e8\u5e8f\u5217\u4e2d\u63d0\u53d6\u8bf4\u8bdd\u8005\u7279\u5b9a\u7684\u8fd0\u52a8\u7279\u5f81\uff0c\u751f\u6210\u591a\u6837\u5316\u7684\u52a8\u753b\u98ce\u683c\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u80fd\u591f\u751f\u6210\u903c\u771f\u3001\u65f6\u95f4\u8fde\u8d2f\u4e14\u5177\u6709\u98ce\u683c\u610f\u8bc6\u7684\u5934\u90e8-\u6ce8\u89c6\u8fd0\u52a8\uff0c\u663e\u8457\u63d0\u5347\u4e86\u97f3\u9891\u9a71\u52a8\u9762\u90e8\u52a8\u753b\u7684\u6027\u80fd\u3002", "conclusion": "StyGazeTalk\u65b9\u6cd5\u53ca\u5176\u5f15\u5165\u7684\u591a\u6a21\u6001\u6570\u636e\u96c6\u4e3a\u5934\u90e8\u548c\u6ce8\u89c6\u63a7\u5236\u6a21\u578b\u7684\u7814\u7a76\u63d0\u4f9b\u4e86\u91cd\u8981\u8d44\u6e90\uff0c\u63a8\u52a8\u4e86\u97f3\u9891\u9a71\u52a8\u9762\u90e8\u52a8\u753b\u6280\u672f\u7684\u8fdb\u6b65\u3002"}}
{"id": "2509.17212", "categories": ["cs.GR", "cs.CV"], "pdf": "https://arxiv.org/pdf/2509.17212", "abs": "https://arxiv.org/abs/2509.17212", "authors": ["Federico Stella", "Nicolas Talabot", "Hieu Le", "Pascal Fua"], "title": "High Resolution UDF Meshing via Iterative Networks", "comment": "Accepted at NeurIPS 2025", "summary": "Unsigned Distance Fields (UDFs) are a natural implicit representation for\nopen surfaces but, unlike Signed Distance Fields (SDFs), are challenging to\ntriangulate into explicit meshes. This is especially true at high resolutions\nwhere neural UDFs exhibit higher noise levels, which makes it hard to capture\nfine details. Most current techniques perform within single voxels without\nreference to their neighborhood, resulting in missing surface and holes where\nthe UDF is ambiguous or noisy. We show that this can be remedied by performing\nseveral passes and by reasoning on previously extracted surface elements to\nincorporate neighborhood information. Our key contribution is an iterative\nneural network that does this and progressively improves surface recovery\nwithin each voxel by spatially propagating information from increasingly\ndistant neighbors. Unlike single-pass methods, our approach integrates newly\ndetected surfaces, distance values, and gradients across multiple iterations,\neffectively correcting errors and stabilizing extraction in challenging\nregions. Experiments on diverse 3D models demonstrate that our method produces\nsignificantly more accurate and complete meshes than existing approaches,\nparticularly for complex geometries, enabling UDF surface extraction at higher\nresolutions where traditional methods fail.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u8fed\u4ee3\u795e\u7ecf\u7f51\u7edc\u65b9\u6cd5\uff0c\u901a\u8fc7\u591a\u6b21\u4f20\u9012\u548c\u90bb\u5c45\u4fe1\u606f\u63a8\u7406\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u5f00\u653e\u8868\u9762\u65e0\u7b26\u53f7\u8ddd\u79bb\u573a\uff08UDF\uff09\u7684\u4e09\u89d2\u7f51\u683c\u751f\u6210\u7cbe\u5ea6\u548c\u5b8c\u6574\u6027\u3002", "motivation": "\u65e0\u7b26\u53f7\u8ddd\u79bb\u573a\uff08UDF\uff09\u662f\u5f00\u653e\u8868\u9762\u7684\u81ea\u7136\u9690\u5f0f\u8868\u793a\uff0c\u4f46\u5728\u9ad8\u5206\u8fa8\u7387\u4e0b\u56e0\u5176\u566a\u58f0\u6c34\u5e73\u8f83\u9ad8\uff0c\u96be\u4ee5\u6355\u6349\u7ec6\u8282\uff0c\u4e14\u73b0\u6709\u65b9\u6cd5\u901a\u5e38\u5728\u5355\u4e2a\u4f53\u7d20\u5185\u5de5\u4f5c\uff0c\u5bfc\u81f4\u8868\u9762\u7f3a\u5931\u548c\u5b54\u6d1e\u3002", "method": "\u901a\u8fc7\u591a\u6b21\u8fed\u4ee3\u795e\u7ecf\u7f51\u7edc\uff0c\u6574\u5408\u65b0\u68c0\u6d4b\u7684\u8868\u9762\u3001\u8ddd\u79bb\u503c\u548c\u68af\u5ea6\u4fe1\u606f\uff0c\u9010\u6b65\u6539\u8fdb\u6bcf\u4e2a\u4f53\u7d20\u5185\u7684\u8868\u9762\u6062\u590d\uff0c\u5e76\u901a\u8fc7\u7a7a\u95f4\u4f20\u64ad\u90bb\u5c45\u4fe1\u606f\u7ea0\u6b63\u9519\u8bef\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u591a\u6837\u5316\u76843D\u6a21\u578b\u4e0a\u663e\u8457\u63d0\u9ad8\u4e86\u7f51\u683c\u7684\u51c6\u786e\u6027\u548c\u5b8c\u6574\u6027\uff0c\u5c24\u5176\u5728\u590d\u6742\u51e0\u4f55\u7ed3\u6784\u548c\u9ad8\u5206\u8fa8\u7387\u60c5\u51b5\u4e0b\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "\u63d0\u51fa\u7684\u8fed\u4ee3\u65b9\u6cd5\u6709\u6548\u5730\u89e3\u51b3\u4e86UDF\u4e09\u89d2\u7f51\u683c\u751f\u6210\u7684\u6311\u6218\uff0c\u9002\u7528\u4e8e\u9ad8\u5206\u8fa8\u7387\u573a\u666f\uff0c\u5f25\u8865\u4e86\u4f20\u7edf\u65b9\u6cd5\u7684\u4e0d\u8db3\u3002"}}
{"id": "2509.17748", "categories": ["cs.GR", "cs.HC"], "pdf": "https://arxiv.org/pdf/2509.17748", "abs": "https://arxiv.org/abs/2509.17748", "authors": ["Siyi Liu", "Kazi Injamamul Haque", "Zerrin Yumak"], "title": "\"I don't like my avatar\": Investigating Human Digital Doubles", "comment": "pre-print, 12 papges, accepted at ACM Siggraph Motion, Interaction\n  and Games 2025 (MIG 2025) conference", "summary": "Creating human digital doubles is becoming easier and much more accessible to\neveryone using consumer grade devices. In this work, we investigate how avatar\nstyle (realistic vs cartoon) and avatar familiarity (self, acquaintance,\nunknown person) affect self/other-identification, perceived realism, affinity\nand social presence with a controlled offline experiment. We created two styles\nof avatars (realistic-looking MetaHumans and cartoon-looking ReadyPlayerMe\navatars) and facial animations stimuli for them using performance capture.\nQuestionnaire responses demonstrate that higher appearance realism leads to a\nhigher level of identification, perceived realism and social presence. However,\navatars with familiar faces, especially those with high appearance realism,\nlead to a lower level of identification, perceived realism, and affinity.\nAlthough participants identified their digital doubles as their own, they\nconsistently did not like their avatars, especially of realistic appearance.\nBut they were less critical and more forgiving about their acquaintance's or an\nunknown person's digital double.", "AI": {"tldr": "\u7814\u7a76\u8868\u660e\uff0c\u9ad8\u5ea6\u771f\u5b9e\u7684\u6570\u5b57\u66ff\u8eab\u66f4\u5bb9\u6613\u88ab\u8bc6\u522b\u548c\u611f\u77e5\u4e3a\u771f\u5b9e\uff0c\u4f46\u719f\u6089\u7684\u9762\u5b54\uff08\u5c24\u5176\u662f\u9ad8\u771f\u5b9e\u6027\u66ff\u8eab\uff09\u4f1a\u964d\u4f4e\u8fd9\u4e9b\u611f\u53d7\u3002", "motivation": "\u7814\u7a76\u6570\u5b57\u66ff\u8eab\u7684\u98ce\u683c\uff08\u771f\u5b9evs\u5361\u901a\uff09\u548c\u719f\u6089\u5ea6\uff08\u81ea\u5df1\u3001\u719f\u4eba\u3001\u964c\u751f\u4eba\uff09\u5982\u4f55\u5f71\u54cd\u81ea\u6211/\u4ed6\u4eba\u8bc6\u522b\u3001\u771f\u5b9e\u611f\u3001\u4eb2\u548c\u529b\u53ca\u793e\u4ea4\u5b58\u5728\u611f\u3002", "method": "\u901a\u8fc7\u79bb\u7ebf\u5b9e\u9a8c\uff0c\u4f7f\u7528MetaHumans\u548cReadyPlayerMe\u5206\u522b\u521b\u5efa\u771f\u5b9e\u548c\u5361\u901a\u98ce\u683c\u7684\u66ff\u8eab\uff0c\u5e76\u5229\u7528\u52a8\u4f5c\u6355\u6349\u751f\u6210\u9762\u90e8\u52a8\u753b\uff0c\u901a\u8fc7\u95ee\u5377\u8c03\u67e5\u6536\u96c6\u6570\u636e\u3002", "result": "\u9ad8\u5ea6\u771f\u5b9e\u7684\u66ff\u8eab\u63d0\u5347\u4e86\u8bc6\u522b\u5ea6\u3001\u771f\u5b9e\u611f\u548c\u793e\u4ea4\u5b58\u5728\u611f\uff0c\u4f46\u719f\u6089\u9762\u5b54\uff08\u5c24\u5176\u662f\u9ad8\u771f\u5b9e\u6027\u66ff\u8eab\uff09\u964d\u4f4e\u4e86\u8fd9\u4e9b\u611f\u53d7\u3002\u53c2\u4e0e\u8005\u666e\u904d\u4e0d\u559c\u6b22\u81ea\u5df1\u7684\u771f\u5b9e\u66ff\u8eab\uff0c\u4f46\u5bf9\u719f\u4eba\u6216\u964c\u751f\u4eba\u7684\u66ff\u8eab\u66f4\u5bbd\u5bb9\u3002", "conclusion": "\u6570\u5b57\u66ff\u8eab\u7684\u771f\u5b9e\u6027\u548c\u719f\u6089\u5ea6\u5bf9\u7528\u6237\u4f53\u9a8c\u6709\u590d\u6742\u5f71\u54cd\uff0c\u771f\u5b9e\u98ce\u683c\u867d\u589e\u5f3a\u8bc6\u522b\u548c\u5b58\u5728\u611f\uff0c\u4f46\u719f\u6089\u9762\u5b54\u53ef\u80fd\u5e26\u6765\u8d1f\u9762\u60c5\u7eea\u3002"}}
{"id": "2509.17803", "categories": ["cs.GR", "cs.HC"], "pdf": "https://arxiv.org/pdf/2509.17803", "abs": "https://arxiv.org/abs/2509.17803", "authors": ["Nabila Amadou", "Kazi Injamamul Haque", "Zerrin Yumak"], "title": "Effect of Appearance and Animation Realism on the Perception of Emotionally Expressive Virtual Humans", "comment": "pre-print, 8 pages, accepted at ACM International Conference on\n  Intelligent Virtual Agents 2023 (IVA 2023)", "summary": "3D Virtual Human technology is growing with several potential applications in\nhealth, education, business and telecommunications. Investigating the\nperception of these virtual humans can help guide to develop better and more\neffective applications. Recent developments show that the appearance of the\nvirtual humans reached to a very realistic level. However, there is not yet\nadequate analysis on the perception of appearance and animation realism for\nemotionally expressive virtual humans. In this paper, we designed a user\nexperiment and analyzed the effect of a realistic virtual human's appearance\nrealism and animation realism in varying emotion conditions. We found that\nhigher appearance realism and higher animation realism leads to higher social\npresence and higher attractiveness ratings. We also found significant effects\nof animation realism on perceived realism and emotion intensity levels. Our\nstudy sheds light into how appearance and animation realism effects the\nperception of highly realistic virtual humans in emotionally expressive\nscenarios and points out to future directions.", "AI": {"tldr": "\u7814\u7a76\u4e863D\u865a\u62df\u4eba\u7c7b\u7684\u611f\u77e5\u6548\u679c\uff0c\u53d1\u73b0\u66f4\u9ad8\u7684\u5916\u89c2\u548c\u52a8\u753b\u771f\u5b9e\u611f\u53ef\u4ee5\u63d0\u9ad8\u793e\u4ea4\u5b58\u5728\u611f\u548c\u5438\u5f15\u529b\u3002", "motivation": "\u865a\u62df\u4eba\u7c7b\u6280\u672f\u5728\u591a\u4e2a\u9886\u57df\u6709\u6f5c\u5728\u5e94\u7528\uff0c\u4f46\u76ee\u524d\u7f3a\u4e4f\u5bf9\u5176\u5916\u89c2\u548c\u52a8\u753b\u771f\u5b9e\u611f\u7684\u60c5\u611f\u8868\u8fbe\u611f\u77e5\u7814\u7a76\u3002", "method": "\u8bbe\u8ba1\u4e86\u7528\u6237\u5b9e\u9a8c\uff0c\u5206\u6790\u4e0d\u540c\u60c5\u611f\u6761\u4ef6\u4e0b\u5916\u89c2\u548c\u52a8\u753b\u771f\u5b9e\u611f\u5bf9\u611f\u77e5\u7684\u5f71\u54cd\u3002", "result": "\u9ad8\u5916\u89c2\u548c\u52a8\u753b\u771f\u5b9e\u611f\u63d0\u5347\u4e86\u793e\u4ea4\u5b58\u5728\u611f\u548c\u5438\u5f15\u529b\uff1b\u52a8\u753b\u771f\u5b9e\u611f\u663e\u8457\u5f71\u54cd\u611f\u77e5\u771f\u5b9e\u611f\u548c\u60c5\u611f\u5f3a\u5ea6\u3002", "conclusion": "\u7814\u7a76\u63ed\u793a\u4e86\u9ad8\u771f\u5b9e\u611f\u865a\u62df\u4eba\u7c7b\u5728\u60c5\u611f\u8868\u8fbe\u4e2d\u7684\u611f\u77e5\u6548\u679c\uff0c\u4e3a\u672a\u6765\u53d1\u5c55\u65b9\u5411\u63d0\u4f9b\u4e86\u6307\u5bfc\u3002"}}
{"id": "2509.17974", "categories": ["cs.GR"], "pdf": "https://arxiv.org/pdf/2509.17974", "abs": "https://arxiv.org/abs/2509.17974", "authors": ["Son Le Thanh", "Tino Weinkauf"], "title": "A Comparative Study of Different Edit Distance-Based Methods for Feature Tracking using Merge Trees on Time-Varying Scalar Fields", "comment": null, "summary": "Feature tracking in time-varying scalar fields is a fundamental task in\nscientific computing. Topological descriptors, which summarize important\nfeatures of data, have proved to be viable tools to facilitate this task. The\nmerge tree is a topological descriptor that captures the connectivity behaviors\nof the sub- or superlevel sets of a scalar field. Edit distances between merge\ntrees play a vital role in effective temporal data tracking. Existing methods\nto compute them fall into two main classes, namely whether they are dependent\nor independent of the branch decomposition. These two classes represent the\nmost prominent approaches for producing tracking results. In this paper, we\ncompare four different merge tree edit distance-based methods for feature\ntracking. We demonstrate that these methods yield distinct results with both\nanalytical and real-world data sets. Furthermore, we investigate how these\nresults vary and identify the factors that influence them. Our experiments\nreveal significant differences in tracked features over time, even among those\nproduced by techniques within the same category.", "AI": {"tldr": "\u8be5\u8bba\u6587\u6bd4\u8f83\u4e86\u56db\u79cd\u57fa\u4e8e\u5408\u5e76\u6811\u7f16\u8f91\u8ddd\u79bb\u7684\u7279\u5f81\u8ddf\u8e2a\u65b9\u6cd5\uff0c\u5c55\u793a\u4e86\u5b83\u4eec\u5728\u5206\u6790\u548c\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\u7684\u4e0d\u540c\u8868\u73b0\uff0c\u5e76\u63a2\u8ba8\u4e86\u7ed3\u679c\u5dee\u5f02\u7684\u5f71\u54cd\u56e0\u7d20\u3002", "motivation": "\u65f6\u95f4\u53d8\u5316\u6807\u91cf\u573a\u4e2d\u7684\u7279\u5f81\u8ddf\u8e2a\u662f\u79d1\u5b66\u8ba1\u7b97\u4e2d\u7684\u57fa\u7840\u4efb\u52a1\uff0c\u5408\u5e76\u6811\u4f5c\u4e3a\u62d3\u6251\u63cf\u8ff0\u7b26\u5728\u6b64\u4efb\u52a1\u4e2d\u5177\u6709\u91cd\u8981\u4f5c\u7528\u3002\u8bba\u6587\u65e8\u5728\u6bd4\u8f83\u4e0d\u540c\u7f16\u8f91\u8ddd\u79bb\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002", "method": "\u8bba\u6587\u91c7\u7528\u4e86\u56db\u79cd\u57fa\u4e8e\u5408\u5e76\u6811\u7f16\u8f91\u8ddd\u79bb\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u5206\u6790\u548c\u771f\u5b9e\u6570\u636e\u96c6\u6bd4\u8f83\u5b83\u4eec\u7684\u8868\u73b0\uff0c\u5e76\u5206\u6790\u4e86\u7ed3\u679c\u5dee\u5f02\u7684\u539f\u56e0\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0c\u5373\u4f7f\u662f\u540c\u4e00\u7c7b\u522b\u7684\u65b9\u6cd5\uff0c\u5176\u8ffd\u8e2a\u7279\u5f81\u7684\u7ed3\u679c\u4e5f\u5b58\u5728\u663e\u8457\u5dee\u5f02\u3002", "conclusion": "\u7814\u7a76\u8868\u660e\uff0c\u4e0d\u540c\u5408\u5e76\u6811\u7f16\u8f91\u8ddd\u79bb\u65b9\u6cd5\u5728\u7279\u5f81\u8ffd\u8e2a\u4e2d\u8868\u73b0\u51fa\u663e\u8457\u5dee\u5f02\uff0c\u8fd9\u4e9b\u5dee\u5f02\u53d7\u591a\u79cd\u56e0\u7d20\u5f71\u54cd\uff0c\u4e3a\u672a\u6765\u7814\u7a76\u63d0\u4f9b\u4e86\u91cd\u8981\u53c2\u8003\u3002"}}
{"id": "2509.17979", "categories": ["cs.GR", "cs.ET", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.17979", "abs": "https://arxiv.org/abs/2509.17979", "authors": ["Yiwen Song", "Hongyang Li", "Kuang Yuan", "Ran Bi", "Swarun Kumar"], "title": "Towards Seeing Bones at Radio Frequency", "comment": null, "summary": "Wireless sensing literature has long aspired to achieve X-ray-like vision at\nradio frequencies. Yet, state-of-the-art wireless sensing literature has yet to\ngenerate the archetypal X-ray image: one of the bones beneath flesh. In this\npaper, we explore MCT, a penetration-based RF-imaging system for imaging bones\nat mm-resolution, one that significantly exceeds prior penetration-based RF\nimaging literature. Indeed the long wavelength, significant attenuation and\ncomplex diffraction that occur as RF propagates through flesh, have long\nlimited imaging resolution (to several centimeters at best). We address these\nconcerns through a novel penetration-based synthetic aperture algorithm,\ncoupled with a learning-based pipeline to correct for diffraction-induced\nartifacts. A detailed evaluation of meat models demonstrates a resolution\nimprovement from sub-decimeter to sub-centimeter over prior art in RF\npenetrative imaging.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aMCT\u7684\u5c04\u9891\u6210\u50cf\u7cfb\u7edf\uff0c\u80fd\u591f\u5728\u6beb\u7c73\u5206\u8fa8\u7387\u4e0b\u6210\u50cf\u9aa8\u9abc\uff0c\u663e\u8457\u4f18\u4e8e\u4ee5\u5f80\u7684\u5c04\u9891\u7a7f\u900f\u6210\u50cf\u6280\u672f\u3002", "motivation": "\u65e0\u7ebf\u4f20\u611f\u9886\u57df\u957f\u671f\u4ee5\u6765\u5e0c\u671b\u901a\u8fc7\u5c04\u9891\u5b9e\u73b0\u7c7b\u4f3cX\u5c04\u7ebf\u7684\u89c6\u89c9\u80fd\u529b\uff0c\u4f46\u73b0\u6709\u6280\u672f\u96be\u4ee5\u6210\u50cf\u9aa8\u9abc\u3002\u672c\u6587\u65e8\u5728\u7a81\u7834\u8fd9\u4e00\u9650\u5236\uff0c\u5b9e\u73b0\u66f4\u9ad8\u5206\u8fa8\u7387\u7684\u5c04\u9891\u9aa8\u9abc\u6210\u50cf\u3002", "method": "\u901a\u8fc7\u7ed3\u5408\u57fa\u4e8e\u7a7f\u900f\u7684\u5408\u6210\u5b54\u5f84\u7b97\u6cd5\u548c\u5b66\u4e60\u578b\u884d\u5c04\u6821\u6b63\u6d41\u6c34\u7ebf\uff0c\u514b\u670d\u4e86\u5c04\u9891\u5728\u7a7f\u900f\u7ec4\u7ec7\u65f6\u7684\u957f\u6ce2\u957f\u3001\u8870\u51cf\u548c\u590d\u6742\u884d\u5c04\u95ee\u9898\u3002", "result": "\u5728\u8089\u7c7b\u6a21\u578b\u4e0a\u7684\u8be6\u7ec6\u8bc4\u4f30\u663e\u793a\uff0c\u5206\u8fa8\u7387\u4ece\u4e9a\u5206\u7c73\u63d0\u5347\u5230\u4e9a\u5398\u7c73\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u6280\u672f\u3002", "conclusion": "MCT\u7cfb\u7edf\u4e3a\u5c04\u9891\u7a7f\u900f\u6210\u50cf\u6280\u672f\u5e26\u6765\u4e86\u663e\u8457\u7684\u5206\u8fa8\u7387\u63d0\u5347\uff0c\u4e3a\u672a\u6765\u7684\u65e0\u7ebf\u4f20\u611f\u5e94\u7528\u5f00\u8f9f\u4e86\u65b0\u7684\u53ef\u80fd\u6027\u3002"}}
{"id": "2509.17985", "categories": ["cs.GR"], "pdf": "https://arxiv.org/pdf/2509.17985", "abs": "https://arxiv.org/abs/2509.17985", "authors": ["Geonung Kim", "Janghyeok Han", "Sunghyun Cho"], "title": "VideoFrom3D: 3D Scene Video Generation via Complementary Image and Video Diffusion Models", "comment": "Project page: https://kimgeonung.github.io/VideoFrom3D/", "summary": "In this paper, we propose VideoFrom3D, a novel framework for synthesizing\nhigh-quality 3D scene videos from coarse geometry, a camera trajectory, and a\nreference image. Our approach streamlines the 3D graphic design workflow,\nenabling flexible design exploration and rapid production of deliverables. A\nstraightforward approach to synthesizing a video from coarse geometry might\ncondition a video diffusion model on geometric structure. However, existing\nvideo diffusion models struggle to generate high-fidelity results for complex\nscenes due to the difficulty of jointly modeling visual quality, motion, and\ntemporal consistency. To address this, we propose a generative framework that\nleverages the complementary strengths of image and video diffusion models.\nSpecifically, our framework consists of a Sparse Anchor-view Generation (SAG)\nand a Geometry-guided Generative Inbetweening (GGI) module. The SAG module\ngenerates high-quality, cross-view consistent anchor views using an image\ndiffusion model, aided by Sparse Appearance-guided Sampling. Building on these\nanchor views, GGI module faithfully interpolates intermediate frames using a\nvideo diffusion model, enhanced by flow-based camera control and structural\nguidance. Notably, both modules operate without any paired dataset of 3D scene\nmodels and natural images, which is extremely difficult to obtain.\nComprehensive experiments show that our method produces high-quality,\nstyle-consistent scene videos under diverse and challenging scenarios,\noutperforming simple and extended baselines.", "AI": {"tldr": "VideoFrom3D\u662f\u4e00\u4e2a\u65b0\u9896\u6846\u67b6\uff0c\u901a\u8fc7\u7c97\u51e0\u4f55\u3001\u76f8\u673a\u8f68\u8ff9\u548c\u53c2\u8003\u56fe\u50cf\u751f\u6210\u9ad8\u8d28\u91cf3D\u573a\u666f\u89c6\u9891\uff0c\u7b80\u5316\u8bbe\u8ba1\u6d41\u7a0b\u5e76\u63d0\u5347\u751f\u6210\u6548\u679c\u3002", "motivation": "\u73b0\u6709\u89c6\u9891\u6269\u6563\u6a21\u578b\u5728\u751f\u6210\u590d\u6742\u573a\u666f\u7684\u9ad8\u4fdd\u771f\u89c6\u9891\u65f6\u8868\u73b0\u4e0d\u4f73\uff0c\u96be\u4ee5\u540c\u65f6\u6ee1\u8db3\u89c6\u89c9\u8d28\u91cf\u3001\u8fd0\u52a8\u548c\u65f6\u5e8f\u4e00\u81f4\u6027\u7684\u8981\u6c42\u3002", "method": "\u7ed3\u5408\u56fe\u50cf\u548c\u89c6\u9891\u6269\u6563\u6a21\u578b\u7684\u4e92\u8865\u4f18\u52bf\uff0c\u6846\u67b6\u5305\u62ec\u7a00\u758f\u951a\u70b9\u89c6\u56fe\u751f\u6210(SAG)\u548c\u51e0\u4f55\u5f15\u5bfc\u751f\u6210\u4e2d\u95f4\u5e27(GGI)\u6a21\u5757\u3002", "result": "\u7efc\u5408\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u591a\u6837\u5316\u548c\u6311\u6218\u6027\u573a\u666f\u4e0b\u751f\u6210\u9ad8\u8d28\u91cf\u3001\u98ce\u683c\u4e00\u81f4\u7684\u89c6\u9891\uff0c\u4f18\u4e8e\u57fa\u7ebf\u548c\u6269\u5c55\u57fa\u7ebf\u3002", "conclusion": "VideoFrom3D\u6846\u67b6\u65e0\u9700\u6210\u5bf9\u76843D\u573a\u666f\u6a21\u578b\u548c\u81ea\u7136\u56fe\u50cf\u6570\u636e\u96c6\uff0c\u4fbf\u80fd\u9ad8\u6548\u751f\u6210\u9ad8\u8d28\u91cf3D\u573a\u666f\u89c6\u9891\uff0c\u5177\u6709\u5b9e\u9645\u5e94\u7528\u4ef7\u503c\u3002"}}
