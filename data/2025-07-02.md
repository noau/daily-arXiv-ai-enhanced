<div id=toc></div>

# Table of Contents

- [cs.DM](#cs.DM) [Total: 5]
- [cs.DS](#cs.DS) [Total: 4]
- [cs.FL](#cs.FL) [Total: 1]
- [cs.GR](#cs.GR) [Total: 4]
- [cs.GT](#cs.GT) [Total: 1]
- [cs.LO](#cs.LO) [Total: 1]
- [cs.MS](#cs.MS) [Total: 1]
- [cs.PL](#cs.PL) [Total: 3]
- [math.CT](#math.CT) [Total: 2]
- [math.GM](#math.GM) [Total: 8]
- [math.LO](#math.LO) [Total: 1]
- [math.RT](#math.RT) [Total: 4]


<div id='cs.DM'></div>

# cs.DM [[Back]](#toc)

### [1] [Reducing Profile-Based Matching to the Maximum Weight Matching Problem](https://arxiv.org/abs/2507.00047)
*Seongbeom Park*

Main category: cs.DM

TL;DR: 该论文研究了基于配置文件的匹配问题，提出了一种将问题转化为最大权重匹配问题的方法，并展示了其在现实数据中的应用效果。


<details>
  <summary>Details</summary>
Motivation: 基于配置文件的匹配问题在多种实际应用中有重要意义，如排名最大化匹配、公平匹配和权重最大化匹配。然而，现有方法因权重过大而导致效率低下，急需一种更高效的解决方案。

Method: 论文提出了一种方法，通过将效用函数表示为基于基数为$(2U_i+1)$的混合基数系统，将基于配置文件的匹配问题转化为最大权重匹配问题，并分析了其计算复杂度。

Result: 结果表明，该方法的时间复杂度为$O(m\sqrt{n}(\log{n} + \sum_{i=1}^{r}\log{U_i}))$，同时证明了在排名最大化、公平匹配和权重最大化匹配中的权重下限，并展示了算法的实际应用效果。

Conclusion: 论文提出的方法不仅能高效解决基于配置文件的匹配问题，还为排名最大化匹配等具体问题提供了更好的计算复杂度和验证算法，实际应用验证了其有效性。

Abstract: The profile-based matching problem is the problem of finding a matching that
optimizes profile from an instance $(G, r, \langle u_1, \dots, u_r \rangle)$,
where $G$ is a bipartite graph $(A \cup B, E)$, $r$ is the number of utility
functions, and $u_i: E \to \{ 0, 1, \dots, U_i \}$ is utility functions for $1
\le i \le r$. A matching is optimal if the matching maximizes the sum of the
1st utility, subject to this, maximizes the sum of the 2nd utility, and so on.
The profile-based matching can express rank-maximal matching
\cite{irving2006rank}, fair matching \cite{huang2016fair}, and weight-maximal
matching \cite{huang2012weight}. These problems can be reduced to maximum
weight matching problems, but the reduction is known to be inefficient due to
the huge weights.
  This paper presents the condition for a weight function to find an optimal
matching by reducing profile-based matching to the maximum weight matching
problem. It is shown that a weight function which represents utilities as a
mixed-radix numeric system with base-$(2U_i+1)$ can be used, so the complexity
of the problem is $O(m\sqrt{n}(\log{n} + \sum_{i=1}^{r}\log{U_i}))$ for $n =
|V|$, $m = |E|$. In addition, it is demonstrated that the weight lower bound
for rank-maximal/fair/weight-maximal matching, better computational complexity
for fair/weight-maximal matching, and an algorithm to verify a maximum weight
matching can be reduced to rank-maximal matching. Finally, the effectiveness of
the profile-based algorithm is evaluated with real data for school choice
lottery.

</details>


### [2] [Verification of Hamiltonian Path Conjecture (BHR Conjecture) for Integers up to p=31](https://arxiv.org/abs/2507.00059)
*Ranjan N Naik*

Main category: cs.DM

TL;DR: 该论文提出了通过频率分区、局部/全局调整操作和回溯策略来解决BHR猜想的思路，并通过Python程序验证了p<37时的哈密顿路径存在性。


<details>
  <summary>Details</summary>
Motivation: BHR猜想在2006年提出，研究了完全图中是否存在满足特定条件的哈密顿路径。论文旨在通过数学策略和计算工具验证并扩展这一猜想。

Method: 论文采用频率分区、局部/全局调整操作和回溯的方法，并实现了一个Python程序来探索p<37时的有效哈密顿路径。

Result: 研究结果表明，对于所有小于37的素数，存在满足BHR猜想的哈密顿路径，比Mariusz Meszka先前的结果（p≤23）有所改进。

Conclusion: 通过数学策略和计算工具的结合，论文验证了BHR猜想在更大范围内的适用性，为未来研究提供了新的思路和数据支持。

Abstract: The BHR (Buratti-Horak-Rosa) Conjecture (2006) proposes that for every p and
a multiset L of (p-1) positive integers modulo p, there exists a Hamiltonian
path in the Complete Graph Kp with consecutive edge lengths given by the
elements of L. In this article, we outline an approach to the conjecture based
on frequency partitions and local/global adjustment operations and
backtracking. We describe the mathematical strategy, experimental evidence, and
implementation in a Python Program to explore valid Hamiltonian paths p < 37.
This is a result an improvement over by Mariusz Meszka for all primes up to 23
(included) with the aid of a computer.

</details>


### [3] [$σ$-Maximal Ancestral Graphs](https://arxiv.org/abs/2507.00093)
*Binghua Yao,Joris M. Mooij*

Main category: cs.DM

TL;DR: 论文提出了σ-最大祖先图（σ-MAGs），扩展了传统MAGs以表示含潜在变量的有向环图（DGs），解决了MAGs无法表达循环因果关系的限制。


<details>
  <summary>Details</summary>
Motivation: 传统的最大祖先图（MAGs）仅能表示有向无环图（DAGs），无法处理循环因果关系的存在。为解决这一限制，作者提出了σ-MAGs。

Method: 作者引入并研究了一类新的图形对象——σ-MAGs，它能够表示含潜在变量的有向图（DGs），包括循环关系。

Result: 论文证明了σ-MAGs的性质，并对其马尔可夫等价类进行了特征化描述。

Conclusion: σ-MAGs为含潜在变量的循环有向图提供了一种抽象表示方法，扩展了MAGs的应用范围。

Abstract: Maximal Ancestral Graphs (MAGs) provide an abstract representation of
Directed Acyclic Graphs (DAGs) with latent (selection) variables. These
graphical objects encode information about ancestral relations and
d-separations of the DAGs they represent. This abstract representation has been
used amongst others to prove the soundness and completeness of the FCI
algorithm for causal discovery, and to derive a do-calculus for its output. One
significant inherent limitation of MAGs is that they rule out the possibility
of cyclic causal relationships. In this work, we address that limitation. We
introduce and study a class of graphical objects that we coin
''$\sigma$-Maximal Ancestral Graphs'' (''$\sigma$-MAGs''). We show how these
graphs provide an abstract representation of (possibly cyclic) Directed Graphs
(DGs) with latent (selection) variables, analogously to how MAGs represent
DAGs. We study the properties of these objects and provide a characterization
of their Markov equivalence classes.

</details>


### [4] [Computational complexity of covering regular trees](https://arxiv.org/abs/2507.00564)
*Jan Bok,Jiří Fiala,Nikola Jedličková,Jan Kratochvíl*

Main category: cs.DM

TL;DR: 本文研究了图形覆盖投影问题的计算复杂性，特别是针对半边边的正则图，证明了这类问题通常是NP完全的。


<details>
  <summary>Details</summary>
Motivation: 图形覆盖投影在拓扑图论、组合数学和理论计算机科学中有广泛应用，但其计算复杂性尚未完全明确，特别是半边边的作用。本文旨在填补这一空白。

Method: 通过将半边边添加到树的顶点上，构造d-正则图（d≥3），并研究其覆盖问题的计算复杂性。

Result: 证明了对于通过添加半边边构造的d-正则图，图形覆盖问题是NP完全的，且这一结果适用于简单输入图。

Conclusion: 本文进一步支持了"强二分猜想"，表明即使在简单输入图的情况下，图形覆盖问题仍然是NP难的。

Abstract: A graph covering projection, also referred to as a locally bijective
homomorphism, is a mapping between the vertices and edges of two graphs that
preserves incidences and is a local bijection. This concept originates in
topological graph theory but has also found applications in combinatorics and
theoretical computer science. In this paper we consider undirected graphs in
the most general setting -- graphs may contain multiple edges, loops, and
semi-edges. This is in line with recent trends in topological graph theory and
mathematical physics.
  We advance the study of the computational complexity of the {\sc $H$-Cover}
problem, which asks whether an input graph allows a covering projection onto a
parameter graph $H$. The quest for a complete characterization started in
1990's. Several results for simple graphs or graphs without semi-edges have
been known, the role of semi-edges in the complexity setting has started to be
investigated only recently. One of the most general known NP-hardness results
states that {\sc $H$}-Cover is NP-complete for every simple connected regular
graph of valency greater than two. We complement this result by considering
regular graphs $H$ arising from connected acyclic graphs by adding semi-edges.
Namely, we prove that any graph obtained by adding semi-edges to the vertices
of a tree making it a $d$-regular graph with $d \geq 3$, defines an NP-complete
graph covering problem. In line with the so called Strong Dichotomy Conjecture,
we prove that the NP-hardness holds even for simple graphs on input.

</details>


### [5] [Temporal Orienteering with Changing Fuel Costs](https://arxiv.org/abs/2507.00728)
*Timothée Corsini,Jessica Enright,Laura Larios-Jones,Kitty Meeks*

Main category: cs.DM

TL;DR: 该论文研究了Orienteering问题的变体，其中边的成本取决于出发和到达时间，并引入了时间图的广义模型。问题已被证明是NP完全的，但通过三种限制条件可以实现高效算法。


<details>
  <summary>Details</summary>
Motivation: 研究这一问题的动机源于实际应用，如轨道物体间的旅行，其中燃料成本与出发时间和旅行时间相关。需要一种新的时间图模型来精确描述此类动态成本问题。

Method: 论文引入了时间图的一种广义模型，并将问题定义为NP完全问题。通过三种输入限制条件（边的使用次数、顶点间隔成员宽度类似物以及访问站点数量）来探索高效算法。

Result: 结果表明，标准问题具有NP完全性，但在三种限制条件下（如边的使用次数有限、顶点间隔成员宽度受限或需访问的站点数量较少），可以设计出高效算法。

Conclusion: 论文提出的时间图广义模型具有独立的理论价值，且通过特定限制条件可以将复杂的动态成本问题转化为可高效求解的问题，适用于实际应用场景。

Abstract: The problem Orienteering asks whether there exists a walk which visits a
number of sites without exceeding some fuel budget. In the variant of the
problem we consider, the cost of each edge in the walk is dependent on the time
we depart one endpoint and the time we arrive at the other endpoint. This
mirrors applications such as travel between orbiting objects where fuel costs
are dependent on both the departure time and the length of time spent
travelling. In defining this problem, we introduce a natural generalisation of
the standard notion of temporal graphs: the pair consisting of the graph of the
sites and a cost function, in which costs as well as shortest travel times
between pairs of objects change over time. We believe this model is likely to
be of independent interest. The problem of deciding whether a stated goal is
feasible is easily seen to be NP-complete; we investigate three different ways
to restrict the input which lead to efficient algorithms. These include the
number of times an edge can be used, an analogue of vertex-interval-membership
width, and the number of sites to be visited.

</details>


<div id='cs.DS'></div>

# cs.DS [[Back]](#toc)

### [6] [A Simple Algorithm for Trimmed Multipoint Evaluation](https://arxiv.org/abs/2507.00196)
*Nick Fischer,Melvin Kallmayer,Leo Wennmann*

Main category: cs.DS

TL;DR: 本文提出了一种简单的递归算法来解决修剪多点评估问题，避免了复杂的计算机代数工具，易于理解。


<details>
  <summary>Details</summary>
Motivation: 修剪多点评估是计算机代数中的基本任务，最近成为某些算法结果的关键子程序，但现有解决方案过于复杂。

Method: 提出了一种简单的递归算法，避免了复杂的计算机代数工具，适用于研究人员无需专业背景即可理解。

Result: 该算法解决了修剪多点评估问题，简化了现有方法，同时保持了近线性时间复杂度。

Conclusion: 本文的递归算法为修剪多点评估问题提供了一种易于理解和实现的新解决方案。

Abstract: Evaluating a polynomial on a set of points is a fundamental task in computer
algebra. In this work, we revisit a particular variant called trimmed
multipoint evaluation: given an $n$-variate polynomial with bounded individual
degree $d$ and total degree $D$, the goal is to evaluate it on a natural class
of input points. This problem arises as a key subroutine in recent algorithmic
results [Dinur; SODA '21], [Dell, Haak, Kallmayer, Wennmann; SODA '25]. It is
known that trimmed multipoint evaluation can be solved in near-linear time [van
der Hoeven, Schost; AAECC '13] by a clever yet somewhat involved algorithm. We
give a simple recursive algorithm that avoids heavy computer-algebraic
machinery, and can be readily understood by researchers without specialized
background.

</details>


### [7] [Lazy B-Trees](https://arxiv.org/abs/2507.00277)
*Casper Moldrup Rysgaard,Sebastian Wild*

Main category: cs.DS

TL;DR: 摘要介绍了懒惰B树的设计，它是一种适合外部内存的懒惰搜索树变体，能够根据实际使用情况自动调整性能，介于优先级队列和二叉搜索树之间。该结构克服了缺乏外部偏置搜索树的技术难题，并在某些情况下作为外部内存优先级队列表现出色。


<details>
  <summary>Details</summary>
Motivation: 懒惰搜索树（Lazy search trees）在不同操作间能够自动调整性能，但缺乏适合外部内存的变体。因此，研究者设计了懒惰B树，以扩展这种性能优势到外部内存环境。

Method: 研究者克服了缺乏外部偏置搜索树的技术难题，提出了懒惰B树的设计。该结构基于B树的优势，并通过新的构造方法实现了在外部内存环境下的高效操作。

Result: 懒惰B树在某些场景下（如外部内存优先级队列）表现出色，特别是在减少键和插入操作方面，性能优于已知的数据结构。

Conclusion: 懒惰B树是懒惰搜索树在外部内存环境下的成功扩展，克服了关键技术难题，并在特定应用中展现出显著优势。

Abstract: Lazy search trees (Sandlund & Wild FOCS 2020, Sandlund & Zhang SODA 2022) are
sorted dictionaries whose update and query performance smoothly interpolates
between that of efficient priority queues and binary search trees -
automatically, depending on actual use; no adjustments are necessary to the
data structure to realize the cost savings. In this paper, we design lazy
B-trees, a variant of lazy search trees suitable for external memory that
generalizes the speedup of B-trees over binary search trees wrt. input/output
operations to the same smooth interpolation regime.
  A key technical difficulty to overcome is the lack of a (fully satisfactory)
external variant of biased search trees, on which lazy search trees crucially
rely. We give a construction for a subset of performance guarantees sufficient
to realize external-memory lazy search trees, which we deem of independent
interest.
  As one special case, lazy B-trees can be used as an external-memory priority
queue, in which case they are competitive with some tailor-made heaps; indeed,
they offer faster decrease-key and insert operations than known data
structures.

</details>


### [8] [On the (In)Approximability of the Monitoring Edge Geodetic Set Problem](https://arxiv.org/abs/2507.00708)
*Davide Bilò,Giodano Colli,Luca Forlizzi,Stefano Leucci*

Main category: cs.DS

TL;DR: 论文研究了最小监控边测地集问题，证明了其非恒定不可逼近性，并提供了在特定图类上的高效近似算法。


<details>
  <summary>Details</summary>
Motivation: 研究最小监控边测地集问题的动机源于其在图论中的基础性以及实际应用中的重要性，尤其是在网络监控和路径分析领域。

Method: 通过理论分析证明了问题的不可逼近性，并针对特定图类（如平面图和有界树宽图）设计了近似算法。

Result: 证明了除非P=NP，否则最小监控边测地集问题无法在多项式时间内获得优于对数级的近似比，同时提供了在特定图类上的高效近似解。

Conclusion: 论文为最小监控边测地集问题提供了重要的理论基础和实用算法，但平面图的NP难性仍为未解决问题。

Abstract: We study the minimum \emph{Monitoring Edge Geodetic Set} (\megset) problem
introduced in [Foucaud et al., CALDAM'23]: given a graph $G$, we say that an
edge is monitored by a pair $u,v$ of vertices if \emph{all} shortest paths
between $u$ and $v$ traverse $e$; the goal of the problem consists in finding a
subset $M$ of vertices of $G$ such that each edge of $G$ is monitored by at
least one pair of vertices in $M$, and $|M|$ is minimized.
  In this paper, we prove that all polynomial-time approximation algorithms for
the minimum \megset problem must have an approximation ratio of $\Omega(\log
n)$, unless \p = \np. To the best of our knowledge, this is the first
non-constant inapproximability result known for this problem. We also
strengthen the known \np-hardness of the problem on $2$-apex graphs by showing
that the same result holds for $1$-apex graphs. This leaves open the problem of
determining whether the problem remains \np-hard on planar (i.e., $0$-apex)
graphs.
  On the positive side, we design an algorithm that computes good approximate
solutions for hereditary graph classes that admit efficiently computable
balanced separators of truly sublinear size. This immediately results in
polynomial-time approximation algorithms achieving an approximation ratio of
$O(n^{\frac{1}{4}} \sqrt{\log n})$ on planar graphs, graphs with bounded genus,
and $k$-apex graphs with $k=O(n^{\frac{1}{4}})$. On graphs with bounded
treewidth, we obtain an approximation ratio of $O(\log^{3/2} n)$ for any
constant $\varepsilon > 0$. This compares favorably with the best-known
approximation algorithm for general graphs, which achieves an approximation
ratio of $O(\sqrt{n \log n})$ via a simple reduction to the \textsc{Set Cover}
problem.

</details>


### [9] [Inverse matroid optimization under subset constraints](https://arxiv.org/abs/2507.00930)
*Kristóf íBérczi,Lydia Mirabel Mendoza-Cadena,José Soto*

Main category: cs.DS

TL;DR: 论文研究了逆拟阵问题的六个变种，提出了在ℓ∞范数下的组合多项式时间算法，并通过改进的最小最大定理简化了计算。


<details>
  <summary>Details</summary>
Motivation: 研究逆拟阵问题的动机是为了解释或强制给定的解决方案，通过最小的扰动输入来实现目标基的最大权重。

Method: 通过改变初始权重函数并施加结构约束，研究了六个变种问题，并在ℓ∞范数下开发了组合多项式时间算法。

Result: 所有变种问题均可在多项式时间内解决，关键是一项改进的最小最大定理，简化了算法并提高了效率。

Conclusion: 该研究扩展了在拟阵上的逆优化问题的范围，尤其是那些通过子集包含或排除约束最优解结构的问题。

Abstract: In the Inverse Matroid problem, we are given a matroid, a fixed basis $B$,
and an initial weight function, and the goal is to minimally modify the weights
-- measured by some function -- so that $B$ becomes a maximum-weight basis. The
problem arises naturally in settings where one wishes to explain or enforce a
given solution by minimally perturbing the input.
  We extend this classical problem by replacing the fixed basis with a subset
$S_0$ of the ground set and imposing various structural constraints on the set
of maximum-weight bases relative to $S_0$. Specifically, we study six variants:
(A) Inverse Matroid Exists, where $S_0$ must contain at least one
maximum-weight basis; (B) Inverse Matroid All, where all bases contained in
$S_0$ are maximum-weight; and (C) Inverse Matroid Only, where $S_0$ contains
exactly the maximum-weight bases, along with their natural negated
counterparts.
  For all variants, we develop combinatorial polynomial-time algorithms under
the $\ell_\infty$-norm. A key ingredient is a refined min-max theorem for
Inverse Matroid under the $\ell_\infty$-norm, which enables simpler and faster
algorithms than previous approaches and may be of independent combinatorial
interest. Our work significantly broadens the range of inverse optimization
problems on matroids that can be solved efficiently, especially those that
constrain the structure of optimal solutions through subset inclusion or
exclusion.

</details>


<div id='cs.FL'></div>

# cs.FL [[Back]](#toc)

### [10] [Eilenberg correspondence for Stone recognition](https://arxiv.org/abs/2507.00409)
*Jorge Almeida,Ondřej Klíma*

Main category: cs.FL

TL;DR: 本文探讨了通过将语言（拓扑代数的子集）视为Stone拓扑代数中的闭开集的前像，建立了Eilenberg对应和Birkhoff/Reiterman型定理，扩展了框架并提出了显示语言不属于某种变体的通用方法。


<details>
  <summary>Details</summary>
Motivation: 研究如何通过连续同态将语言识别为Stone拓扑代数中的闭开集前像，扩展传统语言识别框架，特别是在正则语言的范畴之外。

Method: 通过建立Eilenberg对应和Birkhoff/Reiterman型定理，展示了有序Stone拓扑代数的变体可以通过伪不等式定义，并使用Stone完备性扩展了最小自动机的框架。

Result: 证明了该框架能够超越正则语言的范畴，并提出了通用方法用于显示语言不属于某个语言变体，特别是在处理上下文无关语言的有限交集时。

Conclusion: 通过Stone拓扑代数的理论，本文不仅扩展了语言识别的框架，还提供了新的工具和方法来研究语言的变体性质。

Abstract: We develop and explore the idea of recognition of languages (in the general
sense of subsets of topological algebras) as preimages of clopen sets under
continuous homomorphisms into Stone topological algebras. We obtain an
Eilenberg correspondence between varieties of languages and varieties of
ordered Stone topological algebras and a Birkhoff/Reiterman-type theorem
showing that the latter may me defined by certain pseudo-inequalities. In the
case of classical formal languages, of words over a finite alphabet, we also
show how this extended framework goes beyond the class of regular languages by
working with Stone completions of minimal automata, viewed as unary algebras.
This leads to a general method for showing that a language does not belong to a
variety of languages, expressed in terms of sequences of pairs of words, which
is illustrated when the class consists of all finite intersections of
context-free languages.

</details>


<div id='cs.GR'></div>

# cs.GR [[Back]](#toc)

### [11] [MVGBench: Comprehensive Benchmark for Multi-view Generation Models](https://arxiv.org/abs/2507.00006)
*Xianghui Xie,Chuhang Zou,Meher Gitika Karumuri,Jan Eric Lenssen,Gerard Pons-Moll*

Main category: cs.GR

TL;DR: 论文提出了MVGBench，一个用于评估多视图图像生成模型（MVGs）的全面基准测试，重点评估3D一致性、图像质量和语义理解，并引入了一种新的3D自一致性度量方法。


<details>
  <summary>Details</summary>
Motivation: 现有的MVGs评估方法通常将生成图像与真实目标视图进行比较，这在生成任务中并不适用，因为存在多种解决方案且与真实视图不同。此外，现有方法在泛化能力和鲁棒性方面缺乏系统评估。

Method: MVGBench通过评估最佳配置性能、泛化到真实数据的能力和鲁棒性三个方面，引入了一种新的3D自一致性度量方法，该方法通过比较来自不同生成多视图的3D重建来评估一致性。

Result: 系统比较了12种现有MVGs在4个不同数据集上的表现，发现现有方法在鲁棒性和泛化能力方面存在显著不足，并确定了关键设计选择。基于此，提出的ViFiGen在3D一致性方面优于所有评估的MVGs。

Conclusion: MVGBench为MVGs的评估提供了更全面的框架，ViFiGen展示了其优越性能，代码和基准测试套件将公开发布。

Abstract: We propose MVGBench, a comprehensive benchmark for multi-view image
generation models (MVGs) that evaluates 3D consistency in geometry and texture,
image quality, and semantics (using vision language models). Recently, MVGs
have been the main driving force in 3D object creation. However, existing
metrics compare generated images against ground truth target views, which is
not suitable for generative tasks where multiple solutions exist while
differing from ground truth. Furthermore, different MVGs are trained on
different view angles, synthetic data and specific lightings -- robustness to
these factors and generalization to real data are rarely evaluated thoroughly.
Without a rigorous evaluation protocol, it is also unclear what design choices
contribute to the progress of MVGs. MVGBench evaluates three different aspects:
best setup performance, generalization to real data and robustness. Instead of
comparing against ground truth, we introduce a novel 3D self-consistency metric
which compares 3D reconstructions from disjoint generated multi-views. We
systematically compare 12 existing MVGs on 4 different curated real and
synthetic datasets. With our analysis, we identify important limitations of
existing methods specially in terms of robustness and generalization, and we
find the most critical design choices. Using the discovered best practices, we
propose ViFiGen, a method that outperforms all evaluated MVGs on 3D
consistency. Our code, model, and benchmark suite will be publicly released.

</details>


### [12] [ViscoReg: Neural Signed Distance Functions via Viscosity Solutions](https://arxiv.org/abs/2507.00412)
*Meenakshi Krishnan,Ramani Duraiswami*

Main category: cs.GR

TL;DR: 论文提出了一种基于隐式神经表示（INR）和学习有符号距离函数（SDF）的连续3D场景重建方法，并引入了新的损失函数ViscoReg以提高训练的稳定性。


<details>
  <summary>Details</summary>
Motivation: 尽管Eikonal方程的不适定性，但通过神经SDF可以实现泛化误差估计。然而，使用Eikonal损失训练可能导致不稳定的梯度流，因此需要替代的稳定化技术。

Method: 论文利用粘度正则化的理论，改进了神经SDF的训练，提出了一种名为ViscoReg的新型损失函数。

Result: 理论分析证明了ViscoReg损失函数的梯度流稳定性，实验结果表明其优于SIREN、DiGS和StEik等先进方法，且未增加显著计算成本。

Conclusion: ViscoReg为神经SDF训练提供了一种稳定且高效的解决方案，适用于连续3D场景重建任务。

Abstract: Implicit Neural Representations (INRs) that learn a Signed Distance Function
(SDF) are a powerful tool for continuous 3D scene reconstruction. These models
are trained by enforcing the Eikonal equation. We demonstrate theoretically
that despite the ill-posedness of the Eikonal equation, generalization error
estimates may be obtained for Neural SDFs in terms of the training error.
However, training with the Eikonal loss can lead to unstable gradient flows,
necessitating alternate stabilization techniques. Traditional numerical solvers
for the equation have relied on viscosity approaches for regularization. We
enhance Neural SDF training using this well-developed theory, and introduce a
new loss formulation we call ViscoReg. We theoretically demonstrate the
stability of the gradient flow equation of our proposed loss term. Empirically,
ViscoReg outperforms state-of-the-art approaches such as SIREN, DiGS, and StEik
without adding significant computational cost.

</details>


### [13] [FreNBRDF: A Frequency-Rectified Neural Material Representation](https://arxiv.org/abs/2507.00476)
*Chenliang Zhou,Zheyuan Hu,Cengiz Oztireli*

Main category: cs.GR

TL;DR: 提出了一种基于频率校正的神经BRDF建模方法FreNBRDF，通过球谐函数在频域中优化材料表示，提高了重建和编辑的准确性与鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 为了克服现有神经材料表示在频域行为理解不足的问题，作者希望开发一种新的神经BRDF模型，以提高材料建模的准确性和实用性。

Method: 该方法利用球谐函数，将频域分析引入神经BRDF建模，并提出一种频率校正损失函数，结合到通用的重建和编辑流程中。

Result: 实验表明，FreNBRDF在材料外观重建和编辑上优于现有基准方法，提供了更高的保真度、适应性和效率。

Conclusion: FreNBRDF通过频域优化显著提升了神经BRDF模型的性能，为下游任务和应用提供了更结构化且可解释的框架。

Abstract: Accurate material modeling is crucial for achieving photorealistic rendering,
bridging the gap between computer-generated imagery and real-world photographs.
While traditional approaches rely on tabulated BRDF data, recent work has
shifted towards implicit neural representations, which offer compact and
flexible frameworks for a range of tasks. However, their behavior in the
frequency domain remains poorly understood. To address this, we introduce
FreNBRDF, a frequency-rectified neural material representation. By leveraging
spherical harmonics, we integrate frequency-domain considerations into neural
BRDF modeling. We propose a novel frequency-rectified loss, derived from a
frequency analysis of neural materials, and incorporate it into a generalizable
and adaptive reconstruction and editing pipeline. This framework enhances
fidelity, adaptability, and efficiency. Extensive experiments demonstrate that
\ours improves the accuracy and robustness of material appearance
reconstruction and editing compared to state-of-the-art baselines, enabling
more structured and interpretable downstream tasks and applications.

</details>


### [14] [Analyzing Time-Varying Scalar Fields using Piecewise-Linear Morse-Cerf Theory](https://arxiv.org/abs/2507.00725)
*Amritendu Dhar,Apratim Chakraborty,Vijay Natarajan*

Main category: cs.GR

TL;DR: 本文介绍了一种将Morse-Cerf理论应用于分段线性函数族的方法，提出了顶点图和Cerf图来表示关键点的演变，并引入了一种拓扑描述符用于时间变化的标量场。


<details>
  <summary>Details</summary>
Motivation: 研究如何将Morse-Cerf理论从光滑函数扩展到分段线性函数，以更好地理解和分析时间变化的标量场。

Method: 提出了顶点图和Cerf图作为表示分段线性函数关键点演变的工具，基于顶点下链同调的理论定义了拓扑描述符，并设计了计算Cerf图的算法。

Result: 实验结果表明，该方法能够有效地分析时间变化的标量场，并提供了比较两种Cerf图的度量标准。

Conclusion: 该方法为分析分段线性函数的关键点演变提供了新的理论工具和实践算法，扩展了Morse-Cerf理论的应用范围。

Abstract: Morse-Cerf theory considers a one-parameter family of smooth functions
defined on a manifold and studies the evolution of their critical points with
the parameter. This paper presents an adaptation of Morse-Cerf theory to a
family of piecewise-linear (PL) functions. The vertex diagram and Cerf diagram
are introduced as representations of the evolution of critical points of the PL
function. The characterization of a crossing in the vertex diagram based on the
homology of the lower links of vertices leads to the definition of a
topological descriptor for time-varying scalar fields. An algorithm for
computing the Cerf diagram and a measure for comparing two Cerf diagrams are
also described together with experimental results on time-varying scalar
fields.

</details>


<div id='cs.GT'></div>

# cs.GT [[Back]](#toc)

### [15] [Horus: A Protocol for Trustless Delegation Under Uncertainty](https://arxiv.org/abs/2507.00631)
*David Shi,Kevin Joo*

Main category: cs.GT

TL;DR: 该论文提出了一种通过抵押声明和递归验证游戏来确保AI代理在低信任环境中的正确性的协议。


<details>
  <summary>Details</summary>
Motivation: 在动态、低信任的环境中，自主AI代理需要通过将工作委托给子代理来完成任务，但无法通过前期规范或集中监督确保正确性。

Method: 论文提出了一种协议，其中任务以意图形式发布，解决者竞争完成。被选中的解决者在风险下执行任务，事后由验证者检查正确性。任何挑战者可以通过抵押来挑战结果并触发验证过程。

Result: 错误的代理会受到惩罚，正确的反对者会得到奖励，同时有一个升级路径会惩罚错误的验证者。当激励在解决者、挑战者和验证者之间对齐时，正确性成为纳什均衡。

Conclusion: 通过抵押声明和递归验证游戏，论文展示了如何在低信任环境中实现对AI代理正确性的保障。

Abstract: Correctness is an emergent property of systems where exposing error is
cheaper than committing it. In dynamic, low-trust environments, autonomous AI
agents benefit from delegating work to sub-agents, yet correctness cannot be
assured through upfront specification or centralized oversight. We propose a
protocol that enforces correctness through collateralized claims in a recursive
verification game. Tasks are published as intents, and solvers compete to
fulfill them. Selected solvers carry out tasks under risk, with correctness
checked post hoc by verifiers. Any challenger can challenge a result by staking
against it to trigger the verification process. Incorrect agents are slashed
and correct opposition is rewarded, with an escalation path that penalizes
erroneous verifiers themselves. When incentives are aligned across solvers,
challengers, and verifiers, falsification conditions make correctness the Nash
equilibrium.

</details>


<div id='cs.LO'></div>

# cs.LO [[Back]](#toc)

### [16] [Encoding Peano Arithmetic in a Minimal Fragment of Separation Logic](https://arxiv.org/abs/2507.00465)
*Sohei Ito,Makoto Tatsuta*

Main category: cs.LO

TL;DR: 本文研究了扩展自然数的最小分离逻辑片段的表达能力，证明仅含直觉指向谓词、常数0和后继函数的片段足以编码Peano算术的所有$\Pi^0_1$公式。通过构造从PA到该片段的翻译，展示了$\Pi^0_1$公式在标准算术模型中的有效性与翻译片段中的有效性等价，揭示了该片段的有效性问题的不可判定性。


<details>
  <summary>Details</summary>
Motivation: 研究最小分离逻辑片段的表达能力，探索其在程序验证和逻辑理论中的潜在应用。

Method: 构造从Peano算术到分离逻辑片段的翻译，利用基于堆的算术操作（加法、乘法、不等式）编码，并通过有限模型理论进行不可判定性证明。

Result: 证明该片段的有效性问题为$\Pi^0_1$-完全问题，表明其具有重要的理论意义，且翻译不保留$\Sigma^0_1$公式的有效性。

Conclusion: 本文展示了最小分离逻辑片段的强大表达能力及其在程序验证中的潜在重要性，同时揭示了其理论边界和不可判定性。

Abstract: This paper investigates the expressive power of a minimal fragment of
separation logic extended with natural numbers. Specifically, it demonstrates
that the fragment consisting solely of the intuitionistic points-to predicate,
the constant 0, and the successor function is sufficient to encode all
$\Pi^0_1$ formulas of Peano Arithmetic (PA). The authors construct a
translation from PA into this fragment, showing that a $\Pi^0_1$ formula is
valid in the standard model of arithmetic if and only if its translation is
valid in the standard interpretation of the separation logic fragment. This
result implies the undecidability of validity in the fragment, despite its
syntactic simplicity. The translation leverages a heap-based encoding of
arithmetic operations - addition, multiplication, and inequality - using
structured memory cells. The paper also explores the boundaries of this
encoding, showing that the translation does not preserve validity for
$\Sigma^0_1$ formulas. Additionally, an alternative undecidability proof is
presented via a reduction from finite model theory. Finally, the paper
establishes that the validity problem for this fragment is $\Pi^0_1$-complete,
highlighting its theoretical significance in the landscape of logic and program
verification.

</details>


<div id='cs.MS'></div>

# cs.MS [[Back]](#toc)

### [17] [Anatomy of High-Performance Column-Pivoted QR Decomposition](https://arxiv.org/abs/2507.00976)
*Maksim Melnichenko,Riley Murray,William Killian,James Demmel,Michael W. Mahoney,Piotr Luszczek,Mark Gates*

Main category: cs.MS

TL;DR: 介绍了一种通用矩阵QR分解带列主元（QRCP）的算法框架，通过用户可控的子程序选择设计实用QRCP算法，性能显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 旨在设计一种灵活且高效的QRCP算法框架，以适应现代硬件平台需求，提升性能。

Method: 提供用户可控的子程序选择框架，详细描述CPU和GPU上的实现方法，并在RandLAPACK库中实现。

Result: 在双AMD EPYC 9734系统上，性能比LAPACK标准QRCP提高两个数量级，优于现有随机QRCP算法；在NVIDIA H100 GPU上达到cuSOLVER无主元QR分解65%的性能。

Conclusion: 该框架成功实现了高性能QRCP算法，为现代硬件提供了新的解决方案。

Abstract: We introduce an algorithmic framework for performing QR factorization with
column pivoting (QRCP) on general matrices. The framework enables the design of
practical QRCP algorithms through user-controlled choices for the core
subroutines. We provide a comprehensive overview of how to navigate these
choices on modern hardware platforms, offering detailed descriptions of
alternative methods for both CPUs and GPUs. The practical QRCP algorithms
developed within this framework are implemented as part of the open-source
RandLAPACK library. Our empirical evaluation demonstrates that, on a dual AMD
EPYC 9734 system, the proposed method achieves performance improvements of up
to two orders of magnitude over LAPACK's standard QRCP routine and greatly
surpasses the performance of the current state-of-the-art randomized QRCP
algorithm. Additionally, on an NVIDIA H100 GPU, our method attains
approximately 65 percent of the performance of cuSOLVER's unpivoted QR
factorization.

</details>


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [18] [Estimating Correctness Without Oracles in LLM-Based Code Generation](https://arxiv.org/abs/2507.00057)
*Thomas Valentin,Ardi Madadi,Gaetano Sapia,Marcel Böhme*

Main category: cs.PL

TL;DR: 本文提出了一种称为“不一致性”的度量方法，用于在没有正确实现（即oracle）的情况下，评估大型语言模型（LLMs）生成代码的正确性概率，并证明其高效性和有效性。


<details>
  <summary>Details</summary>
Motivation: LLMs在从自然语言生成代码方面表现优异，但会产生语法正确但事实错误的输出。如何在缺少正确实现（oracle）的情况下量化生成代码的正确性概率，是一个关键问题。

Method: 提出了一种称为“不一致性”的度量方法，可在没有oracle的情况下高效估计生成代码的错误概率，并提供错误的下界。

Result: 实验表明，该方法能自动识别约三分之二的错误程序，且无误报。不一致性评估能可靠替代oracle评估，与pass@1排名高度一致。

Conclusion: 不一致性评估是一种高效且可靠的方法，可用于在没有oracle的情况下评估LLMs生成代码的正确性，为代码生成任务提供了新的评估标准。

Abstract: Generating code from natural language specifications is one of the most
successful applications of Large Language Models (LLMs). Yet, they hallucinate:
LLMs produce outputs that may be grammatically correct but are factually
incorrect. Without an existing, correct implementation (i.e., an oracle), can
we quantify how likely the generated program is correct?
  In this paper, we propose a measure of incorrectness, called incoherence,
that can be estimated efficiently in the absence of an oracle and provides a
lower bound on the error, i.e., the probability that the LLM-generated program
for that specification is incorrect. Our experiments demonstrate an
extraordinary effectiveness. For the average code generation task, our
incoherence-based methodology can automatically identify about two-thirds of
incorrect programs without reports of false positives. In fact, an oracle-based
evaluation of LLMs can be reliably replaced by an incoherence-based evaluation.
In particular, we find a very strong agreement between the ranking of LLMs by
the number of programs deemed correct via an oracle (pass@1) and the ranking of
LLMs by the number of programs deemed correct via our incoherence.

</details>


### [19] [Rust vs. C for Python Libraries: Evaluating Rust-Compatible Bindings Toolchains](https://arxiv.org/abs/2507.00264)
*Isabella Basso do Amaral,Renato Cordeiro Ferreira,Alfredo Goldman*

Main category: cs.PL

TL;DR: Python因其语法和科学库而闻名，但其解释器速度较慢。本研究比较了PyO3、ctypes和cffi的性能与易用性，展示了Rust工具链在Python中的高性能表现。


<details>
  <summary>Details</summary>
Motivation: Python解释器的速度较慢，优化关键部分需要跨语言交互知识，且手动实现复杂。研究旨在评估PyO3工具链的性能和易用性，以解决这些问题。

Method: 研究通过比较PyO3、ctypes和cffi这三种工具链的性能和易用性，利用Rust为Python开发的工具链，测试其在Python中的表现。

Result: 结果显示，PyO3工具链在提供高性能的同时，无需担心API兼容性问题，优于ctypes和cffi。

Conclusion: PyO3工具链为Python提供了高性能的解决方案，简化了跨语言交互的复杂性，是优化Python性能的有效选择。

Abstract: The Python programming language is best known for its syntax and scientific
libraries, but it is also notorious for its slow interpreter. Optimizing
critical sections in Python entails special knowledge of the binary
interactions between programming languages, and can be cumbersome to interface
manually, with implementers often resorting to convoluted third-party
libraries. This comparative study evaluates the performance and ease of use of
the PyO3 Python bindings toolchain for Rust against ctypes and cffi. By using
Rust tooling developed for Python, we can achieve state-of-the-art performance
with no concern for API compatibility.

</details>


### [20] [Have Object-Oriented Languages Missed a Trick with Class Function and its Subclasses?](https://arxiv.org/abs/2507.00488)
*Lloyd Allison*

Main category: cs.PL

TL;DR: 论文探讨了编程语言中函数的分类不足问题，提出了在面向对象语言中引入函数类和其子类的想法，并通过实验验证了这一概念的可行性。


<details>
  <summary>Details</summary>
Motivation: 作者观察到编程语言中的函数缺乏像数学中那样的分类，尤其是在面向对象语言中，虽然有函数类型，但缺乏对函数进行更细粒度分类的机制，这限制了语言的表达能力和灵活性。

Method: 作者提出了一种在面向对象语言中引入函数类及其子类的概念，并通过在几种流行的编程语言中实现这些分类来进行实验研究。

Result: 通过实验，作者展示了在面向对象语言中引入函数类和子类的可行性，证明了这一设计可以增强语言的表达能力和灵活性。

Conclusion: 论文的结论是指出当前编程语言在函数分类上的不足，提出了一种新的设计思路，并通过实验验证了其可行性，为未来编程语言的设计提供了新的方向。

Abstract: Compared to functions in mathematics, functions in programming languages seem
to be under classified. Functional programming languages based on the lambda
calculus famously treat functions as first-class values. Object-oriented
languages have adopted ``lambdas'', notably for call-back routines in
event-based programming. Typically a programming language has functions, a
function has a type, and some functions act on other functions and/or return
functions but there is generally a lack of (i) ``class Function'' in the OO
sense of the word class and particularly (ii) subclasses of Function for
functions having specific properties. Some such classes are presented here and
programmed in some popular programming languages as an experimental
investigation into OO languages missing this opportunity.

</details>


<div id='math.CT'></div>

# math.CT [[Back]](#toc)

### [21] [Semi-strictification of $(\infty, n)$-categories](https://arxiv.org/abs/2507.00146)
*Clémence Chanavat,Amar Hadzihasanovic*

Main category: math.CT

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: We prove the first equivalence between a weak non-algebraic model and a
semi-strict algebraic model of $(\infty, n)$-categories. This takes the form of
a natural semi-strictification, whereby a weak $(\infty, n)$-category is
embedded into a semi-strict one through an acyclic cofibration, in such a way
that weak functors lift to semi-strict functors; this constitutes the derived
unit of a Quillen equivalence between weak model categories whose fibrant
objects are, respectively, the weak $(\infty, n)$-categories and (up to an
acyclic fibration) the semi-strict ones. The semi-strict model has algebraic
units and composition of round pasting diagrams, satisfying a strict form of
associativity and interchange as in Henry's regular version of Simpson's weak
units conjecture; semi-strict functors strictly preserve round composition, but
only weakly preserve units. Globular composition operations are obtained from a
combination of units and round composition. Since the models satisfy the
homotopy hypothesis in the case $n = 0$, this result also exhibits the first
semi-strict model of the classical homotopy types that has algebraic units and
composition. The constructions are based on the combinatorics of regular
directed complexes and are entirely explicit and combinatorial, in the spirit
of Mac Lane's strictification of bicategories.

</details>


### [22] [The concept of nullity in general spaces and contexts](https://arxiv.org/abs/2507.00212)
*Suddhasattwa Das*

Main category: math.CT

TL;DR: 本文提出了一种广义的nullity概念，将其作为范畴间的函子，并通过构造性方法将其扩展到具有更丰富结构的范畴中。


<details>
  <summary>Details</summary>
Motivation: 在各个数学领域中，nullity的概念普遍存在，但如何在不同上下文中统一并扩展这一概念是研究的动机。

Method: 通过将nullity概念作为范畴间的函子，并利用右和左Kan扩展的规则，构造性地将其扩展到更广泛的范畴中。

Result: 提出的方法能够递归地定义如广义向量空间等范畴中的nullity，显示其为一种可扩展的任意构造。

Conclusion: nullity是一种可通过明确定义的规则扩展的构造，而这些规则通过Kan扩展简洁表达。

Abstract: The notion of nullity is present in all discourses of mathematics. The two
most familiar notions of nullity are "almost-every" and "almost none". A notion
of nullity corresponds to a choice of subsets that one interprets as null or
non-empty. The rationale behind this choice depends on the context, such as
Topology or Measure theory. One also expects that the morphisms or
transformations within the contexts preserve the nullity structures. Extending
this idea, a generalized notion of nullity is presented as a functor between
categories. A constructive procedure is presented for extending the notion of
nullity to categories with richer structure. Thus nullity in a category, such
as that of general vector spaces, can be provided a recursive definition. Thus
nullity is an arbitrary construct, which can be extended to broader contexts
using well defined rules. These rules are succinctly expressed by right and
left Kan extensions.

</details>


<div id='math.GM'></div>

# math.GM [[Back]](#toc)

### [23] [A note on D-functions and P-covariances on Hilbert spaces and related inequalities](https://arxiv.org/abs/2507.00009)
*Sergio Scarlatti*

Main category: math.GM

TL;DR: 论文综述了D函数的概念，并引入Hilbert空间上的P协方差概念，证明了当P为一维时，多个著名不等式（如Buzano、Richard和Walker不等式）是P协方差不等式及其与D函数关系的简单推论，并通过这些概念以最小努力改进了前述不等式，还深入分析了Walker不等式并提出Holder不等式的新改进。


<details>
  <summary>Details</summary>
Motivation: 研究动机在于探索与柯西-施瓦茨不等式密切相关的D函数概念，并通过引入P协方差的概念，简化并推广多个著名不等式的证明和应用，进而提出新的改进和金融考虑。

Method: 方法包括回顾D函数的概念，引入Hilbert空间上的P协方差（P为正交投影），并在一维投影情况下展示其与多个著名不等式的关系，进一步通过这些关系改进不等式。

Result: 结果表明，P协方差不等式可以简单推导出Buzano、Richard和Walker等不等式，并且能够以最小努力改进这些不等式。此外，论文还提出了Walker不等式的深入分析和Holder不等式的新改进。

Conclusion: 论文通过引入P协方差的概念，不仅简化了多个著名不等式的推导，还提出了新的改进和金融应用，展示了D函数和P协方差在不等式理论中的重要作用。

Abstract: In this note we first review the concept of D-function, closely connected
with Cauchy-Schwarz inequality, and then introduce the notion of P-covariance
on a Hilbert space, where $P$ is an orthogonal projection. We show that when P
is specialized to be one-dimensional many well-known inequalities such as
Buzano, Richard and Walker inequalities are simple consequences of P-covariance
inequalities and their relation with D-functions. By means of these concepts
enhancements of the previous mentioned inequalities are also established with
minimum effort. A more thorough analysis of Walker inequality is presented
jointly with some novel financial considerations as well as a new refinement of
Holder inequality.

</details>


### [24] [The 2p order Heisenberg-Pauli-Weyl uncertainty principles related to the offset linear canonical transform](https://arxiv.org/abs/2507.00010)
*Jia-Yin Peng,Bing-Zhao Li*

Main category: math.GM

TL;DR: 本文研究了偏移线性正则变换相关的不确定性原理，包括Plancherel-Parseval-Rayleigh恒等式、$2p$阶Heisenberg-Pauli-Weyl不确定性原理及其锐化形式，并通过数值模拟验证结果。


<details>
  <summary>Details</summary>
Motivation: 随着基于傅里叶变换的各种先进时频分析方法的不断发展，研究这些方法相关的不确定性原理成为一个重要且有意义的课题。

Method: 本文通过理论推导和数值模拟，研究了偏移线性正则变换下的不确定性原理，包括Plancherel-Parseval-Rayleigh恒等式和不同阶的Heisenberg-Pauli-Weyl不确定性原理。

Result: 提出了偏移线性正则变换下的Plancherel-Parseval-Rayleigh恒等式、$2p$阶Heisenberg-Pauli-Weyl不确定性原理及其锐化形式，并通过数值模拟验证了这些结果的正确性。

Conclusion: 研究结果表明，偏移线性正则变换下的不确定性原理在时频分析中具有重要应用价值，为相关领域提供了新的理论工具。

Abstract: The uncertainty principle is one of the fundamental tools for time-frequency
analysis in harmonic analysis, revealing the intrinsic trade-off between time
and frequency resolutions. With the continuous development of various advanced
time-frequency analysis methods based on the Fourier transform, investigating
uncertainty principles associated with these methods has become one of the most
interesting topics. This paper studies the uncertainty principles related to
the offset linear canonical transform, including the
Plancherel-Parseval-Rayleigh identity, the $2p$ order Heisenberg-Pauli-Weyl
uncertainty principle and the sharpened Heisenberg-Pauli-Weyl uncertainty
principle. Numerical simulations are also proposed to validate the derived
results.

</details>


### [25] [On a class of coupled fractional nonlinear singular boundary value problems arising in dusty fluid models](https://arxiv.org/abs/2507.00017)
*Lok Nath Kannaujiya,Narendra Kumar,Amit K. Verma*

Main category: math.GM

TL;DR: 本文介绍了一种新型的耦合分数阶Lane-Emden边值问题，并采用分数阶Haar小波配置法与Newton-Raphson方法相结合的新方法进行求解。通过数值实验验证了方法的准确性和有效性。


<details>
  <summary>Details</summary>
Motivation: 研究动机在于解决非线性奇异分数阶边值问题，通过开发一种新的数值方法以提高求解精度和效率。

Method: 采用分数阶Haar小波配置法与Newton-Raphson方法相结合的新方法，分析两种条件下的数值实验，并通过Mathematica软件实现求解。

Result: 通过五个数值实验和图表展示了方法的有效性和收敛性，证明了残差误差随着分辨率$J$的增加而减少，且在固定$J$时，分数阶导数的阶数变化也呈现类似趋势。

Conclusion: 研究表明，所提出的方法能有效解决非线性奇异分数阶边值问题，且Mathematica软件的应用进一步验证了方法的实用性。

Abstract: In this article, we introduce a new class of coupled fractional Lane-Emden
boundary value problems. We employ a novel approach, the fractional Haar
wavelet collocation method with the Newton-Raphson method. We analyze the
conditions in two cases to present numerical experiments related to the defined
system of fractional differential equations. To validate the accuracy of the
proposed method we present the convergence of the method, and we demonstrate
the method's effectiveness through five numerical experiments, highlighting
real-world applications of fractional differential equations. Using figures and
tables, we show that the residual error decreases as we increase the value of
the maximum level of resolution $J$ while keeping the order of derivatives
fixed, and similar trends also observe when $J$ is fixed and vary the order of
fractional derivatives. We demonstrate that Mathematica software can be used
effectively to solve such nonlinear singular fractional boundary value
problems.

</details>


### [26] [An Application of Fractional Calculus to Column Theory](https://arxiv.org/abs/2507.00021)
*José Villa-Morales,Manuel Ramírez-Aranda*

Main category: math.GM

TL;DR: 本文采用分数曲率半径的欧拉方程，推导出Caputo意义上的分数微分方程，并证明在某些分数参数值下存在临界屈曲力，同时提供了数值方案来精确近似该临界力。


<details>
  <summary>Details</summary>
Motivation: 研究动机在于探索分数曲率半径在欧拉柱屈曲方程中的应用，以扩展经典屈曲理论并解决分数参数下的临界屈曲问题。

Method: 方法包括在Caputo意义上推导分数微分方程，并求解该方程以确定临界屈曲力的存在性。此外，还开发了数值方案用于近似计算临界力。

Result: 结果表明，对于某些分数参数值，确实存在临界屈曲力。同时，数值方案的精度验证了这一发现。

Conclusion: 结论证明了分数曲率半径在屈曲理论中的有效性，为分数参数下的临界屈曲问题提供了理论和数值支持。

Abstract: In this article, we employ a fractional version of the radius of curvature in
Euler's equation for column buckling, enabling us to derive a fractional
differential equation in the Caputo sense. We solve this equation and
demonstrate that for certain values of the fractional parameter, there exists a
critical buckling force. Additionally, we provide a numerical scheme for
accurately approximating this critical force.

</details>


### [27] [Conditions for solving polynomial equations using algebraic and hypergeometric functions](https://arxiv.org/abs/2507.00027)
*Nikos Mantzakouras,Carlos López Zapata,Nid Na Ratch*

Main category: math.GM

TL;DR: 本文探讨了使用连续函数或超几何函数求解高次方程的概念，并提供了另一种证明，说明四次以上方程不存在代数解。基于Kolmogorov-Arnold定理，证明了五次以上方程在系数间无特殊关系时无法用超几何函数求解，但一般形式的三项式方程通常可使用超几何函数求解。


<details>
  <summary>Details</summary>
Motivation: 研究动机在于澄清高次方程求解的概念，探讨使用连续函数或超几何函数求解的可能性，并填补相关理论空白。特别是对五次以上方程的代数解的存在性问题进行深入探讨。

Method: 本文基于Kolmogorov-Arnold定理，通过数学证明方法，分析了高次方程求解的限制条件。特别是研究了超几何函数在求解三项式方程中的适用性。

Result: 研究结果表明，对于五次以上的方程，若系数间没有特殊关系，则无法使用超几何函数求解。然而，一般形式的三项式方程通常可以使用超几何函数求解。

Conclusion: 本文的结论强调了高次方程求解的限制性，并指出了超几何函数在特定情况下（如三项式方程）的适用性，为进一步研究提供了理论基础。

Abstract: In this paper, we focus on clarifying the concept of solving equations of
degree greater than six using continuous functions or hypergeometric functions
and providing another proof of the non-existence of algebraic solutions for
equations of degree greater than four. According to the Kolmogorov-Arnold
theorem, we will prove that equations of degree greater than five cannot be
solved without special conditions between their coefficients using
hypergeometric functions. However, we prove that trinomial equations of general
form can in general be solved using hypergeometric functions.

</details>


### [28] [Weakly Compatible Mappings and Common Fixed Points Under Generalized Contractive Conditions](https://arxiv.org/abs/2507.00035)
*Alemayehu Negash,Meaza Bogale*

Main category: math.GM

TL;DR: 本文在度量空间中建立了弱相容映射的新共同不动点定理，放宽了连续性、相容性和互连续性等传统要求。


<details>
  <summary>Details</summary>
Motivation: 传统的不动点定理对映射的连续性、相容性等要求较高，本文旨在通过弱相容映射和更宽松的条件，扩展和推广现有理论。

Method: 通过定义三个自映射$T$、$f$和$g$的收缩条件，结合控制函数$\psi$，提出统一框架，并扩展到对映射和上半连续控制函数。

Result: 结果表明新定理不仅严格推广了前人工作，还消除了对整个空间完备性的假设，并放宽了映射的相容性条件。

Conclusion: 本文的理论框架和结果为不动点定理提供了更广泛的适用性，为后续研究提供了新的方向。

Abstract: This paper establishes new common fixed point theorems for weakly compatible
mappings in metric spaces, relaxing traditional requirements such as
continuity, compatibility, and reciprocal continuity. We present a unified
framework for three self-mappings $T$, $f$, and $g$ with a contractive
condition involving a control function $\psi$, along with corollaries extending
results to pairs of mappings and upper semi-continuous control functions.
Further generalizations include iterated mappings and sequences of mappings.
Rigorous examples demonstrate the necessity of hypotheses and show our results
strictly generalize theorems by Al-Thagafi \emph{et. al.}
\cite{Al-Thagafi2006}, Babu \emph{et. al.} \cite{Babu2007}, Jungck
\cite{Jungck1976,Jungck1986}, Singh \cite{Singh1986,Singh1997a}, Som
\cite{Som2003}, Song \cite{Song2007} and Zhang \emph{et. al.} \cite{Zhang2008}.
Key advancements include eliminating completeness assumptions on the entire
space and relaxing mapping compatibility conditions.

</details>


### [29] [An Addendum to Plouffe's Ramanujan Identities](https://arxiv.org/abs/2507.00040)
*Segun Olofin Akerele*

Main category: math.GM

TL;DR: 本文介绍了一类新的多对数和，与L. Vepščik在2010年研究的一个家族密切相关。这些广义和依赖于两个自由参数，并给出了包含Dirichlet eta函数的闭式表达式。此外，还提供了Ramanujan最初讨论的一个双曲和的替代证明。


<details>
  <summary>Details</summary>
Motivation: 研究动机是扩展和深化对多对数和的理解，特别是与Dirichlet eta函数相关的广义和，同时提供对历史结果的新的数学证明。

Method: 通过引入依赖于两个自由参数的广义多对数和，并利用数学分析技术，推导出闭式表达式。此外，使用替代方法重新证明了Ramanujan的双曲和。

Result: 研究结果表明，新引入的多对数和可以表示为Dirichlet eta函数的闭式表达式，并且通过替代方法验证了Ramanujan的双曲和。

Conclusion: 文章结论展示了广义多对数和在数学分析中的重要性，并提供了一种新的工具来研究类似的问题。对Ramanujan结果的再证明也增强了原始结论的可信度。

Abstract: We introduce a new class of polylogarithm sums closely related to a family
studied by L. Vep\v{s}tas in 2010. These generalized sums depend on two free
parameters and yield closed-form expressions involving the Dirichlet eta
function. Additionally, we present an alternative proof for a hyperbolic sum
originally discussed by Ramanujan.

</details>


### [30] [Fixed Points of the Josephus Function via Fractional Base Expansions](https://arxiv.org/abs/2507.00317)
*Yunier Bello-Cruz,Roy Quintero-Contreras*

Main category: math.GM

TL;DR: 研究了约瑟夫函数 $J_3$ 的不动点性质，发现其与中国剩余定理的联系，并通过模运算在基数 $3/2$ 下的数值模式开发了递归过程。


<details>
  <summary>Details</summary>
Motivation: 探索约瑟夫函数 $J_3$ 的不动点性质及其潜在规律，希望通过数学工具揭示其内在联系。

Method: 利用中国剩余定理分析了不动点序列的性质，并通过基数 $3/2$ 下的数值模式开发了递归方法。

Result: 发现了不动点序列在基数 $3/2$ 下的清晰数值模式，并成功建立了递归过程来描述其扩张形式。

Conclusion: 通过研究约瑟夫函数 $J_3$ 的不动点，不仅揭示了其与中国剩余定理的联系，还开发了有效的递归方法，为相关数学问题提供了新见解。

Abstract: In this paper, we investigated some interesting properties of the fixed
points of the Josephus function $J_3$. First, we establish a connection between
this sequence and the Chinese Remainder Theorem. Next, we observed a clear
numerical pattern in the fixed points sequence when the terms are written in
base $3/2$ using modular arithmetic, which allows us to develop a recursive
procedure to determine the digits of their base $3/2$ expansions.

</details>


<div id='math.LO'></div>

# math.LO [[Back]](#toc)

### [31] [Refinements of provability and consistency principles for the second incompleteness theorem](https://arxiv.org/abs/2507.00955)
*Taishi Kurahashi*

Main category: math.LO

TL;DR: 本文继续作者先前的研究，展示了几种受非正规模态逻辑启发的弱原则足以推导出第二不完备定理的各种精细形式。


<details>
  <summary>Details</summary>
Motivation: 研究的动机是探索非正规模态逻辑中的弱原则如何能够支持第二不完备定理的多种变体形式。

Method: 通过集合 {E, C, D3} 证明了一致性声明 ¬Pr_T(⌜0=1⌝) 的不可证性，并使用集合 {E^U, CB_∃} 推导出形式化的 Σ1-完备性。

Result: 主要结果显示，某些弱原则足以支持第二不完备定理的精细形式，并证明了形式化的 Σ1-完备性。

Conclusion: 结论表明，非正规模态逻辑中的弱原则可以广泛应用于不完备性定理的推导，为相关研究提供了新的视角。

Abstract: This paper continues the author's previous study \cite{Kura20}, showing that
several weak principles inspired by non-normal modal logic suffice to derive
various refined forms of the second incompleteness theorem. Among the main
results of the present paper, we show that the set $\{\mathbf{E},\mathbf{C},
\mathbf{D3}\}$ suffices to establish the unprovability of the consistency
statement $\neg\, \mathrm{Pr}_T(\ulcorner 0=1 \urcorner)$. We also prove that
the set $\{\mathbf{E}^{\mathrm{U}}, \mathbf{CB_{\exists}}\}$ yields formalized
$\Sigma_1$-completeness.

</details>


<div id='math.RT'></div>

# math.RT [[Back]](#toc)

### [32] [$(d,σ)$-twisted Affine-Virasoro superalgebras](https://arxiv.org/abs/2507.00349)
*Rencai Lü,Xizhou You,Kaiming Zhao*

Main category: math.RT

TL;DR: 本文引入了$(d, \sigma)$-扭曲的仿射-Virasoro超代数$\mathfrak{L}$，并确定了其普遍中心扩展$\hat{\mathfrak{L}}$。这一大类无限维Lie超代数包含了许多已知和新的代数结构，并通过权重函子和$A$-覆盖方法分类了尖点模和简单模。


<details>
  <summary>Details</summary>
Motivation: 研究无限维Lie超代数的普遍中心扩展及其模的分类问题，旨在统一和扩展数学和数学物理中已知的Lie代数结构。

Method: 引入$(d, \sigma)$-扭曲的仿射-Virasoro超代数$\mathfrak{L}$，确定其普遍中心扩展$\hat{\mathfrak{L}}$，并利用权重函子和$A$-覆盖方法分类尖点模和简单模。

Result: 成功构造了无限维Lie超代数$\mathfrak{L}$及其中心扩展$\hat{\mathfrak{L}}$，分类了其上的尖点模和简单模，涵盖了多个已知和新的代数结构。

Conclusion: 本文为数学和数学物理中的Lie超代数结构提供了统一的框架和扩展，丰富了相关理论和应用。

Abstract: For any finite dimensional Lie superalgebra $\dot{\mathfrak{g}}$ (maybe a Lie
algebra) with an even derivation $d$ and a finite order automorphism $\sigma$
that commutes with $d$, we introduce the $(d,\sigma)$-twisted Affine-Virasoro
superalgebra $\mathfrak{L}=\mathfrak{L}(\dot{\mathfrak{g}},d,\sigma)$ and
determine its universal central extension
$\hat{\mathfrak{L}}=\hat{\mathfrak{L}}(\dot{\mathfrak{g}},d,\sigma)$. This is a
huge class of infinite-dimensional Lie superalgebras. Such Lie superalgebras
consist of many new and well-known Lie algebras and superalgebras, including
the Affine-Virasoro superalgebras, the twisted Heisenberg-Virasoro algebra, the
mirror Heisenberg-Virasoro algebra, the W-algebra $W(2,2)$, the gap-$p$
Virasoro algebras, the Fermion-Virasoro algebra, the $N=1$ BMS superalgebra,
the planar Galilean conformal algebra. Then we give the classification of
cuspidal $A\mathfrak{L}$-modules by using the weighting functor from
$U(\mathfrak{h})$-free modules to weight modules. Consequently, we give the
classification of simple cuspidal $\mathfrak{L}$-modules by using the $A$-cover
method. Finally, all simple quasi-finite modules over $\mathfrak{L}$ and
$\hat{\mathfrak{L}}$ are classified. Our results recover many known Lie
superalgebra results from mathematics and mathematical physics, and give many
new Lie superalgebras.

</details>


### [33] [Finite-dimensional $\mathbb{Z}$-graded Lie algebras](https://arxiv.org/abs/2507.00384)
*Mark D. Gould,Phillip S. Isaac,Ian Marquette,Jorgen Rasmussen*

Main category: math.RT

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: We investigate the structure and representation theory of finite-dimensional
$\mathbb{Z}$-graded Lie algebras, including the corresponding root systems and
Verma, irreducible, and Harish-Chandra modules. This extends the familiar
theory for finite-dimensional semisimple Lie algebras to a much wider class of
Lie algebras, and opens up for advances and applications in areas relying on
ad-hoc approaches. Physically relevant examples are afforded by the Heisenberg
and conformal Galilei algebras, including the Schr\"odinger algebras, whose
$\mathbb{Z}$-graded structures are yet to be fully exploited.

</details>


### [34] [Study of $p$-Young tableaux, Robinson-Schensted correspondence and the lacunary Cauchy identity of group algebras $KG_{r}$ and $KSG_{r}$](https://arxiv.org/abs/2507.00580)
*M. Parvathi,A. Tamilselvi,D. Hepsi*

Main category: math.RT

TL;DR: 本文通过矩阵单位方法扩展了Robinson-Schensted对应关系，应用于群 $G_{r}$ 和 $SG_{r}$ 的元素，并讨论了其经典性质，同时提出了新的lacunary柯西恒等式。


<details>
  <summary>Details</summary>
Motivation: 研究群 $G_{r}$ 和 $SG_{r}$ 的表示理论及其组合结构，扩展经典Robinson-Schensted对应关系的方法。

Method: 基于矩阵单位方法，利用群代数的正交原始幂等元计算，发展了一种新的Robinson-Schensted对应关系。

Result: 提出了适用于群 $G_{r}$ 和 $SG_{r}$ 的新对应关系，并扩展了经典的柯西恒等式，称为lacunary柯西恒等式。

Conclusion: 研究为这些群的表示理论和组合结构提供了新的见解，展示了新方法的有效性。

Abstract: In this paper, we develop the Robinson-Schensted correspondence between the
elements of the groups $G_{r}$ $(\mathbb{Z}_{p^{r}}\rtimes
\mathbb{Z}^{*}_{p^{r}})$ and $SG_{r}$ $(\mathbb{Z}_{p^{r-1}}\rtimes
\mathbb{Z}^{*}_{p^{r}})$, along with a pair of the standard $p$-Young tableaux.
This approach differs from the classical method, and ours is based on matrix
units arising from orthogonal primitive idempotents computed for every group
algebra. Some classical properties of the Robinson-Schensted correspondence are
discussed. As a by-product, we also extend the Cauchy identity to our setup,
which we refer to as the lacunary Cauchy identity. This study offers new
insights into the representation theory of these groups and their combinatorial
structures.

</details>


### [35] [Dualities of Gaudin models with irregular singularities for general linear Lie (super)algebras](https://arxiv.org/abs/2507.00730)
*Wan Keng Cheong,Ngau Lam*

Main category: math.RT

TL;DR: 论文证明了$	ext{gl}_d$和$	ext{gl}_{p+m|q+n}$的Gaudin代数在不规则奇点下对Fock空间作用的等价性，并建立了$(	ext{gl}_d, 	ext{gl}_{p+m|q+n})$的对偶性。


<details>
  <summary>Details</summary>
Motivation: 研究Gaudin模型在不规则奇点下的代数作用及其对偶性，为理解高维代数结构的对称性提供理论基础。

Method: 通过分析$	ext{gl}_d$和$	ext{gl}_{p+m|q+n}$的Gaudin代数在Fock空间上的作用，利用Takiff超代数的无限维模进行应用验证。

Result: 证明了Gaudin代数在不规则奇点下对特定无限维模的作用是循环且对角化的，并在经典Gaudin模型中展示了类似的对偶性。

Conclusion: 论文确立了$(	ext{gl}_d, 	ext{gl}_{p+m|q+n})$的对偶性，为Gaudin模型的代数结构研究提供了新的视角和方法。

Abstract: We prove an equivalence between the actions of the Gaudin algebras with
irregular singularities for $\mathfrak{gl}_d$ and $\mathfrak{gl}_{p+m|q+n}$ on
the Fock space of $d(p+m)$ bosonic and $d(q+n)$ fermionic oscillators. This
establishes a duality of $(\mathfrak{gl}_d, \mathfrak{gl}_{p+m|q+n})$ for
Gaudin models. As an application, we show that the Gaudin algebra with
irregular singularities for $\mathfrak{gl}_{p+m|q+n}$ acts cyclically on each
weight space of a certain class of infinite-dimensional modules over a direct
sum of Takiff superalgebras over $\mathfrak{gl}_{p+m|q+n}$ and that the action
is diagonalizable with a simple spectrum under a generic condition. We also
study the classical versions of Gaudin algebras with irregular singularities
and demonstrate a duality of $(\mathfrak{gl}_d, \mathfrak{gl}_{p+m|q+n})$ for
classical Gaudin models.

</details>
