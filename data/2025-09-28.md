<div id=toc></div>

# Table of Contents

- [cs.GR](#cs.GR) [Total: 9]
- [cs.PL](#cs.PL) [Total: 2]
- [cs.GT](#cs.GT) [Total: 2]


<div id='cs.GR'></div>

# cs.GR [[Back]](#toc)

### [1] [SeHDR: Single-Exposure HDR Novel View Synthesis via 3D Gaussian Bracketing](https://arxiv.org/abs/2509.20400)
*Yiyu Li,Haoyuan Wang,Ke Xu,Gerhard Petrus Hancke,Rynson W. H. Lau*

Main category: cs.GR

TL;DR: SeHDR是一种新颖的高动态范围3D高斯投影方法，通过单曝光多视图LDR图像学习HDR场景表示，避免了传统方法需要多曝光图像的繁琐和潜在误差。


<details>
  <summary>Details</summary>
Motivation: 传统方法需要多曝光的多视图LDR图像，不仅捕获繁琐，还容易因物体运动模糊和校准误差导致问题。SeHDR旨在通过单曝光图像解决这些问题。

Method: SeHDR首先从单曝光LDR输入学习基础3D高斯，然后通过曝光操作估计具有相同几何但不同线性颜色的多个3D高斯，最后通过可微分神经网络曝光融合（NeEF）整合为HDR高斯。

Result: 实验表明，SeHDR在性能上优于现有方法和精心设计的基线。

Conclusion: SeHDR提供了一种高效、准确的HDR场景表示方法，适用于单曝光多视图LDR图像的场景重建和新视角生成。

Abstract: This paper presents SeHDR, a novel high dynamic range 3D Gaussian Splatting
(HDR-3DGS) approach for generating HDR novel views given multi-view LDR images.
Unlike existing methods that typically require the multi-view LDR input images
to be captured from different exposures, which are tedious to capture and more
likely to suffer from errors (e.g., object motion blurs and
calibration/alignment inaccuracies), our approach learns the HDR scene
representation from multi-view LDR images of a single exposure. Our key insight
to this ill-posed problem is that by first estimating Bracketed 3D Gaussians
(i.e., with different exposures) from single-exposure multi-view LDR images, we
may then be able to merge these bracketed 3D Gaussians into an HDR scene
representation. Specifically, SeHDR first learns base 3D Gaussians from
single-exposure LDR inputs, where the spherical harmonics parameterize colors
in a linear color space. We then estimate multiple 3D Gaussians with identical
geometry but varying linear colors conditioned on exposure manipulations.
Finally, we propose the Differentiable Neural Exposure Fusion (NeEF) to
integrate the base and estimated 3D Gaussians into HDR Gaussians for novel view
rendering. Extensive experiments demonstrate that SeHDR outperforms existing
methods as well as carefully designed baselines.

</details>


### [2] [SGAligner++: Cross-Modal Language-Aided 3D Scene Graph Alignment](https://arxiv.org/abs/2509.20401)
*Binod Singh,Sayan Deb Sarkar,Iro Armeni*

Main category: cs.GR

TL;DR: SGAligner++是一种跨模态、语言辅助的框架，用于3D场景图对齐，解决了部分重叠场景观察的挑战，并在低重叠和噪声条件下实现了精确对齐。


<details>
  <summary>Details</summary>
Motivation: 现有的3D场景图对齐方法通常依赖单模态点云数据，且在输入不完整或噪声较多时表现不佳。SGAligner++旨在解决这些问题，提升场景理解的准确性和鲁棒性。

Method: SGAligner++通过轻量级单模态编码器和基于注意力的融合，学习统一的联合嵌入空间，实现了跨模态对齐能力。

Result: 在真实世界数据集上的评估表明，SGAligner++在噪声重建任务中比现有方法表现优越40%，并展示了跨模态泛化能力。

Conclusion: SGAligner++通过跨模态和语言辅助的方法，显著提升了3D场景图对齐的性能和实用性，适用于视觉定位、3D重建和导航等任务。

Abstract: Aligning 3D scene graphs is a crucial initial step for several applications
in robot navigation and embodied perception. Current methods in 3D scene graph
alignment often rely on single-modality point cloud data and struggle with
incomplete or noisy input. We introduce SGAligner++, a cross-modal,
language-aided framework for 3D scene graph alignment. Our method addresses the
challenge of aligning partially overlapping scene observations across
heterogeneous modalities by learning a unified joint embedding space, enabling
accurate alignment even under low-overlap conditions and sensor noise. By
employing lightweight unimodal encoders and attention-based fusion, SGAligner++
enhances scene understanding for tasks such as visual localization, 3D
reconstruction, and navigation, while ensuring scalability and minimal
computational overhead. Extensive evaluations on real-world datasets
demonstrate that SGAligner++ outperforms state-of-the-art methods by up to 40%
on noisy real-world reconstructions, while enabling cross-modal generalization.

</details>


### [3] [SceneWeaver: All-in-One 3D Scene Synthesis with an Extensible and Self-Reflective Agent](https://arxiv.org/abs/2509.20414)
*Yandan Yang,Baoxiong Jia,Shujie Zhang,Siyuan Huang*

Main category: cs.GR

TL;DR: SceneWeaver是一个反射性代理框架，通过工具化迭代优化实现多样化的室内场景合成，结合语言模型规划器和多种生成工具，显著提升了场景的物理合理性、视觉真实性和语义一致性。


<details>
  <summary>Details</summary>
Motivation: 随着Embodied AI的发展，对视觉真实、物理合理且功能多样的3D环境需求增加，但现有方法在场景类别固定、物体细节缺乏和用户指令对齐方面存在局限。

Method: SceneWeaver采用闭环的"reason-act-reflect"设计，通过语言模型规划器选择生成工具，迭代优化场景的物理合理性、视觉真实性和语义一致性。

Result: 实验表明，SceneWeaver在物理、视觉和语义指标上优于现有方法，并能有效泛化到复杂场景和多样用户指令。

Conclusion: SceneWeaver在通用3D环境生成方面迈出重要一步，为多样化和复杂化的场景合成提供了新思路。

Abstract: Indoor scene synthesis has become increasingly important with the rise of
Embodied AI, which requires 3D environments that are not only visually
realistic but also physically plausible and functionally diverse. While recent
approaches have advanced visual fidelity, they often remain constrained to
fixed scene categories, lack sufficient object-level detail and physical
consistency, and struggle to align with complex user instructions. In this
work, we present SceneWeaver, a reflective agentic framework that unifies
diverse scene synthesis paradigms through tool-based iterative refinement. At
its core, SceneWeaver employs a language model-based planner to select from a
suite of extensible scene generation tools, ranging from data-driven generative
models to visual- and LLM-based methods, guided by self-evaluation of physical
plausibility, visual realism, and semantic alignment with user input. This
closed-loop reason-act-reflect design enables the agent to identify semantic
inconsistencies, invoke targeted tools, and update the environment over
successive iterations. Extensive experiments on both common and open-vocabulary
room types demonstrate that SceneWeaver not only outperforms prior methods on
physical, visual, and semantic metrics, but also generalizes effectively to
complex scenes with diverse instructions, marking a step toward general-purpose
3D environment generation. Project website: https://scene-weaver.github.io/.

</details>


### [4] [ArtUV: Artist-style UV Unwrapping](https://arxiv.org/abs/2509.20710)
*Yuguang Chen,Xinhai Liu,Yang Li,Victor Cheung,Zhuo Chen,Dongyu Zhang,Chunchao Guo*

Main category: cs.GR

TL;DR: ArtUV是一种全自动、端到端的方法，用于生成艺术家风格的UV展开，解决了现有方法在时间消耗、碎片化、缺乏语义性和不规则UV岛屿方面的限制。


<details>
  <summary>Details</summary>
Motivation: 现有UV展开方法存在耗时、碎片化、缺乏语义性和不规则UV岛屿等问题，限制了其实际应用；而艺术家风格的UV映射需满足高质量标准。

Method: ArtUV分为两个阶段：表面接缝预测和艺术家风格UV参数化。接缝预测阶段使用SeamGPT生成语义化切缝，参数化阶段则通过Auto-Encoder优化粗糙UV映射。

Result: ArtUV在多个基准测试中表现优异，确保了语义一致性和拓扑结构保留，适用于专业渲染工具或独立系统。

Conclusion: ArtUV提供了一种高效、高质量的UV生成解决方案，能够满足专业渲染和快速编辑的需求。

Abstract: UV unwrapping is an essential task in computer graphics, enabling various
visual editing operations in rendering pipelines. However, existing UV
unwrapping methods struggle with time-consuming, fragmentation, lack of
semanticity, and irregular UV islands, limiting their practical use. An
artist-style UV map must not only satisfy fundamental criteria, such as
overlap-free mapping and minimal distortion, but also uphold higher-level
standards, including clean boundaries, efficient space utilization, and
semantic coherence. We introduce ArtUV, a fully automated, end-to-end method
for generating artist-style UV unwrapping. We simulates the professional UV
mapping process by dividing it into two stages: surface seam prediction and
artist-style UV parameterization. In the seam prediction stage, SeamGPT is used
to generate semantically meaningful cutting seams. Then, in the
parameterization stage, a rough UV obtained from an optimization-based method,
along with the mesh, is fed into an Auto-Encoder, which refines it into an
artist-style UV map. Our method ensures semantic consistency and preserves
topological structure, making the UV map ready for 2D editing. We evaluate
ArtUV across multiple benchmarks and show that it serves as a versatile
solution, functioning seamlessly as either a plug-in for professional rendering
tools or as a standalone system for rapid, high-quality UV generation.

</details>


### [5] [SeamCrafte: Enhancing Mesh Seam Generation for Artist UV Unwrapping via Reinforcement Learning](https://arxiv.org/abs/2509.20725)
*Duoteng Xu,Yuguang Chen,Jing Li,Xinhai Liu,Xueqi Ma,Zhuo Chen,Dongyu Zhang,Chunchao Guo*

Main category: cs.GR

TL;DR: SeamCrafter是一种基于GPT风格的自回归缝生成器，通过双分支点云编码器分离和捕获拓扑和几何线索，并结合直接偏好优化，显著降低了UV失真和碎片化。


<details>
  <summary>Details</summary>
Motivation: 现有方法在3D表面的UV参数化和纹理映射中往往无法平衡失真和碎片化问题，影响了纹理合成和艺术家的工作流程。

Method: 提出了SeamCrafter，一种基于GPT风格的缝生成器，使用双分支点云编码器分离拓扑和几何线索，并通过直接偏好优化在偏好数据集上微调模型。

Result: 实验表明，SeamCrafter生成的缝显著降低了失真和碎片化，同时保持了拓扑一致性和视觉保真度。

Conclusion: SeamCrafter通过创新的双分支编码和偏好优化框架，有效解决了UV参数化中的缝质量问题，优于现有方法。

Abstract: Mesh seams play a pivotal role in partitioning 3D surfaces for UV
parametrization and texture mapping. Poorly placed seams often result in severe
UV distortion or excessive fragmentation, thereby hindering texture synthesis
and disrupting artist workflows. Existing methods frequently trade one failure
mode for another-producing either high distortion or many scattered islands. To
address this, we introduce SeamCrafter, an autoregressive GPT-style seam
generator conditioned on point cloud inputs. SeamCrafter employs a dual-branch
point-cloud encoder that disentangles and captures complementary topological
and geometric cues during pretraining. To further enhance seam quality, we
fine-tune the model using Direct Preference Optimization (DPO) on a preference
dataset derived from a novel seam-evaluation framework. This framework assesses
seams primarily by UV distortion and fragmentation, and provides pairwise
preference labels to guide optimization. Extensive experiments demonstrate that
SeamCrafter produces seams with substantially lower distortion and
fragmentation than prior approaches, while preserving topological consistency
and visual fidelity.

</details>


### [6] [ARMesh: Autoregressive Mesh Generation via Next-Level-of-Detail Prediction](https://arxiv.org/abs/2509.20824)
*Jiabao Lei,Kewei Shi,Zhihao Liang,Kui Jia*

Main category: cs.GR

TL;DR: 本文提出了一种基于渐进式粗到细的自动回归模型，用于生成3D网格，该方法通过逐步增加几何细节来构建网格，提供对生成质量和时间的直观控制。


<details>
  <summary>Details</summary>
Motivation: 现有的自动回归网格生成模型按字典顺序逐面构建网格，未能有效捕捉与人类感知一致的几何结构，因此需要一种更自然的网格生成方法。

Method: 本文将网格简化为单纯复形，并开发了一种基于Transformer的自动回归模型，通过逆向简化过程逐步从单个点构建网格，逐步增加几何细节。

Result: 实验表明，该方法不仅可以通过提前停止回归过程控制生成质量和时间，还支持网格细化和编辑等应用。

Conclusion: 渐进式网格生成方法提供了一种更直观、灵活的网格生成方式，为图形学应用开辟了新可能性。

Abstract: Directly generating 3D meshes, the default representation for 3D shapes in
the graphics industry, using auto-regressive (AR) models has become popular
these days, thanks to their sharpness, compactness in the generated results,
and ability to represent various types of surfaces. However, AR mesh generative
models typically construct meshes face by face in lexicographic order, which
does not effectively capture the underlying geometry in a manner consistent
with human perception. Inspired by 2D models that progressively refine images,
such as the prevailing next-scale prediction AR models, we propose generating
meshes auto-regressively in a progressive coarse-to-fine manner. Specifically,
we view mesh simplification algorithms, which gradually merge mesh faces to
build simpler meshes, as a natural fine-to-coarse process. Therefore, we
generalize meshes to simplicial complexes and develop a transformer-based AR
model to approximate the reverse process of simplification in the order of
level of detail, constructing meshes initially from a single point and
gradually adding geometric details through local remeshing, where the topology
is not predefined and is alterable. Our experiments show that this novel
progressive mesh generation approach not only provides intuitive control over
generation quality and time consumption by early stopping the auto-regressive
process but also enables applications such as mesh refinement and editing.

</details>


### [7] [ArchGPT: Understanding the World's Architectures with Large Multimodal Models](https://arxiv.org/abs/2509.20858)
*Yuze Wang,Luo Yang,Junyi Wang,Yue Qi*

Main category: cs.GR

TL;DR: 研究者提出ArchGPT，一种多模态建筑视觉问答模型，并开发了一个可扩展的数据管道Arch-300K，用于生成高质量的、特定于建筑的VQA标注。


<details>
  <summary>Details</summary>
Motivation: 现有VR/MR/AR系统通常通过案例开发，依赖于硬编码的注释和任务特定的交互方式，无法适应多样化的建筑环境。

Method: 通过多阶段流程构建Arch-300K数据集，整合3D重建和语义分割筛选图像，并利用LLM指导的文本验证管道生成问题-答案对，最终在ShareGPT4V-7B上进行监督微调。

Result: ArchGPT模型成功生成315,000个图像-问题-答案三元组，并通过多模态骨干网络实现了高质量的建筑视觉问答能力。

Conclusion: ArchGPT及其数据管道为建筑领域的VR/MR/AR系统提供了可扩展的解决方案，提升了建筑信息的可访问性和理解能力。

Abstract: Architecture embodies aesthetic, cultural, and historical values, standing as
a tangible testament to human civilization. Researchers have long leveraged
virtual reality (VR), mixed reality (MR), and augmented reality (AR) to enable
immersive exploration and interpretation of architecture, enhancing
accessibility, public understanding, and creative workflows around architecture
in education, heritage preservation, and professional design practice. However,
existing VR/MR/AR systems are often developed case-by-case, relying on
hard-coded annotations and task-specific interactions that do not scale across
diverse built environments. In this work, we present ArchGPT, a multimodal
architectural visual question answering (VQA) model, together with a scalable
data-construction pipeline for curating high-quality, architecture-specific VQA
annotations. This pipeline yields Arch-300K, a domain-specialized dataset of
approximately 315,000 image-question-answer triplets. Arch-300K is built via a
multi-stage process: first, we curate architectural scenes from Wikimedia
Commons and filter unconstrained tourist photo collections using a novel
coarse-to-fine strategy that integrates 3D reconstruction and semantic
segmentation to select occlusion-free, structurally consistent architectural
images. To mitigate noise and inconsistency in raw textual metadata, we propose
an LLM-guided text verification and knowledge-distillation pipeline to generate
reliable, architecture-specific question-answer pairs. Using these curated
images and refined metadata, we further synthesize formal analysis
annotations-including detailed descriptions and aspect-guided conversations-to
provide richer semantic variety while remaining faithful to the data. We
perform supervised fine-tuning of an open-source multimodal backbone
,ShareGPT4V-7B, on Arch-300K, yielding ArchGPT.

</details>


### [8] [Marching Neurons: Accurate Surface Extraction for Neural Implicit Shapes](https://arxiv.org/abs/2509.21007)
*Christian Stippel,Felix Mujkanovic,Thomas Leimkühler,Pedro Hermosilla*

Main category: cs.GR

TL;DR: 提出了一种从神经隐式函数中解析提取表面的新方法，避免了传统空间分解方法的精度限制，实现了高精度和高效并行的表面提取。


<details>
  <summary>Details</summary>
Motivation: 3D视觉计算中，准确的表面几何表示至关重要。现有的显式和隐式表示各有优势，但传统表面提取方法（如Marching Cubes算法）因固定的空间分解和采样导致精度不足。

Method: 开发了一种深度优先遍历策略，利用神经元的领域划分特性，直接从神经隐式函数中解析提取表面，支持并行操作并适应大型神经网络结构。

Result: 生成的网格准确捕获了网络的完整几何信息，避免了临时的空间离散化，在不同形状和网络结构上实现了前所未有的精度，同时保持了较高的速度。

Conclusion: 该方法在神经隐式函数表面提取中实现了高精度和高效率，为3D视觉计算提供了更优的表面几何表示解决方案。

Abstract: Accurate surface geometry representation is crucial in 3D visual computing.
Explicit representations, such as polygonal meshes, and implicit
representations, like signed distance functions, each have distinct advantages,
making efficient conversions between them increasingly important. Conventional
surface extraction methods for implicit representations, such as the widely
used Marching Cubes algorithm, rely on spatial decomposition and sampling,
leading to inaccuracies due to fixed and limited resolution. We introduce a
novel approach for analytically extracting surfaces from neural implicit
functions. Our method operates natively in parallel and can navigate large
neural architectures. By leveraging the fact that each neuron partitions the
domain, we develop a depth-first traversal strategy to efficiently track the
encoded surface. The resulting meshes faithfully capture the full geometric
information from the network without ad-hoc spatial discretization, achieving
unprecedented accuracy across diverse shapes and network architectures while
maintaining competitive speed.

</details>


### [9] [CHARM: Control-point-based 3D Anime Hairstyle Auto-Regressive Modeling](https://arxiv.org/abs/2509.21114)
*Yuze He,Yanning Zhou,Wang Zhao,Jingwen Ye,Yushi Bai,Kaiwen Xiao,Yong-Jin Liu,Zhongqian Sun,Wei Yang*

Main category: cs.GR

TL;DR: CHARM是一种用于动漫发型建模的新型参数化表示和生成框架，通过控制点参数化和自回归生成模型，高效支持艺术设计与学习生成。


<details>
  <summary>Details</summary>
Motivation: 传统发型建模方法难以处理高度风格化和分段结构的动漫发型，现有技术依赖于密集网格建模或手工样条曲线，效率低下且不适合扩展学习。

Method: CHARM提出了基于控制点的紧凑可逆参数化表示，每个发片由控制点序列表示，并采用自回归生成框架从输入图像或点云生成动漫发型。

Result: CHARM在重建精度和生成质量上表现出色，并通过AnimeHair数据集（包含37K高质量动漫发型）验证了其性能。

Conclusion: CHARM为动漫发型建模提供了一种高效、可扩展的解决方案，支持艺术设计和生成任务。

Abstract: We present CHARM, a novel parametric representation and generative framework
for anime hairstyle modeling. While traditional hair modeling methods focus on
realistic hair using strand-based or volumetric representations, anime
hairstyle exhibits highly stylized, piecewise-structured geometry that
challenges existing techniques. Existing works often rely on dense mesh
modeling or hand-crafted spline curves, making them inefficient for editing and
unsuitable for scalable learning. CHARM introduces a compact, invertible
control-point-based parameterization, where a sequence of control points
represents each hair card, and each point is encoded with only five geometric
parameters. This efficient and accurate representation supports both
artist-friendly design and learning-based generation. Built upon this
representation, CHARM introduces an autoregressive generative framework that
effectively generates anime hairstyles from input images or point clouds. By
interpreting anime hairstyles as a sequential "hair language", our
autoregressive transformer captures both local geometry and global hairstyle
topology, resulting in high-fidelity anime hairstyle creation. To facilitate
both training and evaluation of anime hairstyle generation, we construct
AnimeHair, a large-scale dataset of 37K high-quality anime hairstyles with
separated hair cards and processed mesh data. Extensive experiments demonstrate
state-of-the-art performance of CHARM in both reconstruction accuracy and
generation quality, offering an expressive and scalable solution for anime
hairstyle modeling. Project page: https://hyzcluster.github.io/charm/

</details>


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [10] [Dual-Language General-Purpose Self-Hosted Visual Language and new Textual Programming Language for Applications](https://arxiv.org/abs/2509.20426)
*Mahmoud Samir Fayed*

Main category: cs.PL

TL;DR: PWCT2是一种双语言（阿拉伯语/英语）、通用、自托管的可视化编程语言，通过Ring文本编程语言开发，显著提高了代码生成速度和存储效率。


<details>
  <summary>Details</summary>
Motivation: 现有通用可视化编程语言（VPL）如PWCT需依赖文本编程语言改进，限制了其发展。本文旨在通过开发自托管的PWCT2解决这一问题。

Method: 首先设计了轻量级动态类型文本编程语言Ring，用于开发PWCT2。PWCT2使用Ring开发，并通过PWCT可视化实现Ring编译器和虚拟机。

Result: PWCT2代码生成速度快36倍，视觉源文件存储减少20倍，支持Ring代码转换为视觉代码，实现了自托管开发。

Conclusion: PWCT2通过自托管和高效性能证明了其可行性，用户反馈积极，推动了进一步研究。

Abstract: Most visual programming languages (VPLs) are domain-specific, with few
general-purpose VPLs like Programming Without Coding Technology (PWCT). These
general-purpose VPLs are developed using textual programming languages and
improving them requires textual programming. In this thesis, we designed and
developed PWCT2, a dual-language (Arabic/English), general-purpose,
self-hosting visual programming language. Before doing so, we specifically
designed a textual programming language called Ring for its development. Ring
is a dynamically typed language with a lightweight implementation, offering
syntax customization features. It permits the creation of domain-specific
languages through new features that extend object-oriented programming,
allowing for specialized languages resembling Cascading Style Sheets (CSS) or
Supernova language. The Ring Compiler and Virtual Machine are designed using
the PWCT visual programming language where the visual implementation is
composed of 18,945 components that generate 24,743 lines of C code, which
increases the abstraction level and hides unnecessary details. Using PWCT to
develop Ring allowed us to realize several issues in PWCT, which led to the
development of the PWCT2 visual programming language using the Ring textual
programming language. PWCT2 provides approximately 36 times faster code
generation and requires 20 times less storage for visual source files. It also
allows for the conversion of Ring code into visual code, enabling the creation
of a self-hosting VPL that can be developed using itself. PWCT2 consists of
approximately 92,000 lines of Ring code and comes with 394 visual components.
PWCT2 is distributed to many users through the Steam platform and has received
positive feedback, On Steam, 1772 users have launched the software, and the
total recorded usage time exceeds 17,000 hours, encouraging further research
and development.

</details>


### [11] [Efficient Symbolic Computation vis Hash Consing](https://arxiv.org/abs/2509.20534)
*Bowen Zhu,Aayush Sabharwal,Songchen Tan,Yingbo Ma,Alan Edelman,Christopher Rackauckas*

Main category: cs.PL

TL;DR: 论文提出了一种将hash consing技术集成到JuliaSymbolics中的方法，以减少内存冗余和提升性能，在多个计算领域取得了显著的加速效果。


<details>
  <summary>Details</summary>
Motivation: 传统的符号计算系统因存储结构相同的子表达式而导致内存冗余（表达式膨胀），影响了经典计算机代数和AI驱动的数学推理工具的性能。为此，论文旨在通过hash consing技术解决这一问题。

Method: 论文方法包括在JuliaSymbolics中引入全局弱引用哈希表，对表达式进行规范化处理并消除重复存储。该方法与Julia的元编程和即时编译基础设施无缝集成。

Result: 实验结果表明，该方法在符号计算、内存使用、代码生成、函数编译和数值评估等方面均有显著提升，加速比最高可达100倍。尽管某些工作负载的初始计算阶段收益较小或略有开销，但下游处理始终受益明显。

Conclusion: 论文的研究强调了hash consing在扩展符号计算中的重要性，并为进一步在AI驱动的流程中结合e-graphs实现增强的表达式共享指明了未来的研究方向。

Abstract: Symbolic computation systems suffer from memory inefficiencies due to
redundant storage of structurally identical subexpressions, commonly known as
expression swell, which degrades performance in both classical computer algebra
and emerging AI-driven mathematical reasoning tools. In this paper, we present
the first integration of hash consing into JuliaSymbolics, a high-performance
symbolic toolkit in Julia, by employing a global weak-reference hash table that
canonicalizes expressions and eliminates duplication. This approach reduces
memory consumption and accelerates key operations such as differentiation,
simplification, and code generation, while seamlessly integrating with Julia's
metaprogramming and just-in-time compilation infrastructure. Benchmark
evaluations across different computational domains reveal substantial
improvements: symbolic computations are accelerated by up to 3.2 times, memory
usage is reduced by up to 2 times, code generation is up to 5 times faster,
function compilation up to 10 times faster, and numerical evaluation up to 100
times faster for larger models. While certain workloads with fewer duplicate
unknown-variable expressions show more modest gains or even slight overhead in
initial computation stages, downstream processing consistently benefits
significantly. These findings underscore the importance of hash consing in
scaling symbolic computation and pave the way for future work integrating hash
consing with e-graphs for enhanced equivalence-aware expression sharing in
AI-driven pipelines.

</details>


<div id='cs.GT'></div>

# cs.GT [[Back]](#toc)

### [12] [Efficient Kernelized Learning in Polyhedral Games Beyond Full-Information: From Colonel Blotto to Congestion Games](https://arxiv.org/abs/2509.20919)
*Andreas Kontogiannis,Vasilis Pollatos,Gabriele Farina,Panayotis Mertikopoulos,Ioannis Panageas*

Main category: cs.GT

TL;DR: 该论文探讨了在多面体游戏中高效学习粗相关均衡 (CCE) 的问题，提出了一种基于核化框架的方法，显著提高了部分信息环境下的计算效率。


<details>
  <summary>Details</summary>
Motivation: 在多面体游戏中，行动集规模庞大且具有组合结构，使得学习粗相关均衡的计算复杂度较高。现有方法在部分信息环境下效率不足，因此需要一种更高效的算法来解决这一挑战。

Method: 论文提出了一种基于核化框架的方法，并将其应用于多面体游戏（如 Colonel Blotto、图形拟阵和网络拥塞游戏），设计了计算高效的基于支付的算法。

Result: 该方法显著改进了现有工作的运行时复杂度，能够在部分信息环境下更高效地学习粗相关均衡。

Conclusion: 通过核化框架的应用，论文为多面体游戏中的粗相关均衡学习提供了一种高效且实用的算法，解决了现有方法的局限性。

Abstract: We examine the problem of efficiently learning coarse correlated equilibria
(CCE) in polyhedral games, that is, normal-form games with an exponentially
large number of actions per player and an underlying combinatorial structure.
Prominent examples of such games are the classical Colonel Blotto and
congestion games. To achieve computational efficiency, the learning algorithms
must exhibit regret and per-iteration complexity that scale polylogarithmically
in the size of the players' action sets. This challenge has recently been
addressed in the full-information setting, primarily through the use of
kernelization. However, in the case of the realistic, but mathematically
challenging, partial-information setting, existing approaches result in
suboptimal and impractical runtime complexity to learn CCE. We tackle this
limitation by building a framework based on the kernelization paradigm. We
apply this framework to prominent examples of polyhedral games -- namely the
Colonel Blotto, graphic matroid and network congestion games -- and provide
computationally efficient payoff-based learning algorithms, which significantly
improve upon prior works in terms of the runtime for learning CCE in these
settings.

</details>


### [13] [A Category Theoretic Approach to Approximate Game Theory](https://arxiv.org/abs/2509.20932)
*Neil Ghani*

Main category: cs.GT

TL;DR: 本文利用范畴论为近似博弈论提出了一种全新方法，通过选择函数和开放博弈模型研究了近似均衡的代数性质与组合结构。


<details>
  <summary>Details</summary>
Motivation: 精确解在计算中可能难以实现，尤其是在多智能体系统中存在固有不确定性的情况下。因此，研究近似最优决策具有实际意义。

Method: 首先选择函数进行研究，建立了近似均衡的简单而鲁棒的模型，并分析了其代数性质和组合结构；随后将该方法推广到更高级的开放博弈模型。

Result: 成功建立了选择函数和开放博弈模型中的近似均衡理论，并验证了其代数性质和组合结构的有效性。

Conclusion: 通过范畴论方法，本文为近似博弈论提供了新的理论框架，揭示了近似均衡的关键性质，并为复杂场景下的决策提供了理论基础。

Abstract: This paper uses category theory to develop an entirely new approach to
approximate game theory. Game theory is the study of how different agents
within a multi-agent system take decisions. At its core, game theory asks what
an optimal decision is in a given scenario. Thus approximate game theory asks
what is an approximately optimal decision in a given scenario. This is
important in practice as -- just like in much of computing -- exact answers
maybe too difficult to compute or even impossible to compute given inherent
uncertainty in input.
  We consider first "Selection Functions" which are functions and develop a
simple yet robust model of approximate equilibria. We develop the algebraic
properties of approximation wrt selection functions and also relate
approximation to the compositional structure of selection functions. We then
repeat this process successfully for Open Games -- a more advanced model of
game theory.

</details>
