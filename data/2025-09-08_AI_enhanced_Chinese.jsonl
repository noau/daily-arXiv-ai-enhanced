{"id": "2509.04936", "categories": ["cs.PL", "cs.SE"], "pdf": "https://arxiv.org/pdf/2509.04936", "abs": "https://arxiv.org/abs/2509.04936", "authors": ["Andrea Gilot", "Tobias Wrigstad", "Eva Darulova"], "title": "A Large-Scale Study of Floating-Point Usage in Statically Typed Languages", "comment": null, "summary": "Reasoning about floating-point arithmetic is notoriously hard. While static\nand dynamic analysis techniques or program repair have made significant\nprogress, more work is still needed to make them relevant to real-world code.\nOn the critical path to that goal is understanding what real-world\nfloating-point code looks like. To close that knowledge gap, this paper\npresents the first large-scale empirical study of floating-point arithmetic\nusage in statically typed languages across public GitHub repositories. We\nfollow state-of the art mining practices including random sampling and\nfiltering based on only intrinsic properties to avoid bias, and identify\nfloating-point usage by searching for keywords in the source code, and\nprogramming language constructs (e.g., loops) by parsing the code. Our\nevaluation supports the claim often made in papers that floating-point\narithmetic is widely used. Comparing statistics such as size and usage of\ncertain constructs and functions, we find that benchmarks used in literature to\nevaluate automated reasoning techniques for floating-point arithmetic are in\ncertain aspects representative of 'real-world' code, but not in all. We aim for\nour study and dataset to help future techniques for floating-point arithmetic\nto be designed and evaluated to match actual users' expectations.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u9996\u6b21\u5bf9\u9759\u6001\u7c7b\u578b\u8bed\u8a00\u4e2d\u6d6e\u70b9\u8fd0\u7b97\u5728\u516c\u5171GitHub\u4ed3\u5e93\u4e2d\u7684\u4f7f\u7528\u8fdb\u884c\u4e86\u5927\u89c4\u6a21\u5b9e\u8bc1\u7814\u7a76\uff0c\u63ed\u793a\u4e86\u6d6e\u70b9\u8fd0\u7b97\u5728\u5b9e\u9645\u4ee3\u7801\u4e2d\u7684\u5e7f\u6cdb\u4f7f\u7528\u53ca\u5176\u7279\u6027\u3002", "motivation": "\u7531\u4e8e\u6d6e\u70b9\u7b97\u672f\u7684\u5206\u6790\u548c\u4fee\u590d\u6280\u672f\u5728\u5b9e\u9645\u4ee3\u7801\u4e2d\u7684\u5e94\u7528\u4ecd\u7136\u4e0d\u8db3\uff0c\u7406\u89e3\u5b9e\u9645\u4ee3\u7801\u4e2d\u6d6e\u70b9\u8fd0\u7b97\u7684\u7279\u70b9\u6210\u4e3a\u5173\u952e\u3002\u8bba\u6587\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u77e5\u8bc6\u7a7a\u767d\uff0c\u4e3a\u672a\u6765\u6280\u672f\u8bbe\u8ba1\u548c\u8bc4\u4f30\u63d0\u4f9b\u4f9d\u636e\u3002", "method": "\u8bba\u6587\u91c7\u7528\u6700\u5148\u8fdb\u7684\u6316\u6398\u5b9e\u8df5\uff0c\u5305\u62ec\u968f\u673a\u62bd\u6837\u548c\u57fa\u4e8e\u5185\u5728\u5c5e\u6027\u7684\u8fc7\u6ee4\uff0c\u901a\u8fc7\u89e3\u6790\u6e90\u4ee3\u7801\u4e2d\u7684\u5173\u952e\u5b57\u548c\u7f16\u7a0b\u8bed\u8a00\u6784\u9020\uff08\u5982\u5faa\u73af\uff09\u6765\u8bc6\u522b\u6d6e\u70b9\u8fd0\u7b97\u7684\u4f7f\u7528\u3002", "result": "\u7814\u7a76\u53d1\u73b0\u6d6e\u70b9\u7b97\u672f\u5728\u5b9e\u9645\u4ee3\u7801\u4e2d\u88ab\u5e7f\u6cdb\u4f7f\u7528\uff0c\u4f46\u6587\u732e\u4e2d\u7528\u4e8e\u8bc4\u4f30\u81ea\u52a8\u63a8\u7406\u6280\u672f\u7684\u57fa\u51c6\u5728\u67d0\u4e9b\u65b9\u9762\u4e0e\u201c\u771f\u5b9e\u4e16\u754c\u201d\u4ee3\u7801\u4e00\u81f4\uff0c\u4f46\u5728\u67d0\u4e9b\u65b9\u9762\u4e0d\u4e00\u81f4\u3002", "conclusion": "\u8bba\u6587\u7684\u7814\u7a76\u548c\u6570\u636e\u96c6\u65e8\u5728\u5e2e\u52a9\u672a\u6765\u6d6e\u70b9\u8fd0\u7b97\u6280\u672f\u7684\u8bbe\u8ba1\u548c\u8bc4\u4f30\uff0c\u4ee5\u66f4\u7b26\u5408\u5b9e\u9645\u7528\u6237\u7684\u671f\u671b\u3002"}}
{"id": "2509.05160", "categories": ["cs.PL", "cs.SE"], "pdf": "https://arxiv.org/pdf/2509.05160", "abs": "https://arxiv.org/abs/2509.05160", "authors": ["Steven Smyth", "Daniel Busch", "Moez Ben Haj Hmida", "Edward A. Lee", "Bernhard Steffen"], "title": "AI-Assisted Modeling: DSL-Driven AI Interactions", "comment": "7 pages, 4 figures", "summary": "AI-assisted programming greatly increases software development performance.\nWe enhance this potential by integrating transparency through domain-specific\nmodeling techniques and providing instantaneous, graphical visualizations that\naccurately represent the semantics of AI-generated code. This approach\nfacilitates visual inspection and formal verification, such as model checking.\n  Formal models can be developed using programming, natural language prompts,\nvoice commands, and stage-wise refinement, with immediate feedback after each\ntransformation step. This support can be tailored to specific domains or\nintended purposes, improving both code generation and subsequent validation\nprocesses.\n  To demonstrate the effectiveness of this approach, we have developed a\nprototype as a Visual Studio Code extension for the Lingua Franca language.\nThis prototype showcases the potential for novel domain-specific modeling\npractices, offering an advancement in how models are created, visualized, and\nverified.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86\u4e00\u79cd\u901a\u8fc7\u900f\u660e\u5316AI\u751f\u6210\u4ee3\u7801\u7684\u65b9\u6cd5\uff0c\u7ed3\u5408\u9886\u57df\u7279\u5b9a\u5efa\u6a21\u6280\u672f\u548c\u5373\u65f6\u56fe\u5f62\u53ef\u89c6\u5316\uff0c\u63d0\u5347\u8f6f\u4ef6\u5f00\u53d1\u6548\u7387\u548c\u9a8c\u8bc1\u8fc7\u7a0b\u3002", "motivation": "\u4e3a\u4e86\u63d0\u9ad8AI\u8f85\u52a9\u7f16\u7a0b\u7684\u900f\u660e\u5ea6\u548c\u9a8c\u8bc1\u6548\u7387\uff0c\u4f5c\u8005\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u9886\u57df\u7279\u5b9a\u5efa\u6a21\u548c\u5373\u65f6\u53ef\u89c6\u5316\u7684\u65b9\u6cd5\uff0c\u65e8\u5728\u6539\u5584\u4ee3\u7801\u751f\u6210\u548c\u9a8c\u8bc1\u8fc7\u7a0b\u3002", "method": "\u901a\u8fc7\u7f16\u7a0b\u3001\u81ea\u7136\u8bed\u8a00\u63d0\u793a\u3001\u8bed\u97f3\u547d\u4ee4\u548c\u9010\u6b65\u7ec6\u5316\u5f00\u53d1\u5f62\u5f0f\u5316\u6a21\u578b\uff0c\u5e76\u5728\u6bcf\u4e00\u6b65\u8f6c\u6362\u540e\u63d0\u4f9b\u5373\u65f6\u53cd\u9988\u3002\u6b64\u5916\uff0c\u5f00\u53d1\u4e86\u4e00\u4e2aVisual Studio Code\u6269\u5c55\u539f\u578b\uff0c\u7528\u4e8eLingua Franca\u8bed\u8a00\u3002", "result": "\u539f\u578b\u5c55\u793a\u4e86\u9886\u57df\u7279\u5b9a\u5efa\u6a21\u5b9e\u8df5\u7684\u6f5c\u529b\uff0c\u80fd\u591f\u6539\u8fdb\u6a21\u578b\u7684\u521b\u5efa\u3001\u53ef\u89c6\u5316\u548c\u9a8c\u8bc1\u8fc7\u7a0b\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u901a\u8fc7\u900f\u660e\u5316\u548c\u5373\u65f6\u53cd\u9988\u673a\u5236\uff0c\u663e\u8457\u63d0\u5347\u4e86AI\u8f85\u52a9\u7f16\u7a0b\u7684\u6548\u7387\u548c\u9a8c\u8bc1\u53ef\u9760\u6027\u3002"}}
{"id": "2509.05293", "categories": ["cs.PL", "cs.CL", "cs.SE", "D.3; F.3"], "pdf": "https://arxiv.org/pdf/2509.05293", "abs": "https://arxiv.org/abs/2509.05293", "authors": ["Julien Vanegue", "Jules Villard", "Peter O'Hearn", "Azalea Raad"], "title": "Non-Termination Proving: 100 Million LoC and Beyond", "comment": "14 pages, 4 figures", "summary": "We report on our tool, Pulse Infinite, that uses proof techniques to show\nnon-termination (divergence) in large programs. Pulse Infinite works\ncompositionally and under-approximately: the former supports scale, and the\nlatter ensures soundness for proving divergence. Prior work focused on small\nbenchmarks in the tens or hundreds of lines of code (LoC), and scale limits\ntheir practicality: a single company may have tens of millions, or even\nhundreds of millions of LoC or more. We report on applying Pulse Infinite to\nover a hundred million lines of open-source and proprietary software written in\nC, C++, and Hack, identifying over 30 previously unknown issues, establishing a\nnew state of the art for detecting divergence in real-world codebases.", "AI": {"tldr": "Pulse Infinite \u662f\u4e00\u6b3e\u5229\u7528\u8bc1\u660e\u6280\u672f\u68c0\u6d4b\u5927\u578b\u7a0b\u5e8f\u4e2d\u975e\u7ec8\u6b62\uff08\u53d1\u6563\uff09\u95ee\u9898\u7684\u5de5\u5177\uff0c\u901a\u8fc7\u7ec4\u5408\u548c\u4e0d\u8db3\u8fd1\u4f3c\u65b9\u6cd5\u5b9e\u73b0\u89c4\u6a21\u5316\u4e0e\u53ef\u9760\u6027\uff0c\u6210\u529f\u5e94\u7528\u4e8e\u8d85\u8fc7\u4e00\u4ebf\u884c\u4ee3\u7801\uff0c\u53d1\u73b030\u591a\u4e2a\u672a\u77e5\u95ee\u9898\u3002", "motivation": "\u4ee5\u5f80\u7684\u7814\u7a76\u5c40\u9650\u4e8e\u5c0f\u89c4\u6a21\u4ee3\u7801\uff08\u51e0\u5341\u5230\u51e0\u767e\u884c\uff09\uff0c\u96be\u4ee5\u6ee1\u8db3\u5b9e\u9645\u9700\u6c42\uff08\u5982\u4f01\u4e1a\u4ee3\u7801\u89c4\u6a21\u53ef\u8fbe\u6570\u5343\u4e07\u751a\u81f3\u6570\u4ebf\u884c\uff09\uff0cPulse Infinite \u65e8\u5728\u89e3\u51b3\u5927\u89c4\u6a21\u4ee3\u7801\u4e2d\u7684\u975e\u7ec8\u6b62\u95ee\u9898\u3002", "method": "Pulse Infinite \u4f7f\u7528\u7ec4\u5408\u65b9\u6cd5\u548c\u4e0d\u8db3\u8fd1\u4f3c\u6280\u672f\uff0c\u652f\u6301\u89c4\u6a21\u5316\u5206\u6790\u5e76\u786e\u4fdd\u8bc1\u660e\u7684\u53d1\u6563\u6027\u7ed3\u679c\u7684\u53ef\u9760\u6027\u3002", "result": "\u8be5\u5de5\u5177\u5e94\u7528\u4e8e\u5305\u62ecC\u3001C++\u548cHack\u5728\u5185\u7684\u8d85\u8fc7\u4e00\u4ebf\u884c\u5f00\u6e90\u548c\u4e13\u6709\u4ee3\u7801\uff0c\u6210\u529f\u8bc6\u522b\u51fa30\u591a\u4e2a\u672a\u77e5\u95ee\u9898\uff0c\u8fbe\u5230\u4e86\u68c0\u6d4b\u771f\u5b9e\u4ee3\u7801\u5e93\u4e2d\u53d1\u6563\u95ee\u9898\u7684\u65b0\u6c34\u5e73\u3002", "conclusion": "Pulse Infinite \u5728\u89c4\u6a21\u5316\u68c0\u6d4b\u7a0b\u5e8f\u975e\u7ec8\u6b62\u95ee\u9898\u4e0a\u8868\u73b0\u51fa\u8272\uff0c\u4e3a\u5b9e\u9645\u5927\u89c4\u6a21\u4ee3\u7801\u5e93\u7684\u53d1\u6563\u6027\u95ee\u9898\u6811\u7acb\u4e86\u65b0\u7684\u6807\u6746\u3002"}}
{"id": "2509.04481", "categories": ["cs.GR", "cs.AI", "cs.CL", "cs.MM"], "pdf": "https://arxiv.org/pdf/2509.04481", "abs": "https://arxiv.org/abs/2509.04481", "authors": ["Yi-Chun Chen", "Arnav Jhala"], "title": "Narrative-to-Scene Generation: An LLM-Driven Pipeline for 2D Game Environments", "comment": null, "summary": "Recent advances in large language models(LLMs) enable compelling story\ngeneration, but connecting narrative text to playable visual environments\nremains an open challenge in procedural content generation(PCG). We present a\nlightweight pipeline that transforms short narrative prompts into a sequence of\n2D tile-based game scenes, reflecting the temporal structure of stories. Given\nan LLM-generated narrative, our system identifies three key time frames,\nextracts spatial predicates in the form of \"Object-Relation-Object\" triples,\nand retrieves visual assets using affordance-aware semantic embeddings from the\nGameTileNet dataset. A layered terrain is generated using Cellular Automata,\nand objects are placed using spatial rules grounded in the predicate structure.\nWe evaluated our system in ten diverse stories, analyzing tile-object matching,\naffordance-layer alignment, and spatial constraint satisfaction across frames.\nThis prototype offers a scalable approach to narrative-driven scene generation\nand lays the foundation for future work on multi-frame continuity, symbolic\ntracking, and multi-agent coordination in story-centered PCG.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u8f7b\u91cf\u7ea7\u6d41\u7a0b\uff0c\u5c06\u77ed\u7bc7\u53d9\u4e8b\u63d0\u793a\u8f6c\u5316\u4e3a2D\u6e38\u620f\u573a\u666f\u5e8f\u5217\uff0c\u901a\u8fc7LLM\u751f\u6210\u7684\u53d9\u4e8b\u8bc6\u522b\u5173\u952e\u65f6\u95f4\u5e27\u5e76\u4f7f\u7528\u7a7a\u95f4\u8c13\u8bcd\u548c\u8bed\u4e49\u5d4c\u5165\u751f\u6210\u573a\u666f\u3002", "motivation": "\u867d\u7136\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u5728\u6545\u4e8b\u751f\u6210\u65b9\u9762\u53d6\u5f97\u4e86\u663e\u8457\u8fdb\u5c55\uff0c\u4f46\u5c06\u53d9\u4e8b\u6587\u672c\u8fde\u63a5\u5230\u53ef\u73a9\u7684\u89c6\u89c9\u73af\u5883\u4ecd\u7136\u662f\u7a0b\u5e8f\u5316\u5185\u5bb9\u751f\u6210\uff08PCG\uff09\u4e2d\u7684\u6311\u6218\u3002", "method": "\u7cfb\u7edf\u901a\u8fc7\u8bc6\u522bLLM\u53d9\u4e8b\u4e2d\u7684\u4e09\u4e2a\u5173\u952e\u65f6\u95f4\u5e27\uff0c\u63d0\u53d6\u2018\u5bf9\u8c61-\u5173\u7cfb-\u5bf9\u8c61\u2019\u4e09\u5143\u7ec4\u5f62\u5f0f\u7684\u7a7a\u95f4\u8c13\u8bcd\uff0c\u5e76\u4f7f\u7528GameTileNet\u6570\u636e\u96c6\u4e2d\u7684\u8bed\u4e49\u5d4c\u5165\u68c0\u7d22\u89c6\u89c9\u8d44\u6e90\u3002\u901a\u8fc7\u5143\u80de\u81ea\u52a8\u673a\u751f\u6210\u5730\u5f62\uff0c\u5e76\u6839\u636e\u8c13\u8bcd\u7ed3\u6784\u653e\u7f6e\u5bf9\u8c61\u3002", "result": "\u5728\u5341\u4e2a\u591a\u6837\u5316\u6545\u4e8b\u4e2d\u7684\u8bc4\u4f30\u8868\u660e\uff0c\u7cfb\u7edf\u5728\u74e6\u7247-\u5bf9\u8c61\u5339\u914d\u3001\u529f\u80fd\u5c42\u5bf9\u9f50\u548c\u7a7a\u95f4\u7ea6\u675f\u6ee1\u8db3\u65b9\u9762\u8868\u73b0\u826f\u597d\u3002", "conclusion": "\u8be5\u539f\u578b\u4e3a\u53d9\u4e8b\u9a71\u52a8\u7684\u573a\u666f\u751f\u6210\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u7684\u65b9\u6cd5\uff0c\u5e76\u4e3a\u672a\u6765\u5728\u591a\u5e27\u8fde\u7eed\u6027\u3001\u7b26\u53f7\u8ddf\u8e2a\u548c\u591a\u667a\u80fd\u4f53\u534f\u8c03\u65b9\u9762\u7684\u7814\u7a76\u5960\u5b9a\u4e86\u57fa\u7840\u3002"}}
{"id": "2509.04513", "categories": ["cs.GR", "cs.NA", "math.NA", "physics.app-ph", "65Kxx"], "pdf": "https://arxiv.org/pdf/2509.04513", "abs": "https://arxiv.org/abs/2509.04513", "authors": ["Ming Du", "Volker Rose", "Junjing Deng", "Dileep Singh", "Si Chen", "Mathew J. Cherukara"], "title": "Fidelity-preserving enhancement of ptychography with foundational text-to-image models", "comment": null, "summary": "Ptychographic phase retrieval enables high-resolution imaging of complex\nsamples but often suffers from artifacts such as grid pathology and multislice\ncrosstalk, which degrade reconstructed images. We propose a plug-and-play (PnP)\nframework that integrates physics model-based phase retrieval with text-guided\nimage editing using foundational diffusion models. By employing the alternating\ndirection method of multipliers (ADMM), our approach ensures consensus between\ndata fidelity and artifact removal subproblems, maintaining physics consistency\nwhile enhancing image quality. Artifact removal is achieved using a text-guided\ndiffusion image editing method (LEDITS++) with a pre-trained foundational\ndiffusion model, allowing users to specify artifacts for removal in natural\nlanguage. Demonstrations on simulated and experimental datasets show\nsignificant improvements in artifact suppression and structural fidelity,\nvalidated by metrics such as peak signal-to-noise ratio (PSNR) and diffraction\npattern consistency. This work highlights the combination of text-guided\ngenerative models and model-based phase retrieval algorithms as a transferable\nand fidelity-preserving method for high-quality diffraction imaging.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u7269\u7406\u6a21\u578b\u76f8\u4f4d\u68c0\u7d22\u4e0e\u6587\u672c\u5f15\u5bfc\u56fe\u50cf\u7f16\u8f91\u7684PnP\u6846\u67b6\uff0c\u901a\u8fc7ADMM\u65b9\u6cd5\u786e\u4fdd\u6570\u636e\u4fdd\u771f\u5ea6\u4e0e\u4f2a\u5f71\u53bb\u9664\u7684\u4e00\u81f4\u6027\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6210\u50cf\u8d28\u91cf\u3002", "motivation": "\u4e3a\u4e86\u89e3\u51b3Ptychographic\u76f8\u4f4d\u68c0\u7d22\u4e2d\u5e38\u89c1\u7684\u7f51\u683c\u4f2a\u5f71\u548c\u591a\u5c42\u4e32\u6270\u95ee\u9898\uff0c\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u7269\u7406\u6a21\u578b\u4e0e\u6587\u672c\u5f15\u5bfc\u7f16\u8f91\u7684\u65b0\u65b9\u6cd5\u3002", "method": "\u91c7\u7528\u4ea4\u66ff\u65b9\u5411\u4e58\u5b50\u6cd5(ADMM)\uff0c\u5c06\u6570\u636e\u4fdd\u771f\u5ea6\u4e0e\u57fa\u4e8e\u9884\u8bad\u7ec3\u6269\u6563\u6a21\u578b(LEDITS++)\u7684\u6587\u672c\u5f15\u5bfc\u4f2a\u5f71\u53bb\u9664\u76f8\u7ed3\u5408\u3002", "result": "\u5728\u6a21\u62df\u548c\u5b9e\u9a8c\u6570\u636e\u96c6\u4e0a\uff0c\u8be5\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86\u4f2a\u5f71\u6291\u5236\u548c\u7ed3\u6784\u4fdd\u771f\u5ea6\uff0cPSNR\u548c\u884d\u5c04\u56fe\u6848\u4e00\u81f4\u6027\u7b49\u6307\u6807\u5747\u5f97\u5230\u9a8c\u8bc1\u3002", "conclusion": "\u8be5\u7814\u7a76\u5c55\u793a\u4e86\u6587\u672c\u5f15\u5bfc\u751f\u6210\u6a21\u578b\u4e0e\u57fa\u4e8e\u6a21\u578b\u7684\u76f8\u4f4d\u68c0\u7d22\u7b97\u6cd5\u7684\u7ed3\u5408\uff0c\u4e3a\u9ad8\u8d28\u91cf\u884d\u5c04\u6210\u50cf\u63d0\u4f9b\u4e86\u4e00\u79cd\u53ef\u8fc1\u79fb\u4e14\u4fdd\u771f\u5ea6\u9ad8\u7684\u65b9\u6cd5\u3002"}}
{"id": "2509.05285", "categories": ["cs.GR", "cs.CV"], "pdf": "https://arxiv.org/pdf/2509.05285", "abs": "https://arxiv.org/abs/2509.05285", "authors": ["Haruo Fujiwara", "Yusuke Mukuta", "Tatsuya Harada"], "title": "Improved 3D Scene Stylization via Text-Guided Generative Image Editing with Region-Based Control", "comment": null, "summary": "Recent advances in text-driven 3D scene editing and stylization, which\nleverage the powerful capabilities of 2D generative models, have demonstrated\npromising outcomes. However, challenges remain in ensuring high-quality\nstylization and view consistency simultaneously. Moreover, applying style\nconsistently to different regions or objects in the scene with semantic\ncorrespondence is a challenging task. To address these limitations, we\nintroduce techniques that enhance the quality of 3D stylization while\nmaintaining view consistency and providing optional region-controlled style\ntransfer. Our method achieves stylization by re-training an initial 3D\nrepresentation using stylized multi-view 2D images of the source views.\nTherefore, ensuring both style consistency and view consistency of stylized\nmulti-view images is crucial. We achieve this by extending the style-aligned\ndepth-conditioned view generation framework, replacing the fully shared\nattention mechanism with a single reference-based attention-sharing mechanism,\nwhich effectively aligns style across different viewpoints. Additionally,\ninspired by recent 3D inpainting methods, we utilize a grid of multiple depth\nmaps as a single-image reference to further strengthen view consistency among\nstylized images. Finally, we propose Multi-Region Importance-Weighted Sliced\nWasserstein Distance Loss, allowing styles to be applied to distinct image\nregions using segmentation masks from off-the-shelf models. We demonstrate that\nthis optional feature enhances the faithfulness of style transfer and enables\nthe mixing of different styles across distinct regions of the scene.\nExperimental evaluations, both qualitative and quantitative, demonstrate that\nour pipeline effectively improves the results of text-driven 3D stylization.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u6539\u8fdb\u6587\u672c\u9a71\u52a8\u76843D\u573a\u666f\u7f16\u8f91\u548c\u98ce\u683c\u5316\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u5f15\u5165\u591a\u89c6\u56fe\u98ce\u683c\u4e00\u81f4\u6027\u548c\u533a\u57df\u63a7\u5236\u7684\u98ce\u683c\u8fc1\u79fb\u6280\u672f\uff0c\u663e\u8457\u63d0\u5347\u4e863D\u98ce\u683c\u5316\u7684\u8d28\u91cf\u548c\u89c6\u56fe\u4e00\u81f4\u6027\u3002", "motivation": "\u5c3d\u7ba1\u73b0\u6709\u76842D\u751f\u6210\u6a21\u578b\u57283D\u573a\u666f\u7f16\u8f91\u548c\u98ce\u683c\u5316\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5728\u4fdd\u6301\u9ad8\u8d28\u91cf\u98ce\u683c\u5316\u548c\u89c6\u56fe\u4e00\u81f4\u6027\u7684\u540c\u65f6\uff0c\u786e\u4fdd\u5bf9\u4e0d\u540c\u533a\u57df\u6216\u5bf9\u8c61\u7684\u98ce\u683c\u5316\u8bed\u4e49\u4e00\u81f4\u6027\u4ecd\u7136\u5177\u6709\u6311\u6218\u6027\u3002", "method": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b9\u6cd5\uff0c\u901a\u8fc7\u91cd\u65b0\u8bad\u7ec3\u521d\u59cb3D\u8868\u793a\u6765\u4f7f\u7528\u98ce\u683c\u5316\u7684\u591a\u89c6\u56fe2D\u56fe\u50cf\uff0c\u91c7\u7528\u57fa\u4e8e\u5355\u4e00\u53c2\u8003\u7684\u6ce8\u610f\u529b\u5171\u4eab\u673a\u5236\u6765\u5bf9\u9f50\u98ce\u683c\uff0c\u5e76\u5229\u7528\u591a\u6df1\u5ea6\u56fe\u7f51\u683c\u589e\u5f3a\u89c6\u56fe\u4e00\u81f4\u6027\u3002\u6b64\u5916\uff0c\u5f15\u5165\u4e86\u591a\u533a\u57df\u91cd\u8981\u6027\u52a0\u6743\u5207\u7247Wasserstein\u8ddd\u79bb\u635f\u5931\uff0c\u5b9e\u73b0\u57fa\u4e8e\u5206\u5272\u63a9\u6a21\u7684\u533a\u57df\u5316\u98ce\u683c\u8fc1\u79fb\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u6587\u672c\u9a71\u52a8\u76843D\u98ce\u683c\u5316\u4e2d\u663e\u8457\u63d0\u5347\u4e86\u98ce\u683c\u4e00\u81f4\u6027\u548c\u89c6\u56fe\u4e00\u81f4\u6027\uff0c\u540c\u65f6\u652f\u6301\u5bf9\u4e0d\u540c\u533a\u57df\u7684\u98ce\u683c\u6df7\u5408\u3002", "conclusion": "\u8bba\u6587\u63d0\u51fa\u7684\u6280\u672f\u6709\u6548\u89e3\u51b3\u4e863D\u98ce\u683c\u5316\u4e2d\u7684\u5173\u952e\u6311\u6218\uff0c\u901a\u8fc7\u6539\u8fdb\u7684\u6ce8\u610f\u529b\u673a\u5236\u548c\u533a\u57df\u63a7\u5236\u65b9\u6cd5\uff0c\u4e3a\u672a\u6765\u76843D\u573a\u666f\u7f16\u8f91\u548c\u98ce\u683c\u5316\u63d0\u4f9b\u4e86\u65b0\u7684\u53ef\u80fd\u6027\u3002"}}
