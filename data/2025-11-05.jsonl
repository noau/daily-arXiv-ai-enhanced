{"id": "2511.02157", "categories": ["cs.GT", "cs.AI", "cs.LG", "cs.SY", "eess.SY", "math.OC"], "pdf": "https://arxiv.org/pdf/2511.02157", "abs": "https://arxiv.org/abs/2511.02157", "authors": ["Asrin Efe Yorulmaz", "Tamer Ba≈üar"], "title": "Near Optimal Convergence to Coarse Correlated Equilibrium in General-Sum Markov Games", "comment": null, "summary": "No-regret learning dynamics play a central role in game theory, enabling\ndecentralized convergence to equilibrium for concepts such as Coarse Correlated\nEquilibrium (CCE) or Correlated Equilibrium (CE). In this work, we improve the\nconvergence rate to CCE in general-sum Markov games, reducing it from the\npreviously best-known rate of $\\mathcal{O}(\\log^5 T / T)$ to a sharper\n$\\mathcal{O}(\\log T / T)$. This matches the best known convergence rate for CE\nin terms of $T$, number of iterations, while also improving the dependence on\nthe action set size from polynomial to polylogarithmic-yielding exponential\ngains in high-dimensional settings. Our approach builds on recent advances in\nadaptive step-size techniques for no-regret algorithms in normal-form games,\nand extends them to the Markovian setting via a stage-wise scheme that adjusts\nlearning rates based on real-time feedback. We frame policy updates as an\ninstance of Optimistic Follow-the-Regularized-Leader (OFTRL), customized for\nvalue-iteration-based learning. The resulting self-play algorithm achieves, to\nour knowledge, the fastest known convergence rate to CCE in Markov games."}
{"id": "2511.02746", "categories": ["cs.GT"], "pdf": "https://arxiv.org/pdf/2511.02746", "abs": "https://arxiv.org/abs/2511.02746", "authors": ["Jiaxin Song", "Parnian Shahkar", "Kate Donahue", "Bhaskar Ray Chaudhury"], "title": "Human-AI Collaboration with Misaligned Preferences", "comment": "37 pages, 8 figures, appeared at EAAMO'25", "summary": "In many real-life settings, algorithms play the role of assistants, while\nhumans ultimately make the final decision. Often, algorithms specifically act\nas curators, narrowing down a wide range of options into a smaller subset that\nthe human picks between: consider content recommendation or chatbot responses\nto questions with multiple valid answers. Crucially, humans may not know their\nown preferences perfectly either, but instead may only have access to a noisy\nsampling over preferences. Algorithms can assist humans by curating a smaller\nsubset of items, but must also face the challenge of misalignment: humans may\nhave different preferences from each other (and from the algorithm), and the\nalgorithm may not know the exact preferences of the human they are facing at\nany point in time. In this paper, we model and theoretically study such a\nsetting. Specifically, we show instances where humans benefit by collaborating\nwith a misaligned algorithm. Surprisingly, we show that humans gain more\nutility from a misaligned algorithm (which makes different mistakes) than from\nan aligned algorithm. Next, we build on this result by studying what properties\nof algorithms maximize human welfare when the goals could be either utilitarian\nwelfare or ensuring all humans benefit. We conclude by discussing implications\nfor designers of algorithmic tools and policymakers."}
{"id": "2511.01894", "categories": ["cs.GR", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.01894", "abs": "https://arxiv.org/abs/2511.01894", "authors": ["Fangbing Liu", "Pengfei Duan", "Wen Li", "Yi He"], "title": "LGCC: Enhancing Flow Matching Based Text-Guided Image Editing with Local Gaussian Coupling and Context Consistency", "comment": null, "summary": "Recent advancements have demonstrated the great potential of flow\nmatching-based Multimodal Large Language Models (MLLMs) in image editing.\nHowever, state-of-the-art works like BAGEL face limitations, including detail\ndegradation, content inconsistency, and inefficiency due to their reliance on\nrandom noise initialization. To address these issues, we propose LGCC, a novel\nframework with two key components: Local Gaussian Noise Coupling (LGNC) and\nContent Consistency Loss (CCL). LGNC preserves spatial details by modeling\ntarget image embeddings and their locally perturbed counterparts as coupled\npairs, while CCL ensures semantic alignment between edit instructions and image\nmodifications, preventing unintended content removal. By integrating LGCC with\nthe BAGEL pre-trained model via curriculum learning, we significantly reduce\ninference steps, improving local detail scores on I2EBench by 1.60% and overall\nscores by 0.53%. LGCC achieves 3x -- 5x speedup for lightweight editing and 2x\nfor universal editing, requiring only 40% -- 50% of the inference time of BAGEL\nor Flux. These results demonstrate LGCC's ability to preserve detail, maintain\ncontextual integrity, and enhance inference speed, offering a cost-efficient\nsolution without compromising editing quality."}
{"id": "2511.02491", "categories": ["cs.PL"], "pdf": "https://arxiv.org/pdf/2511.02491", "abs": "https://arxiv.org/abs/2511.02491", "authors": ["Roland Meyer", "Jakob Tepe"], "title": "Oriented Metrics for Bottom-Up Enumerative Synthesis", "comment": "37 pages, 16 figures", "summary": "In syntax-guided synthesis, one of the challenges is to reduce the enormous\nsize of the search space. We observe that most search spaces are not just flat\nsets of programs, but can be endowed with a structure that we call an oriented\nmetric. Oriented metrics measure the distance between programs, like ordinary\nmetrics do, but are designed for settings in which operations have an\norientation. Our focus is on the string and the bitvector domains, where\noperations like concatenation and bitwise conjunction transform an input into\nan output in a way that is not symmetric. We develop several new oriented\nmetrics for these domains. Oriented metrics are designed for search space\nreduction, and we present four techniques: (i) pruning the search space to a\nball around the ground truth, (ii) factorizing the search space by an\nequivalence that is induced by the oriented metric, (iii) abstracting the\noriented metric (and hence the equivalence) and refining it, and (iv) improving\nthe enumeration order by learning from abstract information. We acknowledge\nthat these techniques are inspired by developments in the literature. By\nunderstanding their roots in oriented metrics, we can substantially increase\ntheir applicability and efficiency. We have integrated these techniques into a\nnew synthesis algorithm and implemented the algorithm in a new solver. Notably,\nour solver is generic in the oriented metric over which it computes. We\nconducted experiments in the string and the bitvector domains, and consistently\nimprove the performance over the state-of-the-art by more than an order of\nmagnitude."}
