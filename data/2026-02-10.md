<div id=toc></div>

# Table of Contents

- [cs.GR](#cs.GR) [Total: 6]
- [cs.PL](#cs.PL) [Total: 4]
- [cs.GT](#cs.GT) [Total: 7]


<div id='cs.GR'></div>

# cs.GR [[Back]](#toc)

### [1] [Convex Primitive Decomposition for Collision Detection](https://arxiv.org/abs/2602.07369)
*Julian Knodt,Xifeng Gao*

Main category: cs.GR

TL;DR: 本文提出了一种自底向上的凸体分解方法，用于将复杂网格分解为适合刚性体模拟的凸体，优于现有方法且性能更优。


<details>
  <summary>Details</summary>
Motivation: 手动为3D模型创建碰撞体耗时且低效，现有自动凸体分解方法因性能问题和缺乏灵活性而不适用于游戏等高性能需求场景。

Method: 采用自底向上的分解方法，受四边形网格简化启发，将输入网格分解为凸体，确保包围输入表面并提供合理的模拟性能。

Result: 在60多个模型上测试，该方法在Hausdorff和Chamfer距离上优于V-HACD和CoACD，且复杂度更低；在24个模型中，刚性体模拟性能一致提升。

Conclusion: 该方法在生成碰撞体时更高效，性能更优，适用于需要高性能碰撞检测的应用场景。

Abstract: Creation of collision objects for 3D models is a time-consuming task, requiring modelers to manually place primitives such as bounding boxes, capsules, spheres, and other convex primitives to approximate complex meshes. While there has been work in automatic approximate convex decompositions of meshes using convex hulls, they are not practical for applications with tight performance budgets such as games due to slower collision detection and inability to manually modify the output while maintaining convexity as compared to manually placed primitives. Rather than convex decomposition with convex hulls, we devise an approach for bottom-up decomposition of an input mesh into convex primitives specifically for rigid body simulation inspired by quadric mesh simplification. This approach fits primitives to complex, real-world meshes that provide plausible simulation performance and are guaranteed to enclose the input surface. We test convex primitive decomposition on over 60 models from Sketchfab, showing the algorithm's effectiveness. On this dataset, convex primitive decomposition has lower one-way mean and median Hausdorff and Chamfer distance from the collider to the input compared to V-HACD and CoACD, with less than one-third of the complexity as measured by total bytes for each collider. On top of that, rigid-body simulation performance measured by wall-clock time is consistently improved across 24 tested models.

</details>


### [2] [Low-Rank Koopman Deformables with Log-Linear Time Integration](https://arxiv.org/abs/2602.07687)
*Yue Chang,Peter Yichen Chen,Eitan Grinspun,Maurizio M. Chiaramonte*

Main category: cs.GR

TL;DR: 提出了一种基于低秩Koopman算子的方法，用于加速可变形子空间模拟，通过动态模式分解（DMD）参数化Koopman算子，学习动态演化和预测未来状态，显著提高了计算效率。


<details>
  <summary>Details</summary>
Motivation: 传统方法在可变形模拟中依赖时间积分，计算效率低。为解决这一问题，提出了基于Koopman算子的方法，通过DMD参数化学习动态演化，提升效率。

Method: 使用动态模式分解（DMD）参数化Koopman算子，学习动态演化并通过矩阵运算预测未来状态，避免了顺序时间积分。同时，扩展方法支持跨形状和网格分辨率学习。

Result: 该方法显著提升了计算效率，实现了对数线性时间步长缩放，同时保持了精度，适用于控制和初始状态估计等优化任务。

Conclusion: Koopman算子学习方法为高效的可变形模拟和设计提供了实用工具，扩展了其在图形学中的应用范围。

Abstract: We present a low-rank Koopman operator formulation for accelerating deformable subspace simulation. Using a Dynamic Mode Decomposition (DMD) parameterization of the Koopman operator, our method learns the temporal evolution of deformable dynamics and predicts future states through efficient matrix evaluations instead of sequential time integration. This yields log-linear scaling in the number of time steps and allows large portions of the trajectory to be skipped while retaining accuracy. The resulting temporal efficiency is especially advantageous for optimization tasks such as control and initial-state estimation, where the objective often depends largely on the final configuration.
  To broaden the scope of Koopman-based reduced-order models in graphics, we introduce a discretization-agnostic extension that learns shared dynamic behavior across multiple shapes and mesh resolutions. Prior DMD-based approaches have been restricted to a single shape and discretization, which limits their usefulness for tasks involving geometry variation. Our formulation generalizes across both shape and discretization, which enables fast shape optimization that was previously impractical for DMD models. This expanded capability highlights the potential of Koopman operator learning as a practical tool for efficient deformable simulation and design.

</details>


### [3] [TABI: Tight and Balanced Interactive Atlas Packing](https://arxiv.org/abs/2602.07782)
*Floria Gu,Nicholas Vining,Alla Sheffer*

Main category: cs.GR

TL;DR: 提出了一种面向实时交互的GPU图集打包方法TABI，能够接近离线方法的质量，同时支持用户灵活调整性能与质量的权衡。


<details>
  <summary>Details</summary>
Motivation: 目前实时GPU打包方法在打包质量与交互性能之间存在显著差距，尤其是打包后图表存在大间隙和不对称问题，影响质量。

Method: 采用Tight And Balanced方法（TABI），通过水平和垂直方向上的空隙压缩以及动态调整图集行宽和方向，以高效并行处理提高打包质量。

Result: TABI显著减少了图表缩放，打包质量接近离线方法，同时比离线方法快几个数量级。

Conclusion: TABI是一种高效的实时GPU打包方法，解决了现有实时方法的质量问题，适用于需要高质量交互式打包的应用。

Abstract: Atlas packing is a key step in many computer graphics applications. Packing algorithms seek to arrange a set of charts within a fixed-size atlas with as little downscaling as possible. Many packing applications such as content creation tools, dynamic atlas generation for video games, and texture space shading require on-the-fly interactive atlas packing. Unfortunately, while many methods have been developed for generating tight high-quality packings, they are designed for offline settings and have running times two or more orders of magnitude greater than what is required for interactive performance. While real-time GPU packing methods exist, they significantly downscale packed charts compared to offline methods. We introduce a GPU packing method that targets interactive speeds, provides packing quality approaching that of offline methods, and supports flexible user control over the tradeoff between performance and quality. We observe that current real-time packing methods leave large gaps between charts and often produce asymmetric, or poorly balanced, packings. These artifacts dramatically degrade packing quality. Our Tight And Balanced method eliminates these artifacts while retaining Interactive performance. TABI generates tight packings by compacting empty space between irregularly shaped charts both horizontally and vertically, using two approximations of chart shape that support efficient parallel processing. We balance packing outputs by automatically adjusting atlas row widths and orientations to accommodate varying chart heights. We show that our method significantly reduces chart downscaling compared to existing interactive methods while remaining orders of magnitude faster than offline alternatives.

</details>


### [4] [MPM Lite: Linear Kernels and Integration without Particles](https://arxiv.org/abs/2602.07853)
*Xiang Feng,Yunuo Chen,Chang Yu,Hao Su,Demetri Terzopoulos,Yin Yang,Joe Masterjohn,Alejandro Castro,Chenfanfu Jiang*

Main category: cs.GR

TL;DR: MPM Lite是一种新的混合拉格朗日/欧拉方法，通过固定位置的积分点和紧凑线性核，消除了基于粒子的积分需求，显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 传统的MPM方法存在性能瓶颈，主要源于基于粒子的积分和大范围核函数的使用，导致隐式求解的时间复杂度与粒子数量成正比。MPM Lite旨在解决这一问题。

Method: MPM Lite将粒子视为运动状态和材料历史的载体，通过背景笛卡尔网格作为体素六面体网格，将粒子状态重新采样到固定位置的积分点。采用高效的紧凑线性核函数，避免了粒子访问，使得求解复杂度与粒子数量无关。

Result: 实验表明，MPM Lite在保持传统MPM的鲁棒性和多功能性的同时，在隐式设置中显著提升了速度，并在显式设置中也有所改进。

Conclusion: MPM Lite通过新颖的应力传递和拉伸重建策略，不仅解决了传统MPM的性能问题，还能直接利用现有的非线性求解器和边界条件，具有广泛的应用潜力。

Abstract: In this paper, we introduce MPM Lite, a new hybrid Lagrangian/Eulerian method that eliminates the need for particle-based quadrature at solve time. Standard MPM practices suffer from a performance bottleneck where expensive implicit solves are proportional to particle-per-cell (PPC) counts due to the the choices of particle-based quadrature and wide-stencil kernels. In contrast, MPM Lite treats particles primarily as carriers of kinematic state and material history. By conceptualizing the background Cartesian grid as a voxel hexahedral mesh, we resample particle states onto fixed-location quadrature points using efficient, compact linear kernels. This architectural shift allows force assembly and the entire time-integration process to proceed without accessing particles, making the solver complexity no longer relate to particles. At the core of our method is a novel stress transfer and stretch reconstruction strategy. To avoid non-physical averaging of deformation gradients, we resample the extensive Kirchhoff stress and derive a rotation-free deformation reference solution, which naturally supports an optimization-based incremental potential formulation. Consequently, MPM Lite can be implemented as modular resampling units coupled with an FEM-style integration module, enabling the direct use of off-the-shelf nonlinear solvers, preconditioners, and unambiguous boundary conditions. We demonstrate through extensive experiments that MPM Lite preserves the robustness and versatility of traditional MPM across diverse materials while delivering significant speedups in implicit settings and improving explicit settings at the same time. Check our project page at https://mpmlite.github.io.

</details>


### [5] [Energy-Controllable Time Integration for Elastodynamic Contact](https://arxiv.org/abs/2602.08094)
*Kevin You,Juntian Zheng,Minchen Li*

Main category: cs.GR

TL;DR: 本文提出了一种名为A-search的能量可控时间积分器，该积分器通过简单的隐式欧拉修改实现对能量耗散或守恒的灵活控制，适用于大规模变形和复杂碰撞场景。


<details>
  <summary>Details</summary>
Motivation: 传统的数值积分器在图形学中虽然稳定，但存在能量不可控耗散或能量守恒但稳定性不足的问题。

Method: 提出一类基于哈密顿问题的数值积分器，并在其基础上推导出A-search方法，该方法可通过用户指定的能量目标灵活控制能量耗散或守恒。

Result: 实验表明，A-search在低频运动中倾向于保持能量而非耗散，且在相同运行时间内，其性能优于传统方法（如BDF2），能生成更理想的视觉效果。

Conclusion: A-search是一种稳定且能量可控的积分器，适用于需要高物理保真度和视觉效果的弹性体动态模拟。

Abstract: Dynamic simulation of elastic bodies is a longstanding task in engineering and computer graphics. In graphics, numerical integrators like implicit Euler and BDF2 are preferred due to their stability at large time steps, but they tend to dissipate energy uncontrollably. In contrast, symplectic methods like implicit midpoint can conserve energy but are not unconditionally stable and fail on moderately stiff problems. To address these limitations, we propose a general class of numerical integrators for Hamiltonian problems which are symplectic on linear problems, yet have superior stability on nonlinear problems. With this, we derive a novel energy-controllable time integrator, A-search, a simple modification of implicit Euler that can follow user-specified energy targets, enabling flexible control over energy dissipation or conservation while maintaining stability and physical fidelity. Our method integrates seamlessly with barrier-type energies and allows for inversion-free and penetration-free guarantees, making it well-suited for handling large deformations and complex collisions. Extensive evaluations over a wide range of material parameters and scenes demonstrate that A-search has biases to keep energy in low frequency motion rather than dissipation, and A-search outperforms traditional methods such as BDF2 at similar total running times by maintaining energy and leading to more visually desirable simulations.

</details>


### [6] [Forget Superresolution, Sample Adaptively (when Path Tracing)](https://arxiv.org/abs/2602.08642)
*Martin Bálint,Corentin Salaün,Hans-Peter Seidel,Karol Myszkowski*

Main category: cs.GR

TL;DR: 提出了一种端到端的自适应采样和去噪管道，专为低于1-spp的稀疏采样场景设计，结合梯度估计和人眼感知优化，显著提升了视觉关键细节的恢复效果。


<details>
  <summary>Details</summary>
Motivation: 随着渲染复杂度、分辨率和帧率要求的提高，实时路径追踪通常需要在极低的采样预算（低于每像素一个样本）下运行，现有超分辨率和自适应采样方法在高稀疏条件下效果不佳。

Method: 采用基于随机采样的梯度估计方法训练神经采样器，结合感知优化的训练管道（含可微色调映射和最优感知损失），并提出一种金字塔去噪滤波器和自适应反照率解调方法。

Result: 相比均匀稀疏采样，该方法在视觉关键细节（如高光和阴影边界）的重建上表现更优，验证了在极低采样预算下自适应采样的有效性。

Conclusion: 该方法通过端到端设计和感知优化，显著提升了稀疏采样条件下的渲染质量，证明了自适应采样在极低预算下仍具实用价值。

Abstract: Real-time path tracing increasingly operates under extremely low sampling budgets, often below one sample per pixel, as rendering complexity, resolution, and frame-rate requirements continue to rise. While super-resolution is widely used in production, it uniformly sacrifices spatial detail and cannot exploit variations in noise, reconstruction difficulty, and perceptual importance across the image. Adaptive sampling offers a compelling alternative, but existing end-to-end approaches rely on approximations that break down in sparse regimes.
  We introduce an end-to-end adaptive sampling and denoising pipeline explicitly designed for the sub-1-spp regime. Our method uses a stochastic formulation of sample placement that enables gradient estimation despite discrete sampling decisions, allowing stable training of a neural sampler at low sampling budgets. To better align optimization with human perception, we propose a tonemapping-aware training pipeline that integrates differentiable filmic operators and a state-of-the-art perceptual loss, preventing oversampling of regions with low visual impact.
  In addition, we introduce a gather-based pyramidal denoising filter and a learnable generalization of albedo demodulation tailored to sparse sampling. Our results show consistent improvements over uniform sparse sampling, with notably better reconstruction of perceptually critical details such as specular highlights and shadow boundaries, and demonstrate that adaptive sampling remains effective even at minimal budgets.

</details>


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [7] [Static Analysis Under Non-Deterministic Program Assumptions](https://arxiv.org/abs/2602.07324)
*Abdullah H. Rasheed*

Main category: cs.PL

TL;DR: 论文提出了一种允许用户提供程序假设的静态分析方法，以弥补传统静态分析中的不精确性，从而扩展其应用范围。


<details>
  <summary>Details</summary>
Motivation: 传统静态分析在精确性和自动化之间做出妥协，限制了其应用场景。论文旨在通过引入用户提供的程序假设，减少不精确性，扩展静态分析的使用范围。

Method: 论文提出了一种静态分析方法，该方法接受用户提供的与程序位置相关的假设，通过非确定性方式处理这些假设，生成基于不同假设集合的分析结果函数。

Result: 展示了一种功能，该功能能够在假设的搜索空间上进行优化，这在传统分析方法中是不可行的。

Conclusion: 通过引入用户提供的假设，该方法显著提升了静态分析的精确性和适用性，为更广泛的应用提供了可能。

Abstract: Static analyses overwhelmingly trade precision for soundness and automation. For this reason, their use-cases are restricted to situations where imprecision isn't prohibitive. In this paper, we propose and specify a static analysis that accepts user-supplied program assumptions that are local to program locations. Such assumptions can be used to counteract imprecision in static analyses, enabling their use in a much wider variety of applications. These assumptions are taken by the analyzer non-deterministically, resulting in a function from sets of accepted assumptions to the resulting analysis under those assumptions. We also demonstrate the utility of such a function in two ways, both of which showcase how it can enable optimization over a search space of assumptions that is otherwise infeasible without the specified analysis.

</details>


### [8] [RustCompCert: A Verified and Verifying Compiler for a Sequential Subset of Rust](https://arxiv.org/abs/2602.07455)
*Jinhua Wu,Yuting Wang,Liukun Yu,Linglong Meng*

Main category: cs.PL

TL;DR: 该论文介绍了一种基于CompCert的端到端验证Rust编译器，确保Rust到汇编的语义保留和内存安全。


<details>
  <summary>Details</summary>
Motivation: 开发一种能够保证Rust程序语义保留和内存安全的编译器，以减少验证工具的复杂性并提升程序的功能正确性。

Method: 基于CompCert开发端到端验证的Rust编译器，通过借用检查传递确保内存安全，并保留源代码到目标代码的语义。

Result: 该方法能够确保Rust程序的语义保留和内存安全，简化了验证工具的工作，使其可以专注于功能正确性。

Conclusion: 该研究为Rust程序的验证提供了一种有效的工具，为未来的编译器设计和验证工作奠定了基础。

Abstract: We present our ongoing work on developing an end-to-end verified Rust compiler based on CompCert. It provides two guarantees: one is semantics preservation from Rust to assembly, i.e., the behaviors of source code includes the behaviors of target code, with which the properties verified at the source can be preserved down to the target; the other is memory safety ensured by the verifying compilation -- the borrow checking pass, which can simplify the verification of Rust programs, e.g., by allowing the verification tools focus on the functional correctness.

</details>


### [9] [Series-Parallel-Loop Decompositions of Control-flow Graphs](https://arxiv.org/abs/2602.07627)
*Xuran Cai,Amir Goharshady,S Hitarth,Chun Kit Lam*

Main category: cs.PL

TL;DR: 本文提出了一种基于语法的新分解框架，精确描述结构化程序生成的控制流图（CFG），并改进了两种经典编译器优化问题的算法。


<details>
  <summary>Details</summary>
Motivation: 传统方法使用树宽等图参数近似建模CFG的稀疏性，但这些参数无法精确表示CFG的结构约束，导致优化技术适用于比实际更广泛的图类。

Method: 引入一种新的基于语法的分解框架，完全匹配结构化程序的CFG，并保持与树宽方法的动态规划范式兼容。

Result: 通过新框架改进了寄存器分配和LOSPRE算法，实验表明其性能显著优于现有方法。

Conclusion: 专为CFG设计的分解框架能有效提升编译器优化任务的性能，验证了定制分解方法的优势。

Abstract: Control-flow graphs (CFGs) of structured programs are well known to exhibit strong sparsity properties. Traditionally, this sparsity has been modeled using graph parameters such as treewidth and pathwidth, enabling the development of faster parameterized algorithms for tasks in compiler optimization, model checking, and program analysis. However, these parameters only approximate the structural constraints of CFGs: although every structured CFG has treewidth at most~7, many graphs with treewidth at most~7 cannot arise as CFGs. As a result, existing parameterized techniques are optimized for a substantially broader class of graphs than those encountered in practice.
  In this work, we introduce a new grammar-based decomposition framework that characterizes \emph{exactly} the class of control-flow graphs generated by structured programs. Our decomposition is intuitive, mirrors the syntactic structure of programs, and remains fully compatible with the dynamic-programming paradigm of treewidth-based methods. Using this framework, we design improved algorithms for two classical compiler optimization problems: \emph{Register Allocation} and \emph{Lifetime-Optimal Speculative Partial Redundancy Elimination (LOSPRE)}. Extensive experimental evaluation demonstrates significant performance improvements over previous state-of-the-art approaches, highlighting the benefits of using decompositions tailored specifically to CFGs.

</details>


### [10] [Gillian Debugging: Swinging Through the (Compositional Symbolic Execution) Trees, Extended Version](https://arxiv.org/abs/2602.07742)
*Nat Karmios,Sacha-Élie Ayoun,Philippa Gardner*

Main category: cs.PL

TL;DR: 本文介绍了一种针对符号执行工具的可视化调试界面，集成在Visual Studio Code和Gillian多语言CSE平台中，重点强调了可视化和交互性，并通过用户研究验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 尽管组合符号执行（CSE）工具在实际代码库中的应用越来越广泛，但调试这些工具的输出仍然非常困难，即使对于专业用户也是如此。因此，本文旨在解决这一问题。

Method: 作者提出了一种调试界面，集成到Visual Studio Code和Gillian多语言CSE平台中。该界面注重可视化和交互性，并且设计为工具无关，便于未来迁移到其他符号分析工具。

Result: 通过用户研究，本文展示了该调试界面在帮助早期研究人员理解CSE原理以及验证Gillian中基本数据结构算法方面的有效性。

Conclusion: 本文提出的调试界面显著提升了符号执行工具的调试体验，特别是在可视化和交互性方面，且其工具无关的设计为未来的扩展提供了可能。

Abstract: In recent years, compositional symbolic execution (CSE) tools have been growing in prominence and are becoming more and more applicable to real-world codebases. Still to this day, however, debugging the output of these tools remains difficult, even for specialist users. To address this, we introduce a debugging interface for symbolic execution tools, integrated with Visual Studio Code and the Gillian multi-language CSE platform, with strong focus on visualisation, interactivity, and intuitive representation of symbolic execution trees. We take care in making this interface tool-agnostic, easing its transfer to other symbolic analysis tools in future. We empirically evaluate our work with a user study, the results of which show the debugger's usefulness in helping early researchers understand the principles of CSE and verify fundamental data structure algorithms in Gillian.

</details>


<div id='cs.GT'></div>

# cs.GT [[Back]](#toc)

### [11] [Online Contract Design](https://arxiv.org/abs/2602.07385)
*Elad Lavi,Hadas Shachnai,Inbal Talgam-Cohen*

Main category: cs.GT

TL;DR: 该论文研究了在线合约的设计，结合了经济合约理论和在线算法设计的挑战，提出了一种在代理奖励具有加性结构时的1/2竞争算法。


<details>
  <summary>Details</summary>
Motivation: 结合经济合约理论和在线算法设计的挑战，探索如何在敌对模型中设计最优的在线合约，以最大化委托人的效用。

Method: 论文采用了对抗性在线决策模型，提出了随机化算法，并在代理奖励为加性结构时实现了最佳可能的1/2竞争比率。

Result: 主要的正面结果是提出的随机化算法在加性奖励下具有1/2竞争比率，且证明确定性算法无法达到有界的竞争比率。

Conclusion: 在线合约在敌对模型中具有一定的可行性，但在奖励具有XOS组合结构时，随机化算法也可能失效。

Abstract: We initiate the study of online contracts, which integrate the game-theoretic considerations of economic contract theory, with the algorithmic and informational challenges of online algorithm design. Our starting point is the classic online setting with preemption of Buchbinder et al. [SODA'15], in which a hiring principal faces a sequence of adversarial agent arrivals. Upon arrival, the principal must decide whether to tentatively accept the agent to their team, and whether to dismiss previous tentative choices. Dismissal is irrevocable, giving the setting its online decision-making flavor. In our setting, the agents are rational players: once the team is finalized, a game is played where the principal offers contracts (performance-based payment schemes), and each agent decides whether or not to work. Working agents reward the principal, and the goal is to choose a team that maximizes the principal's utility. Our main positive result is a 1/2-competitive algorithm when agent rewards are additive, which matches the best-possible competitive ratio. Our algorithm is randomized and this is necessary, as we show that no deterministic algorithm can attain a bounded competitive ratio. Moreover, if agent rewards are allowed to exhibit combinatorial structure known as XOS, even randomized algorithms might fail. En route to our competitive algorithm, we develop the technique of balance points, which can be useful for further exploration of online contracts in the adversarial model.

</details>


### [12] [Modeling Concurrent Multi-Agent Systems](https://arxiv.org/abs/2602.08452)
*Senthil Rajasekaran,Moshe Y. Vardi*

Main category: cs.GT

TL;DR: 本论文通过对比显式模型和电路模型在均衡分析中的复杂性问题，提出电路模型能更有效地处理显式模型中存在的问题。


<details>
  <summary>Details</summary>
Motivation: 多智能体系统中，现有的模型往往缺乏表达力或忽略了复杂性理论结果，限制了均衡分析的准确性。本文旨在通过提出电路模型来解决这些问题。

Method: 论文对比分析了显式模型（文献中常用的精确数学模型）和电路模型（新颖模型），并研究了两个模型在实现性和验证问题上的上下界。

Result: 研究发现，电路模型能更好地解决显式模型中存在的问题，特别是在均衡分析中最关键的实现性和验证问题上。

Conclusion: 电路模型在多智能体系统的均衡分析中表现出更强的适用性，解决了现有模型的局限性。

Abstract: Recent work in the field of multi-agent systems has sought to use techniques and concepts from the field of formal methods to provide rigorous theoretical analysis and guarantees on complex systems where multiple agents strategically interact, leading to the creation of the field of equilibrium analysis, which studies equilibria concepts from the field of game theory through a complexity-theoretic lens. Multi-agent systems, however, are complex mathematical objects, and, therefore, defining them in a precise mathematical manner is non-trivial. As a result, researchers often considered more restrictive models that are easier to model but lack expressive power or simply omit critical complexity-theoretic results in their analysis. This paper addresses this problem by carefully analyzing and contrasting complexity-theoretic results in the explicit model, a mathematically precise formulation of the models commonly used in the literature, and the circuit-based model, a novel model that addresses the problems found in the literature. The utility of the circuit-based model is demonstrated through a comprehensive analysis that considers upper and lower bounds for the realizability and verification problems, the two most important decision problems in equilibrium analysis, for both models. By conducting this analysis, we see that problematic issues that are endemic to the explicit model and the equilibrium analysis literature as a whole are adequately handled by the circuit-based model.

</details>


### [13] [A General Theory of Proportionality with Additive Utilities](https://arxiv.org/abs/2602.08504)
*Piotr Skowron*

Main category: cs.GT

TL;DR: 该论文提出了基于选民偏好和一般约束条件的候选子集选择模型，并针对基数选票设计了比例规则，以确保排名前缀满足比例性。


<details>
  <summary>Details</summary>
Motivation: 当前的比例规则仅适用于批准选票（选民提交可接受的候选子集），但在基数选票（选民为每个候选分配数值效用）场景下缺乏有效规则。论文旨在填补这一空白。

Method: 提出了适用于基数选票的比例规则，并引入了生成比例排名的方法，确保排名的每个前缀都满足比例性。

Result: 论文设计的规则扩展了比例规则的适用范围，使其能够处理基数选票，并提供了确保比例性的排名方法。

Conclusion: 通过为基数选票设计比例规则，论文丰富了比例代表性理论，并为多样化的选举和决策场景提供了实用工具。

Abstract: We consider a model where a subset of candidates must be selected based on voter preferences, subject to general constraints that specify which subsets are feasible. This model generalizes committee elections with diversity constraints, participatory budgeting (including constraints specifying how funds must be allocated to projects from different pools), and public decision-making. Axioms of proportionality have recently been defined for this general model, but the proposed rules apply only to approval ballots, where each voter submits a subset of candidates she finds acceptable. We propose proportional rules for cardinal ballots, where each voter assigns a numerical value to each candidate corresponding to her utility if that candidate is selected. In developing these rules, we also introduce methods that produce proportional rankings, ensuring that every prefix of the ranking satisfies proportionality.

</details>


### [14] [An Automata-Based Approach to Games with $ω$-Automatic Preferences](https://arxiv.org/abs/2602.08549)
*Véronique Bruyère,Emmanuel Filiot,Christophe Grandmont,Jean-François Raskin*

Main category: cs.GT

TL;DR: 该论文研究了基于图的多人回合制游戏，其中玩家偏好由确定性奇偶自动机建模为$ω$-自动关系，与大多数现有工作不同。论文通过计算分析，引入值的概念，并证明其可由多项式大小的交替奇偶自动机识别。此外，论文还探讨了多人游戏中的纳什均衡，填补了文献中的计算复杂性空白。


<details>
  <summary>Details</summary>
Motivation: 研究目的是探索基于图的多人回合制游戏中，玩家偏好由$ω$-自动关系建模时的计算问题，与现有工作中特定的奖励函数研究形成对比。

Method: 论文通过确定性奇偶自动机建模玩家偏好，引入值的概念并使用交替奇偶自动机进行识别。此外，还分析了多人游戏中的纳什均衡和相关计算问题。

Result: 结果表明，值可由多项式大小的交替奇偶自动机识别。论文填补了多人游戏中纳什均衡的计算复杂性空白，并证明合作理性综合问题是$ackslashackslashmathsf{PSPACE}$-完全的，而非合作情况下则是不可判定的。

Conclusion: 论文通过引入值的概念和使用交替奇偶自动机，填补了多人回合制游戏中的计算复杂性空白，特别是在非合作情况下的不可判定性问题。

Abstract: This paper studies multiplayer turn-based games on graphs in which player preferences are modeled as $ω$-automatic relations given by deterministic parity automata. This contrasts with most existing work, which focuses on specific reward functions. We conduct a computational analysis of these games, starting with the threshold problem in the antagonistic zero-sum case. As in classical games, we introduce the concept of value, defined here as the set of plays a player can guarantee to improve upon, relative to their preference relation. We show that this set is recognized by an alternating parity automaton APW of polynomial size. We also establish the computational complexity of several problems related to the concepts of value and optimal strategy, taking advantage of the $ω$-automatic characterization of value. Next, we shift to multiplayer games and Nash equilibria, and revisit the threshold problem in this context. Based on an APW construction again, we close complexity gaps left open in the literature, and additionally show that cooperative rational synthesis is $\mathsf{PSPACE}$-complete, while it becomes undecidable in the non-cooperative case.

</details>


### [15] [Approximate-EFX Allocations with Ordinal and Limited Cardinal Information](https://arxiv.org/abs/2602.08714)
*Aris Filos-Ratsikas,Georgios Kalantzis,Alexandros A. Voudouris*

Main category: cs.GT

TL;DR: 研究在有限信息下如何实现离散公平分配问题中的α-EFX公平标准，重点关注查询次数与公平性之间的权衡。


<details>
  <summary>Details</summary>
Motivation: 现有文献主要假设代理人的估值函数完全已知，而本文则探讨在仅有序数偏好和少量查询的情况下实现α-EFX公平性的可能性。

Method: 假设算法有权访问序数偏好排名，并通过少量查询获取代理人对物品的估值，研究在这些条件下的α-EFX公平分配算法。

Result: 展示了α值与查询次数之间的（接近最优）权衡关系，特别是在恒定EFX近似下的表现。此外，在代理人数固定或仅两种估值的情况下取得了改进结果。

Conclusion: 在有限信息条件下，通过合理设计的查询策略可以实现接近最优的公平分配，为实际应用提供了理论基础。

Abstract: We study a discrete fair division problem where $n$ agents have additive valuation functions over a set of $m$ goods. We focus on the well-known $α$-EFX fairness criterion, according to which the envy of an agent for another agent is bounded multiplicatively by $α$, after the removal of any good from the envied agent's bundle. The vast majority of the literature has studied $α$-EFX allocations under the assumption that full knowledge of the valuation functions of the agents is available. Motivated by the established literature on the distortion in social choice, we instead consider $α$-EFX algorithms that operate under limited information on these functions. In particular, we assume that the algorithm has access to the ordinal preference rankings, and is allowed to make a small number of queries to obtain further access to the underlying values of the agents for the goods. We show (near-optimal) tradeoffs between the values of $α$ and the number of queries required to achieve those, with a particular focus on constant EFX approximations. We also consider two interesting special cases, namely instances with a constant number of agents, or with two possible values, and provide improved positive results.

</details>


### [16] [Distortion of Metric Voting with Bounded Randomness](https://arxiv.org/abs/2602.08871)
*Ziyi Cai,D. D. Gao,Prasanna Ramakrishnan,Kangning Wang*

Main category: cs.GT

TL;DR: 研究了在度量失真框架下投票规则的设计，提出了一个既突破3的失真障碍又保持有限随机性的投票规则。


<details>
  <summary>Details</summary>
Motivation: 探讨如何在有限随机性的情况下突破确定性投票规则的失真下限3，平衡透明度和性能。

Method: 提出了一种投票规则，通过从确定的小规模候选列表中随机选择获胜者，实现了失真上限3-ε。

Result: 新投票规则成功将失真降至3-ε，同时保持了较小的随机性范围。

Conclusion: 研究表明，有限随机性可以突破失真界限，同时提供了一种新的投票规则设计方向。

Abstract: We study the design of voting rules in the metric distortion framework. It is known that any deterministic rule suffers distortion of at least $3$, and that randomized rules can achieve distortion strictly less than $3$, often at the cost of reduced transparency and interpretability. In this work, we explore the trade-off between these paradigms by asking whether it is possible to break the distortion barrier of $3$ using only "bounded" randomness. We answer in the affirmative by presenting a voting rule that (1) achieves distortion of at most $3 - \varepsilon$ for some absolute constant $\varepsilon > 0$, and (2) selects a winner uniformly at random from a deterministically identified list of constant size. Our analysis builds on new structural results for the distortion and approximation of Maximal Lotteries and Stable Lotteries.

</details>


### [17] [Maximin Shares with Lower Quotas](https://arxiv.org/abs/2602.08966)
*Hirota Kinoshita,Ayumi Igarashi*

Main category: cs.GT

TL;DR: 研究在异构加性估值下，考虑每个代理的上限和下限配额限制时，不可分割物品的公平分配问题，提出多项式时间算法实现不同场景下的MMS近似分配。


<details>
  <summary>Details</summary>
Motivation: 公平分配在人员配置和计算资源分配等应用中有广泛需求，但现有研究主要关注上限配额，较少考虑下限配额和多种配额组合的情况。

Method: 提出多项式时间算法，分别处理物品和杂务的分配，在任意上下限配额和分类配额下，计算满足特定近似比例的MMS分配。

Result: 在任意上下限配额下，首次证明了物品和杂务分别存在特定比例的MMS分配，并可多项式时间计算；扩展到分类配额场景同样适用。

Conclusion: 本研究填补了异构配额限制下公平分配的理论空白，提供了多项式时间算法，为实际应用提供了理论基础。

Abstract: We study the fair division of indivisible items among $n$ agents with heterogeneous additive valuations, subject to lower and upper quotas on the number of items allocated to each agent. Such constraints are crucial in various applications, ranging from personnel assignments to computing resource distribution. This paper focuses on the fairness criterion known as maximin shares (MMS) and its approximations. Under arbitrary lower and upper quotas, we show that a $\left(\frac{2n}{3n-1}\right)$-MMS allocation of goods exists and can be computed in polynomial time, while we also present a polynomial-time algorithm for finding a $\left(\frac{3n-1}{2n}\right)$-MMS allocation of chores. Furthermore, we consider the generalized scenario where items are partitioned into multiple categories, each with its own lower and upper quotas. In this setting, our algorithm computes an $\left(\frac{n}{2n-1}\right)$-MMS allocation of goods or a $\left(\frac{2n-1}{n}\right)$-MMS allocation of chores in polynomial time. These results extend previous work on the cardinality constraints, i.e., the special case where only upper quotas are imposed.

</details>
