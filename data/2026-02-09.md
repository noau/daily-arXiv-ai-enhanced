<div id=toc></div>

# Table of Contents

- [cs.GR](#cs.GR) [Total: 1]
- [cs.PL](#cs.PL) [Total: 6]
- [cs.GT](#cs.GT) [Total: 4]


<div id='cs.GR'></div>

# cs.GR [[Back]](#toc)

### [1] [Real time, cross platform visualizations with zero dependencies for the N-body package REBOUND](https://arxiv.org/abs/2602.06735)
*Hanno Rein*

Main category: cs.GR

TL;DR: 本研究提出了一种新的实时可视化方法，利用Web浏览器进行GPU加速渲染，无需依赖外部库，并支持跨平台3D交互式可视化。


<details>
  <summary>Details</summary>
Motivation: 现有的可视化工具依赖外部库，带来了依赖性和长期支持的不确定性，本研究旨在提供一种更简单且独立的方法。

Method: 通过WebAssembly重用现有的OpenGL代码，并利用HTTP通信和内置web服务器，实现本地和远程实时可视化。

Result: 该方法成功实现了无需外部库的跨平台3D交互式可视化，并支持多种运行模式，如浏览器内模拟和Jupyter笔记本可视化。

Conclusion: 该方法不仅适用于REBOUND包，还可广泛应用于其他需要实时科学和非科学可视化的领域。

Abstract: Visualizations have become an indispensable part of the scientific process. A vibrant ecosystem of visualization tools exists, catering to a wide variety of different needs. Real-time visualizations of numerical simulations offer scientists immediate feedback about the status of their simulations and can also be valuable educational and outreach tools. Developing a visualization tool with support for different operating systems, CPU/GPU architectures, and programming languages can be a challenge. It is common to use one or more graphics or UI libraries to act as abstraction layers and hide the underlying complexity. Whereas external libraries greatly simplify the initial programming effort, we argue that relying on them introduces new dependencies and problems, such as a higher entry barriers for new developers and users, and uncertainty regarding long-term support. In this paper we present a new approach for real time visualizations which we have implemented for the N-body package REBOUND. We propose to use a web browser to handle GPU accelerated rendering. This enables us to offer 3D, interactive visualizations on all major operating systems. What makes our new approach unique is that we achieve this without the need for any external libraries. We utilize WebAssembly to reuse existing OpenGL visualization code. Using communication via HTTP and a custom built-in web server, we are able to provide both local and remote real time visualizations. In addition to the browser based real time visualization, our approach offers other additional operating modes, including simulations running entirely within the browser, visualizations within jupyter notebooks, and traditional standalone visualizations using OpenGL. We focus on the implementation in REBOUND but the concepts and ideas discussed can be applied to many other areas in need of scientific and non-scientific real time visualizations.

</details>


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [2] [Protean Compiler: An Agile Framework to Drive Fine-grain Phase Ordering](https://arxiv.org/abs/2602.06142)
*Amir H. Ashouri,Shayan Shirahmad Gale Bagi,Kavin Satheeskumar,Tejas Srikanth,Jonathan Zhao,Ibrahim Saidoun,Ziwen Wang,Bryan Chan,Tomasz S. Czajkowski*

Main category: cs.PL

TL;DR: Protean Compiler是一个敏捷框架，为LLVM提供细粒度的阶段排序能力，通过内置的140多种手工静态特征收集方法和机器学习集成，显著提升编译性能。


<details>
  <summary>Details</summary>
Motivation: 解决编译器优化阶段排序长期存在的挑战，避免传统手工调优方法的高成本和局限性。

Method: 在LLVM中集成细粒度阶段排序框架，结合手工特征收集方法和第三方机器学习模型。

Result: 实验显示，在Cbench应用中平均提速4.1%，最高15.7%，且构建时间仅增加几秒；集成第三方模型后，某些应用提速达10.1%。

Conclusion: Protean Compiler成功嵌入LLVM，成为功能完整的编译器，未来将开源发布。

Abstract: The phase ordering problem has been a long-standing challenge since the late 1970s, yet it remains an open problem due to having a vast optimization space and an unbounded nature, making it an open-ended problem without a finite solution, one can limit the scope by reducing the number and the length of optimizations. Traditionally, such locally optimized decisions are made by hand-coded algorithms tuned for a small number of benchmarks, often requiring significant effort to be retuned when the benchmark suite changes. In the past 20 years, Machine Learning has been employed to construct performance models to improve the selection and ordering of compiler optimizations, however, the approaches are not baked into the compiler seamlessly and never materialized to be leveraged at a fine-grained scope of code segments. This paper presents Protean Compiler: An agile framework to enable LLVM with built-in phase-ordering capabilities at a fine-grained scope. The framework also comprises a complete library of more than 140 handcrafted static feature collection methods at varying scopes, and the experimental results showcase speedup gains of up to 4.1% on average and up to 15.7% on select Cbench applications wrt LLVM's O3 by just incurring a few extra seconds of build time on Cbench. Additionally, Protean compiler allows for an easy integration with third-party ML frameworks and other Large Language Models, and this two-step optimization shows a gain of 10.1% and 8.5% speedup wrt O3 on Cbench's Susan and Jpeg applications. Protean compiler is seamlessly integrated into LLVM and can be used as a new, enhanced, full-fledged compiler. We plan to release the project to the open-source community in the near future.

</details>


### [3] [Uniqueness is Separation](https://arxiv.org/abs/2602.06386)
*Liam O'Connor,Pilar Selene Linares Arevalo,Christine Rizkallah*

Main category: cs.PL

TL;DR: 论文探讨了如何在形式验证中平衡值独立性的优势与现实的软件需求，提出将值独立性作为分离逻辑中的断言，以混合保证的形式验证系统。


<details>
  <summary>Details</summary>
Motivation: 值独立性在大规模软件系统推理中具有显著优势，但其严格的类型系统限制在现实软件中难以满足。因此，需要找到一种方式，既能利用值独立性的优势，又能适应现实软件的需求。

Method: 论文提出将值独立性作为分离逻辑中的断言，从而在保持语义优势的同时，允许一定程度的可变性和灵活性。

Result: 通过这种方式，可以在形式验证系统中混合使用值独立性和可变性，从而在保证推理优势的同时，满足现实软件的需求。

Conclusion: 研究表明，将值独立性作为分离逻辑中的断言是一种有效的折衷方案，能够在形式验证中兼顾推理优势和现实的软件限制。

Abstract: Value independence is enormously beneficial for reasoning about software systems at scale. These benefits carry over into the world of formal verification. Reasoning about programs algebraically is a simple affair in a proof assistant, whereas programs with unconstrained mutation necessitate much more complex techniques, such as Separation Logic, where invariants about memory safety, aliasing, and state changes must be established by manual proof. Uniqueness type systems allow programs to be compiled to code that uses mutation for efficiency, while retaining a semantics that enjoys value independence for reasoning. The restrictions of these type systems, however, are often too onerous for realistic software. Thus, most uniqueness type systems include some "escape hatch" where the benefits of value independence for reasoning are lost, but the restrictions of uniqueness types are lifted. To formally verify a system with such mixed guarantees, the value independence guarantees from uniqueness types must be expressed in terms of imperative, mutable semantics. In other words, we ought to express value independence as an assertion in Separation Logic.

</details>


### [4] [Auditing Rust Crates Effectively](https://arxiv.org/abs/2602.06466)
*Lydia Zoghbi,David Thien,Ranjit Jhala,Deian Stefan,Caleb Stanford*

Main category: cs.PL

TL;DR: Cargo Scan是一个交互式程序分析工具，旨在帮助开发者审计Rust第三方代码，通过类型和模块系统减少需要检查的潜在危险代码量。


<details>
  <summary>Details</summary>
Motivation: Rust系统中的第三方依赖与C或JavaScript等其他语言一样危险，但现有的审计方法需要逐行检查代码，效率低下。

Method: Cargo Scan通过定制化的副作用分析，将潜在危险代码建模为效应，并利用调用图跟踪上下文依赖信息。

Result: 在69.2%的情况下，开发者可以本地检查标记的效应；审计hyper及其依赖时，Cargo Scan将潜在危险代码缩减到0.2%的代码量。

Conclusion: Cargo Scan显著降低了审计负担，并能自动分类大量Rust库为安全，同时将危险效应集中在少数库中。

Abstract: We introduce Cargo Scan, the first interactive program analysis tool designed to help developers audit third-party Rust code. Real systems written in Rust rely on thousands of transitive dependencies. These dependencies are as dangerous in Rust as they are in other languages (e.g., C or JavaScript) -- and auditing these dependencies today means manually inspecting every line of code. Unlike for most industrial languages, though, we can take advantage of Rust's type and module system to minimize the amount of code that developers need to inspect to the code that is potentially dangerous. Cargo Scan models such potentially dangerous code as effects and performs a side-effects analysis, tailored to Rust, to identify effects and track them across crate and module boundaries. In most cases (69.2%) developers can inspect flagged effects and decide whether the code is potentially dangerous locally. In some cases, however, the safety of an effect depends on the calling context -- how a function is called, potentially by a crate the developer imports later. Hence, Cargo Scan tracks context-dependent information using a call-graph, and collects audit results into composable and reusable audit files. In this paper, we describe our experience auditing Rust crates with Cargo Scan. In particular, we audit the popular client and server HTTP crate, hyper, and all of its dependencies; our experience shows that Cargo Scan can reduce the auditing burden of potentially dangerous code to a median of 0.2% of lines of code when compared to auditing whole crates. Looking at the Rust ecosystem more broadly, we find that Cargo Scan can automatically classify ~3.5K of the top 10K crates on crates.io as safe; of the crates that do require manual inspection, we find that most of the potentially dangerous side-effects are concentrated in roughly 3% of these crates.

</details>


### [5] [Same Engine, Multiple Gears: Parallelizing Fixpoint Iteration at Different Granularities (Extended Version)](https://arxiv.org/abs/2602.06680)
*Ali Rasim Kocal,Michael Schwarz,Simmo Saan,Helmut Seidl*

Main category: cs.PL

TL;DR: 本文提出了一种参数化任务粒度的通用不动点引擎并行化方法，支持两种不同的并行化哲学：即时方法和独立方法。在Goblint静态分析框架中进行实验，验证了其在大规模现实程序中的有效性。


<details>
  <summary>Details</summary>
Motivation: 现有的静态分析工具在并行化不动点引擎时通常固定任务粒度，导致引擎性能受限。本文旨在通过参数化任务粒度的方式，实现一个能够在不同粒度下运行的通用并行化引擎。

Method: 基于支持混合流敏感性的TD求解器，提出两种并行化哲学：即时方法（所有任务共享线程安全的哈希表）和独立方法（每个任务有自己的状态，并通过发布/订阅数据结构交换数据）。

Result: 在Goblint静态分析框架中实现并测试了这两种方法，验证了其在大规模现实程序中的性能提升。

Conclusion: 参数化任务粒度的并行化方法为静态分析工具提供了更高的灵活性，两种并行化哲学各有优劣，适用于不同的场景。

Abstract: Fixpoint iteration constitutes the algorithmic core of static analyzers. Parallelizing the fixpoint engine can significantly reduce analysis times. Previous approaches typically fix the granularity of tasks upfront, e.g., at the level of program threads or procedures - yielding an engine permanently stuck in one gear. Instead, we propose to parallelize a generic fixpoint engine in a way that is parametric in the task granularity - meaning that our engine can be run in different gears. We build on the top-down solver TD, extended with support for mixed-flow sensitivity, and realize two competing philosophies for parallelization, both building on a task pool that schedules tasks to a fixed number of workers. The nature of tasks differs between the philosophies. In the immediate approach, all tasks access a single thread-safe hash table maintaining solver state, while in the independent approach, each task has its own state and exchanges data with other tasks via a publish/subscribe data structure. We have equipped the fixpoint engine of the static analysis framework Goblint with implementations following both philosophies and report on our results for large real-world programs.

</details>


### [6] [Practical Refinement Session Type Inference (Extended Version)](https://arxiv.org/abs/2602.06715)
*Toby Ueno,Ankush Das*

Main category: cs.PL

TL;DR: 提出了一种用于带有算术精化的会话类型系统的类型推断算法，通过类型模拟理论证明其正确性，并通过Z3 SMT求解器实现可行性优化。


<details>
  <summary>Details</summary>
Motivation: 现有的会话类型系统虽然能静态捕获进程间的交互协议，但算术精化的引入增加了程序员的标注负担。为了减轻这一负担，提出了类型推断算法。

Method: 开发了会话类型的子类型理论，设计了一个基于类型模拟语义的算法，并通过Z3 SMT求解器解决类型和算术约束问题，实现了三个关键优化。

Result: 在Rast语言上实现了该算法，并通过对6个具有挑战性的基准测试（如自然数和线性λ-演算）的评估，展示了优化的性能优势。

Conclusion: 提出的类型推断算法在减轻标注负担的同时，通过优化确保了Z3在合理时间内解决算术约束，具有实际应用潜力。

Abstract: Session types express and enforce safe communication in concurrent message-passing systems by statically capturing the interaction protocols between processes in the type. Recent works extend session types with arithmetic refinements, which enable additional fine-grained description of communication, but impose additional annotation burden on the programmer. To alleviate this burden, we propose a type inference algorithm for a session type system with arithmetic refinements. We develop a theory of subtyping for session types, including an algorithm which we prove sound with respect to a semantic definition based on type simulation. We also provide a formal inference algorithm that generates type and arithmetic constraints, which are then solved using the Z3 SMT solver. The algorithm has been implemented on top of the Rast language, and includes 3 key optimizations that make inference feasible and practical. We evaluate the efficacy of our inference engine by evaluating it on 6 challenging benchmarks, ranging from unary and binary natural numbers to linear $λ$-calculus. We show the performance benefits provided by our optimizations in coercing Z3 into solving the arithmetic constraints in reasonable time.

</details>


### [7] [Implementing Grassroots Logic Programs with Multiagent Transition Systems and AI](https://arxiv.org/abs/2602.06934)
*Ehud Shapiro*

Main category: cs.PL

TL;DR: GLP是一种并发的逻辑编程语言，通过分区变量为‘读者’和‘写者’，并结合线性逻辑和未来/承诺的概念，实现了简洁的多向通信表达。


<details>
  <summary>Details</summary>
Motivation: 设计GLP的目的是为了支持分布式系统（如智能手机之间的点对点通信），这些系统可以独立运行，也可以合并为更大的实例。

Method: 通过定义单代理和多代理的转换系统操作语义（分别为dGLP和madGLP），并为AI提供形式化规范，以实现在工作站和智能手机上的GLP实现。

Result: 开发了dGLP和madGLP，并验证了其正确性；dGLP已用于工作站的GLP实现，madGLP正用于智能手机的maGLP实现。

Conclusion: GLP及其实现为分布式系统提供了强大的多向通信能力，并验证了形式化规范在实现中的有效性。

Abstract: Grassroots Logic Programs (GLP) is a concurrent logic programming language with variables partitioned into paired \emph{readers} and \emph{writers}, conjuring both linear logic and futures/promises: an assignment is produced at most once via the sole occurrence of a writer (promise) and consumed at most once via the sole occurrence of its paired reader (future), and may contain additional readers and/or writers, enabling the concise expression of rich multidirectional communication modalities.
  GLP was designed as a language for grassroots platforms -- distributed systems with multiple instances that can operate independently of each other and of any global resource, and can coalesce into ever larger instances -- with its target architecture being smartphones communicating peer-to-peer. The operational semantics of Concurrent (single-agent) GLP and of multiagent GLP (maGLP) were defined via transition systems/multiagent transition systems, respectively.
  Here, we describe the mathematics developed to facilitate the workstation- and smartphone-based implementations of GLP by AI in Dart. We developed dGLP -- implementation-ready deterministic operational semantics for single-agent GLP -- and proved it correct with respect to the Concurrent GLP operational semantics; dGLP was used by AI as a formal spec, from which it developed a workstation-based implementation of GLP. We developed madGLP -- an implementation-ready multiagent operational semantics for maGLP -- and proved it correct with respect to the maGLP operational semantics; madGLP is deterministic at the agent level (not at the system level due to communication asynchrony), and is being used by AI as a formal spec from which it develops a smartphone-based implementation of maGLP.

</details>


<div id='cs.GT'></div>

# cs.GT [[Back]](#toc)

### [8] [Envy-Free Allocation of Indivisible Goods via Noisy Queries](https://arxiv.org/abs/2602.06361)
*Zihan Li,Yan Hao Ling,Jonathan Scarlett,Warut Suksompong*

Main category: cs.GT

TL;DR: 该论文研究了在无法直接观测代理人估值的情况下，通过噪声查询公平分配不可分割物品的问题。针对两代理人和高斯噪声设置，推导了找到无嫉妒分配所需查询次数的上界和下界。


<details>
  <summary>Details</summary>
Motivation: 公平分配不可分割物品是一个经典问题，但直接观测代理人的估值在实际中可能不可行。论文旨在解决通过噪声查询获取估值时的公平分配问题。

Method: 在二代理人且估值有界的情况下，使用高斯噪声模型。通过非自适应查询和基于阈值的分配算法，推导了查询次数的上界；同时证明了即使使用自适应查询，查询次数的下界依然存在。

Result: 当最优分配的无嫉妒程度 $Δ$ 不太小时（$Δ\gg m^{1/4}$），最优查询次数约为 $\frac{m^{2.5}}{Δ^2}$（对数因子内）。算法在多项式时间内运行。

Conclusion: 论文展示了在高斯噪声下，通过合理设计的查询策略和算法，可以在有限时间内找到公平分配。结果对实际应用中的公平分配问题提供了理论支持。

Abstract: We introduce a problem of fairly allocating indivisible goods (items) in which the agents' valuations cannot be observed directly, but instead can only be accessed via noisy queries. In the two-agent setting with Gaussian noise and bounded valuations, we derive upper and lower bounds on the required number of queries for finding an envy-free allocation in terms of the number of items, $m$, and the negative-envy of the optimal allocation, $Δ$. In particular, when $Δ$ is not too small (namely, $Δ\gg m^{1/4}$), we establish that the optimal number of queries scales as $\frac{\sqrt m }{(Δ/ m)^2} = \frac{m^{2.5}}{Δ^2}$ up to logarithmic factors. Our upper bound is based on non-adaptive queries and a simple thresholding-based allocation algorithm that runs in polynomial time, while our lower bound holds even under adaptive queries and arbitrary computation time.

</details>


### [9] [The Impossibility of Strategyproof Rank Aggregation](https://arxiv.org/abs/2602.06582)
*Manuel Eberl,Patrick Lederer*

Main category: cs.GT

TL;DR: 本文研究了排名聚合中的策略证明性，证明了当有四个或更多备选方案时，不存在满足一致性和策略证明性的匿名社会福利函数，并通过SAT求解和Isabelle验证了这一结果。


<details>
  <summary>Details</summary>
Motivation: 研究排名聚合方法（社会福利函数）在策略证明性方面的表现，探讨是否存在满足一致性和策略证明性的匿名方法。

Method: 使用SAT求解和计算机辅助证明技术（如Isabelle）验证理论结果，并手工证明了策略证明性与多数一致性的不兼容性。

Result: 证明了当备选方案至少为四个时，不存在匿名且满足一致性和策略证明性的社会福利函数；同时显示两类自然社会福利函数具有较高的可操纵性。

Conclusion: 研究表明，在排名聚合中，匿名且满足策略证明性的社会福利函数在多个备选方案下难以实现，且这些方法通常具有较高的可操纵性。

Abstract: In rank aggregation, the goal is to combine multiple input rankings into a single output ranking. In this paper, we analyze rank aggregation methods, so-called social welfare functions (SWFs), with respect to strategyproofness, which requires that no agent can misreport his ranking to obtain an output ranking that is closer to his true ranking in terms of the Kemeny distance. As our main result, we show that no anonymous SWF satisfies unanimity and strategyproofness when there are at least four alternatives. This result is proven by SAT solving, a computer-aided theorem proving technique, and verified by Isabelle, a highly trustworthy interactive proof assistant. Further, we prove by hand that strategyproofness is incompatible with majority consistency, a variant of Condorcet-consistency for SWFs. Lastly, we show that all SWFs in two natural classes have a large incentive ratio and are thus highly manipulable.

</details>


### [10] [Selfish routing games with priority lanes](https://arxiv.org/abs/2602.06598)
*Yang Li,Alexander Skopalik,Marc Uetz*

Main category: cs.GT

TL;DR: 研究了一种自私路由游戏模型，用户可以选择常规或优先级服务，通过边际成本定价实现系统最优，且价格等于1。


<details>
  <summary>Details</summary>
Motivation: 探讨在自私路由游戏中，通过优先级服务的选择和定价机制，是否能实现系统最优和激励兼容。

Method: 使用线性延迟函数建立均衡存在性，并通过边际成本定价机制实现系统最优。

Result: 证明了边际成本定价可以使得均衡流与社会最优流一致，且价格等于1。

Conclusion: 自愿优先级机制提供了一种激励兼容的替代方案，但讨论了统一定价方案的局限性。

Abstract: We study selfish routing games where users can choose between regular and priority service for each network edge on their chosen path. Priority users pay an additional fee, but in turn they may travel the edge prior to non-priority users, hence experiencing potentially less congestion. For this model, we establish existence of equilibria for linear latency functions and prove uniqueness of edge latencies, despite potentially different strategic choices in equilibrium. Our main contribution demonstrates that marginal cost pricing achieves system optimality: When priority fees equal marginal externality costs, the equilibrium flow coincides with the socially optimal flow, hence the price of anarchy equals $1$. This voluntary priority mechanism therefore provides an incentive-compatible alternative to mandatory congestion pricing, whilst achieving the same result. We also discuss the limitations of a uniform pricing scheme for the priority option.

</details>


### [11] [Fair Transit Stop Placement: A Clustering Perspective and Beyond](https://arxiv.org/abs/2602.06776)
*Haris Aziz,Ling Gai,Yuhang Guo,Jeremy Vollen*

Main category: cs.GT

TL;DR: 该论文研究了度量空间中的公交站点布局问题（TrSP），通过合理代表（JR）和核心概念探讨公平性，并提出了一种在JR和核心之间权衡的参数化算法。


<details>
  <summary>Details</summary>
Motivation: 研究公交站点布局中的公平性问题，尤其是通过合理代表（JR）和核心概念来确保乘客在使用公交服务时的公平性。

Method: 通过分析与公平聚类的结构性对应关系，提出了一种参数化算法，能够在JR和核心之间实现可调谐的权衡。此外，还提出了扩展成本算法来实现紧致的JR近似。

Result: 证明了比例公平聚类中的常数因子近似可以保证核心的双参数近似，并建立了JR近似下限为1.366。实验部分使用小型市场拼车数据验证了理论结果。

Conclusion: 研究揭示了公交站点布局中公平性与聚类的结构性联系，并通过参数化算法提供了一种灵活的公平性权衡方案。

Abstract: We study the transit stop placement (TrSP) problem in general metric spaces, where agents travel between source-destination pairs and may either walk directly or utilize a shuttle service via selected transit stops. We investigate fairness in TrSP through the lens of justified representation (JR) and the core, and uncover a structural correspondence with fair clustering. Specifically, we show that a constant-factor approximation to proportional fairness in clustering can be used to guarantee a constant-factor biparameterized approximation to core. We establish a lower bound of 1.366 on the approximability of JR, and moreover show that no clustering algorithm can approximate JR within a factor better than 3. Going beyond clustering, we propose the Expanding Cost Algorithm, which achieves a tight 2.414-approximation for JR, but does not give any bounded core guarantee. In light of this, we introduce a parameterized algorithm that interpolates between these approaches, and enables a tunable trade-off between JR and core. Finally, we complement our results with an experimental analysis using small-market public carpooling data.

</details>
