{"id": "2510.11751", "categories": ["cs.PL"], "pdf": "https://arxiv.org/pdf/2510.11751", "abs": "https://arxiv.org/abs/2510.11751", "authors": ["Jan Pedersen", "Kevin Chalmers"], "title": "Verifying Correctness of Shared Channels in a Cooperatively Scheduled Process-Oriented Language", "comment": null, "summary": "Correct concurrent behaviour is important in understanding how components\nwill act within certain conditions. In this work. we analyse the behaviour of\nshared communicating channels within a coorporatively scheduled runtime. We use\nthe refinement checking and modelling tool FDR to develop both specifications\nof how such shared channels should behave and models of the implementations of\nthese channels in the cooperatively scheduled language ProcessJ. Our results\ndemonstrate that although we can certainly implement the correct behaviour of\nsuch channels, the outcome is dependant on having adequate resources available\nto execute all processes involved. We conclude that modelling the runtime\nenvironment of concurrent components is necessary to ensure components behave\nas specified in the real world."}
{"id": "2510.11759", "categories": ["cs.PL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.11759", "abs": "https://arxiv.org/abs/2510.11759", "authors": ["Hongyu Lin", "Haolin Pan", "Haoran Luo", "Yuchen Li", "Kaichun Yao", "Libo Zhang", "Mingjie Xing", "Yanjun Wu"], "title": "AwareCompiler: Agentic Context-Aware Compiler Optimization via a Synergistic Knowledge-Data Driven Framework", "comment": null, "summary": "Compiler optimization is crucial for enhancing program performance by\ntransforming the sequence of optimization passes while maintaining correctness.\nDespite the promising potential of large language models (LLMs)-based agent for\nsoftware optimization, automating compiler optimization remains challenging due\nto: (1) semantic misalignment between abstract program representations and\nconcrete optimization passes, (2) inefficient interaction mechanisms between\nagents and compiler environments, and (3) reward sparsity from the extensive\ndecision-making process within large optimization spaces. This paper introduces\n\\textbf{AwareCompiler}, an agentic framework for compiler optimization that\naddresses these challenges through three key innovations: structured knowledge\nintegration and dataset construction, knowledge-driven adaptive pass\ngeneration, and data-driven hybrid training pipeline. Experimental results on\nstandard benchmarks demonstrate that AwareCompiler significantly outperforms\nexisting baselines in both performance and efficiency, highlighting the\neffectiveness of our synergistic knowledge-data-driven approach. Our code is\npublicly available at https://github.com/LHY-24/AwareCompiler."}
{"id": "2510.12131", "categories": ["cs.PL"], "pdf": "https://arxiv.org/pdf/2510.12131", "abs": "https://arxiv.org/abs/2510.12131", "authors": ["Haobin Ni", "Robbert van Renesse", "Greg Morrisett"], "title": "Functional Reasoning for Distributed Systems with Failures", "comment": null, "summary": "Distributed system theory literature often argues for correctness using an\ninformal, Hoare-like style of reasoning. While these arguments are intuitive,\nthey have not all been foolproof, and whether they directly correspond to\nformal proofs is in question. We formally ground this kind of reasoning and\nconnect it to standard formal approaches through language design and\nmeta-analysis, which leads to a functional style of compositional formal\nreasoning for a class of distributed systems, including cases involving\nByzantine faults. The core of our approach is twin languages: Sync and Async,\nwhich formalize the insight from distributed system theory that an asynchronous\nsystem can be reduced to a synchronous system for more straightforward\nreasoning under certain conditions. Sync describes a distributed system as a\nsingle, synchronous, data-parallel program. It restricts programs syntactically\nand has a functional denotational semantics suitable for Hoare-style formal\nreasoning. Async models a distributed system as a collection of interacting\nmonadic programs, one for each non-faulty node in the system. It has a standard\ntrace-based operational semantics, modeling asynchrony with interleaving. Sync\ncompiles to Async and can then be extracted to yield executable code. We prove\nthat any safety property proven for a Sync program in its denotational\nsemantics is preserved in the operational semantics of its compiled Async\nprograms. We implement the twin languages in Rocq and verify the safety\nproperties of two fault-tolerant consensus protocols: BOSCO and SeqPaxos."}
{"id": "2510.12295", "categories": ["cs.PL", "cs.LO", "ACM F.3.2", "F.3.2"], "pdf": "https://arxiv.org/pdf/2510.12295", "abs": "https://arxiv.org/abs/2510.12295", "authors": ["Roberto M. Amadio"], "title": "Operational methods in semantics", "comment": null, "summary": "The focus of these lecture notes is on abstract models and basic ideas and\nresults that relate to the operational semantics of programming languages\nlargely conceived. The approach is to start with an abstract description of the\ncomputation steps of programs and then to build on top semantic equivalences,\nspecification languages, and static analyses. While other approaches to the\nsemantics of programming languages are possible, it appears that the\noperational one is particularly effective in that it requires a moderate level\nof mathematical sophistication and scales reasonably well to a large variety of\nprogramming features. In practice, operational semantics is a suitable\nframework to build portable language implementations and to specify and test\nprogram properties. It is also used routinely to tackle more ambitious tasks\nsuch as proving the correctness of a compiler or a static analyzer."}
{"id": "2510.11878", "categories": ["cs.GR", "cs.CV"], "pdf": "https://arxiv.org/pdf/2510.11878", "abs": "https://arxiv.org/abs/2510.11878", "authors": ["Anastasiya Pechko", "Piotr Borycki", "Joanna Waczyńska", "Daniel Barczyk", "Agata Szymańska", "Sławomir Tadeja", "Przemysław Spurek"], "title": "GS-Verse: Mesh-based Gaussian Splatting for Physics-aware Interaction in Virtual Reality", "comment": null, "summary": "As the demand for immersive 3D content grows, the need for intuitive and\nefficient interaction methods becomes paramount. Current techniques for\nphysically manipulating 3D content within Virtual Reality (VR) often face\nsignificant limitations, including reliance on engineering-intensive processes\nand simplified geometric representations, such as tetrahedral cages, which can\ncompromise visual fidelity and physical accuracy. In this paper, we introduce\n\\our{} (\\textbf{G}aussian \\textbf{S}platting for \\textbf{V}irtual\n\\textbf{E}nvironment \\textbf{R}endering and \\textbf{S}cene \\textbf{E}diting), a\nnovel method designed to overcome these challenges by directly integrating an\nobject's mesh with a Gaussian Splatting (GS) representation. Our approach\nenables more precise surface approximation, leading to highly realistic\ndeformations and interactions. By leveraging existing 3D mesh assets, \\our{}\nfacilitates seamless content reuse and simplifies the development workflow.\nMoreover, our system is designed to be physics-engine-agnostic, granting\ndevelopers robust deployment flexibility. This versatile architecture delivers\na highly realistic, adaptable, and intuitive approach to interactive 3D\nmanipulation. We rigorously validate our method against the current\nstate-of-the-art technique that couples VR with GS in a comparative user study\ninvolving 18 participants. Specifically, we demonstrate that our approach is\nstatistically significantly better for physics-aware stretching manipulation\nand is also more consistent in other physics-based manipulations like twisting\nand shaking. Further evaluation across various interactions and scenes confirms\nthat our method consistently delivers high and reliable performance, showing\nits potential as a plausible alternative to existing methods."}
{"id": "2510.11866", "categories": ["cs.GT", "cs.DC", "cs.MA"], "pdf": "https://arxiv.org/pdf/2510.11866", "abs": "https://arxiv.org/abs/2510.11866", "authors": ["Michael Crystal", "Guy Goren", "Scott Duke Kominers"], "title": "Rationally Analyzing Shelby: Proving Incentive Compatibility in a Decentralized Storage Network", "comment": "23 pages, 1 figure", "summary": "Decentralized storage is one of the most natural applications built on\nblockchains and a central component of the Web3 ecosystem. Yet despite a decade\nof active development -- from IPFS and Filecoin to more recent entrants -- most\nof these storage protocols have received limited formal analysis of their\nincentive properties. Claims of incentive compatibility are sometimes made, but\nrarely proven. This gap matters: without well-designed incentives, a system may\ndistribute storage but fail to truly decentralize it.\n  We analyze Shelby -- a storage network protocol recently proposed by Aptos\nLabs and Jump Crypto -- and provide the first formal proof of its incentive\nproperties. Our game-theoretic model shows that while off-chain audits alone\ncollapse to universal shirking, Shelby's combination of peer audits with\noccasional on-chain verification yields incentive compatibility under natural\nparameter settings. We also examine coalition behavior and outline a simple\nmodification that strengthens the protocol's collusion-resilience."}
{"id": "2510.12582", "categories": ["cs.PL", "cs.SE", "quant-ph"], "pdf": "https://arxiv.org/pdf/2510.12582", "abs": "https://arxiv.org/abs/2510.12582", "authors": ["Mark Koch", "Alan Lawrence", "Kartik Singhal", "Seyon Sivarajah", "Ross Duncan"], "title": "GUPPY: Pythonic Quantum-Classical Programming", "comment": "Presented at the Fourth International Workshop on Programming\n  Languages for Quantum Computing (PLanQC 2024)", "summary": "We present ongoing work on Guppy, a domain-specific language embedded in\nPython that allows users to write high-level hybrid quantum programs with\ncomplex control flow in Pythonic syntax, aiming to run them on actual quantum\nhardware."}
{"id": "2510.12053", "categories": ["cs.GR"], "pdf": "https://arxiv.org/pdf/2510.12053", "abs": "https://arxiv.org/abs/2510.12053", "authors": ["Ty Trusty"], "title": "Coordinate Condensation: Subspace-Accelerated Coordinate Descent for Physics-Based Simulation", "comment": null, "summary": "We introduce Coordinate Condensation, a variant of coordinate descent that\naccelerates physics-based simulation by augmenting local coordinate updates\nwith a Schur-complement-based subspace correction. Recent work by Lan et al.\n2025 (JGS2) uses perturbation subspaces to augment local solves to account for\nglobal coupling, but their approach introduces damping that can degrade\nconvergence. We reuse this subspace but solve for local and subspace\ndisplacements independently, eliminating this damping. For problems where the\nsubspace adequately captures global coupling, our method achieves near-Newton\nconvergence while retaining the efficiency and parallelism of coordinate\ndescent. Through experiments across varying material stiffnesses and mesh\nresolutions, we show substantially faster convergence than both standard\ncoordinate descent and JGS2. We also characterize when subspace-based\ncoordinate methods succeed or fail, offering insights for future solver design."}
{"id": "2510.12158", "categories": ["cs.GT"], "pdf": "https://arxiv.org/pdf/2510.12158", "abs": "https://arxiv.org/abs/2510.12158", "authors": ["Kevin Hsu"], "title": "Fair Division of Indivisible Items", "comment": "105 pages, PhD dissertation", "summary": "We study the fair division of indivisible items. In the general model, the\ngoal is to allocate $m$ indivisible items to $n$ agents while satisfying\nfairness criteria such as MMS, EF1, and EFX. We also study a\nrecently-introduced graphical model that represents the fair division problem\nas a multigraph, in which vertices correspond to agents and edges to items. The\ngraphical model stipulates that an item can have non-zero marginal utility to\nan agent only if its corresponding edge is incident to the agent's\ncorresponding vertex. We study orientations (allocations that allocate each\nedge to an endpoint) in this model, as they are particularly desirable.\n  Our first contribution concerns MMS allocations of mixed manna (i.e. a\nmixture of goods and chores) in the general model. It is known that MMS\nallocations of goods exist when $m \\leq n+5$. We generalize this and show that\nwhen $m \\leq n+5$, MMS allocations of mixed manna exist as long as $n \\leq 3$,\nthere is an agent whose MMS threshold is non-negative, or every item is a\nchore. Remarkably, our result leaves only the case where every agent has a\nnegative MMS threshold unanswered.\n  Our second contribution concerns EFX orientations of multigraphs of goods. We\nshow that deciding whether EFX orientations exist for multigraphs is\nNP-complete, even for symmetric bi-valued multigraphs. Complementarily, we show\nsymmetric bi-valued multigraphs that do not contain non-trivial odd multitrees\nhave EFX orientations that can be found in polynomial time.\n  Our third contribution concerns EF1 and EFX orientations of graphs and\nmultigraphs of chores. We obtain polynomial-time algorithms for deciding\nwhether such graphs have EF1 and EFX orientations, resolving a previous\nconjecture and showing a fundamental difference between goods and chores\ndivision. In addition, we show that the analogous problems for multigraphs are\nNP-hard."}
{"id": "2510.12087", "categories": ["cs.GR"], "pdf": "https://arxiv.org/pdf/2510.12087", "abs": "https://arxiv.org/abs/2510.12087", "authors": ["Heng Zhang", "Tianyi Zhang", "Yuling Shi", "Xiaodong Gu", "Yaomin Shen", "Zijian Zhang", "Yilei Yuan", "Hao Zhang", "Jin Huang"], "title": "Can Representation Gaps Be the Key to Enhancing Robustness in Graph-Text Alignment?", "comment": null, "summary": "Representation learning on text-attributed graphs (TAGs) integrates\nstructural connectivity with rich textual semantics, enabling applications in\ndiverse domains. Current methods largely rely on contrastive learning to\nmaximize cross-modal similarity, assuming tighter coupling between graph and\ntext representations improves transfer performance. However, our empirical\nanalysis reveals that both natural gap expansion and forced gap reduction\nresult in performance degradation by disrupting pre-trained knowledge\nstructures and impairing generalization. This arises from the geometric\nincompatibility between encoders, where graph encoders capture topological\npatterns, while text encoders capture semantic structures. Over-alignment\ncompresses these distinct spaces into shared subspaces, causing structure\ncollapse that diminishes both topological reasoning and semantic understanding.\nWe propose \\textbf{LLM4GTA}, a gap-aware alignment framework that preserves\nrepresentation gaps as geometric necessities for maintaining modality-specific\nknowledge and improving transfer performance. LLM4GTA includes an adaptive gap\npreservation module to prevent over-alignment by monitoring similarity\nevolution and an intra-modal compensation mechanism that boosts discriminative\npower using auxiliary classifiers in graph space. Extensive experiments show\nsignificant improvements over existing methods in zero-shot and few-shot\nscenarios."}
{"id": "2510.12641", "categories": ["cs.GT", "cs.DS"], "pdf": "https://arxiv.org/pdf/2510.12641", "abs": "https://arxiv.org/abs/2510.12641", "authors": ["Martin Bullinger", "Adam Dunajski", "Edith Elkind", "Matan Gilboa"], "title": "Single-Deviation Stability in Additively Separable Hedonic Games with Constrained Coalition Sizes", "comment": null, "summary": "We study stability in additively separable hedonic games when coalition sizes\nhave to respect fixed size bounds. We consider four classic notions of\nstability based on single-agent deviations, namely, Nash stability, individual\nstability, contractual Nash stability, and contractual individual stability.\nFor each stability notion, we consider two variants: in one, the coalition left\nbehind by a deviator must still be of a valid size, and in the other there is\nno such constraint. We provide a full picture of the existence of stable\noutcomes with respect to given size parameters. Additionally, when there are\nonly upper bounds, we fully characterize the computational complexity of the\nassociated existence problem. In particular, we obtain polynomial-time\nalgorithms for contractual individual stability and contractual Nash stability,\nwhere the latter requires an upper bound of 2. We obtain further results for\nNash stability and contractual individual stability, when the lower bound is at\nleast 2."}
{"id": "2510.12192", "categories": ["cs.GR"], "pdf": "https://arxiv.org/pdf/2510.12192", "abs": "https://arxiv.org/abs/2510.12192", "authors": ["Xi Cheng", "Pingfa Feng", "Zhichao Liao", "Mingyu Fan", "Long Zeng"], "title": "SDGraph: Multi-Level Sketch Representation Learning by Sparse-Dense Graph Architecture", "comment": null, "summary": "Freehand sketches exhibit unique sparsity and abstraction, necessitating\nlearning pipelines distinct from those designed for images. For sketch learning\nmethods, the central objective is to fully exploit the effective information\nembedded in sketches. However, there is limited research on what constitutes\neffective sketch information, which in turn constrains the performance of\nexisting approaches. To tackle this issue, we first proposed the Multi-Level\nSketch Representation Scheme to systematically identify the effective\ninformation. The scheme organizes sketch representation into three levels:\nsketch-level, stroke-level, and point-level. This design is based on the\ngranularity of analytical elements, from coarse (sketch-level) to fine\n(point-level), thereby ensuring more comprehensive coverage of the sketch\ninformation. For each level, we conducted theoretical analyses and experimental\nevaluations to identify and validate the effective information. Building on the\nabove studies, we developed SDGraph, a deep learning architecture designed to\nexploit the identified effective information across the three levels. SDGraph\ncomprises two complementary modules: a Sparse Graph that treats strokes as\nnodes for sketch-level and stroke-level representation learning, and a Dense\nGraph that treats points as nodes for sketch-level and point-level\nrepresentation learning. Both modules employ graph convolution along with\ndown-sampling and up-sampling operations, enabling them to function as both\nencoder and decoder. Besides that, an information fusion module bridges the two\ngraphs to further enhance feature extraction. SDGraph supports a wide range of\nsketch-related downstream tasks, achieving accuracy improvements of 1.15\\% and\n1.70\\% over the state-of-the-art in classification and retrieval, respectively,\nand 36.58\\% improvement in vector sketch generation quality."}
