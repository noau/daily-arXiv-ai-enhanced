<div id=toc></div>

# Table of Contents

- [cs.GR](#cs.GR) [Total: 8]
- [cs.PL](#cs.PL) [Total: 12]
- [cs.GT](#cs.GT) [Total: 9]


<div id='cs.GR'></div>

# cs.GR [[Back]](#toc)

### [1] [CLoD-GS: Continuous Level-of-Detail via 3D Gaussian Splatting](https://arxiv.org/abs/2510.09997)
*Zhigang Cheng,Mingchao Sun,Yu Liu,Zengye Ge,Luyang Tang,Mu Xu,Yangyan Li,Peng Pan*

Main category: cs.GR

TL;DR: 本文提出了一种基于3D高斯泼溅技术的连续细节层次（CLoD）框架CLoD-GS，解决了传统离散细节层次方法的存储开销和视觉跳跃问题。


<details>
  <summary>Details</summary>
Motivation: 传统的离散细节层次（DLoD）技术在渲染复杂场景时需要存储多个模型副本，并容易产生视觉跳跃现象，影响用户体验。因此，研究者希望通过新兴的3D高斯泼溅技术（3DGS）实现连续的细节层次（CLoD），以避免这些问题。

Method: CLoD-GS框架为每个高斯基元引入了可学习的距离依赖衰减参数，动态调整其不透明度。此外，提出了虚拟距离缩放机制和一种新颖的从粗到细的训练策略，结合渲染点数量正则化，以确保模型在不同距离下的稳健性。

Result: 实验表明，CLoD-GS能够通过单一模型实现平滑、可质量扩展的渲染，显著减少了基元数量和内存占用，避免了传统方法的存储开销和视觉跳跃问题。

Conclusion: CLoD-GS是一种高效的连续细节层次解决方案，不仅提升了视觉保真度，还优化了存储和性能，为实时计算机图形学提供了新的技术路径。

Abstract: Level of Detail (LoD) is a fundamental technique in real-time computer
graphics for managing the rendering costs of complex scenes while preserving
visual fidelity. Traditionally, LoD is implemented using discrete levels
(DLoD), where multiple, distinct versions of a model are swapped out at
different distances. This long-standing paradigm, however, suffers from two
major drawbacks: it requires significant storage for multiple model copies and
causes jarring visual ``popping" artifacts during transitions, degrading the
user experience. We argue that the explicit, primitive-based nature of the
emerging 3D Gaussian Splatting (3DGS) technique enables a more ideal paradigm:
Continuous LoD (CLoD). A CLoD approach facilitates smooth, seamless quality
scaling within a single, unified model, thereby circumventing the core problems
of DLOD. To this end, we introduce CLoD-GS, a framework that integrates a
continuous LoD mechanism directly into a 3DGS representation. Our method
introduces a learnable, distance-dependent decay parameter for each Gaussian
primitive, which dynamically adjusts its opacity based on viewpoint proximity.
This allows for the progressive and smooth filtering of less significant
primitives, effectively creating a continuous spectrum of detail within one
model. To train this model to be robust across all distances, we introduce a
virtual distance scaling mechanism and a novel coarse-to-fine training strategy
with rendered point count regularization. Our approach not only eliminates the
storage overhead and visual artifacts of discrete methods but also reduces the
primitive count and memory footprint of the final model. Extensive experiments
demonstrate that CLoD-GS achieves smooth, quality-scalable rendering from a
single model, delivering high-fidelity results across a wide range of
performance targets.

</details>


### [2] [Sketch Animation: State-of-the-art Report](https://arxiv.org/abs/2510.10218)
*Gaurav Rai,Ojaswa Sharma*

Main category: cs.GR

TL;DR: 本综述探讨了素描动画的最新趋势和创新，重点介绍了关键帧插值、物理动画、深度学习方法等关键技术，并展望了元宇宙和人机交互的未来机遇。


<details>
  <summary>Details</summary>
Motivation: 素描动画已成为连接艺术与科学的变革性技术，广泛应用于娱乐、教育、医疗和虚拟现实等领域。本文旨在提供对当前技术和未来方向的全面理解，为学术和行业专业人士提供参考。

Method: 论文对素描动画的关键方法进行了分类和评估，包括关键帧插值、基于物理的动画、数据驱动、动作捕捉和深度学习方法，并探讨了人工智能、实时渲染和云端解决方案的应用。

Result: 研究总结了人工智能等技术在提升素描动画的真实感、可扩展性和交互性方面的作用，并分析了计算复杂性、可扩展性和用户友好界面等挑战。

Conclusion: 本综述通过综合多方研究，全面呈现了素描动画的现状和未来发展方向，为这一动态领域的创新提供了重要参考。

Abstract: Sketch animation has emerged as a transformative technology, bridging art and
science to create dynamic visual narratives across various fields such as
entertainment, education, healthcare, and virtual reality. This survey explores
recent trends and innovations in sketch animation, with a focus on methods that
have advanced the state of the art. The paper categorizes and evaluates key
methodologies, including keyframe interpolation, physics-based animation,
data-driven, motion capture, and deep learning approaches. We examine the
integration of artificial intelligence, real-time rendering, and cloud-based
solutions, highlighting their impact on enhancing realism, scalability, and
interactivity. Additionally, the survey delves into the challenges of
computational complexity, scalability, and user-friendly interfaces, as well as
emerging opportunities within metaverse applications and human-machine
interaction. By synthesizing insights from a wide array of research, this
survey aims to provide a comprehensive understanding of the current landscape
and future directions of sketch animation, serving as a resource for both
academics and industry professionals seeking to innovate in this dynamic field.

</details>


### [3] [Unlocking Thickness Modeling for Codimensional Contact Simulation](https://arxiv.org/abs/2510.10256)
*Gonzalo Gomez-Nogales,Zhen Chen,Rosalie Martin,Elena Garces,Danny M. Kaufman*

Main category: cs.GR

TL;DR: 本文分析了阻碍可靠应用具有厚度的共维纱线和壳模型模拟真实世界编织和针织织物的基本限制，并提出了一种新的接触处理模型以解决问题。


<details>
  <summary>Details</summary>
Motivation: 现有方法在处理非物理接触力时存在分辨率限制或牺牲鲁棒性的问题，影响模拟的真实性和准确性。

Method: 提出了一种新的接触处理模型，适用于加厚的共维模拟，消除了分辨率限制并保证无接触锁定的非相交模拟。

Result: 该模型在一系列先前无法实现的模拟场景中展示了有效性，包括真实世界的纱线和织物参数、挑战性的模拟条件和网格分辨率。

Conclusion: 新模型成功解决了模拟真实世界材料时的接触锁定问题，为高分辨率和鲁棒的共维模拟提供了实用方案。

Abstract: In this work we analyze and address a fundamental restriction that blocks the
reliable application of codimensional yarn-level and shell models with
thickness, to simulate real-world woven and knit fabrics. As discretizations
refine toward practical and accurate physical modeling, such models can
generate non-physical contact forces with stencil-neighboring elements in the
simulation mesh, leading to severe locking artifacts. While not well-documented
in the literature, this restriction has so far been addressed with two
alternatives with undesirable tradeoffs. One option is to restrict the mesh to
coarse resolutions, however, this eliminates the possibility of accurate (and
consistent) resolution simulations across real-world material variations. A
second alternative instead seeks to cull contact pairs that can create such
locking forces in the first place. This relaxes resolution restrictions but
compromise robustness. Culling can and will generate unacceptable and
unpredictable geometric intersections and tunneling that destroys weaving and
knitting structures and cause unrecoverable pull-throughs. We address these
challenges to simulating real-world materials with a new and practical
contact-processing model for thickened codimensional simulation, that removes
resolution restrictions, while guaranteeing contact-locking-free,
non-intersecting simulations. We demonstrate the application of our model
across a wide range of previously unavailable simulation scenarios, with
real-world material yarn and fabric parameters and patterns, challenging
simulation conditions and mesh resolutions, and both rod and shell models,
integrated with the IPC barrier.

</details>


### [4] [GraphTracer: Graph-Guided Failure Tracing in LLM Agents for Robust Multi-Turn Deep Search](https://arxiv.org/abs/2510.10581)
*Heng Zhang,Yuling Shi,Xiaodong Gu,Haochen You,Zijian Zhang,Lubin Gan,Yilei Yuan,Jin Huang*

Main category: cs.GR

TL;DR: GraphTracer框架通过信息流分析重新定义了多Agent系统中的故障归因方法，解决了多回合深度搜索场景中高失败率的问题，并在准确性和性能上显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有的时间属性方法在多Agent系统中难以准确诊断错误根源，特别是在错误跨Agent传播时。本文旨在解决两个核心挑战：区分多Agent错误传播中的症状与根本原因，以及追踪信息依赖关系。

Method: GraphTracer框架通过构建信息依赖图（IDGs）显式捕捉Agent间的引用和输出依赖关系，并利用图感知合成数据生成瞄准关键节点，从而定位故障根源。

Result: 在Who&When基准测试和生产系统中的评估显示，GraphTracer-8B的归因准确性比现有最优模型高出18.18%，并使部署的多Agent框架性能提升4.8%至14.2%。

Conclusion: GraphTracer为多Agent系统调试提供了稳健的解决方案，显著提升了故障归因准确性和系统性能。

Abstract: Multi-agent systems powered by Large Language Models excel at complex tasks
through coordinated collaboration, yet they face high failure rates in
multi-turn deep search scenarios. Existing temporal attribution methods
struggle to accurately diagnose root causes, particularly when errors propagate
across multiple agents. Attempts to automate failure attribution by analyzing
action sequences remain ineffective due to their inability to account for
information dependencies that span agents. This paper identifies two core
challenges: \textit{(i) distinguishing symptoms from root causes in multi-agent
error propagation}, and \textit{(ii) tracing information dependencies beyond
temporal order}. To address these issues, we introduce \textbf{GraphTracer}, a
framework that redefines failure attribution through information flow analysis.
GraphTracer constructs Information Dependency Graphs (IDGs) to explicitly
capture how agents reference and build on prior outputs. It localizes root
causes by tracing through these dependency structures instead of relying on
temporal sequences. GraphTracer also uses graph-aware synthetic data generation
to target critical nodes, creating realistic failure scenarios. Evaluations on
the Who\&When benchmark and integration into production systems demonstrate
that GraphTracer-8B achieves up to 18.18\% higher attribution accuracy compared
to state-of-the-art models and enables 4.8\% to 14.2\% performance improvements
in deployed multi-agent frameworks, establishing a robust solution for
multi-agent system debugging.

</details>


### [5] [D3MAS: Decompose, Deduce, and Distribute for Enhanced Knowledge Sharing in Multi-Agent Systems](https://arxiv.org/abs/2510.10585)
*Heng Zhang,Yuling Shi,Xiaodong Gu,Haochen You,Zijian Zhang,Lubin Gan,Yilei Yuan,Jin Huang*

Main category: cs.GR

TL;DR: D3MAS是一种分层协调框架，旨在解决多智能体系统中知识冗余的问题，通过分解、推理和分发三个协调层来提高效率和准确性。


<details>
  <summary>Details</summary>
Motivation: 当前多智能体系统在协作解决问题时存在严重的知识冗余问题，导致检索和推理过程中重复劳动，效率低下。

Method: 提出D3MAS框架，通过任务分解、协作推理和分布式记忆三个层次的协调，利用结构化消息传递来减少冗余。

Result: 在四个数据集上的实验表明，D3MAS平均提高推理准确性8.7%至15.6%，并减少46%的知识冗余。

Conclusion: D3MAS通过分层设计和结构化消息传递，显著减少了知识冗余并提升了多智能体系统的协作效率。

Abstract: Multi-agent systems powered by large language models exhibit strong
capabilities in collaborative problem-solving. However, these systems suffer
from substantial knowledge redundancy. Agents duplicate efforts in retrieval
and reasoning processes. This inefficiency stems from a deeper issue: current
architectures lack mechanisms to ensure agents share minimal sufficient
information at each operational stage. Empirical analysis reveals an average
knowledge duplication rate of 47.3\% across agent communications. We propose
D3MAS (Decompose, Deduce, and Distribute), a hierarchical coordination
framework addressing redundancy through structural design rather than explicit
optimization. The framework organizes collaboration across three coordinated
layers. Task decomposition filters irrelevant sub-problems early. Collaborative
reasoning captures complementary inference paths across agents. Distributed
memory provides access to non-redundant knowledge. These layers coordinate
through structured message passing in a unified heterogeneous graph. This
cross-layer alignment ensures information remains aligned with actual task
needs. Experiments on four challenging datasets show that D3MAS consistently
improves reasoning accuracy by 8.7\% to 15.6\% and reduces knowledge redundancy
by 46\% on average.

</details>


### [6] [VLM-Guided Adaptive Negative Prompting for Creative Generation](https://arxiv.org/abs/2510.10715)
*Shelly Golan,Yotam Nitzan,Zongze Wu,Or Patashnik*

Main category: cs.GR

TL;DR: 本文提出了一种无需训练、推理时间的方法VLM-Guided Adaptive Negative-Prompting，利用视觉语言模型（VLM）引导生成过程，避免传统视觉概念，从而促进新颖且有效的图像生成。


<details>
  <summary>Details</summary>
Motivation: 当前文本到图像扩散模型虽然能精确匹配用户提示生成逼真场景，但在生成真正新颖内容方面仍有不足，且现有方法存在限制或计算成本高的问题。

Method: 通过VLM分析生成过程的中间输出，自适应地引导生成远离传统视觉概念，以鼓励新颖且有效的图像生成。

Result: 实验表明，该方法在创意新颖性方面有显著提升，且计算开销极小，适用于复杂场景和连贯创意对象集的生成。

Conclusion: 该方法无缝集成到现有扩散流程中，为生成超越文本描述限制的创意输出提供了实用途径。

Abstract: Creative generation is the synthesis of new, surprising, and valuable samples
that reflect user intent yet cannot be envisioned in advance. This task aims to
extend human imagination, enabling the discovery of visual concepts that exist
in the unexplored spaces between familiar domains. While text-to-image
diffusion models excel at rendering photorealistic scenes that faithfully match
user prompts, they still struggle to generate genuinely novel content. Existing
approaches to enhance generative creativity either rely on interpolation of
image features, which restricts exploration to predefined categories, or
require time-intensive procedures such as embedding optimization or model
fine-tuning. We propose VLM-Guided Adaptive Negative-Prompting, a
training-free, inference-time method that promotes creative image generation
while preserving the validity of the generated object. Our approach utilizes a
vision-language model (VLM) that analyzes intermediate outputs of the
generation process and adaptively steers it away from conventional visual
concepts, encouraging the emergence of novel and surprising outputs. We
evaluate creativity through both novelty and validity, using statistical
metrics in the CLIP embedding space. Through extensive experiments, we show
consistent gains in creative novelty with negligible computational overhead.
Moreover, unlike existing methods that primarily generate single objects, our
approach extends to complex scenarios, such as generating coherent sets of
creative objects and preserving creativity within elaborate compositional
prompts. Our method integrates seamlessly into existing diffusion pipelines,
offering a practical route to producing creative outputs that venture beyond
the constraints of textual descriptions.

</details>


### [7] [MATStruct: High-Quality Medial Mesh Computation via Structure-aware Variational Optimization](https://arxiv.org/abs/2510.10751)
*Ningna Wang,Rui Xu,Yibo Yin,Zichun Zhong,Taku Komura,Wenping Wang,Xiaohu Guo*

Main category: cs.GR

TL;DR: 提出了一种新颖的优化框架，用于计算中轴线变换，同时保留中轴结构并确保高质量的中轴网格。


<details>
  <summary>Details</summary>
Motivation: 中轴结构（由相互连接的片、缝和节点组成）提供了3D形状的自然体积分解。现有方法在中轴结构的准确性和网格质量上存在不足，因此需要一种能够兼顾结构和质量的方法。

Method: 采用结构感知的粒子优化流程，基于受限功率图（RPD）划分输入体积为凸包单元，并通过球形二次误差度量（SQEM）投影约束球体运动，高斯核能量鼓励均匀分布。

Result: 相比MATFP和MATTopo等方法，该方法产生了更干净、更准确的中轴结构，并显著提高了网格质量。

Conclusion: 该框架首次将结构感知集成到优化过程中，产生了具有卓越几何保真度、拓扑正确性和明确结构分解的中轴网格。

Abstract: We propose a novel optimization framework for computing the medial axis
transform that simultaneously preserves the medial structure and ensures high
medial mesh quality. The medial structure, consisting of interconnected sheets,
seams, and junctions, provides a natural volumetric decomposition of a 3D
shape. Our method introduces a structure-aware, particle-based optimization
pipeline guided by the restricted power diagram (RPD), which partitions the
input volume into convex cells whose dual encodes the connectivity of the
medial mesh. Structure-awareness is enforced through a spherical quadratic
error metric (SQEM) projection that constrains the movement of medial spheres,
while a Gaussian kernel energy encourages an even spatial distribution.
Compared to feature-preserving methods such as MATFP and MATTopo, our approach
produces cleaner and more accurate medial structures with significantly
improved mesh quality. In contrast to voxel-based, point-cloud-based, and
variational methods, our framework is the first to integrate structural
awareness into the optimization process, yielding medial meshes with superior
geometric fidelity, topological correctness, and explicit structural
decomposition.

</details>


### [8] [The Fire We Share](https://arxiv.org/abs/2510.10841)
*Chen Wang,Mengtan Lin*

Main category: cs.GR

TL;DR: 《The Fire We Share》提出了一种以关怀为中心的视觉化框架，将野火数据重新构想为一个有生命的、生态和社会交织的档案。


<details>
  <summary>Details</summary>
Motivation: 研究动机在于将野火数据从静态指标转变为动态的、生态和社会交织的档案，以强调其伦理和关系性。

Method: 通过结合植物启发的数据形式、基于事件的映射和叙事分层，开发了一种多层次的视觉化框架。

Result: 该项目成功地将野火数据重新构想为一个有纹理的、受伤的档案，突出了其时间和关系性。

Conclusion: 研究表明，通过这种视觉化方法，野火数据可以更有效地传达其生态和社会影响，从而促进更伦理和关怀的理解。

Abstract: The Fire We Share proposes a care-centered, consequence-aware visualization
framework for engaging with wildfire data not as static metrics, but as living
archives of ecological and social entanglement. By combining plants-inspired
data forms, event-based mapping, and narrative layering, the project
foregrounds fire as a shared temporal condition-one that cuts across natural
cycles and human systems. Rather than simplifying wildfire data into digestible
visuals, The Fire We Share reimagines it as a textured, wounded
archive-embodied, relational, and radically ethical.

</details>


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [9] [Herb.jl: A Unifying Program Synthesis Library](https://arxiv.org/abs/2510.09726)
*Tilman Hinnerichs,Reuben Gardos Reid,Jaap de Jong,Bart Swinkels,Pamela Wochner,Nicolae Filat,Tudor Magurescu,Issa Hanou,Sebastijan Dumancic*

Main category: cs.PL

TL;DR: Herb.jl 是一个用 Julia 编程语言编写的统一程序合成库，旨在模块化合成算法的核心组件，便于复用和扩展。


<details>
  <summary>Details</summary>
Motivation: 现有的程序合成工具虽然多样，但复用和混合之前开发的方法耗时且繁琐。Herb.jl 通过模块化合成算法的底层组件，解决这一问题。

Method: Herb.jl 将合成算法分解为可通信和完全可扩展的子模块，支持简单的重新应用。

Result: 通过三个常见用例展示了 Herb.jl 的优势：实现简单问题与语法、用少量代码实现已有合成器，以及对基准测试运行合成器。

Conclusion: Herb.jl 提供了一个灵活且高效的模块化框架，显著简化了程序合成工具的开发和复用。

Abstract: Program synthesis -- the automatic generation of code given a specification
-- is one of the most fundamental tasks in artificial intelligence (AI) and
many programmers' dream. Numerous synthesizers have been developed to tackle
program synthesis, manifesting different ideas to approach the exponentially
growing program space. While numerous smart program synthesis tools exist,
reusing and remixing previously developed methods is tedious and
time-consuming. We propose Herb.jl, a unifying program synthesis library
written in the Julia programming language, to address these issues. Since
current methods rely on similar building blocks, we aim to modularize the
underlying synthesis algorithm into communicating and fully extendable
sub-compartments, allowing for straightforward reapplication of these modules.
To demonstrate the benefits of using Herb.jl, we show three common use cases:
1. how to implement a simple problem and grammar, and how to solve it, 2. how
to implement a previously developed synthesizer with just a few lines of code,
and 3. how to run a synthesizer against a benchmark.

</details>


### [10] [ACT: Automatically Generating Compiler Backends from Tensor Accelerator ISA Descriptions](https://arxiv.org/abs/2510.09932)
*Devansh Jain,Akash Pardeshi,Marco Frigo,Krut Patel,Kaustubh Khulbe,Jai Arora,Charith Mendis*

Main category: cs.PL

TL;DR: ACT是一种编译器后端生成器，能够根据指令集架构（ISA）描述自动为张量加速器生成编译器后端，提高开发效率并支持快速迭代。


<details>
  <summary>Details</summary>
Motivation: 现有的张量加速器缺乏编译器后端支持，且设计迭代迅速，手动开发编译器后端成本高。为了提高新加速器的采用率和软件开发效率，需要更敏捷的编译器后端构建方法。

Method: ACT通过形式化指定编译器后端生成问题，引入新的张量加速器ISA描述规范。其设计支持用户可编程内存和复杂参数化指令，并使用参数化等式饱和的指令选择阶段和基于约束编程的内存分配阶段。

Result: ACT生成的编译器后端在三个学术和工业加速器平台上性能优于或等于手写优化的内核库代码，同时保持较低的编译开销。

Conclusion: ACT为张量加速器提供了一种高效、自动化生成编译器后端的方法，解决了手动开发的高成本问题，推动了新加速器的快速采用和软件开发。

Abstract: Tensor compilers play a key role in enabling high-performance implementations
of deep learning workloads. These compilers rely on existing CPU and GPU code
generation backends to generate device-specific code. Recently, many tensor
accelerators (neural processing units) have been proposed to further accelerate
these workloads. Compared to commodity hardware, however, most of the proposed
tensor accelerators do not have compiler backends with code generation support.
Moreover, the accelerator designs are subject to fast iteration cycles, making
it difficult to manually develop compiler backends similar to commodity
hardware platforms. Therefore, to increase adoption and enable faster software
development cycles for novel tensor accelerator designs, we need to make the
compiler backend construction process more agile.
  To address this gap, we introduce ACT, a compiler backend generator that
automatically generates compiler backends for tensor accelerators, given just
the instruction set architecture (ISA) descriptions. We first formally specify
the compiler backend generation problem that introduces a novel specification
for describing tensor accelerator ISAs. Next, we design ACT such that it
supports user-programmable memories and complex parameterized instructions that
are prevalent in tensor accelerators. ACT uses a novel parameterized equality
saturation-based instruction selection phase and a constraint programming-based
memory allocation phase. We prove that compiler backends generated by ACT are
sound and complete. Finally, we generate compiler backends for three
accelerator platforms from industry and academia, and show that they match or
outperform code written using hand-optimized kernel libraries while maintaining
low compilation overheads.

</details>


### [11] [End-to-end Compositional Verification of Program Safety through Verified and Verifying Compilation](https://arxiv.org/abs/2510.10015)
*Jinhua Wu,Yuting Wang,Liukun Yu,Linglong Meng*

Main category: cs.PL

TL;DR: 该论文提出了一种名为开放安全的模块化安全性定义，以解决现代安全编程语言（如Rust）中混合安全和未安全模块时的端到端安全性验证问题。


<details>
  <summary>Details</summary>
Motivation: 现代安全编程语言（如Rust）在某些功能实现时需要混合使用安全和未安全模块，这给端到端安全性验证带来了新的挑战。论文旨在通过模块化和通用的安全性定义解决这一问题。

Method: 论文提出了一种基于开放标记转移系统（LTS）的开放安全定义，支持模块间的组合性验证和验证组合编译。此外，论文还开发了一个受Rust启发的所有权语言（Owlang）的验证编译器。

Result: 通过Owlang和C实现的哈希映射的组合安全性验证，论文证明了其框架在结合验证和验证编译时的有效性。

Conclusion: 开放安全定义和验证组合编译的方法为解决现代编程语言中的端到端安全性问题提供了可行的解决方案。

Abstract: Program safety (i.e., absence of undefined behaviors) is critical for correct
operation of computer systems. It is usually verified at the source level
(e.g., by separation logics) and preserved to the target by verified compilers
(e.g., CompCert), thereby achieving end-to-end verification of safety. However,
modern safe programming languages like Rust pose new problems in achieving
end-to-end safety. Because not all functionalities can be implemented in the
safe language, mixing safe and unsafe modules is needed. Therefore, verified
compilation must preserve a modular notion of safety which can be composed at
the target level. Furthermore, certain classes of errors (e.g., memory errors)
are automatically excluded by verifying compilation (e.g., borrow checking) for
modules written in safe languages. As a result, verified compilation needs to
cooperate with verifying compilation to ensure end-to-end safety.
  To address the above problems, we propose a modular and generic definition of
safety called open safety based on program semantics described as open labeled
transition systems (LTS). Open safety is composable at the boundary of modules
and can be modularly preserved by verified compositional compilation. Those
properties enable separate verification of safety for heterogeneous modules and
composition of the safety results at the target level. Open safety can be
generalized to partial safety (i.e., only a certain class of errors can occur).
By this we formalized the correctness of verifying compilation as derivation of
total safety from partial safety. We demonstrate how our framework can combine
verified and verifying compilation by developing a verified compiler for an
ownership language (called Owlang) inspired by Rust. We evaluate our approach
on the compositional safety verification using a hash map implemented by Owlang
and C.

</details>


### [12] [LOOPerSet: A Large-Scale Dataset for Data-Driven Polyhedral Compiler Optimization](https://arxiv.org/abs/2510.10209)
*Massinissa Merouani,Afif Boudaoud,Riyadh Baghdadi*

Main category: cs.PL

TL;DR: LOOPerSet是一个包含2800万个标记数据点的公共数据集，用于解决机器学习在编译器优化中数据稀缺的问题，支持成本模型训练和新模型架构的基准测试。


<details>
  <summary>Details</summary>
Motivation: 机器学习在编译器优化，尤其是多面体模型中的应用受到大规模公开性能数据集稀缺的限制，导致研究进程缓慢且难以复现。

Method: 通过合成220,000个独特的多面体程序，生成2800万个标记数据点，每个数据点将程序和语义保留的转换序列映射到执行时间。

Result: LOOPerSet数据集因其规模和多样性，成为训练和评估学习成本模型的宝贵资源，同时支持自动化多面体调度的前沿探索。

Conclusion: LOOPerSet的发布旨在降低数据驱动编译器优化的入门门槛，促进可复现研究。

Abstract: The advancement of machine learning for compiler optimization, particularly
within the polyhedral model, is constrained by the scarcity of large-scale,
public performance datasets. This data bottleneck forces researchers to
undertake costly data generation campaigns, slowing down innovation and
hindering reproducible research learned code optimization. To address this gap,
we introduce LOOPerSet, a new public dataset containing 28 million labeled data
points derived from 220,000 unique, synthetically generated polyhedral
programs. Each data point maps a program and a complex sequence of
semantics-preserving transformations (such as fusion, skewing, tiling, and
parallelism)to a ground truth performance measurement (execution time). The
scale and diversity of LOOPerSet make it a valuable resource for training and
evaluating learned cost models, benchmarking new model architectures, and
exploring the frontiers of automated polyhedral scheduling. The dataset is
released under a permissive license to foster reproducible research and lower
the barrier to entry for data-driven compiler optimization.

</details>


### [13] [Learning to Guarantee Type Correctness in Code Generation through Type-Guided Program Synthesis](https://arxiv.org/abs/2510.10216)
*Zhechong Huang,Zhao Zhang,Ruyi Ji,Tingxuan Xia,Qihao Zhu,Qinxiang Cao,Zeyu Sun,Yingfei Xiong*

Main category: cs.PL

TL;DR: 本文提出了一种名为TyFlow的新系统，通过将类型推理内化到代码生成中，使语言模型能够学习类型系统，从而提高代码的功能正确性。


<details>
  <summary>Details</summary>
Motivation: 尽管语言模型在代码生成方面表现出色，但确保类型正确性仍是一个挑战。传统方法（如约束解码）只能外部拒绝不可类型化的代码，模型并未有效学习类型推理，从而限制了整体性能。

Method: TyFlow是一种新型的类型指导的程序合成系统，通过保持类型派生树和合成派生树之间的同构关系，生成基于合成决策序列的新代码表示，而非传统的基于文本的标记序列。

Result: TyFlow不仅消除了类型错误，还显著提高了代码的功能正确性，这表明将语言模型与类型系统内部对齐的重要性。

Conclusion: TyFlow通过内化类型推理和优化代码表示，有效提升了语言模型在代码生成中的性能，展示了类型系统学习对模型能力的重要性。

Abstract: Language models have shown remarkable proficiency in code generation;
nevertheless, ensuring type correctness remains a challenge. Although
traditional methods, such as constrained decoding, alleviate this problem by
externally rejecting untypable code, the model itself does not effectively
learn type reasoning internally, which ultimately limits its overall
performance. This paper introduces TyFlow, a novel system that internalizes
type reasoning within code generation to guide the model to learn the type
system. The core of our approach is a novel type-guided program synthesis
system that maintains an isomorphism between type derivation trees and
synthesis derivation trees, enabling a new code representation based on
synthesis decision sequences rather than traditional text-based token
sequences. By offloading the complexity of type system learning to the
representation itself, models can redirect their computational resources toward
higher-level program semantics. Our evaluation shows that TyFlow not only
eliminates type errors but also significantly improves functional correctness,
highlighting the importance of aligning LMs with type systems internally.

</details>


### [14] [Old is Gold: Optimizing Single-threaded Applications with Exgen-Malloc](https://arxiv.org/abs/2510.10219)
*Ruihao Li,Lizy K. John,Neeraja J. Yadwadkar*

Main category: cs.PL

TL;DR: Exgen-Malloc是一种专为单线程应用设计的内存分配器，通过简化元数据和控制流，显著提高了分配效率并减少了内存开销。


<details>
  <summary>Details</summary>
Motivation: 内存分配器的性能对数据中心成本有显著影响，尤其是在超大规模环境中。现有分配器在多线程环境下的复杂性带来了不必要的开销，而在单线程场景中这些开销是可以避免的。

Method: Exgen-Malloc通过集中式堆、单一空闲块列表以及平衡的内存提交和重定位策略，简化了设计并减少了开销。同时，它借鉴了多线程分配器的现代设计原则。

Result: 在两个Intel Xeon平台上，Exgen-Malloc在SPEC CPU2017、redis-benchmark和mimalloc-bench上的速度分别提高了1.17倍、1.10倍和1.93倍，并且内存节省分别为6.2%、0.1%和25.2%。

Conclusion: Exgen-Malloc证明了在单线程应用中专一化设计的有效性，通过简化设计和减少开销，显著提升了性能并降低了内存消耗。

Abstract: Memory allocators hide beneath nearly every application stack, yet their
performance footprint extends far beyond their code size. Even small
inefficiencies in the allocators ripple through caches and the rest of the
memory hierarchy, collectively imposing what operators often call a "datacenter
tax". At hyperscale, even a 1% improvement in allocator efficiency can unlock
millions of dollars in savings and measurable reductions in datacenter energy
consumption. Modern memory allocators are designed to optimize allocation speed
and memory fragmentation in multi-threaded environments, relying on complex
metadata and control logic to achieve high performance. However, the overhead
introduced by this complexity prompts a reevaluation of allocator design.
Notably, such overhead can be avoided in single-threaded scenarios, which
continue to be widely used across diverse application domains.
  In this paper, we introduce Exgen-Malloc, a memory allocator purpose-built
for single-threaded applications. By specializing for single-threaded
execution, Exgen-Malloc eliminates unnecessary metadata, simplifies the control
flow, thereby reducing overhead and improving allocation efficiency. Its core
design features include a centralized heap, a single free-block list, and a
balanced strategy for memory commitment and relocation. Additionally,
Exgen-Malloc incorporates design principles in modern multi-threaded
allocators, which do not exist in legacy single-threaded allocators such as
dlmalloc. We evaluate Exgen-Malloc on two Intel Xeon platforms. Across both
systems, Exgen-Malloc achieves a speedup of 1.17x, 1.10x, and 1.93x over
dlmalloc on SPEC CPU2017, redis-benchmark, and mimalloc-bench, respectively. In
addition to performance, Exgen-Malloc achieves 6.2%, 0.1%, and 25.2% memory
savings over mimalloc on SPEC CPU2017, redis-benchmark, and mimalloc-bench,
respectively.

</details>


### [15] [A Trace-based Approach for Code Safety Analysis](https://arxiv.org/abs/2510.10410)
*Hui Xu*

Main category: cs.PL

TL;DR: 本文通过分析Rust的安全性设计及实际项目，建立了理解不安全代码和未定义行为的系统性框架，并总结了Rust代码的正确性标准。


<details>
  <summary>Details</summary>
Motivation: Rust虽然是一种内存安全的编程语言，但其不安全代码仍是关键问题，本文旨在系统性地理解这些问题并提出解决方案。

Method: 通过审查Rust的安全性设计及分析实际项目中的数据，建立了系统性框架。

Result: 总结了Rust代码的正确性标准，并提出了实现正确封装的可操作指导。

Conclusion: 本文为理解和解决Rust中的不安全代码问题提供了系统性框架和实用建议。

Abstract: Rust is a memory-safe programming language that disallows undefined behavior.
Its safety guarantees have been extensively examined by the community through
empirical studies, which has led to its remarkable success. However, unsafe
code remains a critical concern in Rust. By reviewing the safety design of Rust
and analyzing real-world Rust projects, this paper establishes a systematic
framework for understanding unsafe code and undefined behavior, and summarizes
the soundness criteria for Rust code. It further derives actionable guidance
for achieving sound encapsulation.

</details>


### [16] [ECO: Enhanced Code Optimization via Performance-Aware Prompting for Code-LLMs](https://arxiv.org/abs/2510.10517)
*Su-Hyeon Kim,Joonghyuk Hahn,Sooyoung Cha,Yo-Sub Han*

Main category: cs.PL

TL;DR: 论文介绍了一种名为ECO的性能感知提示框架，用于代码优化，通过提取运行时优化指令（ROIs）并结合符号顾问的诊断结果，显著提升了代码-LM生成高效代码的能力。


<details>
  <summary>Details</summary>
Motivation: 现有的基于代码-LLM的优化方法往往只能模仿表面模式，而无法深入理解性能提升的根本原因。因此，需要一种能够提供性能推理的新方法来改进代码优化。

Method: ECO框架通过从参考的快慢代码对中提取运行时优化指令（ROIs），并结合符号顾问的瓶颈诊断和ROI检索器，生成了一个性能感知的提示，用于指导代码-LLM的优化过程。

Result: ECO框架显著提升了代码-LM的性能优化能力，最高实现了7.81倍的加速，同时最大程度地减少了正确性的损失。

Conclusion: ECO框架通过提供性能感知的提示，有效地改进了代码优化的质量，证明了其在代码-LM优化任务中的实用性和高效性。

Abstract: Code runtime optimization-the task of rewriting a given code to a faster
one-remains challenging, as it requires reasoning about performance trade-offs
involving algorithmic and structural choices. Recent approaches employ
code-LLMs with slow-fast code pairs provided as optimization guidance, but such
pair-based methods obscure the causal factors of performance gains and often
lead to superficial pattern imitation rather than genuine performance
reasoning. We introduce ECO, a performance-aware prompting framework for code
optimization. ECO first distills runtime optimization instructions (ROIs) from
reference slow-fast code pairs; Each ROI describes root causes of inefficiency
and the rationales that drive performance improvements. For a given input code,
ECO in parallel employs (i) a symbolic advisor to produce a bottleneck
diagnosis tailored to the code, and (ii) an ROI retriever to return related
ROIs. These two outputs are then composed into a performance-aware prompt,
providing actionable guidance for code-LLMs. ECO's prompts are model-agnostic,
require no fine-tuning, and can be easily prepended to any code-LLM prompt. Our
empirical studies highlight that ECO prompting significantly improves
code-LLMs' ability to generate efficient code, achieving speedups of up to
7.81x while minimizing correctness loss.

</details>


### [17] [A Verified High-Performance Composable Object Library for Remote Direct Memory Access (Extended Version)](https://arxiv.org/abs/2510.10531)
*Guillaume Ambal,George Hodgkins,Mark Madler,Gregory Chockler,Brijesh Dongol,Joseph Izraelevitz,Azalea Raad,Viktor Vafeiadis*

Main category: cs.PL

TL;DR: LOCO是一个形式化验证的多节点对象库，用于在RDMA上构建对象，填补共享内存与分布式系统编程之间的空白。


<details>
  <summary>Details</summary>
Motivation: RDMA因其高吞吐量和低延迟特性适用于数据中心和AI/ML负载，但其弱内存模型难以使用且缺乏形式化验证。

Method: 开发了LOCO库和Mowgli验证框架，前者利用RDMA的局部性和弱一致性，后者为多节点对象提供模块化声明性验证。

Result: LOCO库性能与定制RDMA系统相当，且编程模型更简单，适合形式化正确性证明。

Conclusion: LOCO和Mowgli的结合为RDMA提供了形式化验证的解决方案，简化了编程模型并确保正确性。

Abstract: Remote Direct Memory Access (RDMA) is a memory technology that allows remote
devices to directly write to and read from each other's memory, bypassing
components such as the CPU and operating system. This enables low-latency
high-throughput networking, as required for many modern data centres, HPC
applications and AI/ML workloads. However, baseline RDMA comprises a highly
permissive weak memory model that is difficult to use in practice and has only
recently been formalised. In this paper, we introduce the Library of Composable
Objects (LOCO), a formally verified library for building multi-node objects on
RDMA, filling the gap between shared memory and distributed system programming.
LOCO objects are well-encapsulated and take advantage of the strong locality
and the weak consistency characteristics of RDMA. They have performance
comparable to custom RDMA systems (e.g. distributed maps), but with a far
simpler programming model amenable to formal proofs of correctness. To support
verification, we develop a novel modular declarative verification framework,
called Mowgli, that is flexible enough to model multinode objects and is
independent of a memory consistency model. We instantiate Mowgli with the RDMA
memory model, and use it to verify correctness of LOCO libraries.

</details>


### [18] [Abstract String Domain Defined with Word Equations as a Reduced Product (Extended Version)](https://arxiv.org/abs/2510.11007)
*Antonina Nepeivoda,Ilya Afanasyev*

Main category: cs.PL

TL;DR: 提出了一种基于字符串区间的抽象域，通过字符串方程和不等式描述字符串区间，构建了字符串对象的半格结构，并定义了抽象操作以减少计算开销，用于分析JavaScript字符串操作程序的属性。


<details>
  <summary>Details</summary>
Motivation: 为了解决JavaScript字符串操作程序的分析问题，引入了一种新的字符串区间的抽象域，旨在更高效地描述和操作字符串区间。

Method: 利用字符串方程和不等式定义了字符串区间，构建了半格结构，并通过长度非递增的态射确定字符串对象的抽象域。同时，提出了多种缩减策略并定义了基本抽象操作。

Result: 研究表明，在所提出的缩减策略下，字符串对象域形成了一种格结构，并能够高效地分析JavaScript字符串操作程序的属性。

Conclusion: 本文提出的字符串区间抽象域和缩减策略为字符串操作程序的分析提供了一种高效且结构化的方法。

Abstract: We introduce a string-interval abstract domain, where string intervals are
characterized by systems of word equations (encoding lower bounds on string
values) and word disequalities (encoding upper bounds). Building upon the
lattice structure of string intervals, we define an abstract string object as a
reduced product on a string property semilattice, determined by
length-non-increasing morphisms. We consider several reduction strategies for
abstract string objects and show that upon these strategies the string object
domain forms a lattice. We define basic abstract string operations on the
domain, aiming to minimize computational overheads on the reduction, and show
how the domain can be used to analyse properties of JavaScript string
manipulating programs.

</details>


### [19] [HUGR: A Quantum-Classical Intermediate Representation](https://arxiv.org/abs/2510.11420)
*Mark Koch,Agustín Borgna,Seyon Sivarajah,Alan Lawrence,Alec Edgington,Douglas Wilson,Craig Roy,Luca Mondada,Lukas Heidemann,Ross Duncan*

Main category: cs.PL

TL;DR: HUGR是一种新颖的基于图的中间表示法，用于混合量子-经典程序，具有高表达性和可扩展性，支持强大的基于模式匹配的编译技术。


<details>
  <summary>Details</summary>
Motivation: 为了捕捉近未来量子计算设备的能力以及新兴量子编程范式的抽象，需要一种高度表达且可扩展的中间表示法。

Method: HUGR基于图结构设计，受MLIR启发，支持多层次抽象的程序推理和平滑降级，同时提供严格的静态类型和线性量子类型的安全性保障。

Result: HUGR的设计实现了高表达性和可扩展性，并提供了开源规范和参考实现。

Conclusion: HUGR为混合量子-经典程序提供了一种高效且安全的中间表示法，推动了量子编译工具的发展。

Abstract: We introduce the Hierarchical Unified Graph Representation (HUGR): a novel
graph based intermediate representation for mixed quantum-classical programs.
HUGR's design features high expressivity and extensibility to capture the
capabilities of near-term and forthcoming quantum computing devices, as well as
new and evolving abstractions from novel quantum programming paradigms. The
graph based structure is machine-friendly and supports powerful pattern
matching based compilation techniques. Inspired by MLIR, HUGR's extensibility
further allows compilation tooling to reason about programs at multiple levels
of abstraction, lowering smoothly between them. Safety guarantees in the
structure including strict, static typing and linear quantum types allow rapid
development of compilation tooling without fear of program invalidation. A full
specification of HUGR and reference implementation are open-source and
available online.

</details>


### [20] [(Dis)Proving Spectre Security with Speculation-Passing Style](https://arxiv.org/abs/2510.11573)
*Santiago Arranz-Olmos,Gilles Barthe,Lionel Blatter,Xingyu Xie,Zhiyuan Zhang*

Main category: cs.PL

TL;DR: 本文提出了一种名为“推测传递风格”（SPS）的程序转换方法，将推测常数时间（SCT）验证转化为常数时间（CT）验证，利用现有CT验证工具实现SCT的证明。


<details>
  <summary>Details</summary>
Motivation: 现有的SCT工具多为CT工具的扩展，但其转换过程缺乏精确的定义和形式化分析，本文旨在填补这一空白，并为这些转换提供形式化基础以展示实际效益。

Method: 引入SPS程序转换方法，通过修改程序以跟踪攻击者控制的预测输入，从而将SCT验证转化为CT验证问题。该方法与EasyCrypt、BINSEC和ctgrind等工具结合进行了实现。

Result: 实验结果表明，SPS与现有CT验证工具的结合能够有效验证Spectre-v1漏洞，并在Kocher的基准测试中取得了成功。

Conclusion: SPS方法不仅适用于Spectre-v1的标准CT泄漏模型，还可扩展到其他Spectre变体和泄漏模型，展示了其广泛的应用潜力。

Abstract: Constant-time (CT) verification tools are commonly used for detecting
potential side-channel vulnerabilities in cryptographic libraries. Recently, a
new class of tools, called speculative constant-time (SCT) tools, has also been
used for detecting potential Spectre vulnerabilities. In many cases, these SCT
tools have emerged as liftings of CT tools. However, these liftings are seldom
defined precisely and are almost never analyzed formally. The goal of this
paper is to address this gap, by developing formal foundations for these
liftings, and to demonstrate that these foundations can yield practical
benefits.
  Concretely, we introduce a program transformation, coined Speculation-Passing
Style (SPS), for reducing SCT verification to CT verification. Essentially, the
transformation instruments the program with a new input that corresponds to
attacker-controlled predictions and modifies the program to follow them. This
approach is sound and complete, in the sense that a program is SCT if and only
if its SPS transform is CT. Thus, we can leverage existing CT verification
tools to prove SCT; we illustrate this by combining SPS with three standard
methodologies for CT verification, namely reducing it to non-interference,
assertion safety and dynamic taint analysis. We realize these combinations with
three existing tools, EasyCrypt, BINSEC, and ctgrind, and we evaluate them on
Kocher's benchmarks for Spectre-v1. Our results focus on Spectre-v1 in the
standard CT leakage model; however, we also discuss applications of our method
to other variants of Spectre and other leakage models.

</details>


<div id='cs.GT'></div>

# cs.GT [[Back]](#toc)

### [21] [Stability in Online Assignment Games](https://arxiv.org/abs/2510.09814)
*Emile Martinez,Felipe Garrido-Lucero,Umberto Grandi*

Main category: cs.GT

TL;DR: 论文研究了在线分配游戏中的不稳定性，提出了两种不稳定性度量方法，并分析了随机算法在稳定性上的表现。


<details>
  <summary>Details</summary>
Motivation: 尽管Shapley和Shubik证明了稳定分配必须基于最大社会福利匹配，但在线环境中匹配往往是次优的，导致稳定分配难以实现。因此，论文旨在分析次优匹配下的不稳定性。

Method: 论文引入了两种互补的不稳定性度量方法，并将其与底层匹配的最优比率联系起来，以此研究在线分配游戏中随机算法的稳定性能。

Result: 研究发现两种不稳定性度量方法可以有效分析次优匹配下的不稳定性，并为评估随机算法的稳定性提供了框架。

Conclusion: 论文通过分析次优匹配下的不稳定性，提出了适用于在线分配游戏的稳定性评估方法，对实际应用有重要指导意义。

Abstract: The assignment game models a housing market where buyers and sellers are
matched, and transaction prices are set so that the resulting allocation is
stable. Shapley and Shubik showed that every stable allocation is necessarily
built on a maximum social welfare matching. In practice, however, stable
allocations are rarely attainable, as matchings are often sub-optimal,
particularly in online settings where eagents arrive sequentially to the
market. In this paper, we introduce and compare two complementary measures of
instability for allocations with sub-optimal matchings, establish their
connections to the optimality ratio of the underlying matching, and use this
framework to study the stability performances of randomized algorithms in
online assignment games.

</details>


### [22] [Proportional and Pareto-Optimal Allocation of Chores with Subsidy](https://arxiv.org/abs/2510.10335)
*Jugal Garg,Eklavya Sharma,Xiaowei Wu*

Main category: cs.GT

TL;DR: 该论文提出了一种多项式时间算法，用于公平高效地分配不可分割的家务任务，并在保证比例公平的同时最小化总补贴，且算法简化。


<details>
  <summary>Details</summary>
Motivation: 解决不可分割家务任务在代理人之间公平和高效分配的问题，尤其是在比例公平和帕累托最优的条件下，现有方法的复杂性及其缺乏经济效率的不足。

Method: 通过计算比例公平的竞争均衡，并应用以最小痛苦每美元边为引导的舍入过程，设计了一个多项式时间算法。

Result: 该算法能够在保证帕累托最优的同时，达到与现有方法相同的补贴上限（最多n/3 - 1/6），且计算过程更为简化。

Conclusion: 该研究成果显著简化了现有的社会福利优化方法，同时确保了公平性与效率性，为实际应用提供了更可行的解决方案。

Abstract: We consider the problem of allocating $m$ indivisible chores among $n$ agents
with possibly different weights, aiming for a solution that is both fair and
efficient. Specifically, we focus on the classic fairness notion of
proportionality and efficiency notion of Pareto-optimality. Since proportional
allocations may not always exist in this setting, we allow the use of subsidies
(monetary compensation to agents) to ensure agents are
proportionally-satisfied, and aim to minimize the total subsidy required. Wu
and Zhou (WINE 2024) showed that when each chore has disutility at most 1, a
total subsidy of at most $n/3 - 1/6$ is sufficient to guarantee
proportionality. However, their approach is based on a complex technique, which
does not guarantee economic efficiency - a key desideratum in fair division.
  In this work, we give a polynomial-time algorithm that achieves the same
subsidy bound while also ensuring Pareto-optimality. Moreover, both our
algorithm and its analysis are significantly simpler than those of Wu and Zhou
(WINE 2024). Our approach first computes a proportionally-fair competitive
equilibrium, and then applies a rounding procedure guided by
minimum-pain-per-buck edges.

</details>


### [23] [Improved Maximin Share Guarantee for Additive Valuations](https://arxiv.org/abs/2510.10423)
*Ehsan Heidari,Alireza Kaviani,Masoud Seddighin,AmirMohammad Shahrezaei*

Main category: cs.GT

TL;DR: 本文改进了不可分割物品公平分配中最大最小份额（MMS）的最佳近似保证，从当前已知的3/4 + 3/3836提高到10/13。


<details>
  <summary>Details</summary>
Motivation: 最大最小份额（MMS）是公平分配不可分割物品中最突出的基于份额的公平概念。近年来的研究致力于为不同估值类别（特别是加法估值）的MMS提供更好的近似保证，但目前的最佳结果仍存在较大差距。

Method: N/A

Result: 本研究将MMS的最佳近似保证从3/4 + 3/3836提高到10/13，缩小了理论上的差距。

Conclusion: 通过改进近似保证，本研究在公平分配领域为最大最小份额的公平性提供了更强的理论支持。

Abstract: The maximin share ($\textsf{MMS}$) is the most prominent share-based fairness
notion in the fair allocation of indivisible goods. Recent years have seen
significant efforts to improve the approximation guarantees for $\textsf{MMS}$
for different valuation classes, particularly for additive valuations. For the
additive setting, it has been shown that for some instances, no allocation can
guarantee a factor better than $1-\tfrac{1}{n^4}$ of maximin share value to all
agents. However, the best currently known algorithm achieves an approximation
guarantee of $\tfrac{3}{4} + \tfrac{3}{3836}$ for $\textsf{MMS}$. In this work,
we narrow this gap and improve the best-known approximation guarantee for
$\textsf{MMS}$ to $\tfrac{10}{13}$.

</details>


### [24] [Fair Assignment of Indivisible Chores to Asymmetric Agents](https://arxiv.org/abs/2510.10698)
*Masoud Seddighin,Saeed Seddighin*

Main category: cs.GT

TL;DR: 论文研究了在代理人具有不同权益情况下，如何将不可分割的任务分配给他们，并改进了现有的加权极大极小值（WMMS）保证上限，从对数级别提升到常数级别。


<details>
  <summary>Details</summary>
Motivation: 在对称设置下，对于不可分割的物品和任务，常数级别的MMS分配是可行的。但当代理人具有不同权益时，情况变得复杂。此前的研究表明，对于物品分配，n-WMMS是最好的保证；而对于任务分配，只能保证O(log n)-WMMS的存在。本文旨在改进任务分配的上限。

Method: 本研究通过理论分析，提出了一种改进的方法，证明了在任务分配中，常数-WMMS保证的存在性（如20-WMMS）。

Result: 论文证明了存在一个20-WMMS的任务分配方案，并通过更严格的分析表明，方法还可以进一步优化常数因子。

Conclusion: 研究结果表明，对于具有不同权益的代理人，任务分配的常数-WMMS保证是可行的，这一发现显著改进了之前的对数级别上限。

Abstract: We consider the problem of assigning indivisible chores to agents with
different entitlements in the maximin share value (\MMS) context. While
constant-\MMS\ allocations/assignments are guaranteed to exist for both goods
and chores in the symmetric setting, the situation becomes much more complex
when agents have different entitlements. For the allocation of indivisible
goods, it has been proven that an $n$-\WMMS\ (weighted \MMS) guarantee is the
best one can hope for. For indivisible chores, however, it was recently
discovered that an $O(\log n)$-\WMMS\ assignment is guaranteed to exist. In
this work, we improve this upper bound to a constant-\WMMS\
guarantee.\footnote{We prove the existence of a 20-\WMMS\ assignment, but we
did not attempt to optimize the constant factor. We believe our methods already
yield a slightly better bound with a tighter analysis.}

</details>


### [25] [Achieving Coordination in Non-Cooperative Joint Replenishment Games](https://arxiv.org/abs/2510.10929)
*Junjie Luo,Changjun Wang*

Main category: cs.GT

TL;DR: 该论文分析了无限时间确定性联合补货模型的非合作博弈理论方法，设计了成本分配规则以减少长期平均系统成本，同时证明了纳什均衡的存在性及其效率。


<details>
  <summary>Details</summary>
Motivation: 研究动机在于设计一种成本分配规则，能够在零售商各自独立选择补货间隔以最小化自身成本的情况下，最小化长期平均系统成本。

Method: 研究方法包括引入一类成本分配规则，按零售商预定义权重分配主要启动成本，并建立代理更好响应的单调性属性，证明纳什均衡的存在及其高效计算。

Result: 研究结果表明，某些规则（如基于零售商持有成本率的规则）能实现近最优的价格稳定性（PoS）为1.25，而不依赖私有信息的规则也能获得良好的PoS。

Conclusion: 结论表明，某些成本分配规则不仅能高效计算纳什均衡，还能在不同信息设置下实现良好的系统成本效率。

Abstract: We analyze an infinite-horizon deterministic joint replenishment model from a
non-cooperative game-theoretical approach. In this model, a group of retailers
can choose to jointly place an order, which incurs a major setup cost
independent of the group, and a minor setup cost for each retailer.
Additionally, each retailer is associated with a holding cost. Our objective is
to design cost allocation rules that minimize the long-run average system cost
while accounting for the fact that each retailer independently selects its
replenishment interval to minimize its own cost. We introduce a class of cost
allocation rules that distribute the major setup cost among the associated
retailers in proportion to their predefined weights. For these rules, we
establish a monotonicity property of agent better responses, which enables us
to prove the existence of a payoff dominant pure Nash equilibrium that can also
be computed efficiently. We then analyze the efficiency of these equilibria by
examining the price of stability (PoS), the ratio of the best Nash
equilibrium's system cost to the social optimum, across different information
settings. In particular, our analysis reveals that one rule, which leverages
retailers' own holding cost rates, achieves a near-optimal PoS of 1.25, while
another rule that does not require access to retailers' private information
also yields a favorable PoS.

</details>


### [26] [Likes, Budgets, and Equilibria: Designing Contests for Socially Optimal Advertising](https://arxiv.org/abs/2510.11253)
*Sayantika Mandal,Harman Agrawal,Swaprava Nath*

Main category: cs.GT

TL;DR: 企业在社交网络上广告投放以提升品牌知名度，竞争性博弈中需优化预算分配。研究提出双时间尺度模型，证明在标准条件下最佳响应动态收敛至纯策略纳什均衡，并提供设计案例以实现社会福利最大化。


<details>
  <summary>Details</summary>
Motivation: 企业在社交网络上的竞争性广告投放需要优化预算分配，以最大化品牌知名度。研究旨在通过双时间尺度模型解决这一问题。

Method: 采用双时间尺度模型，其中顶点间通信发生在快速时间尺度，企业策略更新发生在慢速时间尺度。分析最佳响应动态的收敛性。

Result: 在标准条件下，最佳响应动态收敛至纯策略纳什均衡，但可能偏离社会最优。研究提供了设计案例以实现社会福利最大化。

Conclusion: 研究表明，通过合适的竞赛成功函数设计，纳什均衡可以唯一且社会福利最大化，实验验证其在现实场景中表现良好。

Abstract: Firms (businesses, service providers, entertainment organizations, political
parties, etc.) advertise on social networks to draw people's attention and
improve their awareness of the brands of the firms. In all such cases, the
competitive nature of their engagements gives rise to a game where the firms
need to decide how to distribute their budget over the agents on a network to
maximize their brand's awareness. The firms (players) therefore need to
optimize how much budget they should put on the vertices of the network so that
the spread improves via direct (via advertisements or free promotional offers)
and indirect marketing (words-of-mouth). We propose a two-timescale model of
decisions where the communication between the vertices happen in a faster
timescale and the strategy update of the firms happen in a slower timescale. We
show that under fairly standard conditions, the best response dynamics of the
firms converge to a pure strategy Nash equilibrium. However, such equilibria
can be away from a socially optimal one. We provide a characterization of the
contest success functions and provide examples for the designers of such
contests (e.g., regulators, social network providers, etc.) such that the Nash
equilibrium becomes unique and social welfare maximizing. Our experiments show
that for realistic scenarios, such contest success functions perform fairly
well.

</details>


### [27] [Temporal Cooperative Games](https://arxiv.org/abs/2510.11255)
*Ashwin Goyal,Drashthi Doshi,Swaprava Nath*

Main category: cs.GT

TL;DR: 论文引入了时间合作游戏（TCG），其中联盟的价值不仅取决于参与的代理人集合，还取决于他们的到达顺序。提出了一种奖励分配机制，但与传统Shapley值的直接应用存在冲突。


<details>
  <summary>Details</summary>
Motivation: 传统合作游戏理论假设联盟的价值仅取决于参与的代理人集合，而实践中可能还受代理人到达顺序的影响。因此，论文引入了时间合作游戏（TCG）来研究这一问题。

Method: 定义了时间合作游戏（TCG），并提出三个关键性质：最优到达激励（I4OA）、在线个体合理性（OIR）和序列效率（SE）。在此基础上，构建了两种Shapley值的变体：序列世界和扩展世界。

Result: 研究发现，满足I4OA、OIR和SE的奖励分配机制与Shapley值的变体存在根本性冲突，即使在凸和简单TCG等受限类别中亦然。

Conclusion: 结果表明，在代理人按顺序到达的情况下，满足理想时间性质的奖励分配机制必须与Shapley值的变体不同，这为定义TCG中公平和高效的解决方案提出了新问题。

Abstract: Classical cooperative game theory assumes that the worth of a coalition
depends only on the set of agents involved, but in practice, it may also depend
on the order in which agents arrive. Motivated by such scenarios, we introduce
temporal cooperative games (TCG), where the worth $v$ becomes a function of the
sequence of agents $\pi$ rather than just the set $S$. This shift calls for
rethinking the underlying axioms. A key property in this temporal framework is
the incentive for optimal arrival (I4OA), which encourages agents to join in
the order maximizing total worth. Alongside, we define two additional
properties: online individual rationality (OIR), incentivizing earlier agents
to invite more participants, and sequential efficiency (SE), ensuring that the
total worth of any sequence is fully distributed among its agents. We identify
a class of reward-sharing mechanisms uniquely characterized by these three
properties. The classical Shapley value does not directly apply here, so we
construct its natural analogs in two variants: the sequential world, where
rewards are defined for each sequence-player pair, and the extended world,
where rewards are defined for each player alone. Properties of efficiency,
additivity, and null player uniquely determine these Shapley analogs in both
worlds. Importantly, the Shapley analogs are disjoint from mechanisms
satisfying I4OA, OIR, and SE, and this conflict persists even for restricted
classes such as convex and simple TCGs. Our findings thus uncover a fundamental
tension: when players arrive sequentially, reward-sharing mechanisms satisfying
desirable temporal properties must inherently differ from Shapley-inspired
ones, opening new questions for defining fair and efficient solution concepts
in TCGs.

</details>


### [28] [On the Complexity of Stationary Nash Equilibria in Discounted Perfect Information Stochastic Games](https://arxiv.org/abs/2510.11550)
*Kristoffer Arnsfelt Hansen,Xinhao Nie*

Main category: cs.GT

TL;DR: 该论文研究了在完美信息随机游戏中计算稳态纳什均衡的计算复杂度问题，对两人游戏证明了其PPAD完备性，并提供了更简单的PPAD-hard证明；对于三人游戏，展示了有理值稳态纳什均衡不一定存在，并证明了四人游戏中计算稳态纳什均衡的SqrtSum-hard性。


<details>
  <summary>Details</summary>
Motivation: 研究在完美信息随机游戏中计算稳态纳什均衡的计算复杂度，以解决多人博弈中的均衡计算问题。

Method: 通过理论证明和构造性反例，分析两人游戏和多人游戏中稳态纳什均衡的计算复杂度。

Result: 两人游戏的稳态纳什均衡计算问题被证明为PPAD完备；三人游戏中可能出现有理值稳态纳什均衡不存在的情况；四人游戏的该问题为SqrtSum-hard。

Conclusion: 论文精确分类了两人游戏的稳态纳什均衡计算问题，并揭示了多人游戏中均衡计算的更高复杂度。

Abstract: We study the problem of computing stationary Nash equilibria in discounted
perfect information stochastic games from the viewpoint of computational
complexity. For two-player games we prove the problem to be in PPAD, which
together with a previous PPAD-hardness result precisely classifies the problem
as PPAD-complete. In addition to this we give an improved and simpler
PPAD-hardness proof for computing a stationary epsilon-Nash equilibrium. For
3-player games we construct games showing that rational-valued stationary Nash
equilibria are not guaranteed to exist, and we use these to prove
SqrtSum-hardness of computing a stationary Nash equilibrium in 4-player games.

</details>


### [29] [Multiwinner Voting with Interval Preferences under Incomplete Information](https://arxiv.org/abs/2510.11625)
*Drew Springham,Edith Elkind,Bart de Keijzer,Maria Polukarov*

Main category: cs.GT

TL;DR: 在多赢家批准选举中，选民可能难以确定对所有候选人的偏好。本文提出了一种概率偏好模型，展示了如何通过减少通信量来实现公平性保证。


<details>
  <summary>Details</summary>
Motivation: 在多赢家批准选举中，选民可能因候选人数量过多而难以表达完整偏好。因此，研究在减少通信量的情况下仍能提供公平性保证的方法具有重要意义。

Method: 本文采用了一种基于一维偏好的概率模型，选民被分为若干组，每组关联一个实数区间上的分布。提出了一种算法，通过查询选民的偏好来计算满足PJR+条件的委员会。

Result: 算法在期望情况下，每选民仅需进行$ackslash$mathcal{O}(ackslash log( ackslash sigmaackslash cdot k))$次查询，其中$ackslash sigma$为组数，$k$为委员会规模。

Conclusion: 研究表明，在减少通信量的情况下，仍能通过合理的算法设计实现公平性保证，特别是在一维偏好模型中。

Abstract: In multiwinner approval elections with many candidates, voters may struggle
to determine their preferences over the entire slate of candidates. It is
therefore of interest to explore which (if any) fairness guarantees can be
provided under reduced communication. In this paper, we consider voters with
one-dimensional preferences: voters and candidates are associated with points
in $\mathbb R$, and each voter's approval set forms an interval of $\mathbb R$.
We put forward a probabilistic preference model, where the voter set consists
of $\sigma$ different groups; each group is associated with a distribution over
an interval of $\mathbb R$, so that each voter draws the endpoints of her
approval interval from the distribution associated with her group. We present
an algorithm for computing committees that provide Proportional Justified
Representation + (PJR+), which proceeds by querying voters' preferences, and
show that, in expectation, it makes $\mathcal{O}(\log( \sigma\cdot k))$ queries
per voter, where $k$ is the desired committee size.

</details>
