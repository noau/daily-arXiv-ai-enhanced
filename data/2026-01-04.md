<div id=toc></div>

# Table of Contents

- [cs.GR](#cs.GR) [Total: 4]
- [cs.PL](#cs.PL) [Total: 4]
- [cs.GT](#cs.GT) [Total: 3]


<div id='cs.GR'></div>

# cs.GR [[Back]](#toc)

### [1] [PartMotionEdit: Fine-Grained Text-Driven 3D Human Motion Editing via Part-Level Modulation](https://arxiv.org/abs/2512.24200)
*Yujie Yang,Zhichao Zhang,Jiazhou Chen,Zichao Wu*

Main category: cs.GR

TL;DR: 提出了PartMotionEdit，一种细粒度运动编辑框架，通过部分级语义调制实现精确控制。


<details>
  <summary>Details</summary>
Motivation: 现有的基于文本的3D人体运动编辑方法难以精确控制局部运动，因此需要一种更精细的编辑方法。

Method: 提出Part-aware Motion Modulation (PMM)模块和Bidirectional Motion Interaction (BMI)模块，结合部分级相似性曲线监督机制和双向跨模态注意力。

Result: 在知名基准测试中，PartMotionEdit在定量和定性评估中均优于现有方法。

Conclusion: PartMotionEdit通过部分级语义调制和双向交互机制实现了更精确和可解释的运动编辑。

Abstract: Existing text-driven 3D human motion editing methods have demonstrated significant progress, but are still difficult to precisely control over detailed, part-specific motions due to their global modeling nature. In this paper, we propose PartMotionEdit, a novel fine-grained motion editing framework that operates via part-level semantic modulation. The core of PartMotionEdit is a Part-aware Motion Modulation (PMM) module, which builds upon a predefined five-part body decomposition. PMM dynamically predicts time-varying modulation weights for each body part, enabling precise and interpretable editing of local motions. To guide the training of PMM, we also introduce a part-level similarity curve supervision mechanism enhanced with dual-layer normalization. This mechanism assists PMM in learning semantically consistent and editable distributions across all body parts. Furthermore, we design a Bidirectional Motion Interaction (BMI) module. It leverages bidirectional cross-modal attention to achieve more accurate semantic alignment between textual instructions and motion semantics. Extensive quantitative and qualitative evaluations on a well-known benchmark demonstrate that PartMotionEdit outperforms the state-of-the-art methods.

</details>


### [2] [BATISNet: Instance Segmentation of Tooth Point Clouds with Boundary Awareness](https://arxiv.org/abs/2512.24201)
*Yating Cai,Yanghui Xu,Zehua Hu,Jiazhou Chen,Jing Huang*

Main category: cs.GR

TL;DR: 该论文提出了一种称为BATISNet的边界感知实例网络，用于牙齿点云分割，旨在解决传统语义分割方法在处理复杂牙齿情况时的不足，并通过实验验证其优越性。


<details>
  <summary>Details</summary>
Motivation: 由于牙齿紧密排列、边界模糊以及复杂病例（如缺牙、错位牙）的多样性，现有语义分割方法在处理复杂牙科病例时效果不佳。因此，论文提出了一种新的网络模型，以提升牙齿实例分割的鲁棒性和准确性。

Method: 论文提出的BATISNet包括一个特征提取骨干网络和一个实例分割模块。该模型不仅提取不同类型牙齿的语义特征，还学习单个牙齿的实例特征。此外，论文设计了一种边界感知损失函数，专门监督实例间的边界分割。

Result: 实验结果表明，BATISNet在牙齿完整性分割方面优于现有方法，尤其是在处理缺牙和错位牙等复杂临床场景时，提供了更可靠和详细的数据支持。

Conclusion: BATISNet通过结合语义和实例特征，并引入边界感知损失函数，有效缓解了牙齿粘连和边界模糊问题，为实际临床应用提供了更精准的分割结果。

Abstract: Accurate segmentation of the tooth point cloud is of great significance for diagnosis clinical assisting and treatment planning. Existing methods mostly employ semantic segmentation, focusing on the semantic feature between different types of teeth. However, due to the tightly packed structure of teeth, unclear boundaries, and the diversity of complex cases such as missing teeth, malposed teeth, semantic segmentation often struggles to achieve satisfactory results when dealing with complex dental cases. To address these issues, this paper propose BATISNet, a boundary-aware instance network for tooth point cloud segmentation. This network model consists of a feature extraction backbone and an instance segmentation module. It not only focuses on extracting the semantic features of different types of teeth but also learns the instance features of individual teeth. It helps achieve more robust and accurate tooth instance segmentation in complex clinical scenarios such as missing teeth and malposed teeth. Additionally, to further enhance the completeness and accuracy of tooth boundary segmentation, a boundary-aware loss function is designed to specifically supervise the boundary segmentation between instances. It mitigates effectively tooth adhesion and boundary ambiguity issues. Extensive experimental results show that BATISNet outperforms existing methods in tooth integrity segmentation, providing more reliable and detailed data support for practical clinical applications.

</details>


### [3] [The Uncanny Valley in medical simulation-based training: a visual summary](https://arxiv.org/abs/2512.24240)
*Eleni Grigoriou,Manos Kamarianakis,George Papagiannakis*

Main category: cs.GR

TL;DR: 这篇综述文章旨在提供一个基于证据的视觉指南，探讨"恐怖谷"（UV）现象如何深刻影响医学虚拟现实模拟训练的效果。


<details>
  <summary>Details</summary>
Motivation: 理解和解决"恐怖谷"现象对于医学训练至关重要，因为真实感和沉浸感是有效学习的关键。

Method: 研究团队由计算机图形学、虚拟现实和医学教育领域的专家组成，采用跨学科和多机构合作的方法进行研究。

Result: 研究团队通过开发和创新VR技术在医学训练中的应用，推动了该领域的边界。

Conclusion: "恐怖谷"现象对医学虚拟现实模拟训练的影响是多方面的，需要跨学科合作来优化VR应用的沉浸感和学习效果。

Abstract: The purpose of this review article is to provide a bibliographical as well as evidence-based visual guide regarding the effect of ``Uncanny Valley'' (UV) and how it profoundly influences medical virtual reality simulation-based training. The phenomenon, where increasingly realistic virtual humans elicit discomfort due to subtle imperfections, is crucial to understand and address in the context of medical training, where realism and immersion are key to effective learning.
  Our research team, consisting of experts in computer graphics, virtual reality, and medical education, brings a diverse and multidisciplinary perspective to this subject. Our collective experience spans developing advanced computer graphics systems, VR character simulation, and innovative educational technologies. We have collaborated across institutions and industries to push the boundaries of VR applications in medical training.

</details>


### [4] [PhysTalk: Language-driven Real-time Physics in 3D Gaussian Scenes](https://arxiv.org/abs/2512.24986)
*Luca Collorone,Mert Kiray,Indro Spinelli,Fabio Galasso,Benjamin Busam*

Main category: cs.GR

TL;DR: PhysTalk利用3D Gaussian Splatting（3DGS）和大型语言模型（LLM）实现开放的文本驱动的4D物理动画，无需训练且计算轻量。


<details>
  <summary>Details</summary>
Motivation: 传统的视觉模拟需要大量时间和专业知识，而现有的文本驱动解决方案缺乏物理真实性和高效的语言接口。PhysTalk旨在解决这些问题，提供一个实时、交互式的物理动画生成框架。

Method: PhysTalk采用3DGS作为输入，通过LLM生成可直接修改3DGS参数的代码，并结合轻量级代理和粒子动力学实现物理模拟。这是首个无需耗时网格提取即可直接耦合3DGS和物理模拟器的框架。

Result: PhysTalk实现了基于物理的交互式4D动画，支持多材质对象的碰撞感知操纵，且无需训练和复杂的计算资源，大大提升了动画生成的效率。

Conclusion: PhysTalk为开放式词汇驱动的物理动画提供了一种高效、轻量的解决方案，推动了传统的“渲染等待”模式向交互式物理模拟的转变。

Abstract: Realistic visual simulations are omnipresent, yet their creation requires computing time, rendering, and expert animation knowledge. Open-vocabulary visual effects generation from text inputs emerges as a promising solution that can unlock immense creative potential. However, current pipelines lack both physical realism and effective language interfaces, requiring slow offline optimization. In contrast, PhysTalk takes a 3D Gaussian Splatting (3DGS) scene as input and translates arbitrary user prompts into real time, physics based, interactive 4D animations. A large language model (LLM) generates executable code that directly modifies 3DGS parameters through lightweight proxies and particle dynamics. Notably, PhysTalk is the first framework to couple 3DGS directly with a physics simulator without relying on time consuming mesh extraction. While remaining open vocabulary, this design enables interactive 3D Gaussian animation via collision aware, physics based manipulation of arbitrary, multi material objects. Finally, PhysTalk is train-free and computationally lightweight: this makes 4D animation broadly accessible and shifts these workflows from a "render and wait" paradigm toward an interactive dialogue with a modern, physics-informed pipeline.

</details>


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [5] [Enforcing Temporal Constraints for LLM Agents](https://arxiv.org/abs/2512.23738)
*Adharsh Kamath,Sishen Zhang,Calvin Xu,Shubham Ugare,Gagandeep Singh,Sasa Misailovic*

Main category: cs.PL

TL;DR: Agent-C是一个创新框架，通过运行时保证确保LLM代理遵守形式化的时间安全策略，实现了100%的安全性和更高的任务效用。


<details>
  <summary>Details</summary>
Motivation: 当前的安全护栏系统无法防止LLM代理违反时间安全策略（如未经认证访问敏感数据），这需要针对动作序列而非单一动作的推理。

Method: Agent-C提出了一种领域特定语言来表达时间属性，将规范转化为一阶逻辑，并使用SMT求解在令牌生成期间检测不合规动作，同时利用约束生成技术确保生成的每个动作都符合规范。

Result: 在零售客户服务和机票预订系统的评估中，Agent-C实现了100%的安全性和更高的任务效用（如Claude Sonnet 4.5和GPT-5的合规性分别从77.4%和83.7%提升至100%）。

Conclusion: Agent-C为可靠的代理推理设定了新的技术前沿，同时提升了安全性和实用性。

Abstract: LLM-based agents are deployed in safety-critical applications, yet current guardrail systems fail to prevent violations of temporal safety policies, requirements that govern the ordering and sequencing of agent actions. For instance, agents may access sensitive data before authenticating users or process refunds to unauthorized payment methods, violations that require reasoning about sequences of action rather than an individual action. Existing guardrails rely on imprecise natural language instructions or post-hoc monitoring, and provide no formal guarantees that agents will satisfy temporal constraints. We present Agent-C, a novel framework that provides run-time guarantees ensuring LLM agents adhere to formal temporal safety properties. Agent-C introduces a domain-specific language for expressing temporal properties (e.g., authenticate before accessing data), translates specifications to first-order logic, and uses SMT solving to detect non-compliant agent actions during token generation. When the LLM attempts to generate a non-compliant tool call, Agent-C leverages constrained generation techniques to ensure that every action generated by the LLM complies with the specification, and to generate a compliant alternative to a non-compliant agent action. We evaluate Agent-C across two real-world applications: retail customer service and airline ticket reservation system, and multiple language models (open and closed-source). Our results demonstrate that Agent-C achieves perfect safety (100% conformance, 0% harm), while improving task utility compared to state-of-the-art guardrails and unrestricted agents. On SoTA closed-source models, Agent-C improves conformance (77.4% to 100% for Claude Sonnet 4.5 and 83.7% to 100% for GPT-5), while simultaneously increasing utility (71.8% to 75.2% and 66.1% to 70.6%, respectively), representing a new SoTA frontier for reliable agentic reasoning.

</details>


### [6] [Towards representation agnostic probabilistic programming](https://arxiv.org/abs/2512.23740)
*Ole Fenske,Maximilian Popko,Sebastian Bader,Thomas Kirste*

Main category: cs.PL

TL;DR: 这篇论文引入了一个因子抽象框架，通过五个基本操作作为通用接口，实现了表示无关的概率编程，解决了现有工具因为紧耦合模型表示与特定推理算法而无法支持混合离散-连续模型的问题。


<details>
  <summary>Details</summary>
Motivation: 当前的概率编程语言和工具将模型表示与特定推理算法紧密耦合，限制了新表示形式的实验以及对混合离散-连续模型的支持。

Method: 论文提出了一种因子抽象，包含五个基本操作，作为独立于底层表示的通用接口。

Result: 该方法实现了表示无关的概率编程，允许用户在一个统一的框架中自由混合不同表示（如离散表、高斯分布、基于样本的方法），从而在复杂混合模型中实现实际推理。

Conclusion: 因子抽象为概率编程提供了一个灵活的解决方案，克服了现有工具无法充分表达复杂混合模型的限制。

Abstract: Current probabilistic programming languages and tools tightly couple model representations with specific inference algorithms, preventing experimentation with novel representations or mixed discrete-continuous models. We introduce a factor abstraction with five fundamental operations that serve as a universal interface for manipulating factors regardless of their underlying representation. This enables representation-agnostic probabilistic programming where users can freely mix different representations (e.g. discrete tables, Gaussians distributions, sample-based approaches) within a single unified framework, allowing practical inference in complex hybrid models that current toolkits cannot adequately express.

</details>


### [7] [VGC: A High-Performance Zone-Based Garbage Collector Architecture for Python with Partitioning and Parallel Execution](https://arxiv.org/abs/2512.23768)
*Abdulla M*

Main category: cs.PL

TL;DR: VGC是一种新型内存管理框架，通过双层架构（Active VGC和Passive VGC）优化性能，适用于从嵌入式设备到高性能并行架构的多样化系统。


<details>
  <summary>Details</summary>
Motivation: 传统垃圾回收器在高性能并行架构和资源受限设备中表现不足，VGC旨在通过编译时和运行时优化解决这一问题。

Method: VGC采用双层架构：Active VGC动态管理运行时对象，使用并发标记清除策略；Passive VGC在编译时优化静态对象分配，减少内存碎片。

Result: 在并行测试中，Active VGC减少暂停时间30%；Passive VGC减少总内存使用25%，并提升现代并行应用的可扩展性。

Conclusion: VGC通过结合编译时和运行时优化，为内存密集型系统提供了高效且适应性强的解决方案。

Abstract: The Virtual Garbage Collector (VGC) introduces a novel memory management framework designed to optimize performance across diverse systems, ranging from resource constrained embedded devices to high performance parallel architectures. Unlike conventional garbage collectors, VGC employs a dual layer architecture consisting of Active VGC and Passive VGC to enable efficient, low overhead memory management. Active VGC dynamically manages runtime objects using a concurrent mark and sweep strategy tailored for parallel workloads, reducing pause times by up to 30 percent compared to generational collectors in multithreaded benchmarks. Passive VGC operates at compile time and optimizes static object allocation through predictive memory mapping, minimizing fragmentation by aligning objects to cache boundaries. This separation of responsibilities ensures predictable memory access patterns, reduces total memory usage by up to 25 percent, and improves scalability for modern parallel applications. By integrating compile time and runtime optimizations, VGC provides a robust and adaptable solution for memory intensive systems across both low level and high level programming environments.

</details>


### [8] [State Space Estimation for DPOR-based Model Checkers](https://arxiv.org/abs/2512.23996)
*A. R. Balasubramanian,Mohammad Hossein Khoshechin Jorshari,Rupak Majumdar,Umang Mathur,Minjian Zhang*

Main category: cs.PL

TL;DR: 本文研究并发程序的估计问题，提出了一种基于蒙特卡洛方法的无偏估计器，用于计算Mazurkiewicz迹等价类的数量，解决了模型检查中资源分配的实际问题。


<details>
  <summary>Details</summary>
Motivation: 该研究的动机是为并发程序的Mazurkiewicz迹等价类数量提供一个高效的估计方法，以指导模型检查的运行时间和搜索空间覆盖率的评估。

Method: 作者首先证明了计数问题的复杂性，并提出了一种蒙特卡洛方法，将无状态最优DPOR算法转化为无偏估计器，通过随机枚举控制方差。

Result: 实验表明，该方法在JMC模型检查器中实现后，即使在状态空间包含10^5-10^6类时，也能在数百次试验内稳定估计，误差通常在20%以内。

Conclusion: 本文提供了首个可证明的多项式时间无偏估计器，解决了模型检查资源分配中的关键问题，为实际应用提供了有效工具。

Abstract: We study the estimation problem for concurrent programs: given a bounded program $P$, estimate the number of Mazurkiewicz trace-equivalence classes induced by its interleavings. This quantity informs two practical questions for enumeration-based model checking: how long a model checking run is likely to take, and what fraction of the search space has been covered so far. We first show the counting problem is #P-hard even for restricted programs and, unless $P=NP$, inapproximable within any subexponential factor, ruling out efficient exact or randomized approximation algorithms. We give a Monte Carlo approach to obtain a poly-time unbiased estimator: we convert a stateless optimal DPOR algorithm into an unbiased estimator by viewing its exploration as a bounded-depth, bounded-width tree whose leaves are the maximal Mazurkiewicz traces. A classical estimator by Knuth, when run on this tree, yields an unbiased estimate. To control the variance, we apply stochastic enumeration by maintaining a small population of partial paths per depth whose evolution is coupled. We have implemented our estimator in the JMC model checker and evaluated it on shared-memory benchmarks. With modest budgets, our estimator yields stable estimates, typically within a 20% band, within a few hundred trials, even when the state space has $10^5$--$10^6$ classes. We also show how the same machinery estimates model-checking cost by weighting all explored graphs, not only complete traces. Our algorithms provide the first provable poly-time unbiased estimators for counting traces, a problem of considerable importance when allocating model checking resources.

</details>


<div id='cs.GT'></div>

# cs.GT [[Back]](#toc)

### [9] [Personalized Promotions in Practice: Dynamic Allocation and Reference Effects](https://arxiv.org/abs/2512.23781)
*Jackie Baek,Will Ma,Dmitry Mitrofanov*

Main category: cs.GT

TL;DR: 该论文提出了一种高效的个性化促销策略，用于每日为超过2000万顾客提供定制促销（10%至20%折扣），并在实际部署中实现了4.5%的收入增长。此外，论文还构建了一个基于顾客记忆效应的理论模型，解释了最优促销策略的周期性结构。


<details>
  <summary>Details</summary>
Motivation: 在线零售商面临每日为大规模用户群（超过2000万顾客）提供个性化促销的挑战，同时需要满足全局分配约束并最大化收入。

Method: 论文提出了一种高效的促销分配策略，并通过A/B测试验证其效果。此外，还构建了一个组合模型，假设顾客会记住过去$\ell$天内看到的最佳促销作为"参考值"，并基于此模型分析了最优策略的结构。

Result: 在实际部署中，该策略实现了4.5%的收入增长。理论分析表明，最优策略会在$\ell$次提供较差促销值和一次提供较好促销值之间循环。

Conclusion: 论文证明了所提策略在实践中的有效性，并通过理论模型揭示了最优促销策略的周期性特征。

Abstract: Partnering with a large online retailer, we consider the problem of sending daily personalized promotions to a userbase of over 20 million customers. We propose an efficient policy for determining, every day, the promotion that each customer should receive (10%, 12%, 15%, 17%, or 20% off), while respecting global allocation constraints. This policy was successfully deployed to see a 4.5% revenue increase during an A/B test, by better targeting promotion-sensitive customers and also learning intertemporal patterns across customers.
  We also consider theoretically modeling the intertemporal state of the customer. The data suggests a simple new combinatorial model of pricing with reference effects, where the customer remembers the best promotion they saw over the past $\ell$ days as the "reference value", and is more likely to purchase if this value is poor. We tightly characterize the structure of optimal policies for maximizing long-run average revenue under this model -- they cycle between offering poor promotion values $\ell$ times and offering good values once.

</details>


### [10] [Multilevel Fair Allocation](https://arxiv.org/abs/2512.24105)
*Maxime Lucet,Nawal Benabbou,Aurélie Beynier,Nicolas Maudet*

Main category: cs.GT

TL;DR: 本文提出了在多级树状层次关系下公平分配资源的概念，并设计了两种算法以解决公平性和效率问题。


<details>
  <summary>Details</summary>
Motivation: 在多级层次结构中，如何设计既能保持公平性又能高效分配资源的算法是一个主要挑战。

Method: 本文提出了两种算法，一种基于多项式时间顺序的通用算法，另一种扩展了Yankee Swap方法，以在多级公平分配问题中保持公平与效率。

Result: 两种算法在多级公平分配问题中表现出良好的公平性和效率，特别是Yankee Swap扩展在实践中表现出色。

Conclusion: 多级公平分配问题可以通过提出的算法有效解决，兼具理论保证和实践优势。

Abstract: We introduce the concept of multilevel fair allocation of resources with tree-structured hierarchical relations among agents. While at each level it is possible to consider the problem locally as an allocation of an agent to its children, the multilevel allocation can be seen as a trace capturing the fact that the process is iterated until the leaves of the tree. In principle, each intermediary node may have its own local allocation mechanism. The main challenge is then to design algorithms which can retain good fairness and efficiency properties. In this paper we propose two original algorithms under the assumption that leaves of the tree have matroid-rank utility functions and the utility of any internal node is the sum of the utilities of its children. The first one is a generic polynomial-time sequential algorithm that comes with theoretical guarantees in terms of efficiency and fairness. It operates in a top-down fashion -- as commonly observed in real-world applications -- and is compatible with various local algorithms. The second one extends the recently proposed General Yankee Swap to the multilevel setting. This extension comes with efficiency guarantees only, but we show that it preserves excellent fairness properties in practice.

</details>


### [11] [On the Difficulty of Measuring Divisiveness of Proposals under Ranked Preferences](https://arxiv.org/abs/2512.24467)
*Ulle Endriss*

Main category: cs.GT

TL;DR: 本文探讨了如何在在线参与式平台中选择最具争议性的公共政策提案，并通过社会选择理论的公理化方法揭示了满足某些规范性要求的困难。


<details>
  <summary>Details</summary>
Motivation: 设计数字民主平台时，识别最具争议性的提案有助于分析和引导讨论，但缺乏明确的定义和方法。

Method: 采用社会选择理论的公理化方法，探索选择最具争议性提案的定义及其规范性要求。

Result: 研究发现，满足看似温和的规范性要求的选择最具争议性提案的任务面临根本性困难。

Conclusion: 研究指出了在选择最具争议性提案时面临的挑战，为未来数字民主平台的设计提供了理论参考。

Abstract: Given the stated preferences of several people over a number of proposals regarding public policy initiatives, some of those proposals might be judged to be more ``divisive'' than others. When designing online participatory platforms to support digital democracy initiatives enabling citizens to deliberate over such proposals, we might wish to equip those platforms with the functionality to retrieve the most divisive proposals currently under discussion. Such a service would be useful for analysing the progress of deliberation and steering discussion towards issues that still require further debate. Guided by this use case, we explore possibilities for providing a clear definition of what it means to select a set of most divisive proposals on the basis of people's stated preferences over proposals. Then, employing the axiomatic method familiar from social choice theory, we show that the task of selecting the most divisive proposals in a manner that satisfies certain seemingly mild normative requirements faces a number of fundamental difficulties.

</details>
