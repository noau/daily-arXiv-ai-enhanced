{"id": "2601.19207", "categories": ["cs.PL"], "pdf": "https://arxiv.org/pdf/2601.19207", "abs": "https://arxiv.org/abs/2601.19207", "authors": ["Matthew Britton", "Sasha Pak", "Alex Potanin"], "title": "Refactoring and Equivalence in Rust: Expanding the REM Toolchain with a Novel Approach to Automated Equivalence Proofs", "comment": null, "summary": "Refactoring tools are central to modern development, with extract-function refactorings used heavily in day-to-day work. For Rust, however, ownership, borrowing, and advanced type features make automated extract-function refactoring challenging. Existing tools either rely on slow compiler-based analysis, support only restricted language fragments, or provide little assurance beyond \"it still compiles.\" This paper presents REM2.0, a new extract-function and verification toolchain for Rust. REM2.0 works atop rust-analyzer as a persistent daemon, providing low-latency refactorings with a VSCode front-end. It adds a repairer that automatically adjusts lifetimes and signatures when extraction exposes borrow-checker issues, and an optional verification pipeline connecting to CHARON and AENEAS to generate Coq equivalence proofs for a supported Rust subset. The architecture is evaluated on three benchmark suites. On the original REM artefact, REM2.0 achieves 100% compatibility while reducing latency from ~1000ms to single-digit milliseconds in the daemon. On 40 feature-focused extractions from 20 highly starred GitHub repositories, REM2.0 handles most examples involving async/await, const fn, non-local control flow, generics, and higher-ranked trait bounds. On twenty verification benchmarks, the CHARON/AENEAS pipeline constructs end-to-end equivalence proofs for cases within its current subset. Overall, results show that a rust-analyzer-based design can provide fast, feature-rich extract-function refactoring for real Rust programs, while opt-in verification delivers machine-checked behaviour preservation."}
{"id": "2601.19426", "categories": ["cs.PL", "cs.LO", "math.CT"], "pdf": "https://arxiv.org/pdf/2601.19426", "abs": "https://arxiv.org/abs/2601.19426", "authors": ["Samy Avrillon", "Ambrus Kaposi", "Ambroise Lafont", "Niyousha Najmaei", "Johann Rosain"], "title": "For Generalised Algebraic Theories, Two Sorts Are Enough", "comment": null, "summary": "Generalised algebraic theories (GATs) allow multiple sorts indexed over each other. For example, the theories of categories or Martin-L{ö}f type theories form GATs. Categories have two sorts, objects and morphisms, and the latter are double-indexed over the former. Martin-L{ö}f type theory has four sorts: contexts, substitutions, types and terms. For example, types are indexed over contexts, and terms are indexed over both contexts and types. In this paper we show that any GAT can be reduced to a GAT with only two sorts, and there is a section-retraction correspondence (formally, a strict coreflection) between models of the original and the reduced GAT. In particular, any model of the original GAT can be turned into a model of the reduced (two-sorted) GAT and back, and this roundtrip is the identity.\n  The reduced GAT is simpler than the original GAT in the following aspects: it does not have sort equalities; it does not have interleaved sorts and operations; if the original GAT did not have interleaved sorts and operations, then the reduced GAT won't have operations interleaved between different sorts. In a type-theoretic metatheory, the initial algebra of a GAT is called a quotient inductive-inductive type (QIIT). Our reduction provides a way to implement QIITs with sort equalities or interleaved constructors which are not allowed by Cubical Agda. An instance of our reduction is the well-known method of reducing mutual inductive types to a single indexed family. Our approach is semantic in that it does not rely on a syntactic description of GATs, but instead, on Uemura's bi-initial characterisation of the category of (finite) GATs in the 2-category of finitely complete categories with a chosen exponentiable morphism."}
{"id": "2601.18813", "categories": ["cs.GT"], "pdf": "https://arxiv.org/pdf/2601.18813", "abs": "https://arxiv.org/abs/2601.18813", "authors": ["Matthias Gehnen", "Julius Stannat"], "title": "Fog of War Chess", "comment": "21 pages, 13 figures", "summary": "Fog of War chess is a popular variant of classical chess, in which both players have only partial information about the position of the opponent's pieces. This study provides the first theoretical analysis of endgames in Fog of War chess. In particular, we analyze the setups king and queen versus king, king and rook versus king, and king and two rooks versus king. We show that a king and queen can always guarantee a win against a lone king. In contrast to classical chess, a king and a rook cannot guarantee a win against a lone king. However, adding one more rook guarantees a win."}
{"id": "2601.19036", "categories": ["cs.GR"], "pdf": "https://arxiv.org/pdf/2601.19036", "abs": "https://arxiv.org/abs/2601.19036", "authors": ["Tianxin Tao", "Han Liu", "Hung Yu Ling"], "title": "The Last Mile to Production Readiness: Physics-Based Motion Refinement for Video-Based Capture", "comment": null, "summary": "High-quality motion data underpins games, film, XR, and robotics. Vision-based motion capture tools have made significant progress, offering accessible and visually convincing results, yet often fall short in the final stretch -- the last mile -- when it comes to physical realism and production readiness, due to various artifacts introduced during capture. In this paper, we summarize key issues through case studies and feedback from professional animators to set a stepping stone for future research in motion cleanup. We then present a physics-based motion refinement framework to bridge the gap, with the goal of reducing labor-intensive manual cleanup and enhancing visual quality and physical realism. Our framework supports both single- and multi-character sequences and can be integrated into animator workflows for further refinement, such as stylizing motions via keyframe editing."}
{"id": "2601.18824", "categories": ["cs.GT", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.18824", "abs": "https://arxiv.org/abs/2601.18824", "authors": ["Zhiyu An", "Duaa Nakshbandi", "Wan Du"], "title": "Differential Voting: Loss Functions For Axiomatically Diverse Aggregation of Heterogeneous Preferences", "comment": null, "summary": "Reinforcement learning from human feedback (RLHF) implicitly aggregates heterogeneous human preferences into a single utility function, even though the underlying utilities of the participants are in practice diverse. Hence, RLHF can be viewed as a form of voting, where the aggregation mechanism is defined by the loss function. Although Arrow's Impossibility Theorem suggests that different mechanisms satisfy different sets of desirable axioms, most existing methods rely on a single aggregation principle, typically the Bradley-Terry-Luce (BTL) model, which corresponds to Borda count voting. This restricts the axiomatic properties of the learned reward and obscures the normative assumptions embedded in optimization. In this work, we introduce Differential Voting, a unifying framework that constructs instance-wise, differentiable loss functions whose population-level optima provably correspond to distinct classical voting rules. We develop differentiable surrogates for majority-based aggregation (BTL), Copeland, and Kemeny rules, and formally analyze their calibration properties, gradient fields, and limiting behavior as smoothing parameters vanish. For each loss, we establish consistency with the corresponding social choice rule and characterize the axioms it satisfies or violates. Our analysis shows how design choices in loss geometry-such as margin sensitivity and boundary concentration-directly translate into normative aggregation behavior. Differential Voting makes preference aggregation an explicit and controllable design choice in RLHF, enabling principled trade-offs between axiomatic guarantees and optimization stability. Code to reproduce our experiments is open-sourced."}
{"id": "2601.19233", "categories": ["cs.GR"], "pdf": "https://arxiv.org/pdf/2601.19233", "abs": "https://arxiv.org/abs/2601.19233", "authors": ["Zeyu Xiao", "Mingyang Sun", "Yimin Cong", "Lintao Wang", "Dongliang Kou", "Zhenyi Wu", "Dingkang Yang", "Peng Zhai", "Zeyu Wang", "Lihua Zhang"], "title": "UniMGS: Unifying Mesh and 3D Gaussian Splatting with Single-Pass Rasterization and Proxy-Based Deformation", "comment": "conference", "summary": "Joint rendering and deformation of mesh and 3D Gaussian Splatting (3DGS) have significant value as both representa tions offer complementary advantages for graphics applica tions. However, due to differences in representation and ren dering pipelines, existing studies render meshes and 3DGS separately, making it difficult to accurately handle occlusions and transparency. Moreover, the deformed 3DGS still suffers from visual artifacts due to the sensitivity to the topology quality of the proxy mesh. These issues pose serious obsta cles to the joint use of 3DGS and meshes, making it diffi cult to adapt 3DGS to conventional mesh-oriented graphics pipelines. We propose UniMGS, the first unified framework for rasterizing mesh and 3DGS in a single-pass anti-aliased manner, with a novel binding strategy for 3DGS deformation based on proxy mesh. Our key insight is to blend the col ors of both triangle and Gaussian fragments by anti-aliased α-blending in a single pass, achieving visually coherent re sults with precise handling of occlusion and transparency. To improve the visual appearance of the deformed 3DGS, our Gaussian-centric binding strategy employs a proxy mesh and spatially associates Gaussians with the mesh faces, signifi cantly reducing rendering artifacts. With these two compo nents, UniMGS enables the visualization and manipulation of 3D objects represented by mesh or 3DGS within a unified framework, opening up new possibilities in embodied AI, vir tual reality, and gaming. We will release our source code to facilitate future research."}
{"id": "2601.19435", "categories": ["cs.GT", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2601.19435", "abs": "https://arxiv.org/abs/2601.19435", "authors": ["Shengwei Xu", "Zhaohua Chen", "Xiaotie Deng", "Zhiyi Huang", "Grant Schoenebeck"], "title": "Ad Insertion in LLM-Generated Responses", "comment": "31 pages, 8 figures", "summary": "Sustainable monetization of Large Language Models (LLMs) remains a critical open challenge. Traditional search advertising, which relies on static keywords, fails to capture the fleeting, context-dependent user intents--the specific information, goods, or services a user seeks--embedded in conversational flows. Beyond the standard goal of social welfare maximization, effective LLM advertising imposes additional requirements on contextual coherence (ensuring ads align semantically with transient user intents) and computational efficiency (avoiding user interaction latency), as well as adherence to ethical and regulatory standards, including preserving privacy and ensuring explicit ad disclosure. Although various recent solutions have explored bidding on token-level and query-level, both categories of approaches generally fail to holistically satisfy this multifaceted set of constraints.\n  We propose a practical framework that resolves these tensions through two decoupling strategies. First, we decouple ad insertion from response generation to ensure safety and explicit disclosure. Second, we decouple bidding from specific user queries by using ``genres'' (high-level semantic clusters) as a proxy. This allows advertisers to bid on stable categories rather than sensitive real-time response, reducing computational burden and privacy risks. We demonstrate that applying the VCG auction mechanism to this genre-based framework yields approximately dominant strategy incentive compatibility (DSIC) and individual rationality (IR), as well as approximately optimal social welfare, while maintaining high computational efficiency. Finally, we introduce an \"LLM-as-a-Judge\" metric to estimate contextual coherence. Our experiments show that this metric correlates strongly with human ratings (Spearman's $ρ\\approx 0.66$), outperforming 80% of individual human evaluators."}
{"id": "2601.19294", "categories": ["cs.GR", "cs.HC"], "pdf": "https://arxiv.org/pdf/2601.19294", "abs": "https://arxiv.org/abs/2601.19294", "authors": ["Joffrey Guilmet", "Suzanne Sorli", "Diego Vilela Monteiro"], "title": "Words have Weight: Comparing the use of pressure and weight as a metaphor in a User Interface in Virtual Reality", "comment": null, "summary": "This work investigates how weight and pressure can function as haptic metaphors to support user interface notifications in Virtual Reality (VR). While prior research has explored ungrounded weight simulation and pneumatic feedback, their combined role in conveying information through UI elements remains underexplored. We developed a wearable haptic device that transfers liquid and air into flexible containers mounted on the back of the user's hand, allowing us to independently manipulate weight and pressure. Through an initial evaluation using three conditions-no feedback, weight only, and weight combined with pressure-we examined how these signals affect perceived heaviness, coherence with visual cues, and the perceived urgency of notifications. Our results validate that pressure amplifies the perception of weight, but this increased heaviness does not translate into higher perceived urgency. These findings suggest that while pressure___enhanced weight can enrich haptic rendering of UI elements in VR, its contribution to communicating urgency may require further investigation, alternative pressure profiles, or different types of notifications."}
{"id": "2601.19653", "categories": ["cs.GT"], "pdf": "https://arxiv.org/pdf/2601.19653", "abs": "https://arxiv.org/abs/2601.19653", "authors": ["Niclas Boehmer", "Jessica Dierking"], "title": "Single-Winner Voting on Matchings", "comment": "Accepted to AAMAS '26", "summary": "We introduce a single-winner perspective on voting on matchings, in which voters have preferences over possible matchings in a graph, and the goal is to select a single collectively desirable matching. Unlike in classical matching problems, voters in our model are not part of the graph; instead, they have preferences over the entire matching. In the resulting election, the candidate space consists of all feasible matchings, whose exponential size renders standard algorithms for identifying socially desirable outcomes computationally infeasible. We study whether the computational tractability of finding such outcomes can be regained by exploiting the matching structure of the candidate space. Specifically, we provide a complete complexity landscape for questions concerning the maximization of social welfare, the construction and verification of Pareto optimal outcomes, and the existence and verification of Condorcet winners under one affine and two approval-based utility models. Our results consist of a mix of algorithmic and intractability results, revealing sharp boundaries between tractable and intractable cases, with complexity jumps arising from subtle changes in the utility model or solution concept."}
{"id": "2601.19310", "categories": ["cs.GR", "cs.HC"], "pdf": "https://arxiv.org/pdf/2601.19310", "abs": "https://arxiv.org/abs/2601.19310", "authors": ["Yuqi Tong", "Ruiyang Li", "Chengkun Li", "Qixuan Liu", "Shi Qiu", "Pheng-Ann Heng"], "title": "ClipGS-VR: Immersive and Interactive Cinematic Visualization of Volumetric Medical Data in Mobile Virtual Reality", "comment": "IEEE VR 2026 Posters", "summary": "High-fidelity cinematic medical visualization on mobile virtual reality (VR) remains challenging. Although ClipGS enables cross-sectional exploration via 3D Gaussian Splatting, it lacks arbitrary-angle slicing on consumer-grade VR headsets. To achieve real-time interactive performance, we introduce ClipGS-VR and restructure ClipGS's neural inference into a consolidated dataset, integrating high-fidelity layers from multiple pre-computed slicing states into a unified rendering structure. Our framework further supports arbitrary-angle slicing via gradient-based opacity modulation for smooth, visually coherent rendering. Evaluations confirm our approach maintains visual fidelity comparable to offline results while offering superior usability and interaction efficiency."}
{"id": "2601.19706", "categories": ["cs.GT"], "pdf": "https://arxiv.org/pdf/2601.19706", "abs": "https://arxiv.org/abs/2601.19706", "authors": ["Piotr Faliszewski", "Grzegorz Gawron", "Bartosz Kusek"], "title": "Robustness of Approval-Based Multiwinner Voting Rules", "comment": "arXiv admin note: text overlap with arXiv:2208.00750", "summary": "We investigate how robust approval-based multiwinner voting rules are to small perturbations in the votes. In particular, we consider the extent to which a committee can change after we add/remove/swap one approval, and we consider the computational complexity of deciding how many such operations are necessary to change the set of winning committees. We also consider the counting variants of our problems, which can be interpreted as computing the probability that the result of an election changes after a given number of random perturbations of the given election."}
{"id": "2601.19425", "categories": ["cs.GR"], "pdf": "https://arxiv.org/pdf/2601.19425", "abs": "https://arxiv.org/abs/2601.19425", "authors": ["Sophie Kergaßner", "Piotr Didyk"], "title": "It's Not Just a Phase: Creating Phase-Aligned Peripheral Metamers", "comment": "10 pages including references and figure only pages; 2 pages Supplementary Material", "summary": "Novel display technologies can deliver high-quality images across a wide field of view, creating immersive experiences. While rendering for such devices is expensive, most of the content falls into peripheral vision, where human perception differs from that in the fovea. Consequently, it is critical to understand and leverage the limitations of visual perception to enable efficient rendering. A standard approach is to exploit the reduced sensitivity to spatial details in the periphery by reducing rendering resolution, so-called foveated rendering. While this strategy avoids rendering part of the content altogether, an alternative promising direction is to replace accurate and expensive rendering with inexpensive synthesis of content that is perceptually indistinguishable from the ground-truth image. In this paper, we propose such a method for the efficient generation of an image signal that substitutes the rendering of high-frequency details. The method is grounded in findings from image statistics, which show that preserving appropriate local statistics is critical for perceived image quality. Based on this insight, we extrapolate several local image statistics from foveated content into higher spatial frequency ranges that are attenuated or omitted in the rendering process. This rich set of statistics is later used to synthesize a signal that is added to the initial rendering, boosting its perceived quality. We focus on phase information, demonstrating the importance of its alignment across space and frequencies. We calibrate and compare our method with state-of-the-art strategies, showing a significant reduction in the content that must be accurately rendered at a relatively small extra cost for synthesizing the additional signal."}
{"id": "2601.19716", "categories": ["cs.GT"], "pdf": "https://arxiv.org/pdf/2601.19716", "abs": "https://arxiv.org/abs/2601.19716", "authors": ["Piotr Faliszewski", "Piotr Skowron", "Arkadii Slinko", "Krzysztof Sornat", "Stanisław Szufa", "Nimrod Talmon"], "title": "How Similar Are Two Elections?", "comment": null, "summary": "We introduce and study isomorphic distances between ordinal\n  elections (with the same numbers of candidates and voters). The main\n  feature of these distances is that they are invariant to renaming\n  the candidates and voters, and two elections are at distance zero if\n  and only if they are isomorphic. Specifically, we consider\n  isomorphic extensions of distances between preference orders: Given\n  such a distance d, we extend it to distance d-ID between\n  elections by unifying candidate names and finding a matching between\n  the votes, so that the sum of the d-distances between the matched\n  votes is as small as possible.\n  We show that testing isomorphism of two elections can be done in\n  polynomial time so, in principle, such distances can be tractable.\n  Yet, we show that two very natural isomorphic distances are\n  NP-complete and hard to approximate. We attempt to rectify the\n  situation by showing FPT algorithms for several natural\n  parameterizations."}
{"id": "2601.19843", "categories": ["cs.GR"], "pdf": "https://arxiv.org/pdf/2601.19843", "abs": "https://arxiv.org/abs/2601.19843", "authors": ["Doga Yilmaz", "Jialin Zhu", "Deshan Gong", "He Wang"], "title": "Graphical X Splatting (GraphiXS): A Graphical Model for 4D Gaussian Splatting under Uncertainty", "comment": null, "summary": "We propose a new framework to systematically incorporate data uncertainty in Gaussian Splatting. Being the new paradigm of neural rendering, Gaussian Splatting has been investigated in many applications, with the main effort in extending its representation, improving its optimization process, and accelerating its speed. However, one orthogonal, much needed, but under-explored area is data uncertainty. In standard 4D Gaussian Splatting, data uncertainty can manifest as view sparsity, missing frames, camera asynchronization, etc. So far, there has been little research to holistically incorporating various types of data uncertainty under a single framework. To this end, we propose Graphical X Splatting, or GraphiXS, a new probabilistic framework that considers multiple types of data uncertainty, aiming for a fundamental augmentation of the current 4D Gaussian Splatting paradigm into a probabilistic setting. GraphiXS is general and can be instantiated with a range of primitives, e.g. Gaussians, Student's-t. Furthermore, GraphiXS can be used to `upgrade' existing methods to accommodate data uncertainty. Through exhaustive evaluation and comparison, we demonstrate that GraphiXS can systematically model various uncertainties in data, outperform existing methods in many settings where data are missing or polluted in space and time, and therefore is a major generalization of the current 4D Gaussian Splatting research."}
