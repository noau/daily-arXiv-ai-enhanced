<div id=toc></div>

# Table of Contents

- [cs.GR](#cs.GR) [Total: 2]


<div id='cs.GR'></div>

# cs.GR [[Back]](#toc)

### [1] [B2F: End-to-End Body-to-Face Motion Generation with Style Reference](https://arxiv.org/abs/2511.13988)
*Bokyung Jang,Eunho Jung,Yoonsang Lee*

Main category: cs.GR

TL;DR: B2F是一个模型，用于生成与身体动作一致的面部表情，通过解耦内容和风格学习一致的面部动画。


<details>
  <summary>Details</summary>
Motivation: 虚拟角色的面部表情与身体动作不一致会导致整体感知减弱，因此提出B2F模型以解决这一问题。

Method: B2F通过Gumbel-Softmax技巧学习离散潜在编码表示风格，并使用对齐和一致性目标生成FLAME格式的面部动画。

Result: B2F能够生成与身体动作同步且符合风格意图的面部动画，减少感知失调，适用于多种角色和风格。

Conclusion: B2F成功解决了面部表情与身体动作不一致的问题，并能广泛应用于不同虚拟角色。

Abstract: Human motion naturally integrates body movements and facial expressions, forming a unified perception. If a virtual character's facial expression does not align well with its body movements, it may weaken the perception of the character as a cohesive whole. Motivated by this, we propose B2F, a model that generates facial motions aligned with body movements. B2F takes a facial style reference as input, generating facial animations that reflect the provided style while maintaining consistency with the associated body motion. To achieve this, B2F learns a disentangled representation of content and style, using alignment and consistency-based objectives. We represent style using discrete latent codes learned via the Gumbel-Softmax trick, enabling diverse expression generation with a structured latent representation. B2F outputs facial motion in the FLAME format, making it compatible with SMPL-X characters, and supports ARKit-style avatars through a dedicated conversion module. Our evaluations show that B2F generates expressive and engaging facial animations that synchronize with body movements and style intent, while mitigating perceptual dissonance from mismatched cues, and generalizing across diverse characters and styles.

</details>


### [2] [FreeMusco: Motion-Free Learning of Latent Control for Morphology-Adaptive Locomotion in Musculoskeletal Characters](https://arxiv.org/abs/2511.14205)
*Minkwan Kim,Yoonsang Lee*

Main category: cs.GR

TL;DR: FreeMusco是一种无需运动数据的框架，通过联合学习潜在表示和控制策略，实现了能量感知和形态适应的运动控制，适用于多种形态的角色。


<details>
  <summary>Details</summary>
Motivation: 传统的运动控制方法依赖于大量的运动捕捉数据，这在某些情况下不切实际。作者旨在开发一种无需运动数据的方法，直接从生物力学模型中学习运动控制策略。

Method: FreeMusco利用生物力学模型作为先验，通过基于模型的强化学习方法学习潜在空间和控制策略。引入了时间平均损失函数，并结合控制、平衡和生物力学目标，随机化目标姿态和能量水平以提高多样性。

Result: FreeMusco在人类、非人类和合成形态的角色上均表现出多样且物理合理的运动行为，能够自然产生能量高效的策略，如四足动物的步态和人形角色的双足步态。

Conclusion: 该研究表明，无需运动捕捉数据也能实现多样化和自适应的运动控制，为无法或不切实际收集数据的角色运动模拟提供了新方向。

Abstract: We propose FreeMusco, a motion-free framework that jointly learns latent representations and control policies for musculoskeletal characters. By leveraging the musculoskeletal model as a strong prior, our method enables energy-aware and morphology-adaptive locomotion to emerge without motion data. The framework generalizes across human, non-human, and synthetic morphologies, where distinct energy-efficient strategies naturally appear--for example, quadrupedal gaits in Chimanoid versus bipedal gaits in Humanoid. The latent space and corresponding control policy are constructed from scratch, without demonstration, and enable downstream tasks such as goal navigation and path following--representing, to our knowledge, the first motion-free method to provide such capabilities. FreeMusco learns diverse and physically plausible locomotion behaviors through model-based reinforcement learning, guided by the locomotion objective that combines control, balancing, and biomechanical terms. To better capture the periodic structure of natural gait, we introduce the temporally averaged loss formulation, which compares simulated and target states over a time window rather than on a per-frame basis. We further encourage behavioral diversity by randomizing target poses and energy levels during training, enabling locomotion to be flexibly modulated in both form and intensity at runtime. Together, these results demonstrate that versatile and adaptive locomotion control can emerge without motion capture, offering a new direction for simulating movement in characters where data collection is impractical or impossible.

</details>
