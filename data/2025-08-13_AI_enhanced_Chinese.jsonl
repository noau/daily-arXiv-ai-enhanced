{"id": "2508.08669", "categories": ["cs.GT"], "pdf": "https://arxiv.org/pdf/2508.08669", "abs": "https://arxiv.org/abs/2508.08669", "authors": ["Yizhou Zhang", "Eric Mazumdar"], "title": "Convergent Q-Learning for Infinite-Horizon General-Sum Markov Games through Behavioral Economics", "comment": null, "summary": "Risk-aversion and bounded rationality are two key characteristics of human\ndecision-making. Risk-averse quantal-response equilibrium (RQE) is a solution\nconcept that incorporates these features, providing a more realistic depiction\nof human decision making in various strategic environments compared to a Nash\nequilibrium. Furthermore a class of RQE has recently been shown in\narXiv:2406.14156 to be universally computationally tractable in all\nfinite-horizon Markov games, allowing for the development of multi-agent\nreinforcement learning algorithms with convergence guarantees. In this paper,\nwe expand upon the study of RQE and analyze their computation in both\ntwo-player normal form games and discounted infinite-horizon Markov games. For\nnormal form games we adopt a monotonicity-based approach allowing us to\ngeneralize previous results. We first show uniqueness and Lipschitz continuity\nof RQE with respect to player's payoff matrices under monotonicity assumptions,\nand then provide conditions on the players' degrees of risk aversion and\nbounded rationality that ensure monotonicity. We then focus on discounted\ninfinite-horizon Markov games. We define the risk-averse quantal-response\nBellman operator and prove its contraction under further conditions on the\nplayers' risk-aversion, bounded rationality, and temporal discounting. This\nyields a Q-learning based algorithm with convergence guarantees for all\ninfinite-horizon general-sum Markov games.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7814\u7a76\u4e86\u98ce\u9669\u89c4\u907f\u91cf\u5316\u54cd\u5e94\u5747\u8861\uff08RQE\uff09\u5728\u4e24\u4eba\u4e00\u822c\u5f62\u5f0f\u535a\u5f08\u548c\u65e0\u9650\u89c6\u754c\u9a6c\u5c14\u53ef\u592b\u535a\u5f08\u4e2d\u7684\u8ba1\u7b97\u95ee\u9898\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5355\u8c03\u6027\u7684\u65b9\u6cd5\uff0c\u5e76\u5728\u65e0\u9650\u89c6\u754c\u9a6c\u5c14\u53ef\u592b\u535a\u5f08\u4e2d\u5b9a\u4e49\u4e86\u98ce\u9669\u89c4\u907f\u91cf\u5316\u54cd\u5e94Bellman\u7b97\u5b50\uff0c\u8bc1\u660e\u4e86\u5176\u6536\u655b\u6027\u3002", "motivation": "\u98ce\u9669\u89c4\u907f\u548c\u6709\u9650\u7406\u6027\u662f\u4eba\u7c7b\u51b3\u7b56\u7684\u4e24\u4e2a\u5173\u952e\u7279\u5f81\uff0cRQE\u4f5c\u4e3a\u4e00\u79cd\u89e3\u51b3\u65b9\u6848\u6982\u5ff5\uff0c\u6bd4\u7eb3\u4ec0\u5747\u8861\u66f4\u80fd\u771f\u5b9e\u5730\u53cd\u6620\u4eba\u7c7b\u5728\u5404\u79cd\u6218\u7565\u73af\u5883\u4e2d\u7684\u51b3\u7b56\u884c\u4e3a\u3002\u8bba\u6587\u65e8\u5728\u6269\u5c55\u5bf9RQE\u7684\u7814\u7a76\uff0c\u5e76\u5206\u6790\u5176\u5728\u4e0d\u540c\u535a\u5f08\u6a21\u578b\u4e2d\u7684\u8ba1\u7b97\u95ee\u9898\u3002", "method": "\u5728\u4e00\u822c\u5f62\u5f0f\u535a\u5f08\u4e2d\uff0c\u91c7\u7528\u57fa\u4e8e\u5355\u8c03\u6027\u7684\u65b9\u6cd5\uff1b\u5728\u65e0\u9650\u89c6\u754c\u9a6c\u5c14\u53ef\u592b\u535a\u5f08\u4e2d\uff0c\u5b9a\u4e49\u4e86\u98ce\u9669\u89c4\u907f\u91cf\u5316\u54cd\u5e94Bellman\u7b97\u5b50\uff0c\u5e76\u5728\u7279\u5b9a\u6761\u4ef6\u4e0b\u8bc1\u660e\u5176\u6536\u655b\u6027\u3002", "result": "\u8bc1\u660e\u4e86\u4e00\u822c\u5f62\u5f0f\u535a\u5f08\u4e2dRQE\u7684\u552f\u4e00\u6027\u548cLipschitz\u8fde\u7eed\u6027\uff1b\u5728\u65e0\u9650\u89c6\u754c\u9a6c\u5c14\u53ef\u592b\u535a\u5f08\u4e2d\uff0c\u8bc1\u660e\u4e86\u98ce\u9669\u89c4\u907f\u91cf\u5316\u54cd\u5e94Bellman\u7b97\u5b50\u7684\u6536\u655b\u6027\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u5177\u6709\u6536\u655b\u4fdd\u8bc1\u7684Q\u5b66\u4e60\u7b97\u6cd5\u3002", "conclusion": "\u8be5\u7814\u7a76\u6269\u5c55\u4e86\u5bf9RQE\u7684\u7406\u89e3\uff0c\u5e76\u63d0\u4f9b\u4e86\u5728\u591a\u79cd\u535a\u5f08\u6a21\u578b\u4e2d\u8ba1\u7b97RQE\u7684\u6709\u6548\u65b9\u6cd5\uff0c\u5c24\u5176\u5728\u65e0\u9650\u89c6\u754c\u9a6c\u5c14\u53ef\u592b\u535a\u5f08\u4e2d\u63d0\u51fa\u7684\u7b97\u6cd5\u5177\u6709\u91cd\u8981\u7684\u7406\u8bba\u548c\u5b9e\u8df5\u610f\u4e49\u3002"}}
{"id": "2508.08682", "categories": ["cs.GT", "cs.CC"], "pdf": "https://arxiv.org/pdf/2508.08682", "abs": "https://arxiv.org/abs/2508.08682", "authors": ["Matthias Bentert", "Robert Bredereck", "Eva Deltl", "Pallavi Jain", "Leon Kellerhals"], "title": "How to Resolve Envy by Adding Goods", "comment": null, "summary": "We consider the problem of resolving the envy of a given initial allocation\nby adding elements from a pool of goods. We give a characterization of the\ninstances where envy can be resolved by adding an arbitrary number of copies of\nthe items in the pool. From this characterization, we derive a polynomial-time\nalgorithm returning a respective solution if it exists. If the number of copies\nor the total number of added items are bounded, the problem becomes\ncomputationally intractable even in various restricted cases. We perform a\nparameterized complexity analysis, focusing on the number of agents and the\npool size as parameters. Notably, although not every instance admits an\nenvy-free solution, our approach allows us to efficiently determine, in\npolynomial time, whether a solution exists-an aspect that is both theoretically\ninteresting and far from trivial.", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u4e86\u901a\u8fc7\u4ece\u7269\u54c1\u6c60\u4e2d\u6dfb\u52a0\u7269\u54c1\u6765\u89e3\u51b3\u521d\u59cb\u5206\u914d\u4e2d\u7684\u5ac9\u5992\u95ee\u9898\uff0c\u63d0\u4f9b\u4e86\u5728\u7269\u54c1\u6c60\u4e2d\u6dfb\u52a0\u4efb\u610f\u6570\u91cf\u526f\u672c\u65f6\u53ef\u89e3\u51b3\u5ac9\u5992\u7684\u5b9e\u4f8b\u7684\u7279\u5f81\uff0c\u5e76\u63a8\u5bfc\u4e86\u591a\u9879\u5f0f\u65f6\u95f4\u7b97\u6cd5\u3002", "motivation": "\u7814\u7a76\u5982\u4f55\u901a\u8fc7\u6dfb\u52a0\u7269\u54c1\u6765\u89e3\u51b3\u5206\u914d\u4e2d\u7684\u5ac9\u5992\u95ee\u9898\uff0c\u63a2\u8ba8\u5728\u7269\u54c1\u6c60\u4e2d\u65e0\u9650\u6216\u6709\u9650\u6dfb\u52a0\u7269\u54c1\u65f6\u7684\u8ba1\u7b97\u590d\u6742\u6027\u548c\u53ef\u884c\u6027\u3002", "method": "\u901a\u8fc7\u7279\u5f81\u5316\u5b9e\u4f8b\uff0c\u63a8\u5bfc\u51fa\u591a\u9879\u5f0f\u65f6\u95f4\u7b97\u6cd5\uff0c\u5e76\u5728\u53c2\u6570\u5316\u590d\u6742\u5ea6\u5206\u6790\u4e2d\u5173\u6ce8\u4ee3\u7406\u6570\u91cf\u548c\u7269\u54c1\u6c60\u5927\u5c0f\u4f5c\u4e3a\u53c2\u6570\u3002", "result": "\u5728\u7269\u54c1\u6c60\u4e2d\u65e0\u9650\u6dfb\u52a0\u7269\u54c1\u65f6\uff0c\u5b58\u5728\u591a\u9879\u5f0f\u65f6\u95f4\u7b97\u6cd5\u89e3\u51b3\u95ee\u9898\uff1b\u4f46\u5728\u526f\u672c\u6570\u91cf\u6216\u6dfb\u52a0\u7269\u54c1\u603b\u6570\u6709\u9650\u65f6\uff0c\u95ee\u9898\u8ba1\u7b97\u56f0\u96be\u3002", "conclusion": "\u5c3d\u7ba1\u5e76\u975e\u6240\u6709\u5b9e\u4f8b\u90fd\u80fd\u65e0\u5ac9\u5992\u89e3\u51b3\uff0c\u4f46\u8bba\u6587\u63d0\u51fa\u7684\u65b9\u6cd5\u80fd\u9ad8\u6548\u5224\u65ad\u89e3\u7684\u5b58\u5728\u6027\uff0c\u5177\u6709\u7406\u8bba\u548c\u5b9e\u9645\u610f\u4e49\u3002"}}
{"id": "2508.08772", "categories": ["cs.GT"], "pdf": "https://arxiv.org/pdf/2508.08772", "abs": "https://arxiv.org/abs/2508.08772", "authors": ["Huanyu Yan", "Yu Huo", "Min Lu", "Weitong Ou", "Xingyan Shi", "Ruihe Shi", "Xiaoying Tang"], "title": "Optimal Boost Design for Auto-bidding Mechanism with Publisher Quality Constraints", "comment": "18 pages, 23 figures, conference", "summary": "Online bidding is crucial in mobile ecosystems, enabling real-time ad\nallocation across billions of devices to optimize performance and user\nexperience. Improving ad allocation efficiency is a long-standing research\nproblem, as it directly enhances the economic outcomes for all participants in\nadvertising platforms. This paper investigates the design of optimal boost\nfactors in online bidding while incorporating quality value (the impact of\ndisplayed ads on publishers' long-term benefits). To address the divergent\ninterests on quality, we establish a three-party auction framework with a\nunified welfare metric of advertiser and publisher. Within this framework, we\nderive the theoretical efficiency lower bound for C-competitive boost in\nsecond-price single-slot auctions, then design a novel quality-involved\nBoosting (q-Boost) algorithm for computing the optimal boost factor.\nExperimental validation on Alibaba's public dataset (AuctionNet) demonstrates\n2%-6% welfare improvements over conventional approaches, proving our method's\neffectiveness in real-world settings.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u5728\u7ebf\u7ade\u4ef7\u4e2d\u7ed3\u5408\u8d28\u91cf\u4ef7\u503c\u7684\u4f18\u5316\u63d0\u5347\u56e0\u7d20\u8bbe\u8ba1\uff0c\u63d0\u51fa\u4e86\u8d28\u91cf\u53c2\u4e0e\u7684\u63d0\u5347\u7b97\u6cd5\uff08q-Boost\uff09\uff0c\u5b9e\u9a8c\u8bc1\u660e\u5176\u5728\u5b9e\u9645\u73af\u5883\u4e2d\u80fd\u63d0\u9ad82%-6%\u7684\u798f\u5229\u3002", "motivation": "\u63d0\u5347\u5e7f\u544a\u5206\u914d\u6548\u7387\u662f\u957f\u671f\u7684\u7814\u7a76\u95ee\u9898\uff0c\u56e0\u4e3a\u5b83\u76f4\u63a5\u5f71\u54cd\u5e7f\u544a\u5e73\u53f0\u4e2d\u6240\u6709\u53c2\u4e0e\u8005\u7684\u7ecf\u6d4e\u6210\u679c\u3002\u672c\u6587\u65e8\u5728\u8bbe\u8ba1\u5728\u7ebf\u7ade\u4ef7\u4e2d\u7684\u6700\u4f18\u63d0\u5347\u56e0\u7d20\uff0c\u540c\u65f6\u7eb3\u5165\u8d28\u91cf\u4ef7\u503c\uff08\u5c55\u793a\u5e7f\u544a\u5bf9\u53d1\u5e03\u8005\u957f\u671f\u5229\u76ca\u7684\u5f71\u54cd\uff09\u3002", "method": "\u901a\u8fc7\u5efa\u7acb\u4e00\u4e2a\u4e09\u65b9\u62cd\u5356\u6846\u67b6\uff0c\u7edf\u4e00\u4e86\u5e7f\u544a\u4e3b\u548c\u53d1\u5e03\u8005\u7684\u798f\u5229\u6307\u6807\uff0c\u5e76\u5728\u7b2c\u4e8c\u4ef7\u683c\u5355\u63d2\u69fd\u62cd\u5356\u4e2d\u63a8\u5bfc\u4e86\u7406\u8bba\u6548\u7387\u4e0b\u754c\u3002\u968f\u540e\u8bbe\u8ba1\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u8d28\u91cf\u53c2\u4e0e\u63d0\u5347\u7b97\u6cd5\uff08q-Boost\uff09\u6765\u8ba1\u7b97\u6700\u4f18\u63d0\u5347\u56e0\u7d20\u3002", "result": "\u5728\u963f\u91cc\u5df4\u5df4\u7684\u516c\u5f00\u6570\u636e\u96c6\uff08AuctionNet\uff09\u4e0a\u7684\u5b9e\u9a8c\u9a8c\u8bc1\u663e\u793a\uff0c\u8be5\u65b9\u6cd5\u6bd4\u4f20\u7edf\u65b9\u6cd5\u63d0\u9ad8\u4e862%-6%\u7684\u798f\u5229\u3002", "conclusion": "\u7814\u7a76\u8bc1\u660e\u4e86\u8d28\u91cf\u53c2\u4e0e\u7684\u63d0\u5347\u7b97\u6cd5\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u7684\u6709\u6548\u6027\uff0c\u80fd\u591f\u663e\u8457\u63d0\u5347\u5e7f\u544a\u5e73\u53f0\u7684\u7ecf\u6d4e\u6548\u76ca\u3002"}}
{"id": "2508.08810", "categories": ["cs.GT", "cs.AI", "cs.MA", "econ.TH"], "pdf": "https://arxiv.org/pdf/2508.08810", "abs": "https://arxiv.org/abs/2508.08810", "authors": ["Edith Elkind", "Tzeh Yuan Neoh", "Nicholas Teh"], "title": "Not in My Backyard! Temporal Voting Over Public Chores", "comment": "Appears in the 34th International Joint Conference on Artificial\n  Intelligence (IJCAI), 2025", "summary": "We study a temporal voting model where voters have dynamic preferences over a\nset of public chores -- projects that benefit society, but impose individual\ncosts on those affected by their implementation. We investigate the\ncomputational complexity of optimizing utilitarian and egalitarian welfare. Our\nresults show that while optimizing the former is computationally\nstraightforward, minimizing the latter is computationally intractable, even in\nvery restricted cases. Nevertheless, we identify several settings where this\nproblem can be solved efficiently, either exactly or by an approximation\nalgorithm. We also examine the effects of enforcing temporal fairness and its\nimpact on social welfare, and analyze the competitive ratio of online\nalgorithms. We then explore the strategic behavior of agents, providing\ninsights into potential malfeasance in such decision-making environments.\nFinally, we discuss a range of fairness measures and their suitability for our\nsetting.", "AI": {"tldr": "\u7814\u7a76\u4e86\u4e00\u4e2a\u65f6\u95f4\u6295\u7968\u6a21\u578b\uff0c\u5206\u6790\u4e86\u4f18\u5316\u529f\u5229\u4e3b\u4e49\u548c\u5e73\u5747\u4e3b\u4e49\u798f\u5229\u7684\u8ba1\u7b97\u590d\u6742\u6027\uff0c\u53d1\u73b0\u524d\u8005\u7b80\u5355\u800c\u540e\u8005\u56f0\u96be\uff0c\u4f46\u67d0\u4e9b\u60c5\u51b5\u4e0b\u53ef\u6709\u6548\u89e3\u51b3\u3002\u63a2\u8ba8\u4e86\u65f6\u95f4\u516c\u5e73\u6027\u548c\u7b56\u7565\u884c\u4e3a\u7684\u5f71\u54cd\u3002", "motivation": "\u7814\u7a76\u52a8\u6001\u504f\u597d\u4e0b\u516c\u5171\u4efb\u52a1\u5206\u914d\u7684\u6295\u7968\u6a21\u578b\uff0c\u4ee5\u7406\u89e3\u793e\u4f1a\u798f\u5229\u4f18\u5316\u7684\u8ba1\u7b97\u590d\u6742\u6027\u548c\u516c\u5e73\u6027\u95ee\u9898\u3002", "method": "\u901a\u8fc7\u8ba1\u7b97\u590d\u6742\u6027\u5206\u6790\u548c\u8fd1\u4f3c\u7b97\u6cd5\uff0c\u7814\u7a76\u4e86\u529f\u5229\u4e3b\u4e49\u548c\u5e73\u5747\u4e3b\u4e49\u798f\u5229\u7684\u4f18\u5316\uff0c\u5e76\u63a2\u8ba8\u4e86\u65f6\u95f4\u516c\u5e73\u6027\u548c\u7b56\u7565\u884c\u4e3a\u3002", "result": "\u4f18\u5316\u529f\u5229\u4e3b\u4e49\u798f\u5229\u8ba1\u7b97\u7b80\u5355\uff0c\u800c\u4f18\u5316\u5e73\u5747\u4e3b\u4e49\u798f\u5229\u5728\u53d7\u9650\u60c5\u51b5\u4e0b\u4ecd\u56f0\u96be\uff0c\u4f46\u67d0\u4e9b\u573a\u666f\u53ef\u9ad8\u6548\u89e3\u51b3\u3002\u65f6\u95f4\u516c\u5e73\u6027\u5f71\u54cd\u793e\u4f1a\u798f\u5229\uff0c\u7b56\u7565\u884c\u4e3a\u53ef\u80fd\u5bfc\u81f4\u51b3\u7b56\u73af\u5883\u4e2d\u7684\u4e0d\u5f53\u884c\u4e3a\u3002", "conclusion": "\u7814\u7a76\u63ed\u793a\u4e86\u52a8\u6001\u504f\u597d\u4e0b\u793e\u4f1a\u798f\u5229\u4f18\u5316\u7684\u590d\u6742\u6027\uff0c\u5e76\u63d0\u4f9b\u4e86\u516c\u5e73\u6027\u5ea6\u91cf\u548c\u7b56\u7565\u884c\u4e3a\u7684\u6df1\u5165\u5206\u6790\u3002"}}
{"id": "2508.08384", "categories": ["cs.GR", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2508.08384", "abs": "https://arxiv.org/abs/2508.08384", "authors": ["Mutian Tong", "Rundi Wu", "Changxi Zheng"], "title": "Spatiotemporally Consistent Indoor Lighting Estimation with Diffusion Priors", "comment": "11 pages. Accepted by SIGGRAPH 2025 as Conference Paper", "summary": "Indoor lighting estimation from a single image or video remains a challenge\ndue to its highly ill-posed nature, especially when the lighting condition of\nthe scene varies spatially and temporally. We propose a method that estimates\nfrom an input video a continuous light field describing the spatiotemporally\nvarying lighting of the scene. We leverage 2D diffusion priors for optimizing\nsuch light field represented as a MLP. To enable zero-shot generalization to\nin-the-wild scenes, we fine-tune a pre-trained image diffusion model to predict\nlighting at multiple locations by jointly inpainting multiple chrome balls as\nlight probes. We evaluate our method on indoor lighting estimation from a\nsingle image or video and show superior performance over compared baselines.\nMost importantly, we highlight results on spatiotemporally consistent lighting\nestimation from in-the-wild videos, which is rarely demonstrated in previous\nworks.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u4ece\u5355\u5f20\u56fe\u50cf\u6216\u89c6\u9891\u4e2d\u4f30\u8ba1\u5ba4\u5185\u5149\u7167\u6761\u4ef6\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u5229\u75282D\u6269\u6563\u5148\u9a8c\u4f18\u5316MLP\u8868\u793a\u7684\u5149\u573a\uff0c\u5b9e\u73b0\u5728\u771f\u5b9e\u573a\u666f\u4e2d\u7684\u96f6\u6837\u672c\u6cdb\u5316\u3002", "motivation": "\u5ba4\u5185\u5149\u7167\u4f30\u8ba1\u56e0\u5176\u9ad8\u5ea6\u4e0d\u9002\u5b9a\u6027\u800c\u5177\u6709\u6311\u6218\u6027\uff0c\u5c24\u5176\u662f\u5728\u5149\u7167\u6761\u4ef6\u5728\u7a7a\u95f4\u548c\u65f6\u95f4\u4e0a\u53d8\u5316\u7684\u60c5\u51b5\u4e0b\u3002\u672c\u6587\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u5229\u75282D\u6269\u6563\u5148\u9a8c\u4f18\u5316MLP\u8868\u793a\u7684\u5149\u573a\uff0c\u5e76\u901a\u8fc7\u8c03\u6574\u9884\u8bad\u7ec3\u7684\u6269\u6563\u6a21\u578b\u5728\u591a\u4f4d\u7f6e\u9884\u6d4b\u5149\u7167\uff0c\u4ee5\u8054\u5408\u4fee\u590d\u591a\u4e2a\u9540\u94ec\u7403\u4f5c\u4e3a\u5149\u63a2\u5934\u3002", "result": "\u5728\u5355\u5f20\u56fe\u50cf\u6216\u89c6\u9891\u7684\u5ba4\u5185\u5149\u7167\u4f30\u8ba1\u4efb\u52a1\u4e0a\u8868\u73b0\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\uff0c\u5c24\u5176\u662f\u5728\u771f\u5b9e\u89c6\u9891\u4e2d\u5b9e\u73b0\u4e86\u65f6\u7a7a\u4e00\u81f4\u7684\u5149\u7167\u4f30\u8ba1\u3002", "conclusion": "\u672c\u6587\u65b9\u6cd5\u5728\u5ba4\u5185\u5149\u7167\u4f30\u8ba1\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u7279\u522b\u5728\u771f\u5b9e\u89c6\u9891\u4e2d\u5c55\u73b0\u4e86\u65f6\u7a7a\u4e00\u81f4\u6027\uff0c\u8d85\u8d8a\u4e86\u5148\u524d\u7684\u5de5\u4f5c\u3002"}}
{"id": "2508.08429", "categories": ["cs.GR", "cs.CV"], "pdf": "https://arxiv.org/pdf/2508.08429", "abs": "https://arxiv.org/abs/2508.08429", "authors": ["Dalton Omens", "Allise Thurman", "Jihun Yu", "Ronald Fedkiw"], "title": "Improving Facial Rig Semantics for Tracking and Retargeting", "comment": null, "summary": "In this paper, we consider retargeting a tracked facial performance to either\nanother person or to a virtual character in a game or virtual reality (VR)\nenvironment. We remove the difficulties associated with identifying and\nretargeting the semantics of one rig framework to another by utilizing the same\nframework (3DMM, FLAME, MetaHuman, etc.) for both subjects. Although this does\nnot constrain the choice of framework when retargeting from one person to\nanother, it does force the tracker to use the game/VR character rig when\nretargeting to a game/VR character. We utilize volumetric morphing in order to\nfit facial rigs to both performers and targets; in addition, a carefully chosen\nset of Simon-Says expressions is used to calibrate each rig to the motion\nsignatures of the relevant performer or target. Although a uniform set of\nSimon-Says expressions can likely be used for all person to person retargeting,\nwe argue that person to game/VR character retargeting benefits from Simon-Says\nexpressions that capture the distinct motion signature of the game/VR character\nrig. The Simon-Says calibrated rigs tend to produce the desired expressions\nwhen exercising animation controls (as expected). Unfortunately, these\nwell-calibrated rigs still lead to undesirable controls when tracking a\nperformance (a well-behaved function can have an arbitrarily ill-conditioned\ninverse), even though they typically produce acceptable geometry\nreconstructions. Thus, we propose a fine-tuning approach that modifies the rig\nused by the tracker in order to promote the output of more semantically\nmeaningful animation controls, facilitating high efficacy retargeting. In order\nto better address real-world scenarios, the fine-tuning relies on implicit\ndifferentiation so that the tracker can be treated as a (potentially\nnon-differentiable) black box.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u901a\u8fc7\u4f7f\u7528\u76f8\u540c\u7684\u9762\u90e8\u5efa\u6a21\u6846\u67b6\uff08\u59823DMM\u3001FLAME\u7b49\uff09\u6765\u7b80\u5316\u9762\u90e8\u52a8\u4f5c\u91cd\u5b9a\u5411\u7684\u65b9\u6cd5\uff0c\u5e76\u901a\u8fc7Simon-Says\u8868\u8fbe\u5f0f\u6821\u51c6\u548c\u7ec6\u8c03\u6280\u672f\u63d0\u9ad8\u91cd\u5b9a\u5411\u6548\u679c\u3002", "motivation": "\u89e3\u51b3\u9762\u90e8\u52a8\u4f5c\u91cd\u5b9a\u5411\u4e2d\u56e0\u4f7f\u7528\u4e0d\u540c\u9762\u90e8\u5efa\u6a21\u6846\u67b6\u800c\u5bfc\u81f4\u7684\u8bed\u4e49\u8bc6\u522b\u548c\u91cd\u5b9a\u5411\u56f0\u96be\u95ee\u9898\uff0c\u5e76\u4f18\u5316\u4ece\u771f\u5b9e\u4eba\u7269\u5230\u865a\u62df\u89d2\u8272\u7684\u9762\u90e8\u52a8\u753b\u63a7\u5236\u3002", "method": "\u91c7\u7528\u76f8\u540c\u7684\u9762\u90e8\u5efa\u6a21\u6846\u67b6\u8fdb\u884c\u91cd\u5b9a\u5411\uff0c\u4f7f\u7528\u4f53\u79ef\u53d8\u5f62\u6280\u672f\u62df\u5408\u9762\u90e8\u6a21\u578b\uff0c\u5e76\u901a\u8fc7Simon-Says\u8868\u8fbe\u5f0f\u6821\u51c6\u6bcf\u4e2a\u6a21\u578b\uff1b\u63d0\u51fa\u57fa\u4e8e\u9690\u5f0f\u5fae\u5206\u7684\u65b9\u6cd5\u7ec6\u8c03\u8ddf\u8e2a\u5668\u4ee5\u751f\u6210\u66f4\u5177\u8bed\u4e49\u610f\u4e49\u7684\u52a8\u753b\u63a7\u5236\u3002", "result": "\u6821\u51c6\u540e\u7684\u6a21\u578b\u80fd\u591f\u751f\u6210\u9884\u671f\u7684\u8868\u60c5\u52a8\u753b\uff0c\u4f46\u5728\u8ddf\u8e2a\u6027\u80fd\u65f6\u4ecd\u53ef\u80fd\u51fa\u73b0\u4e0d\u826f\u63a7\u5236\uff1b\u63d0\u51fa\u7684\u7ec6\u8c03\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86\u91cd\u5b9a\u5411\u7684\u8bed\u4e49\u6548\u679c\u548c\u5b9e\u7528\u6027\u3002", "conclusion": "\u901a\u8fc7\u6807\u51c6\u5316\u6846\u67b6\u548c\u7ec6\u8c03\u6280\u672f\uff0c\u672c\u7814\u7a76\u5b9e\u73b0\u4e86\u66f4\u9ad8\u6548\u7684\u9762\u90e8\u52a8\u4f5c\u91cd\u5b9a\u5411\uff0c\u9002\u7528\u4e8e\u4ece\u4eba\u7269\u5230\u4eba\u7269\u7684\u91cd\u5b9a\u5411\u4ee5\u53ca\u4eba\u7269\u5230\u865a\u62df\u89d2\u8272\u7684\u573a\u666f\u3002"}}
{"id": "2508.08542", "categories": ["cs.GR", "cs.CV"], "pdf": "https://arxiv.org/pdf/2508.08542", "abs": "https://arxiv.org/abs/2508.08542", "authors": ["Dasith de Silva Edirimuni", "Xuequan Lu", "Ajmal Saeed Mian", "Lei Wei", "Gang Li", "Scott Schaefer", "Ying He"], "title": "Hybrid Long and Short Range Flows for Point Cloud Filtering", "comment": null, "summary": "Point cloud capture processes are error-prone and introduce noisy artifacts\nthat necessitate filtering/denoising. Recent filtering methods often suffer\nfrom point clustering or noise retaining issues. In this paper, we propose\nHybrid Point Cloud Filtering ($\\textbf{HybridPF}$) that considers both\nshort-range and long-range filtering trajectories when removing noise. It is\nwell established that short range scores, given by $\\nabla_{x}\\log p(x_t)$, may\nprovide the necessary displacements to move noisy points to the underlying\nclean surface. By contrast, long range velocity flows approximate constant\ndisplacements directed from a high noise variant patch $x_0$ towards the\ncorresponding clean surface $x_1$. Here, noisy patches $x_t$ are viewed as\nintermediate states between the high noise variant and the clean patches. Our\nintuition is that long range information from velocity flow models can guide\nthe short range scores to align more closely with the clean points. In turn,\nscore models generally provide a quicker convergence to the clean surface.\nSpecifically, we devise two parallel modules, the ShortModule and LongModule,\neach consisting of an Encoder-Decoder pair to respectively account for\nshort-range scores and long-range flows. We find that short-range scores,\nguided by long-range features, yield filtered point clouds with good point\ndistributions and convergence near the clean surface. We design a joint loss\nfunction to simultaneously train the ShortModule and LongModule, in an\nend-to-end manner. Finally, we identify a key weakness in current displacement\nbased methods, limitations on the decoder architecture, and propose a dynamic\ngraph convolutional decoder to improve the inference process. Comprehensive\nexperiments demonstrate that our HybridPF achieves state-of-the-art results\nwhile enabling faster inference speed.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u6df7\u5408\u70b9\u4e91\u6ee4\u6ce2\u65b9\u6cd5\uff08HybridPF\uff09\uff0c\u7ed3\u5408\u77ed\u7a0b\u548c\u8fdc\u7a0b\u6ee4\u6ce2\u8f68\u8ff9\uff0c\u901a\u8fc7\u5e76\u884c\u6a21\u5757\u548c\u52a8\u6001\u56fe\u5377\u79ef\u89e3\u7801\u5668\uff0c\u6709\u6548\u53bb\u9664\u566a\u58f0\u5e76\u63d0\u5347\u63a8\u7406\u901f\u5ea6\u3002", "motivation": "\u73b0\u6709\u70b9\u4e91\u6ee4\u6ce2\u65b9\u6cd5\u5e38\u5b58\u5728\u70b9\u805a\u7c7b\u6216\u566a\u58f0\u6b8b\u7559\u95ee\u9898\uff0c\u672c\u6587\u65e8\u5728\u901a\u8fc7\u7ed3\u5408\u77ed\u7a0b\u548c\u8fdc\u7a0b\u4fe1\u606f\uff0c\u63d0\u51fa\u4e00\u79cd\u66f4\u6709\u6548\u7684\u70b9\u4e91\u53bb\u566a\u65b9\u6cd5\u3002", "method": "\u8bbe\u8ba1\u4e86\u5e76\u884c\u6a21\u5757ShortModule\u548cLongModule\uff0c\u5206\u522b\u5904\u7406\u77ed\u7a0b\u8bc4\u5206\u548c\u8fdc\u7a0b\u6d41\u901f\u4fe1\u606f\uff0c\u5e76\u901a\u8fc7\u8054\u5408\u635f\u5931\u51fd\u6570\u8fdb\u884c\u7aef\u5230\u7aef\u8bad\u7ec3\uff0c\u540c\u65f6\u5f15\u5165\u52a8\u6001\u56fe\u5377\u79ef\u89e3\u7801\u5668\u4f18\u5316\u63a8\u7406\u8fc7\u7a0b\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cHybridPF\u5728\u70b9\u4e91\u53bb\u566a\u4efb\u52a1\u4e2d\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u540c\u65f6\u63d0\u5347\u4e86\u63a8\u7406\u901f\u5ea6\u3002", "conclusion": "\u7ed3\u5408\u77ed\u7a0b\u548c\u8fdc\u7a0b\u4fe1\u606f\u7684\u6df7\u5408\u6ee4\u6ce2\u65b9\u6cd5\u80fd\u591f\u663e\u8457\u63d0\u5347\u70b9\u4e91\u53bb\u566a\u6548\u679c\uff0c\u52a8\u6001\u56fe\u5377\u79ef\u89e3\u7801\u5668\u8fdb\u4e00\u6b65\u4f18\u5316\u4e86\u63a8\u7406\u6548\u7387\u3002"}}
{"id": "2508.08561", "categories": ["cs.GR"], "pdf": "https://arxiv.org/pdf/2508.08561", "abs": "https://arxiv.org/abs/2508.08561", "authors": ["Aysan Mokhtarimousavi", "Michael Kleiss", "Mostafa Alani", "Sida Dai"], "title": "Revisiting the City Tower Project: Geometric Principles and Structural Morphology in the Works of Louis I. Kahn and Anne Tyng", "comment": "8 pages, ARCC Conference", "summary": "This paper presents a study of computation and morphology of Louis Kahn City\nTower project. The City Tower is an unbuilt design by Louis I. Kahn and Anne\nTyng that integrates form and structure using 3D space triangular geometries.\nAlthough never built, the City Tower geometrical framework anticipated later\ndevelopments in design of space-frame structures. Initially envisioned in the\n1950s, the City Tower project is a skyscraper structure based on a tetrahedral\nand octahedral space frame called Octet-Truss. The aim of this study is to\nanalyze the geometry of the City Tower structure and how it can be used to\ndevelop modular and adaptable architectural forms. The study is based on an\nanalytical shape grammar that is used to recreate the original structure, and\nlater to generate new structural configurations based on the City Tower's\nmorphology. This study also investigates the potential applications of these\nfindings in architecture and reveals the possibilities of using tetrahedrons\nand octahedrons as fundamental geometries for creating scalable and modular\ndesigns and presents initial findings.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86Louis Kahn City Tower\u9879\u76ee\u7684\u8ba1\u7b97\u4e0e\u5f62\u6001\u5b66\uff0c\u5206\u6790\u4e86\u5176\u57fa\u4e8e\u56db\u9762\u4f53\u548c\u516b\u9762\u4f53\u7a7a\u95f4\u6846\u67b6\u7684\u51e0\u4f55\u7ed3\u6784\uff0c\u5e76\u63a2\u8ba8\u4e86\u5176\u5728\u6a21\u5757\u5316\u548c\u9002\u5e94\u6027\u5efa\u7b51\u8bbe\u8ba1\u4e2d\u7684\u5e94\u7528\u3002", "motivation": "\u7814\u7a76\u7684\u52a8\u673a\u5728\u4e8e\u5206\u6790Louis Kahn\u548cAnne Tyng\u672a\u5efa\u6210\u7684City Tower\u9879\u76ee\u7684\u51e0\u4f55\u6846\u67b6\uff0c\u63ed\u793a\u4e86\u5176\u5bf9\u7a7a\u95f4\u6846\u67b6\u7ed3\u6784\u8bbe\u8ba1\u7684\u9884\u89c1\u6027\uff0c\u5e76\u63a2\u8ba8\u4e86\u5176\u5728\u73b0\u4ee3\u5efa\u7b51\u4e2d\u7684\u6f5c\u5728\u5e94\u7528\u3002", "method": "\u7814\u7a76\u91c7\u7528\u4e86\u5f62\u72b6\u8bed\u6cd5\u5206\u6790\u65b9\u6cd5\uff0c\u9996\u5148\u91cd\u5efa\u4e86City Tower\u7684\u539f\u59cb\u7ed3\u6784\uff0c\u968f\u540e\u57fa\u4e8e\u5176\u51e0\u4f55\u5f62\u6001\u751f\u6210\u4e86\u65b0\u7684\u7ed3\u6784\u914d\u7f6e\u3002", "result": "\u7814\u7a76\u7ed3\u679c\u8868\u660e\uff0c\u56db\u9762\u4f53\u548c\u516b\u9762\u4f53\u51e0\u4f55\u53ef\u4ee5\u4f5c\u4e3a\u6a21\u5757\u5316\u548c\u53ef\u6269\u5c55\u8bbe\u8ba1\u7684\u57fa\u7840\uff0c\u4e3a\u5efa\u7b51\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u65b0\u7684\u53ef\u80fd\u6027\u3002", "conclusion": "\u7ed3\u8bba\u6307\u51fa\uff0cCity Tower\u7684\u51e0\u4f55\u6846\u67b6\u53ca\u5176\u884d\u751f\u914d\u7f6e\u5c55\u793a\u4e86\u9002\u5e94\u6027\u5efa\u7b51\u8bbe\u8ba1\u7684\u6f5c\u529b\uff0c\u4e3a\u672a\u6765\u7684\u5efa\u7b51\u521b\u65b0\u63d0\u4f9b\u4e86\u542f\u793a\u3002"}}
{"id": "2508.08572", "categories": ["cs.GR"], "pdf": "https://arxiv.org/pdf/2508.08572", "abs": "https://arxiv.org/abs/2508.08572", "authors": ["Michael Kleiss", "Seyedehaysan Mokhtarimousavi", "Sida Dai", "Mostafa Alani"], "title": "Bio-Generative Design Morphology with Radiolaria: An application of a Nature-Based Generative Shape Grammar for Geometrical Design of Space Frames", "comment": "12 pages, SiGradi Conference", "summary": "This paper presents a study on using Radiolaria as a basis for generation of\nspace-based geometry for structural design with shape grammars. Radiolaria has\nbeen a source of inspiration for architectural design with its intricate\nstructural features and geometric patterns (Lim, 2012). We use the basis of the\nRadiolaria geometry to create a generative shape grammar as a computational\nsystem; then use the shape grammar to create spatial configurations for\npotential applications in design of 3D space structural frames. This study\nbegins with the geometric analysis of Radiolaria and the dissection of its\nstructure and geometry into a simplified morphological source, in this case a\ntetrahedral structure. Tetrahedrons are used in combination with octahedrons to\ngenerate spatial configurations to generate 3D spatial structural frames. The\npaper presents the Radiolaria spatial analysis, the shape grammar, the\ncollection of generated designs, and possible applications in space frame\nstructures.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63a2\u8ba8\u4e86\u5229\u7528\u653e\u5c04\u866b\uff08Radiolaria\uff09\u7684\u51e0\u4f55\u7ed3\u6784\u4f5c\u4e3a\u57fa\u7840\uff0c\u901a\u8fc7\u5f62\u72b6\u8bed\u6cd5\u751f\u6210\u7a7a\u95f4\u7ed3\u6784\u8bbe\u8ba1\u7684\u65b9\u6cd5\uff0c\u5e76\u5c55\u793a\u4e86\u5176\u6f5c\u5728\u5e94\u7528\u3002", "motivation": "\u653e\u5c04\u866b\u56e0\u5176\u590d\u6742\u7684\u7ed3\u6784\u548c\u51e0\u4f55\u56fe\u6848\u5728\u5efa\u7b51\u8bbe\u8ba1\u4e2d\u5177\u6709\u542f\u53d1\u610f\u4e49\uff0c\u7814\u7a76\u65e8\u5728\u63a2\u7d22\u5982\u4f55\u5c06\u5176\u51e0\u4f55\u7279\u5f81\u8f6c\u5316\u4e3a\u7a7a\u95f4\u7ed3\u6784\u8bbe\u8ba1\u7684\u8ba1\u7b97\u7cfb\u7edf\u3002", "method": "\u7814\u7a76\u9996\u5148\u5206\u6790\u4e86\u653e\u5c04\u866b\u7684\u51e0\u4f55\u7ed3\u6784\uff0c\u5e76\u5c06\u5176\u7b80\u5316\u4e3a\u56db\u9762\u4f53\u7ed3\u6784\uff0c\u7ed3\u5408\u516b\u9762\u4f53\u751f\u62103D\u7a7a\u95f4\u7ed3\u6784\u6846\u67b6\uff0c\u8fdb\u800c\u5f00\u53d1\u751f\u6210\u5f62\u72b6\u8bed\u6cd5\u3002", "result": "\u7814\u7a76\u6210\u529f\u5f00\u53d1\u4e86\u57fa\u4e8e\u653e\u5c04\u866b\u51e0\u4f55\u7684\u5f62\u72b6\u8bed\u6cd5\uff0c\u5e76\u751f\u6210\u4e86\u4e00\u7cfb\u5217\u7a7a\u95f4\u7ed3\u6784\u914d\u7f6e\uff0c\u5c55\u793a\u4e86\u5728\u7a7a\u95f4\u6846\u67b6\u7ed3\u6784\u4e2d\u7684\u6f5c\u5728\u5e94\u7528\u3002", "conclusion": "\u8be5\u7814\u7a76\u901a\u8fc7\u653e\u5c04\u866b\u7684\u51e0\u4f55\u5206\u6790\u548c\u5f62\u72b6\u8bed\u6cd5\u5f00\u53d1\uff0c\u4e3a3D\u7a7a\u95f4\u7ed3\u6784\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u65b0\u7684\u65b9\u6cd5\u548c\u6f5c\u5728\u5e94\u7528\u65b9\u5411\u3002"}}
{"id": "2508.08754", "categories": ["cs.GR", "cs.CV", "cs.MM"], "pdf": "https://arxiv.org/pdf/2508.08754", "abs": "https://arxiv.org/abs/2508.08754", "authors": ["Qianru Qiu", "Jiafeng Mao", "Xueting Wang"], "title": "Exploring Palette based Color Guidance in Diffusion Models", "comment": "Accepted to ACM MM 2025", "summary": "With the advent of diffusion models, Text-to-Image (T2I) generation has seen\nsubstantial advancements. Current T2I models allow users to specify object\ncolors using linguistic color names, and some methods aim to personalize\ncolor-object association through prompt learning. However, existing models\nstruggle to provide comprehensive control over the color schemes of an entire\nimage, especially for background elements and less prominent objects not\nexplicitly mentioned in prompts. This paper proposes a novel approach to\nenhance color scheme control by integrating color palettes as a separate\nguidance mechanism alongside prompt instructions. We investigate the\neffectiveness of palette guidance by exploring various palette representation\nmethods within a diffusion-based image colorization framework. To facilitate\nthis exploration, we construct specialized palette-text-image datasets and\nconduct extensive quantitative and qualitative analyses. Our results\ndemonstrate that incorporating palette guidance significantly improves the\nmodel's ability to generate images with desired color schemes, enabling a more\ncontrolled and refined colorization process.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u901a\u8fc7\u5728\u6269\u6563\u6a21\u578b\u6846\u67b6\u4e2d\u5f15\u5165\u8c03\u8272\u677f\u5f15\u5bfc\u673a\u5236\u7684\u65b0\u65b9\u6cd5\uff0c\u4ee5\u589e\u5f3a\u6587\u672c\u5230\u56fe\u50cf\u751f\u6210\u4e2d\u5bf9\u6574\u4f53\u8272\u5f69\u65b9\u6848\u7684\u63a7\u5236\u3002", "motivation": "\u73b0\u6709\u7684\u6587\u672c\u5230\u56fe\u50cf\u751f\u6210\u6a21\u578b\u5728\u63a7\u5236\u56fe\u50cf\u7684\u6574\u4f53\u8272\u5f69\u65b9\u6848\uff0c\u5c24\u5176\u662f\u80cc\u666f\u5143\u7d20\u548c\u672a\u660e\u786e\u63d0\u53ca\u5bf9\u8c61\u7684\u8272\u5f69\u65f6\u8868\u73b0\u4e0d\u4f73\uff0c\u7f3a\u4e4f\u5bf9\u8272\u5f69\u7ec4\u5408\u7684\u5168\u9762\u63a7\u5236\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u5c06\u8c03\u8272\u677f\u4f5c\u4e3a\u72ec\u7acb\u5f15\u5bfc\u673a\u5236\u7684\u65b9\u6cd5\uff0c\u7ed3\u5408\u63d0\u793a\u6307\u4ee4\uff0c\u63a2\u7d22\u4e86\u591a\u79cd\u8c03\u8272\u677f\u8868\u793a\u65b9\u6cd5\uff0c\u5e76\u6784\u5efa\u4e86\u4e13\u95e8\u7684\u8c03\u8272\u677f-\u6587\u672c-\u56fe\u50cf\u6570\u636e\u96c6\u8fdb\u884c\u5b9e\u9a8c\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8c03\u8272\u677f\u5f15\u5bfc\u663e\u8457\u63d0\u5347\u4e86\u6a21\u578b\u751f\u6210\u5177\u6709\u6240\u9700\u8272\u5f69\u65b9\u6848\u56fe\u50cf\u7684\u80fd\u529b\uff0c\u5b9e\u73b0\u4e86\u66f4\u53ef\u63a7\u548c\u7cbe\u7ec6\u7684\u8272\u5f69\u5316\u8fc7\u7a0b\u3002", "conclusion": "\u901a\u8fc7\u5f15\u5165\u8c03\u8272\u677f\u5f15\u5bfc\u673a\u5236\uff0c\u672c\u6587\u6709\u6548\u6539\u8fdb\u4e86\u6587\u672c\u5230\u56fe\u50cf\u751f\u6210\u4e2d\u5bf9\u6574\u4f53\u8272\u5f69\u65b9\u6848\u7684\u63a7\u5236\u80fd\u529b\uff0c\u4e3a\u66f4\u7cbe\u7ec6\u7684\u56fe\u50cf\u751f\u6210\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2508.08826", "categories": ["cs.GR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.08826", "abs": "https://arxiv.org/abs/2508.08826", "authors": ["Meng Gai", "Guoping Wang", "Sheng Li"], "title": "Geometry-Aware Global Feature Aggregation for Real-Time Indirect Illumination", "comment": "10 pages", "summary": "Real-time rendering with global illumination is crucial to afford the user\nrealistic experience in virtual environments. We present a learning-based\nestimator to predict diffuse indirect illumination in screen space, which then\nis combined with direct illumination to synthesize globally-illuminated high\ndynamic range (HDR) results. Our approach tackles the challenges of capturing\nlong-range/long-distance indirect illumination when employing neural networks\nand is generalized to handle complex lighting and scenarios.\n  From the neural network thinking of the solver to the rendering equation, we\npresent a novel network architecture to predict indirect illumination. Our\nnetwork is equipped with a modified attention mechanism that aggregates global\ninformation guided by spacial geometry features, as well as a monochromatic\ndesign that encodes each color channel individually.\n  We conducted extensive evaluations, and the experimental results demonstrate\nour superiority over previous learning-based techniques. Our approach excels at\nhandling complex lighting such as varying-colored lighting and environment\nlighting. It can successfully capture distant indirect illumination and\nsimulates the interreflections between textured surfaces well (i.e., color\nbleeding effects); it can also effectively handle new scenes that are not\npresent in the training dataset.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5b66\u4e60\u7684\u5c4f\u5e55\u7a7a\u95f4\u6f2b\u53cd\u5c04\u95f4\u63a5\u5149\u7167\u9884\u6d4b\u65b9\u6cd5\uff0c\u901a\u8fc7\u7ed3\u5408\u76f4\u63a5\u5149\u7167\u5408\u6210\u9ad8\u52a8\u6001\u8303\u56f4\u7ed3\u679c\uff0c\u4ee5\u89e3\u51b3\u795e\u7ecf\u7f51\u7edc\u5904\u7406\u957f\u8ddd\u79bb\u95f4\u63a5\u5149\u7167\u7684\u6311\u6218\u3002", "motivation": "\u5b9e\u65f6\u5168\u5c40\u5149\u7167\u6e32\u67d3\u5bf9\u4e8e\u865a\u62df\u73af\u5883\u4e2d\u7684\u7528\u6237\u4f53\u9a8c\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u795e\u7ecf\u7f51\u7edc\u5728\u6355\u6349\u957f\u8ddd\u79bb\u95f4\u63a5\u5149\u7167\u65b9\u9762\u5b58\u5728\u6311\u6218\u3002\u8bba\u6587\u65e8\u5728\u901a\u8fc7\u5b66\u4e60\u65b9\u6cd5\u63d0\u4f9b\u4e00\u79cd\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u578b\u7f51\u7edc\u67b6\u6784\uff0c\u91c7\u7528\u6539\u8fdb\u7684\u6ce8\u610f\u529b\u673a\u5236\u805a\u5408\u5168\u5c40\u4fe1\u606f\uff0c\u5e76\u901a\u8fc7\u5355\u8272\u8bbe\u8ba1\u72ec\u7acb\u7f16\u7801\u6bcf\u4e2a\u989c\u8272\u901a\u9053\uff0c\u4ece\u800c\u9884\u6d4b\u95f4\u63a5\u5149\u7167\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u5904\u7406\u590d\u6742\u5149\u7167\uff08\u5982\u591a\u8272\u5149\u7167\u548c\u73af\u5883\u5149\u7167\uff09\u548c\u6355\u6349\u8fdc\u8ddd\u79bb\u95f4\u63a5\u5149\u7167\u65b9\u9762\u4f18\u4e8e\u73b0\u6709\u5b66\u4e60\u6280\u672f\uff0c\u5e76\u80fd\u6709\u6548\u5904\u7406\u8bad\u7ec3\u6570\u636e\u4e2d\u672a\u51fa\u73b0\u8fc7\u7684\u65b0\u573a\u666f\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u6210\u529f\u89e3\u51b3\u4e86\u957f\u8ddd\u79bb\u95f4\u63a5\u5149\u7167\u6355\u6349\u7684\u96be\u9898\uff0c\u5e76\u5728\u590d\u6742\u5149\u7167\u548c\u573a\u666f\u4e2d\u5c55\u73b0\u51fa\u4f18\u8d8a\u6027\u80fd\uff0c\u4e3a\u5b9e\u65f6\u5168\u5c40\u5149\u7167\u6e32\u67d3\u63d0\u4f9b\u4e86\u65b0\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2508.08831", "categories": ["cs.GR", "cs.CV", "cs.RO"], "pdf": "https://arxiv.org/pdf/2508.08831", "abs": "https://arxiv.org/abs/2508.08831", "authors": ["Bo-Hsun Chen", "Nevindu M. Batagoda", "Dan Negrut"], "title": "DiffPhysCam: Differentiable Physics-Based Camera Simulation for Inverse Rendering and Embodied AI", "comment": "19 pages, 17 figures, and 4 tables", "summary": "We introduce DiffPhysCam, a differentiable camera simulator designed to\nsupport robotics and embodied AI applications by enabling gradient-based\noptimization in visual perception pipelines. Generating synthetic images that\nclosely mimic those from real cameras is essential for training visual models\nand enabling end-to-end visuomotor learning. Moreover, differentiable rendering\nallows inverse reconstruction of real-world scenes as digital twins,\nfacilitating simulation-based robotics training. However, existing virtual\ncameras offer limited control over intrinsic settings, poorly capture optical\nartifacts, and lack tunable calibration parameters -- hindering sim-to-real\ntransfer. DiffPhysCam addresses these limitations through a multi-stage\npipeline that provides fine-grained control over camera settings, models key\noptical effects such as defocus blur, and supports calibration with real-world\ndata. It enables both forward rendering for image synthesis and inverse\nrendering for 3D scene reconstruction, including mesh and material texture\noptimization. We show that DiffPhysCam enhances robotic perception performance\nin synthetic image tasks. As an illustrative example, we create a digital twin\nof a real-world scene using inverse rendering, simulate it in a multi-physics\nenvironment, and demonstrate navigation of an autonomous ground vehicle using\nimages generated by DiffPhysCam.", "AI": {"tldr": "DiffPhysCam\u662f\u4e00\u79cd\u53ef\u5fae\u5206\u7684\u76f8\u673a\u6a21\u62df\u5668\uff0c\u65e8\u5728\u901a\u8fc7\u652f\u6301\u68af\u5ea6\u4f18\u5316\u6765\u63d0\u5347\u673a\u5668\u4eba\u548cAI\u7684\u89c6\u89c9\u611f\u77e5\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u7684\u865a\u62df\u76f8\u673a\u5728\u63a7\u5236\u5185\u5728\u8bbe\u7f6e\u3001\u5149\u5b66\u6548\u679c\u6a21\u62df\u548c\u6821\u51c6\u53c2\u6570\u65b9\u9762\u5b58\u5728\u5c40\u9650\uff0c\u5f71\u54cd\u4e86\u4ece\u6a21\u62df\u5230\u73b0\u5b9e\u7684\u8f6c\u6362\u6548\u679c\u3002DiffPhysCam\u65e8\u5728\u514b\u670d\u8fd9\u4e9b\u9650\u5236\u3002", "method": "DiffPhysCam\u91c7\u7528\u591a\u9636\u6bb5\u6d41\u7a0b\uff0c\u63d0\u4f9b\u5bf9\u76f8\u673a\u8bbe\u7f6e\u7684\u7cbe\u7ec6\u63a7\u5236\uff0c\u6a21\u62df\u5149\u5b66\u6548\u679c\uff08\u5982\u6563\u7126\u6a21\u7cca\uff09\uff0c\u5e76\u652f\u6301\u57fa\u4e8e\u771f\u5b9e\u6570\u636e\u7684\u6821\u51c6\u3002\u540c\u65f6\u652f\u6301\u6b63\u5411\u6e32\u67d3\u548c\u9006\u5411\u6e32\u67d3\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cDiffPhysCam\u63d0\u5347\u4e86\u673a\u5668\u4eba\u611f\u77e5\u6027\u80fd\uff0c\u5e76\u5728\u9006\u5411\u6e32\u67d3\u4e2d\u6210\u529f\u521b\u5efa\u4e86\u771f\u5b9e\u573a\u666f\u7684\u6570\u5b57\u5b6a\u751f\uff0c\u6a21\u62df\u4e86\u591a\u7269\u7406\u73af\u5883\u4e2d\u7684\u81ea\u4e3b\u5730\u9762\u8f66\u8f86\u5bfc\u822a\u3002", "conclusion": "DiffPhysCam\u901a\u8fc7\u63d0\u4f9b\u5168\u9762\u7684\u76f8\u673a\u6a21\u62df\u548c\u4f18\u5316\u529f\u80fd\uff0c\u4e3a\u673a\u5668\u4eba\u548cAI\u89c6\u89c9\u4efb\u52a1\u63d0\u4f9b\u4e86\u9ad8\u6548\u7684\u5de5\u5177\u3002"}}
{"id": "2508.08930", "categories": ["cs.GR"], "pdf": "https://arxiv.org/pdf/2508.08930", "abs": "https://arxiv.org/abs/2508.08930", "authors": ["Juyeong Hwang", "Seong-Eun Hon", "JaeYoung Seon", "Hyeongyeop Kang"], "title": "How Does a Virtual Agent Decide Where to Look? -- Symbolic Cognitive Reasoning for Embodied Head Rotation", "comment": null, "summary": "Natural head rotation is critical for believable embodied virtual agents, yet\nthis micro-level behavior remains largely underexplored. While head-rotation\nprediction algorithms could, in principle, reproduce this behavior, they\ntypically focus on visually salient stimuli and overlook the cognitive motives\nthat guide head rotation. This yields agents that look at conspicuous objects\nwhile overlooking obstacles or task-relevant cues, diminishing realism in a\nvirtual environment. We introduce SCORE, a Symbolic Cognitive Reasoning\nframework for Embodied Head Rotation, a data-agnostic framework that produces\ncontext-aware head movements without task-specific training or hand-tuned\nheuristics. A controlled VR study (N=20) identifies five motivational drivers\nof human head movements: Interest, Information Seeking, Safety, Social Schema,\nand Habit. SCORE encodes these drivers as symbolic predicates, perceives the\nscene with a Vision-Language Model (VLM), and plans head poses with a Large\nLanguage Model (LLM). The framework employs a hybrid workflow: the VLM-LLM\nreasoning is executed offline, after which a lightweight FastVLM performs\nonline validation to suppress hallucinations while maintaining responsiveness\nto scene dynamics. The result is an agent that predicts not only where to look\nbut also why, generalizing to unseen scenes and multi-agent crowds while\nretaining behavioral plausibility.", "AI": {"tldr": "\u8bba\u6587\u4ecb\u7ecd\u4e86\u4e00\u79cd\u540d\u4e3aSCORE\u7684\u7b26\u53f7\u8ba4\u77e5\u63a8\u7406\u6846\u67b6\uff0c\u7528\u4e8e\u751f\u6210\u865a\u62df\u4ee3\u7406\u7684\u81ea\u7136\u5934\u90e8\u65cb\u8f6c\u884c\u4e3a\uff0c\u901a\u8fc7\u5206\u6790\u4eba\u7c7b\u5934\u90e8\u8fd0\u52a8\u7684\u4e94\u79cd\u52a8\u673a\u9a71\u52a8\u56e0\u7d20\uff0c\u5e76\u7ed3\u5408\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u548c\u5927\u8bed\u8a00\u6a21\u578b\uff0c\u5b9e\u73b0\u4e86\u4e0a\u4e0b\u6587\u611f\u77e5\u7684\u5934\u90e8\u8fd0\u52a8\u3002", "motivation": "\u5f53\u524d\u5934\u90e8\u65cb\u8f6c\u9884\u6d4b\u7b97\u6cd5\u4e3b\u8981\u5173\u6ce8\u89c6\u89c9\u663e\u7740\u523a\u6fc0\uff0c\u5ffd\u7565\u4e86\u8ba4\u77e5\u52a8\u673a\uff0c\u5bfc\u81f4\u865a\u62df\u4ee3\u7406\u884c\u4e3a\u4e0d\u771f\u5b9e\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\uff0c\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u80fd\u6a21\u62df\u4eba\u7c7b\u5934\u90e8\u8fd0\u52a8\u52a8\u673a\u7684\u6846\u67b6\u3002", "method": "SCORE\u6846\u67b6\u901a\u8fc7\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u611f\u77e5\u573a\u666f\uff0c\u5e76\u7ed3\u5408\u5927\u8bed\u8a00\u6a21\u578b\u89c4\u5212\u5934\u90e8\u59ff\u6001\u3002\u91c7\u7528\u79bb\u7ebf\u63a8\u7406\u548c\u5728\u7ebf\u9a8c\u8bc1\u7684\u6df7\u5408\u5de5\u4f5c\u6d41\uff0c\u786e\u4fdd\u751f\u6210\u7684\u5934\u90e8\u8fd0\u52a8\u65e2\u7b26\u5408\u4e0a\u4e0b\u6587\u53c8\u4fdd\u6301\u5b9e\u65f6\u54cd\u5e94\u3002", "result": "\u901a\u8fc7\u865a\u62df\u73b0\u5b9e\u5b9e\u9a8c\u9a8c\u8bc1\uff0cSCORE\u80fd\u591f\u6a21\u62df\u4e94\u79cd\u52a8\u673a\u9a71\u52a8\u7684\u5934\u90e8\u8fd0\u52a8\uff08\u5174\u8da3\u3001\u4fe1\u606f\u5bfb\u6c42\u3001\u5b89\u5168\u3001\u793e\u4f1a\u4e60\u60ef\u548c\u4e60\u60ef\uff09\uff0c\u5e76\u5728\u672a\u89c1\u8fc7\u7684\u65b0\u573a\u666f\u548c\u591a\u4ee3\u7406\u7fa4\u4f53\u4e2d\u4fdd\u6301\u884c\u4e3a\u5408\u7406\u6027\u3002", "conclusion": "SCORE\u6846\u67b6\u4e3a\u865a\u62df\u4ee3\u7406\u7684\u5934\u90e8\u8fd0\u52a8\u63d0\u4f9b\u4e86\u4e00\u79cd\u65e0\u9700\u4efb\u52a1\u7279\u5b9a\u8bad\u7ec3\u6216\u624b\u52a8\u8c03\u6574\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u663e\u8457\u63d0\u5347\u4e86\u865a\u62df\u73af\u5883\u4e2d\u7684\u884c\u4e3a\u548c\u4ea4\u4e92\u7684\u771f\u5b9e\u6027\u3002"}}
{"id": "2508.09062", "categories": ["cs.GR", "cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.09062", "abs": "https://arxiv.org/abs/2508.09062", "authors": ["Xiang Zhang", "Yawar Siddiqui", "Armen Avetisyan", "Chris Xie", "Jakob Engel", "Henry Howard-Jenkins"], "title": "VertexRegen: Mesh Generation with Continuous Level of Detail", "comment": "ICCV 2025. Project Page: https://vertexregen.github.io/", "summary": "We introduce VertexRegen, a novel mesh generation framework that enables\ngeneration at a continuous level of detail. Existing autoregressive methods\ngenerate meshes in a partial-to-complete manner and thus intermediate steps of\ngeneration represent incomplete structures. VertexRegen takes inspiration from\nprogressive meshes and reformulates the process as the reversal of edge\ncollapse, i.e. vertex split, learned through a generative model. Experimental\nresults demonstrate that VertexRegen produces meshes of comparable quality to\nstate-of-the-art methods while uniquely offering anytime generation with the\nflexibility to halt at any step to yield valid meshes with varying levels of\ndetail.", "AI": {"tldr": "VertexRegen\u662f\u4e00\u79cd\u65b0\u578b\u7684\u7f51\u683c\u751f\u6210\u6846\u67b6\uff0c\u652f\u6301\u8fde\u7eed\u7ec6\u8282\u7ea7\u522b\u7684\u751f\u6210\uff0c\u901a\u8fc7\u9006\u5411\u8fb9\u7f18\u6298\u53e0\uff08\u9876\u70b9\u5206\u88c2\uff09\u5b66\u4e60\u751f\u6210\u6a21\u578b\uff0c\u5b9e\u73b0\u968f\u65f6\u505c\u6b62\u751f\u6210\u5e76\u8f93\u51fa\u6709\u6548\u7f51\u683c\u3002", "motivation": "\u73b0\u6709\u7684\u81ea\u56de\u5f52\u65b9\u6cd5\u4ee5\u90e8\u5206\u5230\u5b8c\u6574\u7684\u65b9\u5f0f\u751f\u6210\u7f51\u683c\uff0c\u4e2d\u95f4\u6b65\u9aa4\u4ee3\u8868\u4e0d\u5b8c\u6574\u7ed3\u6784\uff0c\u65e0\u6cd5\u7075\u6d3b\u63a7\u5236\u7ec6\u8282\u7ea7\u522b\u3002VertexRegen\u7684\u76ee\u6807\u662f\u514b\u670d\u8fd9\u4e00\u9650\u5236\u3002", "method": "VertexRegen\u53d7\u6e10\u8fdb\u7f51\u683c\u542f\u53d1\uff0c\u5c06\u751f\u6210\u8fc7\u7a0b\u91cd\u65b0\u5b9a\u4e49\u4e3a\u901a\u8fc7\u751f\u6210\u6a21\u578b\u5b66\u4e60\u7684\u9006\u5411\u8fb9\u7f18\u6298\u53e0\uff08\u9876\u70b9\u5206\u88c2\uff09\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cVertexRegen\u751f\u6210\u7684\u7f51\u683c\u8d28\u91cf\u4e0e\u6700\u5148\u8fdb\u65b9\u6cd5\u76f8\u5f53\uff0c\u540c\u65f6\u5177\u6709\u968f\u65f6\u751f\u6210\u548c\u7075\u6d3b\u505c\u6b62\u7684\u4f18\u52bf\u3002", "conclusion": "VertexRegen\u901a\u8fc7\u9876\u70b9\u5206\u88c2\u5b9e\u73b0\u4e86\u8fde\u7eed\u7ec6\u8282\u7ea7\u522b\u7684\u7f51\u683c\u751f\u6210\uff0c\u4e3a\u7f51\u683c\u751f\u6210\u63d0\u4f9b\u4e86\u66f4\u9ad8\u7684\u7075\u6d3b\u6027\u548c\u5b9e\u7528\u6027\u3002"}}
{"id": "2508.09131", "categories": ["cs.GR", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2508.09131", "abs": "https://arxiv.org/abs/2508.09131", "authors": ["Zixin Yin", "Xili Dai", "Ling-Hao Chen", "Deyu Zhou", "Jianan Wang", "Duomin Wang", "Gang Yu", "Lionel M. Ni", "Heung-Yeung Shum"], "title": "Training-Free Text-Guided Color Editing with Multi-Modal Diffusion Transformer", "comment": null, "summary": "Text-guided color editing in images and videos is a fundamental yet unsolved\nproblem, requiring fine-grained manipulation of color attributes, including\nalbedo, light source color, and ambient lighting, while preserving physical\nconsistency in geometry, material properties, and light-matter interactions.\nExisting training-free methods offer broad applicability across editing tasks\nbut struggle with precise color control and often introduce visual\ninconsistency in both edited and non-edited regions. In this work, we present\nColorCtrl, a training-free color editing method that leverages the attention\nmechanisms of modern Multi-Modal Diffusion Transformers (MM-DiT). By\ndisentangling structure and color through targeted manipulation of attention\nmaps and value tokens, our method enables accurate and consistent color\nediting, along with word-level control of attribute intensity. Our method\nmodifies only the intended regions specified by the prompt, leaving unrelated\nareas untouched. Extensive experiments on both SD3 and FLUX.1-dev demonstrate\nthat ColorCtrl outperforms existing training-free approaches and achieves\nstate-of-the-art performances in both edit quality and consistency.\nFurthermore, our method surpasses strong commercial models such as FLUX.1\nKontext Max and GPT-4o Image Generation in terms of consistency. When extended\nto video models like CogVideoX, our approach exhibits greater advantages,\nparticularly in maintaining temporal coherence and editing stability. Finally,\nour method also generalizes to instruction-based editing diffusion models such\nas Step1X-Edit and FLUX.1 Kontext dev, further demonstrating its versatility.", "AI": {"tldr": "ColorCtrl\u662f\u4e00\u79cd\u65e0\u9700\u8bad\u7ec3\u7684\u989c\u8272\u7f16\u8f91\u65b9\u6cd5\uff0c\u5229\u7528\u591a\u6a21\u6001\u6269\u6563\u53d8\u6362\u5668\u7684\u6ce8\u610f\u529b\u673a\u5236\uff0c\u5b9e\u73b0\u4e86\u5bf9\u56fe\u50cf\u548c\u89c6\u9891\u4e2d\u989c\u8272\u7684\u7cbe\u786e\u7f16\u8f91\uff0c\u540c\u65f6\u4fdd\u6301\u7269\u7406\u4e00\u81f4\u6027\u3002", "motivation": "\u73b0\u6709\u7684\u65e0\u9700\u8bad\u7ec3\u7684\u989c\u8272\u7f16\u8f91\u65b9\u6cd5\u5728\u7cbe\u786e\u63a7\u5236\u989c\u8272\u548c\u4fdd\u6301\u89c6\u89c9\u4e00\u81f4\u6027\u65b9\u9762\u8868\u73b0\u4e0d\u4f73\uff0c\u5c24\u5176\u662f\u5728\u7f16\u8f91\u548c\u975e\u7f16\u8f91\u533a\u57df\u4e4b\u95f4\u5bb9\u6613\u4ea7\u751f\u4e0d\u4e00\u81f4\u3002", "method": "ColorCtrl\u901a\u8fc7\u89e3\u6784\u7ed3\u6784\u548c\u989c\u8272\uff0c\u5e76\u5b9a\u5411\u64cd\u4f5c\u6ce8\u610f\u529b\u56fe\u548c\u503c\u6807\u8bb0\uff0c\u5229\u7528\u591a\u6a21\u6001\u6269\u6563\u53d8\u6362\u5668\u7684\u6ce8\u610f\u529b\u673a\u5236\uff0c\u5b9e\u73b0\u4e86\u51c6\u786e\u7684\u533a\u57df\u5316\u989c\u8272\u7f16\u8f91\u3002", "result": "ColorCtrl\u5728SD3\u548cFLUX.1-dev\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\u5176\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5e76\u5728\u7f16\u8f91\u8d28\u91cf\u548c\u4e00\u81f4\u6027\u65b9\u9762\u8fbe\u5230\u6700\u65b0\u6c34\u5e73\uff0c\u751a\u81f3\u8d85\u8fc7\u4e86FLUX.1 Kontext Max\u548cGPT-4o\u7b49\u5546\u4e1a\u6a21\u578b\u3002", "conclusion": "ColorCtrl\u4e0d\u4ec5\u5728\u56fe\u50cf\u548c\u89c6\u9891\u7f16\u8f91\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u8fd8\u80fd\u6269\u5c55\u5230\u57fa\u4e8e\u6307\u4ee4\u7684\u7f16\u8f91\u6269\u6563\u6a21\u578b\uff0c\u5c55\u73b0\u4e86\u5176\u5e7f\u6cdb\u7684\u9002\u7528\u6027\u548c\u4f18\u52bf\u3002"}}
