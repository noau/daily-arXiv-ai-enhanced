<div id=toc></div>

# Table of Contents

- [cs.GR](#cs.GR) [Total: 1]
- [cs.PL](#cs.PL) [Total: 8]
- [cs.GT](#cs.GT) [Total: 5]


<div id='cs.GR'></div>

# cs.GR [[Back]](#toc)

### [1] [OpenPBR: Novel Features and Implementation Details](https://arxiv.org/abs/2512.23696)
*Jamie Portsmouth,Peter Kutz,Stephen Hill*

Main category: cs.GR

TL;DR: OpenPBR是一个基于物理的标准超级着色器，旨在为VFX、动画和设计可视化工作流提供可互操作的材料创作和渲染。本文档深入探讨了模型开发、实现细节和技术主题。


<details>
  <summary>Details</summary>
Motivation: 开发OpenPBR的目的是为了在VFX、动画和设计可视化等领域中，提供一个标准化、基于物理的超级着色器，以实现材料的互操作性。

Method: OpenPBR结合了分层理论（如基于板的分层、统计混合和微面理论）以及物理组件（如金属、电介质、次表面和光泽-漫反射基底），同时支持特殊案例模式（如薄壁对象渲染）。

Result: 文档提供了详细的实现指导，包括代码示例和数学推导，并探讨了技术主题如解耦反射与传输、次表面散射参数化选择等。

Conclusion: OpenPBR是一个全面且灵活的模型，支持多种材料和渲染需求，并计划未来扩展如雾状镜面反射和回射等功能。

Abstract: OpenPBR is a physically based, standardized uber-shader developed for interoperable material authoring and rendering across VFX, animation, and design visualization workflows. This document serves as a companion to the official specification, offering deeper insight into the model's development and more detailed implementation guidance, including code examples and mathematical derivations.
  We begin with a description of the model's formal structure and theoretical foundations - covering slab-based layering, statistical mixing, and microfacet theory - before turning to its physical components. These include metallic, dielectric, subsurface, and glossy-diffuse base substrates, followed by thin-film iridescence, coat, and fuzz layers. A special-case mode for rendering thin-walled objects is also described.
  Additional sections explore technical topics in greater depth, such as the decoupling of specular reflectivity from transmission, the choice of parameterization for subsurface scattering, and the detailed physics of coat darkening and thin-film interference. We also discuss planned extensions, including hazy specular reflection and retroreflection.

</details>


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [2] [Symbolic Specification and Reasoning for Quantum Data and Operations](https://arxiv.org/abs/2512.22383)
*Mingsheng Ying*

Main category: cs.PL

TL;DR: 本文提出了一种名为符号操作逻辑（SOL）的通用逻辑框架，用于量子数据和操作的符号化规范和推理，填补了量子计算领域缺乏形式理论的空白。


<details>
  <summary>Details</summary>
Motivation: 量子计算领域缺乏关于量子数据和操作的符号化规范与推理的形式理论，限制了自动化验证技术的实际应用。为此，作者提出SOL框架来解决这一问题。

Method: 作者设计了SOL框架，将经典的一阶逻辑嵌入到形式操作语言中，以规范量子数据及操作，包括递归定义，并利用现有经典计算的自动化验证工具进行推理。

Result: SOL框架为量子计算和信息的形式化验证提供了理论基础，适用于Lean、Coq等证明辅助系统中的自动化定理证明。

Conclusion: SOL框架填补了量子计算符号化规范与推理的理论空白，为量子算法和程序的自动化验证提供了有力支持。

Abstract: In quantum information and computation research, symbolic methods have been widely used for human specification and reasoning about quantum states and operations. At the same time, they are essential for ensuring the scalability and efficiency of automated reasoning and verification tools for quantum algorithms and programs. However, a formal theory for symbolic specification and reasoning about quantum data and operations is still lacking, which significantly limits the practical applicability of automated verification techniques in quantum computing.
  In this paper, we present a general logical framework, called Symbolic Operator Logic $\mathbf{SOL}$, which enables symbolic specification and reasoning about quantum data and operations. Within this framework, a classical first-order logical language is embedded into a language of formal operators used to specify quantum data and operations, including their recursive definitions. This embedding allows reasoning about their properties modulo a chosen theory of the underlying classical data (e.g., Boolean algebra or group theory), thereby leveraging existing automated verification tools developed for classical computing. It should be emphasised that this embedding of classical first-order logic into $\mathbf{SOL}$ is precisely what makes the symbolic method possible.
  We envision that this framework can provide a conceptual foundation for the formal verification and automated theorem proving of quantum computation and information in proof assistants such as Lean, Coq, and related systems.

</details>


### [3] [Eliminate Branches by Melding IR Instructions](https://arxiv.org/abs/2512.22390)
*Yuze Li,Srinivasan Ramachandra Sharma,Charitha Saumya,Ali R. Butt,Kirshanthan Sundararajah*

Main category: cs.PL

TL;DR: MERIT是一种编译器优化技术，通过对齐和融合指令级的分支路径相似操作来消除分支，显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 现代处理器中分支预测失败会导致严重的性能损失，现有的硬件预测和配置文件指导技术难以应对数据依赖性分支及不规则模式，传统if-conversion在x86等架构上存在局限性。

Method: MERIT是一种编译器转换技术，通过指令级对齐和融合相似操作（使用序列对齐发现合并机会，并通过安全操作数级保护确保语义正确性）来消除分支。

Result: 在四个基准测试套件的102个程序上评估，MERIT实现了10.9%的几何平均加速，最高性能提升达32倍，静态指令开销显著减少。

Conclusion: MERIT通过软件方法有效替代硬件分支预测，展示了在高性能计算中的潜力与优势。

Abstract: Branch mispredictions cause catastrophic performance penalties in modern processors, leading to performance loss. While hardware predictors and profile-guided techniques exist, data-dependent branches with irregular patterns remain challenging. Traditional if-conversion eliminates branches via software predication but faces limitations on architectures like x86. It often fails on paths containing memory instructions or incurs excessive instruction overhead by fully speculating large branch bodies.
  This paper presents Melding IR Instructions (MERIT), a compiler transformation that eliminates branches by aligning and melding similar operations from divergent paths at the IR instruction level. By observing that divergent paths often perform structurally similar operations with different operands, MERIT adapts sequence alignment to discover merging opportunities and employs safe operand-level guarding to ensure semantic correctness without hardware predication. Implemented as an LLVM pass and evaluated on 102 programs from four benchmark suites, MERIT achieves a geometric mean speedup of 10.9% with peak improvements of 32x compared to hardware branch predictor, demonstrating the effectiveness with reduced static instruction overhead.

</details>


### [4] [A Bounded Game Semantics Checker for Precise Smart Contract Analysis](https://arxiv.org/abs/2512.22417)
*Vasileios Koutavas,Yu-Yang Lin,Nikos Tzevelekos*

Main category: cs.PL

TL;DR: 本文提出了一种基于游戏语义的新方法来精确检测智能合约漏洞，该方法在EVM-Yul解释器中无假阳性，有界完备，并通过领域知识扩展到实际合约。


<details>
  <summary>Details</summary>
Motivation: 智能合约漏洞检测需要精确且可扩展的方法，以减少假阳性并覆盖实际应用中的漏洞。

Method: 方法基于游戏语义，将计算建模为合约与环境之间的交互，并通过YulToolkit工具实现有界游戏语义检查。

Result: YulToolkit在DAO、PredyPool和Lendf.Me等实际案例中成功检测到已知漏洞并生成违规触发路径，且在修复后未报告违规。

Conclusion: 有界游戏语义探索是智能合约分析工具箱的有效补充，尤其适用于如重入漏洞等难以精确检测的问题。

Abstract: We present a new approach to finding smart contract vulnerabilities that is precise (no false positives up to our EVM-Yul interpreter), bounded-complete, and, when instrumented with domain knowledge, scales to real-world contracts. Our method is based on game semantics, modelling computation as an interaction between a contract and its environment, reducing reasoning about unknown or malicious external contracts to trace enumeration. We implement this in a tool we refer to as YulToolkit, a bounded game-semantics checker for Yul, the intermediate language of Solidity. By exploring only feasible interactions, YulToolkit avoids over-approximation, and by relying on the theory of game semantics it achieves bounded completeness. To make exploration tractable, YulToolkit supports instrumentation written in Solidity and propagated to Yul, comparable in effort to creating a test harness. Unlike tests, however, our technique explores all admissible traces within the chosen parameters and bounds. We evaluate YulToolkit on three real-world incidents: The DAO, PredyPool, and Lendf.Me, as well as benchmark contracts. In all cases, YulToolkit detects the known vulnerabilities (producing a violation-triggering trace), and after applying fixes, reports no further violations within bounds. These results show that bounded game semantics exploration is an effective and precise addition to the smart contract analysis toolbox, particularly for vulnerabilities such as reentrancy that are hard to detect precisely in real code.

</details>


### [5] [Compiling Gradual Types with Evidence](https://arxiv.org/abs/2512.22684)
*José Luis Romero,Cristóbal Isla,Matías Toro,Éric Tanter*

Main category: cs.PL

TL;DR: 这篇论文探讨了如何在结构性类型语言中有效支持渐进类型，提出并实现了基于证据的编译器GrEv，其性能在基准测试中表现优异，甚至优于基于强制转换的编译器。


<details>
  <summary>Details</summary>
Motivation: 当前渐进类型在结构性类型语言中的实现主要依赖于强制转换和单调引用，而基于抽象解释的证据语义虽表达力强，但其在实际实现中的效率尚不明确。本文旨在探索证据语义是否能有效实现高效的渐进类型编译器。

Method: 论文设计并实现了基于证据的编译器GrEv，提出了新颖的单调语义，并解释了如何弥合形式语义与编译器实现之间的差距。此外，论文使用Grift基准测试套件对GrEv的性能进行了评估。

Result: 实验结果表明，GrEv在性能上可以与基于强制转换的编译器竞争，甚至在某些配置下表现更优，同时在静态到动态谱系中表现出更高的稳定性。

Conclusion: 这项工作不仅丰富了渐进类型编译器的设计空间，还为AGTs形式推导的多种高级渐进类型语言特征的高效实现开辟了新的研究途径。

Abstract: Efficiently supporting sound gradual typing in a language with structural types is challenging. To date, the Grift compiler is the only close-to-the-metal implementation of gradual typing in this setting, exploiting coercions for runtime checks, and further extended with monotonic references for efficient access to statically-typed data structures. On the language design and semantics side, the Abstracting Gradual Typing (AGT) methodology has proven fruitful to elucidate existing designs and to innovate by deriving gradualizations of a wide variety of typing disciplines and language features. Grounded in abstract interpretation, the Curry-Howard inspired runtime semantics of AGT is based on the notion of evidence for consistent judgments that evolve during reduction, monitoring the plausibility of well-typedness. While expressive and versatile, it is unclear whether such evidence-based semantics are a viable route to realize an efficient implementation of gradual typing.
  In this work, we explore this question by designing, implementing, and evaluating an evidence-based compiler, called GrEv. We explain how to bridge the gap between the formal semantics and the GrEv compiler implementation, and identify novel monotonic semantics. We empirically evaluate the performance of GrEv on the Grift benchmark suite. The results show that an evidence-based compiler can be competitive with, and even faster than, a coercion-based compiler, exhibiting more stability across configurations on the static-to-dynamic spectrum. In addition to enriching the space of gradual typing compilers, this work opens a direct door to exploring efficient implementations of the many advanced gradual typing disciplines formally derived with AGT in the literature.

</details>


### [6] [Fancy Some Chips for Your TeaStore? Modeling the Control of an Adaptable Discrete System](https://arxiv.org/abs/2512.23496)
*Anna Gallone,Simon Bliudze,Sophie Cerf,Olga Kouchnarenko*

Main category: cs.PL

TL;DR: Chips是一种语言，旨在简化由多种交织组件组成的模型设计。它允许以功能块的形式描述应用，并结合控制理论和通用编程语言的概念，生成稳健的基于组件的模型。


<details>
  <summary>Details</summary>
Motivation: 在设计新网络应用时，开发者需应对多种资源约束（如软件、硬件、网络、在线微服务等），这些实体形成复杂的通信依赖系统。确保系统稳健性以提供优质服务至关重要。

Method: 本文介绍了Chips语言，它通过混合控制理论和通用编程语言的概念，支持以功能块的形式建模应用。以一个改进版的Adaptable TeaStore应用为例，系统展示了如何使用Chips设计、建模和分析复杂系统项目。

Result: Chips能够高效地描述和生成稳健的组件模型，适用于复杂系统的设计需求，并通过实际案例验证了其可行性和有效性。

Conclusion: Chips语言通过结合控制理论和编程语言的优势，为复杂系统的设计和建模提供了高效且稳健的解决方案，具有实际应用价值。

Abstract: When designing new web applications, developers must cope with different kinds of constraints relative to the resources they rely on: software, hardware, network, online micro-services, or any combination of the mentioned entities. Together, these entities form a complex system of communicating interdependent processes, physical or logical. It is very desirable that such system ensures its robustness to provide a good quality of service. In this paper we introduce Chips, a language that aims at facilitating the design of models made of various entwined components. It allows the description of applications in the form of functional blocks. Chips mixes notions  from control theory and general purpose programming languages to generate robust component-based models. This paper presents how to use Chips to systematically design, model and analyse a complex system project, using a variation of the Adaptable TeaStore application as running example.

</details>


### [7] [Adaptable TeaStore: A Choreographic Approach](https://arxiv.org/abs/2512.23497)
*Giuseppe De Palma,Saverio Giallorenzo,Ivan Lanese,Gianluigi Zavattaro*

Main category: cs.PL

TL;DR: Adaptable TeaStore 是基于 AIOCJ 的可适应微服务架构参考模型，展示了动态适应和多系统正确性的优势及其局限性。


<details>
  <summary>Details</summary>
Motivation: 为了提供一种可适应微服务架构的参考模型，并探索在动态适应场景中确保通信正确性的方法。

Method: 使用 AIOCJ（一种编排语言）实现 Adaptable TeaStore，确保在运行时动态适应时通信的正确性。

Result: 展示了 AIOCJ 在动态适应中的优势，同时指出了其当前局限性，并提出了未来改进方向。

Conclusion: AIOCJ 为可适应微服务架构提供了有效的解决方案，但仍需进一步优化以更好地适应现实云架构需求。

Abstract: The Adaptable TeaStore has recently been proposed as a reference model for adaptable microservice architectures. It includes different configurations, as well as scenarios requiring to transition between them. We describe an implementation of the Adaptable TeaStore based on AIOCJ, a choreographic language that allows one to program multiparty systems that can adapt at runtime to different conditions. Following the choreographic tradition, AIOCJ ensures by-construction correctness of communications (e.g., no deadlocks) before, during, and after adaptation. Adaptation is dynamic, and the adaptation scenarios need to be fully specified only at runtime. Using AIOCJ to model the Adaptable TeaStore, we showcase the strengths of the approach and its current limitations, providing suggestions for future directions for refining the paradigm (and the AIOCJ language, in particular), to better align it with real-world Cloud architectures.

</details>


### [8] [Beyond Per-Thread Lock Sets: Multi-Thread Critical Sections and Dynamic Deadlock Prediction](https://arxiv.org/abs/2512.23552)
*Martin Sulzmann*

Main category: cs.PL

TL;DR: 该论文提出了一种改进的锁集构造方法，通过放宽临界区限制为单一线程的限制，减少了死锁预测中的误报和漏报，同时保持了高效性和准确性。


<details>
  <summary>Details</summary>
Motivation: 传统的锁集构造方法仅考虑同一线程中获取的锁，而忽略了其他线程的锁获取事件，导致死锁预测中出现误报和漏报。这源于临界区的常规定义对其他线程事件的忽视。

Method: 论文提出了一种基于追踪的临界区表征方法，放宽了临界区仅限于单一线程的限制。通过偏序关系对追踪表征进行近似，实现了改进的锁集构造，能够有效计算并减少误报和漏报。

Result: 改进的锁集构造方法显著减少了DIRK死锁预测器的误报，并通过扩展SPDOffline死锁预测器减少了漏报。实验表明，性能未受影响，同时提高了预测的完整性和精确性。

Conclusion: 通过重新定义临界区并改进锁集构造方法，论文解决了传统方法的局限，提升了死锁预测的准确性和完整性，同时保持了计算效率。

Abstract: Lock sets are commonly used for dynamic analysis of deadlocks. The standard per-thread lock set construction only considers locks acquired in the same thread, but is unaware of locks acquired in another thread. This leads to false positives and false negatives. The underlying issue is that the commonly used notion of a critical section on which the lock set construction relies ignores events from other threads. We give a trace-based characterization of critical sections that drops this restriction. Critical sections are no longer restricted to a single thread and can cover multiple threads. Such forms of critical sections exist, are natural, and correct the standard formulation.
  We show how to soundly approximate the trace-based characterization via partial order relations. Thus, we obtain an improved lock set construction that can still be efficiently computed and allows us to remove false positives reported by the DIRK deadlock predictor and remove false negatives by extending the SPDOffline deadlock predictor. We integrate various lock set constructions with increased precision in an extension of SPDOffline. Our extensions remain sound (no false positives) but are more complete (fewer false negatives) w.r.t. SPDOffline. For an extensive standard benchmark suite we can also show that the performance is not affected.

</details>


### [9] [Automating the Analysis of Parsing Algorithms (and other Dynamic Programs)](https://arxiv.org/abs/2512.23665)
*Tim Vieira,Ryan Cotterell,Jason Eisner*

Main category: cs.PL

TL;DR: 本文开发了一个系统，帮助程序员分析NLP算法的类型、冗余代码以及运行时和空间复杂度界限。


<details>
  <summary>Details</summary>
Motivation: NLP算法研究中，设计者通常需要为算法提供性能保证，例如运行时或空间复杂度的上界。他们还需要验证类型错误和合成高效数据结构。本文旨在帮助程序员完成这些分析任务。

Method: 开发了一个分析系统，用于推断算法的类型、冗余代码以及运行时和空间复杂度的参数化界限。

Result: 该系统成功应用于多个NLP算法，能够准确推断类型、冗余代码以及运行时和空间复杂度的界限。

Conclusion: 本文提出的系统为程序员提供了一个有效的工具，用于分析和优化NLP算法的性能和结构。

Abstract: Much algorithmic research in NLP aims to efficiently manipulate rich formal structures. An algorithm designer typically seeks to provide guarantees about their proposed algorithm -- for example, that its running time or space complexity is upper-bounded as a certain function of its input size. They may also wish to determine the necessary properties of the quantities derived by the algorithm to synthesize efficient data structures and verify type errors. In this paper, we develop a system for helping programmers to perform these types of analyses. We apply our system to a number of NLP algorithms and find that it successfully infers types, dead and redundant code, and parametric runtime and space complexity bounds.

</details>


<div id='cs.GT'></div>

# cs.GT [[Back]](#toc)

### [10] [Analyzing Skill Element in Online Fantasy Cricket](https://arxiv.org/abs/2512.22254)
*Sarthak Sarkar,Supratim Das,Purushottam Saha,Diganta Mukherjee,Tridib Mukherjee*

Main category: cs.GT

TL;DR: 本文通过统计框架分析在线幻想板球平台的团队选择策略，评估技能在胜负中的作用，并通过动态锦标赛模型验证技能的重要性。


<details>
  <summary>Details</summary>
Motivation: 在线幻想板球平台的大量增长引发了对胜负是否主要由技能或运气驱动的疑问，本文旨在定量评估技能的作用。

Method: 构建了基于近期表现、历史数据、统计优化和多准则决策的确定性及随机性策略，并通过Mega和4x或Nothing两种竞赛结构评估表现。此外，引入动态锦标赛模型以模拟适应性行为。

Result: 通过IPL 2024数据集的大量数值实验，结果表明在线幻想板球平台的成功确实受技能影响。

Conclusion: 研究为在线幻想板球平台的技能要素提供了定量证据，支持技能在胜负中的重要作用。

Abstract: Online fantasy cricket has emerged as large-scale competitive systems in which participants construct virtual teams and compete based on real-world player performances. This massive growth has been accompanied by important questions about whether outcomes are primarily driven by skill or chance. We develop a statistical framework to assess the role of skill in determining success on these platforms. We construct and analyze a range of deterministic and stochastic team selection strategies, based on recent form, historical statistics, statistical optimization, and multi-criteria decision making. Strategy performance is evaluated based on points, ranks, and payoff under two contest structures Mega and 4x or Nothing. An extensive comparison between different strategies is made to find an optimal set of strategies. To capture adaptive behavior, we further introduce a dynamic tournament model in which agent populations evolve through a softmax reweighting mechanism proportional to positive payoff realizations. We demonstrate our work by running extensive numerical experiments on the IPL 2024 dataset. The results provide quantitative evidence in favor of the skill element present in online fantasy cricket platforms.

</details>


### [11] [Computing Pure-Strategy Nash Equilibria in a Two-Party Policy Competition: Existence and Algorithmic Approaches](https://arxiv.org/abs/2512.22552)
*Chuang-Chieh Lin,Chi-Jen Lu,Po-An Chen,Chih-Chieh Hung*

Main category: cs.GT

TL;DR: 本文通过将两党政策竞争建模为非合作博弈，验证了等向性假设，证明了纯策略纳什均衡的存在性，并提出了高效算法以近似求解均衡。


<details>
  <summary>Details</summary>
Motivation: 研究两党政策竞争的不确定性和策略选择对选民效用的影响，旨在验证等向性假设并提出均衡求解方法。

Method: 将政策竞争建模为两玩家非合作博弈，使用欧几里得空间中的紧子集作为策略集，并通过仿真验证等向性假设。

Result: 证明了在一维和多维场景下纯策略纳什均衡的存在性，提出了梯度算法和网格搜索算法以高效求解近似均衡。

Conclusion: 本文验证了政策竞争的等向性假设，提出并验证了高效的均衡求解算法，为相关研究提供了理论支持。

Abstract: We formulate two-party policy competition as a two-player non-cooperative game, generalizing Lin et al.'s work (2021). Each party selects a real-valued policy vector as its strategy from a compact subset of Euclidean space, and a voter's utility for a policy is given by the inner product with their preference vector. To capture the uncertainty in the competition, we assume that a policy's winning probability increases monotonically with its total utility across all voters, and we formalize this via an affine isotonic function. A player's payoff is defined as the expected utility received by its supporters. In this work, we first test and validate the isotonicity hypothesis through voting simulations. Next, we prove the existence of a pure-strategy Nash equilibrium (PSNE) in both one- and multi-dimensional settings. Although we construct a counterexample demonstrating the game's non-monotonicity, our experiments show that a decentralized gradient-based algorithm typically converges rapidly to an approximate PSNE. Finally, we present a grid-based search algorithm that finds an $ε$-approximate PSNE of the game in time polynomial in the input size and $1/ε$.

</details>


### [12] [Facility Location Games for Multi-Location Agents with Satisfaction](https://arxiv.org/abs/2512.22873)
*Huanjun Wang,Qizhi Fang,Wenjing Liu*

Main category: cs.GT

TL;DR: 研究了单设施位置游戏中的机制设计，目标是设计激励机制以最大化代理人满意度，同时确保代理人真实报告位置。


<details>
  <summary>Details</summary>
Motivation: 研究如何在代理人拥有多个私有位置的情况下，设计机制以最大化满意度并确保真实性。

Method: 提出了针对理想和厌恶设施位置游戏的策略证明机制，包括近似比为2和5/4的方法。

Result: 在理想设施游戏中，设计了两种机制，分别实现了2和5/4的近似比；在厌恶设施游戏中，提出了两种最优机制和两种随机机制。

Conclusion: 证明了所提出机制在理论和实践上的有效性，为设施位置游戏提供了新的解决方案。

Abstract: In this paper, we study mechanism design for single-facility location games where each agent has multiple private locations in [0, 1]. The individual objective is a satisfaction function that measures the discrepancy between the optimal facility location for an agent and the location provided by the mechanism. Based on different distance functions from agents to the facility, we consider two types of individual objectives: the sum-variant satisfaction and the max-variant satisfaction. Our goal is to design mechanisms that locate one facility to maximize the sum (or the minimum) of all agents' satisfactions, while incentivizing agents to truthfully report their locations. In this paper, we mainly focus on desirable and obnoxious facility location games. For desirable facility location games, we propose two group strategy-proof mechanisms with approximation ratios of 2 and 5/4 for maximizing the sum of the sum-variant and max-variant satisfaction, respectively. Moreover, another mechanism achieves an approximation ratio of 2 for simultaneously maximizing the minimum of the sum-variant satisfaction and the minimum of the max-variant satisfaction. For obnoxious facility location games, we establish that two group strategy-proof mechanisms are the best possible, providing an approximation ratio of 2 for maximizing the sum of the sum-variant satisfaction and the sum of the max-variant satisfaction, respectively. Additionally, we devise two 4/3-approximation randomized group strategy-proof mechanisms, and provide two lower bounds of 1.0625 and 1.0448 of randomized strategy-proof mechanisms for maximizing the sum of the sum-variant satisfaction and the sum of the max-variant satisfaction, respectively.

</details>


### [13] [Impact of Volatility on Time-Based Transaction Ordering Policies](https://arxiv.org/abs/2512.23386)
*Sunghun Ko,Jinsuk Park*

Main category: cs.GT

TL;DR: Arbitrum的Express Lane Auction（ELA）是一种提前进行的次价拍卖，获胜者在一分钟内享有独占的延迟优势。研究发现，由于短期波动性预测困难和竞标者的风险厌恶，优先级访问的价值相对于风险中性估值有所折扣。


<details>
  <summary>Details</summary>
Motivation: 研究旨在验证在风险厌恶的单轮拍卖模型中，优先级访问的市场价值是否会因短期波动性和竞标者风险厌恶而低于风险中性估值。

Method: 基于单轮拍卖模型，分析ELA的竞标记录，并将其与高频ETH价格数据匹配。

Result: 实证结果与模型预测一致，显示优先级访问的价值因风险厌恶和波动性预测困难而出现折扣。

Conclusion: 研究证实了风险厌恶和短期波动性是影响ELA竞标者对优先级访问估值的关键因素。

Abstract: We study Arbitrum's Express Lane Auction (ELA), an ahead-of-time second-price auction that grants the winner an exclusive latency advantage for one minute. Building on a single-round model with risk-averse bidders, we propose a hypothesis that the value of priority access is discounted relative to risk-neutral valuation due to the difficulty of forecasting short-horizon volatility and bidders' risk aversion. We test these predictions using ELA bid records matched to high-frequency ETH prices and find that the result is consistent with the model.

</details>


### [14] [Verifiable Off-Chain Governance](https://arxiv.org/abs/2512.23618)
*Jake Hartnell,Eugenio Battaglia*

Main category: cs.GT

TL;DR: 该论文提出了一种基于可验证链下计算的框架，旨在克服当前 DAO 治理实践的局限性，提高组织表达的复杂性和效率。


<details>
  <summary>Details</summary>
Motivation: 当前 DAO 治理实践受限于链上计算能力，使得复杂的组织决策简化为代币加权投票，限制了组织的表达能力。

Method: 论文提出了利用可验证服务、可信执行环境（TEEs）和零知识证明（ZK proofs）的可验证链下计算框架，并探索了三种新型治理机制：基于认证的多维利益相关者合法性计算、通过可验证偏好处理的集体智能，以及基于策略即代码的自主策略执行。

Result: 该框架提供了架构规范、安全模型和实施考虑，验证了其在实际应用中的可行性，并展示了增强 DAO 表达性和操作效率的潜力。

Conclusion: 通过可验证链下计算框架，DAO 能够在不牺牲加密经济安全性的前提下实现更高分辨率的表达性和更高的操作效率。

Abstract: Current DAO governance praxis limits organizational expressivity and reduces complex organizational decisions to token-weighted voting due to on-chain computational limits. This paper proposes verifiable off-chain computation (leveraging Verifiable Services, TEEs, and ZK proofs) as a framework to transcend these constraints while maintaining cryptoeconomic security. This paper explores three novel governance mechanisms: (1) attestation-based systems that compute multi-dimensional stakeholder legitimacy, (2) collective intelligence through verifiable preference processing, and (3) autonomous policy execution via Policy-as-Code. The framework provides architectural specifications, security models, and implementation considerations for DAOs seeking higher-resolution expressivity and increased operational efficiency, with validation from pioneering implementations demonstrating practical viability.

</details>
