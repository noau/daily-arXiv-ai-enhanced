<div id=toc></div>

# Table of Contents

- [cs.PL](#cs.PL) [Total: 1]
- [cs.GT](#cs.GT) [Total: 2]


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [1] [Semantically Reflected Programs](https://arxiv.org/abs/2509.03318)
*Eduard Kamburjan,Vidar Norstein Klungre,Yuanwei Qu,Rudolf Schlatte,Egor V. Kostylev,Martin Giese,Einar Broch Johnsen*

Main category: cs.PL

TL;DR: 本文通过语义提升程序探讨了结构知识和行为知识的形式化之间的二分法，将程序状态提升为知识图谱，并在编程语言中引入语义反射层。


<details>
  <summary>Details</summary>
Motivation: 解决知识图谱与编程语言在表示系统静态知识和动态行为之间的差异，提供一种将领域知识融入程序的方法。

Method: 提出一种面向对象编程语言的语义提升方法，将程序状态转换为知识图谱，并在SMOL语言中实现语义反射层。

Result: 通过地质建模案例展示了语义提升和语义反射的应用，验证了方法的可行性和实用性。

Conclusion: 语义提升和反射技术为程序员提供了一种在程序中利用领域知识的新途径，相关实现已开源。

Abstract: This paper addresses the dichotomy between the formalization of structural
and the formalization of behavioral knowledge by means of semantically lifted
programs, which explore an intuitive connection between programs and knowledge
graphs. While knowledge graphs and ontologies are eminently useful to represent
formal knowledge about a system's individuals and universals, programming
languages are designed to describe the system's evolution. To address this
dichotomy, we introduce a semantic lifting of the program states of an
executing program into a knowledge graph, for an object-oriented programming
language. The resulting graph is exposed as a semantic reflection layer within
the programming language, allowing programmers to leverage knowledge of the
application domain in their programs. In this paper, we formalize semantic
lifting and semantic reflection for a small programming language, SMOL, explain
the operational aspects of the language, and consider type correctness and
virtualisation for runtime program queries through the semantic reflection
layer. We illustrate semantic lifting and semantic reflection through a case
study of geological modelling and discuss different applications of the
technique. The language implementation is open source and available online.

</details>


<div id='cs.GT'></div>

# cs.GT [[Back]](#toc)

### [2] [Towards Performatively Stable Equilibria in Decision-Dependent Games for Arbitrary Data Distribution Maps](https://arxiv.org/abs/2509.02619)
*Guangzheng Zhong,Yang Liu,Jiming Liu*

Main category: cs.GT

TL;DR: 该论文提出了一种基于梯度的敏感性度量方法，用于量化决策引起的分布偏移影响，并通过一种敏感性指导的重复训练算法，保证在任意数据分布映射下收敛到表现稳定均衡。


<details>
  <summary>Details</summary>
Motivation: 现有的方法依赖于β平滑性假设，这在实践中不可行，因为数据分布映射通常是未知的。论文旨在克服这一限制，直接量化决策引起的分布偏移影响。

Method: 论文提出了一种梯度敏感性度量方法，并结合强单调性假设，开发了一种敏感性指导的重复训练算法。

Result: 实验表明，该方法在预测误差最小化游戏、Cournot竞争和收入最大化游戏中优于现有基线，具有更低的损失和更快的收敛速度。

Conclusion: 论文的创新性梯度敏感性度量方法和算法在实践中能够有效解决决策依赖游戏中的分布偏移问题，优于现有方法。

Abstract: In decision-dependent games, multiple players optimize their decisions under
a data distribution that shifts with their joint actions, creating complex
dynamics in applications like market pricing. A practical consequence of these
dynamics is the \textit{performatively stable equilibrium}, where each player's
strategy is a best response under the induced distribution. Prior work relies
on $\beta$-smoothness, assuming Lipschitz continuity of loss function gradients
with respect to the data distribution, which is impractical as the data
distribution maps, i.e., the relationship between joint decision and the
resulting distribution shifts, are typically unknown, rendering $\beta$
unobtainable. To overcome this limitation, we propose a gradient-based
sensitivity measure that directly quantifies the impact of decision-induced
distribution shifts. Leveraging this measure, we derive convergence guarantees
for performatively stable equilibria under a practically feasible assumption of
strong monotonicity. Accordingly, we develop a sensitivity-informed repeated
retraining algorithm that adjusts players' loss functions based on the
sensitivity measure, guaranteeing convergence to performatively stable
equilibria for arbitrary data distribution maps. Experiments on prediction
error minimization game, Cournot competition, and revenue maximization game
show that our approach outperforms state-of-the-art baselines, achieving lower
losses and faster convergence.

</details>


### [3] [Generative Auto-Bidding in Large-Scale Competitive Auctions via Diffusion Completer-Aligner](https://arxiv.org/abs/2509.03348)
*Yewen Li,Jingtong Gao,Nan Jiang,Shuai Mao,Ruyi An,Fei Pan,Xiangyu Zhao,Bo An,Qingpeng Cai,Peng Jiang*

Main category: cs.GT

TL;DR: 论文提出了一种基于扩散模型的自适应竞价方法CBD，通过增强生成轨迹的动态合法性和对齐广告商目标，显著提升了竞价性能。


<details>
  <summary>Details</summary>
Motivation: 传统的自动竞价技术在复杂的竞争环境中表现不足，尤其是扩散模型在生成动态合法性方面的不确定性，导致竞价效果不佳。

Method: 提出了CBD方法，通过引入额外随机变量t，增强扩散模型生成的序列合法性，并使用轨迹级回报模型优化生成轨迹。

Result: 实验结果显示，CBD在稀疏奖励拍卖环境中提升了29.9%的转化价值，并在快手广告平台上实现了2.0%的目标成本提升。

Conclusion: CBD方法通过增强动态合法性和轨迹对齐，显著提升了自动竞价的性能，尤其在复杂竞争环境中表现优越。

Abstract: Auto-bidding is central to computational advertising, achieving notable
commercial success by optimizing advertisers' bids within economic constraints.
Recently, large generative models show potential to revolutionize auto-bidding
by generating bids that could flexibly adapt to complex, competitive
environments. Among them, diffusers stand out for their ability to address
sparse-reward challenges by focusing on trajectory-level accumulated rewards,
as well as their explainable capability, i.e., planning a future trajectory of
states and executing bids accordingly. However, diffusers struggle with
generation uncertainty, particularly regarding dynamic legitimacy between
adjacent states, which can lead to poor bids and further cause significant loss
of ad impression opportunities when competing with other advertisers in a
highly competitive auction environment. To address it, we propose a Causal
auto-Bidding method based on a Diffusion completer-aligner framework, termed
CBD. Firstly, we augment the diffusion training process with an extra random
variable t, where the model observes t-length historical sequences with the
goal of completing the remaining sequence, thereby enhancing the generated
sequences' dynamic legitimacy. Then, we employ a trajectory-level return model
to refine the generated trajectories, aligning more closely with advertisers'
objectives. Experimental results across diverse settings demonstrate that our
approach not only achieves superior performance on large-scale auto-bidding
benchmarks, such as a 29.9% improvement in conversion value in the challenging
sparse-reward auction setting, but also delivers significant improvements on
the Kuaishou online advertising platform, including a 2.0% increase in target
cost.

</details>
