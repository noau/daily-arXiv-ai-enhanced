{"id": "2509.05498", "categories": ["cs.GT", "cs.CR"], "pdf": "https://arxiv.org/pdf/2509.05498", "abs": "https://arxiv.org/abs/2509.05498", "authors": ["Ya-Ting Yang", "Quanyan Zhu"], "title": "Bi-Level Game-Theoretic Planning of Cyber Deception for Cognitive Arbitrage", "comment": null, "summary": "Cognitive vulnerabilities shape human decision-making and arise primarily\nfrom two sources: (1) cognitive capabilities, which include disparities in\nknowledge, education, expertise, or access to information, and (2) cognitive\nbiases, such as rational inattention, confirmation bias, and base rate neglect,\nwhich influence how individuals perceive and process information. Exploiting\nthese vulnerabilities allows an entity with superior cognitive awareness to\ngain a strategic advantage, a concept referred to as cognitive arbitrage. This\npaper investigates how to exploit the cognitive vulnerabilities of Advanced\nPersistent Threat (APT) attackers and proposes cognition-aware defenses that\nleverage windows of superiority to counteract attacks. Specifically, the\nproposed bi-level cyber warfare game focuses on \"strategic-level\" design for\ndefensive deception mechanisms, which then facilitates \"operational-level\"\nactions and tactical-level execution of Tactics, Techniques, and Procedures\n(TTPs). Game-theoretic reasoning and analysis play a significant role in the\ncross-echelon quantitative modeling and design of cognitive arbitrage\nstrategies. Our numerical results demonstrate that although the defender's\ninitial advantage diminishes over time, strategically timed and deployed\ndeception techniques can turn a negative value for the attacker into a positive\none during the planning phase, and achieve at least a 40% improvement in total\nrewards during execution. This demonstrates that the defender can amplify even\nsmall initial advantages, sustain a strategic edge over the attacker, and\nsecure long-term objectives, such as protecting critical assets throughout the\nattacker's lifecycle."}
{"id": "2509.05956", "categories": ["cs.GT"], "pdf": "https://arxiv.org/pdf/2509.05956", "abs": "https://arxiv.org/abs/2509.05956", "authors": ["Zohar Barak", "Asnat Berlin", "Ilan Reuven Cohen", "Alon Eden", "Omri Porat", "Inbal Talgam-Cohen"], "title": "Knapsack Contracts and the Importance of Return-on-Investment", "comment": null, "summary": "We formulate the Knapsack Contracts problem -- a strategic version of the\nclassic Stochastic Knapsack problem, which builds upon the inherent randomness\nshared by stochastic optimization and contract design. In this problem, the\nprincipal incentivizes agents to perform jobs with stochastic processing times,\nthe realization of which depends on the agents' efforts.\n  Algorithmically, we show that Knapsack Contracts can be viewed as Stochastic\nKnapsack with costs and multi-choice, features that introduce significant new\nchallenges. We identify a crucial and economically meaningful parameter -- the\nReturn on Investment (ROI) value. We show that the Inverse of ROI (or IOR for\nshort) precisely characterizes the extent to which the approximation guarantees\nfor Stochastic Knapsack extend to its strategic counterpart.\n  For IOR of $\\alpha$, we develop an algorithm that finds an\n$O(\\alpha)$-approximation policy that does not rely on adaptivity. We establish\nmatching $\\Omega(\\alpha)$ lower bounds, both on the adaptivity gap, and on what\ncan be achieved without full distributional knowledge of the processing times.\nTaken together, our results show that IOR is fundamental to understanding the\ncomplexity and approximability of Knapsack Contracts, and bounding it is both\nnecessary and sufficient for achieving non-trivial approximation guarantees.\nOur results highlight the computational challenges arising when stochasticity\nin optimization problems is controlled by strategic effort."}
{"id": "2509.06187", "categories": ["cs.GT"], "pdf": "https://arxiv.org/pdf/2509.06187", "abs": "https://arxiv.org/abs/2509.06187", "authors": ["Ramiro N. Deo-Campo Vuong", "Robert Kleinberg", "Aditya Prasad", "Eric Xiao", "Haifeng Xu"], "title": "The Keychain Problem: On Minimizing the Opportunity Cost of Uncertainty", "comment": null, "summary": "In this paper, we introduce a family of sequential decision-making problems,\ncollectively called the Keychain Problem, that involve exploring a set of\nactions to maximize expected payoff when only a subset of actions are available\nin each stage. In an instance of the Keychain Problem, a locksmith faces a\nsequence of choices, each of which involves selecting one key from a specified\nsubset (a keychain) to attempt to open a lock. Given a Bayesian prior on the\neffectiveness of keys, the locksmith's goal is to maximize the expected number\nof rounds in which the lock is opened -- or equivalently, minimize the\nopportunity cost which is the expected number of rounds in which the chain has\na correct key but our selected key is incorrect. We investigate Keychain\nProblems under three assumptions on the order in which keychains are tested by\nthe locksmith: a fixed, known order; a random order sampled from a known\ndistribution on a set of ``scenarios''; or an order selected by the locksmith\nthemself. We present an exact algorithm for the simplest of these settings, and\nwe present approximation algorithms and hardness results for the others. In the\nProbabilistic Scenarios setting, our approximation algorithm is based on a\nnovel connection between combinatorial auctions and policy design for\nsequential decision-making problems. To illustrate the generality of this\ntechnique, we apply the same ideas to obtain Philosopher Inequalities for\nOnline Bipartite Matching and some of its extensions."}
{"id": "2509.05504", "categories": ["cs.PL", "cs.AR"], "pdf": "https://arxiv.org/pdf/2509.05504", "abs": "https://arxiv.org/abs/2509.05504", "authors": ["Karl Aaron Rudkowski", "Sallar Ahmadi-Pour", "Rolf Drechsler"], "title": "Comparing Methods for the Cross-Level Verification of SystemC Peripherals with Symbolic Execution", "comment": null, "summary": "Virtual Prototypes (VPs) are important tools in modern hardware development.\nAt high abstractions, they are often implemented in SystemC and offer early\nanalysis of increasingly complex designs. These complex designs often combine\none or more processors, interconnects, and peripherals to perform tasks in\nhardware or interact with the environment. Verifying these subsystems is a\nwell-suited task for VPs, as they allow reasoning across different abstraction\nlevels. While modern verification techniques like symbolic execution can be\nseamlessly integrated into VP-based workflows, they require modifications in\nthe SystemC kernel. Hence, existing approaches therefore modify and replace the\nSystemC kernel, or ignore the opportunity of cross-level scenarios completely,\nand would not allow focusing on special challenges of particular subsystems\nlike peripherals. We propose CrosSym and SEFOS, two opposing approaches for a\nversatile symbolic execution of peripherals. CrosSym modifies the SystemC\nkernel, while SEFOS instead modifies a modern symbolic execution engine. Our\nextensive evaluation applies our tools to various peripherals on different\nlevels of abstractions. Both tools extensive sets of features are demonstrated\nfor (1) different verification scenarios, and (2) identifying 300+ mutants. In\ncomparison with each other, SEFOS convinces with the unmodified SystemC kernel\nand peripheral, while CrosSym offers slightly better runtime and memory usage.\nIn comparison to the state-of-the-art, that is limited to Transaction Level\nModelling (TLM), our tools offered comparable runtime, while enabling\ncross-level verification with symbolic execution."}
{"id": "2509.05595", "categories": ["cs.GR"], "pdf": "https://arxiv.org/pdf/2509.05595", "abs": "https://arxiv.org/abs/2509.05595", "authors": ["Seonghun Oh", "Xiaodi Yuan", "Xinyue Wei", "Ruoxi Shi", "Fanbo Xiang", "Minghua Liu", "Hao Su"], "title": "PaMO: Parallel Mesh Optimization for Intersection-Free Low-Poly Modeling on the GPU", "comment": null, "summary": "Reducing the triangle count in complex 3D models is a basic geometry\npreprocessing step in graphics pipelines such as efficient rendering and\ninteractive editing. However, most existing mesh simplification methods exhibit\na few issues. Firstly, they often lead to self-intersections during decimation,\na major issue for applications such as 3D printing and soft-body simulation.\nSecond, to perform simplification on a mesh in the wild, one would first need\nto perform re-meshing, which often suffers from surface shifts and losses of\nsharp features. Finally, existing re-meshing and simplification methods can\ntake minutes when processing large-scale meshes, limiting their applications in\npractice. To address the challenges, we introduce a novel GPU-based mesh\noptimization approach containing three key components: (1) a parallel\nre-meshing algorithm to turn meshes in the wild into watertight, manifold, and\nintersection-free ones, and reduce the prevalence of poorly shaped triangles;\n(2) a robust parallel simplification algorithm with intersection-free\nguarantees; (3) an optimization-based safe projection algorithm to realign the\nsimplified mesh with the input, eliminating the surface shift introduced by\nre-meshing and recovering the original sharp features. The algorithm\ndemonstrates remarkable efficiency, simplifying a 2-million-face mesh to 20k\ntriangles in 3 seconds on RTX4090. We evaluated the approach on the Thingi10K\ndataset and showcased its exceptional performance in geometry preservation and\nspeed."}
{"id": "2509.05586", "categories": ["cs.PL", "cs.CC"], "pdf": "https://arxiv.org/pdf/2509.05586", "abs": "https://arxiv.org/abs/2509.05586", "authors": ["Lee Zheng Han", "Umang Mathur"], "title": "Fixed Parameter Tractable Linearizability Monitoring for Stack, Queue and Anagram Agnostic Data Types", "comment": null, "summary": "Verifying linearizability of concurrent data structures is NP-hard, even for\nsimple types. We present fixed-parameter tractable algorithms for monitoring\nstacks, queues, and anagram-agnostic data types (AADTs), parameterized by the\nmaximum concurrency. Our approach leverages frontier graphs and partition\nstates to bound the search space. For AADTs, equivalence of linearizations\nenables monitoring in log-linear time. For stacks, we introduce a grammar-based\nmethod with a sub-cubic reduction to matrix multiplication, and for queues, a\nsplit-sequence transition system supporting efficient dynamic programming.\nThese results unify tractability guarantees for both order-sensitive and\nanagram-agnostic data types under bounded concurrency."}
{"id": "2509.05855", "categories": ["cs.GR", "cs.RO"], "pdf": "https://arxiv.org/pdf/2509.05855", "abs": "https://arxiv.org/abs/2509.05855", "authors": ["Thijs Masmeijer", "Caleb Swain", "Jeff Hill", "Ed Habtour"], "title": "Programming tension in 3D printed networks inspired by spiderwebs", "comment": null, "summary": "Each element in tensioned structural networks -- such as tensegrity,\narchitectural fabrics, or medical braces/meshes -- requires a specific tension\nlevel to achieve and maintain the desired shape, stability, and compliance.\nThese structures are challenging to manufacture, 3D print, or assemble because\nflattening the network during fabrication introduces multiplicative\ninaccuracies in the network's final tension gradients. This study overcomes\nthis challenge by offering a fabrication algorithm for direct 3D printing of\nsuch networks with programmed tension gradients, an approach analogous to the\nspinning of spiderwebs. The algorithm: (i) defines the desired network and\nprescribes its tension gradients using the force density method; (ii) converts\nthe network into an unstretched counterpart by numerically optimizing vertex\nlocations toward target element lengths and converting straight elements into\narcs to resolve any remaining error; and (iii) decomposes the network into\nprintable toolpaths; Optional additional steps are: (iv) flattening curved 2D\nnetworks or 3D networks to ensure 3D printing compatibility; and (v)\nautomatically resolving any unwanted crossings introduced by the flattening\nprocess. The proposed method is experimentally validated using 2D unit cells of\nviscoelastic filaments, where accurate tension gradients are achieved with an\naverage element strain error of less than 1.0\\%. The method remains effective\nfor networks with element minimum length and maximum stress of 5.8 mm and 7.3\nMPa, respectively. The method is used to demonstrate the fabrication of three\ncomplex cases: a flat spiderweb, a curved mesh, and a tensegrity system. The\nprogrammable tension gradient algorithm can be utilized to produce compact,\nintegrated cable networks, enabling novel applications such as moment-exerting\nstructures in medical braces and splints."}
{"id": "2509.06724", "categories": ["cs.PL"], "pdf": "https://arxiv.org/pdf/2509.06724", "abs": "https://arxiv.org/abs/2509.06724", "authors": ["Florian Kohn", "Arthur Correnson", "Jan Baumeister", "Bernd Finkbeiner"], "title": "Pacing Types: Safe Monitoring of Asynchronous Streams", "comment": null, "summary": "Stream-based monitoring is a real-time safety assurance mechanism for complex\ncyber-physical systems such as unmanned aerial vehicles. In this context, a\nmonitor aggregates streams of input data from sensors and other sources to give\nreal-time statistics and assessments of the system's health. Since monitors are\nsafety-critical components, it is crucial to ensure that they are free of\npotential runtime errors. One of the central challenges in designing reliable\nstream-based monitors is to deal with the asynchronous nature of data streams:\nin concrete applications, the different sensors being monitored produce values\nat different speeds, and it is the monitor's responsibility to correctly react\nto the asynchronous arrival of different streams of values. To ease this\nprocess, modern frameworks for stream-based monitoring such as RTLola feature\nan expressive specification language that allows to finely specify data\nsynchronization policies. While this feature dramatically simplifies the design\nof monitors, it can also lead to subtle runtime errors. To mitigate this issue,\nthis paper presents pacing types, a novel type system implemented in RTLola to\nensure that monitors for asynchronous streams are well-behaved at runtime. We\nformalize the essence of pacing types for a core fragment of RTLola, and\npresent a soundness proof of the pacing type system using a new logical\nrelation."}
{"id": "2509.06573", "categories": ["cs.GR"], "pdf": "https://arxiv.org/pdf/2509.06573", "abs": "https://arxiv.org/abs/2509.06573", "authors": ["Jie Zhou", "Linzi Qu", "Miu-Ling Lam", "Hongbo Fu"], "title": "From Rigging to Waving: 3D-Guided Diffusion for Natural Animation of Hand-Drawn Characters", "comment": null, "summary": "Hand-drawn character animation is a vibrant field in computer graphics,\npresenting challenges in achieving geometric consistency while conveying\nexpressive motion. Traditional skeletal animation methods maintain geometric\nconsistency but struggle with complex non-rigid elements like flowing hair and\nskirts, leading to unnatural deformation. Conversely, video diffusion models\nsynthesize realistic dynamics but often create geometric distortions in\nstylized drawings due to domain gaps. This work proposes a hybrid animation\nsystem that combines skeletal animation and video diffusion. Initially, coarse\nimages are generated from characters retargeted with skeletal animations for\ngeometric guidance. These images are then enhanced in texture and secondary\ndynamics using video diffusion priors, framing this enhancement as an\ninpainting task. A domain-adapted diffusion model refines user-masked regions\nneeding improvement, especially for secondary dynamics. To enhance motion\nrealism further, we introduce a Secondary Dynamics Injection (SDI) strategy in\nthe denoising process, incorporating features from a pre-trained diffusion\nmodel enriched with human motion priors. Additionally, to tackle unnatural\ndeformations from low-poly single-mesh character modeling, we present a Hair\nLayering Modeling (HLM) technique that uses segmentation maps to separate hair\nfrom the body, allowing for more natural animation of long-haired characters.\nExtensive experiments show that our system outperforms state-of-the-art methods\nin both quantitative and qualitative evaluations."}
{"id": "2509.06752", "categories": ["cs.PL", "cs.LO"], "pdf": "https://arxiv.org/pdf/2509.06752", "abs": "https://arxiv.org/abs/2509.06752", "authors": ["Amir M. Ben-Amram", "Samir Genaim", "Joël Ouaknine", "James Worrell"], "title": "Termination Analysis of Linear-Constraint Programs", "comment": null, "summary": "This Survey provides an overview of techniques in termination analysis for\nprograms with numerical variables and transitions defined by linear\nconstraints. This subarea of program analysis is challenging due to the\nexistence of undecidable problems, and this Survey systematically explores\napproaches that mitigate this inherent difficulty. These include foundational\ndecidability results, the use of ranking functions, and disjunctive\nwell-founded transition invariants. The Survey also discusses non-termination\nwitnesses, used to prove that a program will not halt. We examine the\nalgorithmic and complexity aspects of these methods, showing how different\napproaches offer a trade-off between expressive power and computational\ncomplexity. The Survey does not discuss how termination analysis is performed\non real-world programming languages, nor does it consider more expressive\nabstract models that include non-linear arithmetic, probabilistic choice, or\nterm rewriting systems."}
{"id": "2509.06607", "categories": ["cs.GR", "cs.CV"], "pdf": "https://arxiv.org/pdf/2509.06607", "abs": "https://arxiv.org/abs/2509.06607", "authors": ["Marilyn Keller", "Keenon Werling", "Soyong Shin", "Scott Delp", "Sergi Pujades", "C. Karen Liu", "Michael J. Black"], "title": "From Skin to Skeleton: Towards Biomechanically Accurate 3D Digital Humans", "comment": null, "summary": "Great progress has been made in estimating 3D human pose and shape from\nimages and video by training neural networks to directly regress the parameters\nof parametric human models like SMPL. However, existing body models have\nsimplified kinematic structures that do not correspond to the true joint\nlocations and articulations in the human skeletal system, limiting their\npotential use in biomechanics. On the other hand, methods for estimating\nbiomechanically accurate skeletal motion typically rely on complex motion\ncapture systems and expensive optimization methods. What is needed is a\nparametric 3D human model with a biomechanically accurate skeletal structure\nthat can be easily posed. To that end, we develop SKEL, which re-rigs the SMPL\nbody model with a biomechanics skeleton. To enable this, we need training data\nof skeletons inside SMPL meshes in diverse poses.\n  We build such a dataset by optimizing biomechanically accurate skeletons\ninside SMPL meshes from AMASS sequences. We then learn a regressor from SMPL\nmesh vertices to the optimized joint locations and bone rotations. Finally, we\nre-parametrize the SMPL mesh with the new kinematic parameters. The resulting\nSKEL model is animatable like SMPL but with fewer, and\nbiomechanically-realistic, degrees of freedom. We show that SKEL has more\nbiomechanically accurate joint locations than SMPL, and the bones fit inside\nthe body surface better than previous methods. By fitting SKEL to SMPL meshes\nwe are able to \"upgrade\" existing human pose and shape datasets to include\nbiomechanical parameters. SKEL provides a new tool to enable biomechanics in\nthe wild, while also providing vision and graphics researchers with a better\nconstrained and more realistic model of human articulation. The model, code,\nand data are available for research at https://skel.is.tue.mpg.de.."}
{"id": "2509.06794", "categories": ["cs.PL", "cs.AR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.06794", "abs": "https://arxiv.org/abs/2509.06794", "authors": ["Shihan Fang", "Hongzheng Chen", "Niansong Zhang", "Jiajie Li", "Han Meng", "Adrian Liu", "Zhiru Zhang"], "title": "Dato: A Task-Based Programming Model for Dataflow Accelerators", "comment": null, "summary": "Recent deep learning workloads increasingly push computational demand beyond\nwhat current memory systems can sustain, with many kernels stalling on data\nmovement rather than computation. While modern dataflow accelerators\nincorporate on-chip streaming to mitigate off-chip bandwidth limitations,\nexisting programming models struggle to harness these capabilities effectively.\nLow-level interfaces provide fine-grained control but impose significant\ndevelopment overhead, whereas high-level tile-based languages abstract away\ncommunication details, restricting optimization and forcing compilers to\nreconstruct the intended dataflow. We present Dato, a Python-embedded,\ntask-based programming model for dataflow accelerators that elevates data\ncommunication and sharding to first-class type constructs. Developers write\nprograms as a graph of tasks connected via explicit stream types, with sharded\ninputs specified using layout types. These tasks are first mapped virtually\nonto the accelerator's spatial fabric, and the compiler then generates a\nphysical mapping that respects hardware constraints. Experimental results on\nboth AMD Ryzen AI NPU and Alveo FPGA devices demonstrate that Dato achieves\nhigh performance while significantly reducing the burden of writing optimized\ncode. On the NPU, Dato attains up to 84% hardware utilization for GEMM and\ndelivers a 2.81x speedup on attention kernels compared to a state-of-the-art\ncommercial framework. On the FPGA, Dato surpasses leading frameworks in\nperformance when generating custom systolic arrays, achieving 98% of the\ntheoretical peak performance."}
{"id": "2509.06950", "categories": ["cs.GR", "cs.CV"], "pdf": "https://arxiv.org/pdf/2509.06950", "abs": "https://arxiv.org/abs/2509.06950", "authors": ["Nithin Gopalakrishnan Nair", "Srinivas Kaza", "Xuan Luo", "Vishal M. Patel", "Stephen Lombardi", "Jungyeon Park"], "title": "Scaling Transformer-Based Novel View Synthesis Models with Token Disentanglement and Synthetic Data", "comment": "Accepted at ICCV 2025", "summary": "Large transformer-based models have made significant progress in\ngeneralizable novel view synthesis (NVS) from sparse input views, generating\nnovel viewpoints without the need for test-time optimization. However, these\nmodels are constrained by the limited diversity of publicly available scene\ndatasets, making most real-world (in-the-wild) scenes out-of-distribution. To\novercome this, we incorporate synthetic training data generated from diffusion\nmodels, which improves generalization across unseen domains. While synthetic\ndata offers scalability, we identify artifacts introduced during data\ngeneration as a key bottleneck affecting reconstruction quality. To address\nthis, we propose a token disentanglement process within the transformer\narchitecture, enhancing feature separation and ensuring more effective\nlearning. This refinement not only improves reconstruction quality over\nstandard transformers but also enables scalable training with synthetic data.\nAs a result, our method outperforms existing models on both in-dataset and\ncross-dataset evaluations, achieving state-of-the-art results across multiple\nbenchmarks while significantly reducing computational costs. Project page:\nhttps://scaling3dnvs.github.io/"}
{"id": "2509.06845", "categories": ["cs.PL", "cs.SE"], "pdf": "https://arxiv.org/pdf/2509.06845", "abs": "https://arxiv.org/abs/2509.06845", "authors": ["Tom Lauwaerts", "Maarten Steevens", "Christophe Scholliers"], "title": "MIO: Multiverse Debugging in the Face of Input/Output -- Extended Version with Additional Appendices", "comment": "This extended version provides auxiliary material to the article of\n  the same title that will appear in the ACM Digital Library as part of the\n  PACMPL issue for OOPSLA 2025", "summary": "Debugging non-deterministic programs on microcontrollers is notoriously\nchallenging, especially when bugs manifest in unpredictable, input-dependent\nexecution paths. A recent approach, called multiverse debugging, makes it\neasier to debug non-deterministic programs by allowing programmers to explore\nall potential execution paths. Current multiverse debuggers enable both forward\nand backward traversal of program paths, and some facilitate jumping to any\npreviously visited states, potentially branching into alternative execution\npaths within the state space.\n  Unfortunately, debugging programs that involve input/output operations using\nexisting multiverse debuggers can reveal inaccessible program states, i.e.\nstates which are not encountered during regular execution. This can\nsignificantly hinder the debugging process, as the programmer may spend\nsubstantial time exploring and examining inaccessible program states, or worse,\nmay mistakenly assume a bug is present in the code, when in fact, the issue is\ncaused by the debugger.\n  This paper presents a novel approach to multiverse debugging, which can\naccommodate a broad spectrum of input/output operations. We provide the\nsemantics of our approach and prove the correctness of our debugger, ensuring\nthat despite having support for a wide range of input/output operations the\ndebugger will only explore those program states which can be reached during\nregular execution.\n  We have developed a prototype, called MIO, leveraging the WARDuino\nWebAssembly virtual machine to demonstrate the feasibility and efficiency of\nour techniques. As a demonstration of the approach we highlight a color dial\nbuilt with a Lego Mindstorms motor, and color sensor, providing a tangible\nexample of how our approach enables multiverse debugging for programs running\non an STM32 microcontroller."}
{"id": "2509.06872", "categories": ["cs.PL"], "pdf": "https://arxiv.org/pdf/2509.06872", "abs": "https://arxiv.org/abs/2509.06872", "authors": ["Zachary Kent", "Ugur Y. Yavuz", "Siddhartha Jayanti", "Stephanie Balzer", "Guy Blelloch"], "title": "Mechanized Metatheory of Forward Reasoning for End-to-End Linearizability Proofs", "comment": null, "summary": "In the past decade, many techniques have been developed to prove\nlinearizability, the gold standard of correctness for concurrent data\nstructures. Intuitively, linearizability requires that every operation on a\nconcurrent data structure appears to take place instantaneously, even when\ninterleaved with other operations. Most recently, Jayanti et al. presented the\nfirst sound and complete \"forward reasoning\" technique for proving\nlinearizability that relates the behavior of a concurrent data structure to a\nreference atomic data structure as time moves forward. This technique can be\nused to produce machine-checked proofs of linearizability in TLA+. However,\nwhile Jayanti et al.'s approach is shown to be sound and complete, a\nmechanization of this important metatheoretic result is still outstanding. As a\nresult, it is not possible to produce verified end-to-end proofs of\nlinearizability. To reduce the size of this trusted computing base, we\nformalize this forward reasoning technique and mechanize proofs of its\nsoundness and completeness in Rocq. As a case study, we use the approach to\nproduce a verified end-to-end proof of linearizability for a simple concurrent\nregister."}
