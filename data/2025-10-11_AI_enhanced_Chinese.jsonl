{"id": "2510.07582", "categories": ["cs.PL"], "pdf": "https://arxiv.org/pdf/2510.07582", "abs": "https://arxiv.org/abs/2510.07582", "authors": ["Yuyan Bao", "Tiark Rompf"], "title": "Type, Ability, and Effect Systems: Perspectives on Purity, Semantics, and Expressiveness", "comment": null, "summary": "Programming benefits from a clear separation between pure, mathematical\ncomputation and impure, effectful interaction with the world. Existing\napproaches to enforce this separation include monads, type-and-effect systems,\nand capability systems. All share a tension between precision and usability,\nand each one has non-obvious strengths and weaknesses.\n  This paper aims to raise the bar in assessing such systems. First, we propose\na semantic definition of purity, inspired by contextual equivalence, as a\nbaseline independent of any specific typing discipline. Second, we propose that\nexpressiveness should be measured by the degree of completeness, i.e., how many\nsemantically pure terms can be typed as pure. Using this measure, we focus on\nminimal meaningful effect and capability systems and show that they are\nincomparable, i.e., neither subsumes the other in terms of expressiveness.\n  Based on this result, we propose a synthesis and show that type, ability, and\neffect systems combine their respective strengths while avoiding their\nweaknesses. As part of our formal model, we provide a logical relation to\nfacilitate proofs of purity and other properties for a variety of effect typing\ndisciplines.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u8bed\u4e49\u7684\u7eaf\u5ea6\u5b9a\u4e49\uff0c\u5e76\u5f15\u5165\u8868\u8fbe\u6027\u5ea6\u91cf\u6807\u51c6\uff0c\u6bd4\u8f83\u4e86\u4e0d\u540c\u7c7b\u578b\u7cfb\u7edf\u5728\u8868\u8fbe\u6027\u4e0a\u7684\u4f18\u52a3\uff0c\u6700\u7ec8\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u7c7b\u578b\u3001\u80fd\u529b\u548c\u6548\u5e94\u7cfb\u7edf\u4f18\u52bf\u7684\u89e3\u51b3\u65b9\u6848\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\uff08\u5982\u5355\u5b50\u3001\u7c7b\u578b\u4e0e\u6548\u5e94\u7cfb\u7edf\u3001\u80fd\u529b\u7cfb\u7edf\uff09\u5728\u5f3a\u5236\u5206\u79bb\u7eaf\u8ba1\u7b97\u4e0e\u4e0d\u7eaf\u6548\u5e94\u4e4b\u95f4\u5b58\u5728\u7cbe\u5ea6\u4e0e\u53ef\u7528\u6027\u7684\u77db\u76fe\uff0c\u672c\u6587\u65e8\u5728\u63d0\u5347\u8bc4\u4f30\u8fd9\u4e9b\u7cfb\u7edf\u7684\u6807\u51c6\u3002", "method": "\u63d0\u51fa\u4e86\u57fa\u4e8e\u4e0a\u4e0b\u6587\u7b49\u4ef7\u6027\u7684\u8bed\u4e49\u7eaf\u5ea6\u5b9a\u4e49\uff0c\u5e76\u901a\u8fc7\u5b8c\u6574\u6027\u5ea6\u91cf\u6bd4\u8f83\u4e86\u6700\u5c0f\u6548\u5e94\u548c\u80fd\u529b\u7cfb\u7edf\u7684\u8868\u8fbe\u6027\u3002", "result": "\u7814\u7a76\u8868\u660e\uff0c\u6700\u5c0f\u6548\u5e94\u548c\u80fd\u529b\u7cfb\u7edf\u5728\u8868\u8fbe\u6027\u4e0a\u4e0d\u53ef\u6bd4\u8f83\uff0c\u4e14\u7ed3\u5408\u7c7b\u578b\u3001\u80fd\u529b\u548c\u6548\u5e94\u7cfb\u7edf\u53ef\u4ee5\u6574\u5408\u5404\u81ea\u7684\u4f18\u52bf\u3002", "conclusion": "\u672c\u6587\u7684\u7efc\u5408\u65b9\u6cd5\u4e3a\u6548\u5e94\u7c7b\u578b\u7cfb\u7edf\u63d0\u4f9b\u4e86\u4e00\u79cd\u65b0\u7684\u7814\u7a76\u65b9\u5411\uff0c\u5e76\u5f00\u53d1\u4e86\u903b\u8f91\u5173\u7cfb\u4ee5\u652f\u6301\u7eaf\u5ea6\u8bc1\u660e\u3002"}}
{"id": "2510.07851", "categories": ["cs.PL", "cs.LO"], "pdf": "https://arxiv.org/pdf/2510.07851", "abs": "https://arxiv.org/abs/2510.07851", "authors": ["Willem Heijltjes"], "title": "The Functional Machine Calculus III: Control", "comment": null, "summary": "The Functional Machine Calculus (Heijltjes 2022) is a new approach to\nunifying the imperative and functional programming paradigms. It extends the\nlambda-calculus, preserving the key features of confluent reduction and typed\ntermination, to embed computational effects, evaluation strategies, and control\nflow operations. The first instalment modelled sequential higher-order\ncomputation with global store, input/output, probabilities, and\nnon-determinism, and embedded both the call-by-name and call-by-value\nlambda-calculus, as well as Moggi's computational metalanguage and Levy's\ncall-by-push-value. The present paper extends the calculus from sequential to\nbranching and looping control flow. This allows the faithful embedding of a\nminimal but complete imperative language, including conditionals, exception\nhandling, and iteration, as well as constants and algebraic data types.\n  The calculus is defined through a simple operational semantics, extending the\n(simplified) Krivine machine for the lambda-calculus with multiple operand\nstacks to model effects and a continuation stack to model sequential,\nbranching, and looping computation. It features a confluent reduction relation\nand a system of simple types that guarantees termination of the machine and\nstrong normalization of reduction (in the absence of iteration). These\nproperties carry over to the embedded imperative language, providing a unified\nfunctional-imperative model of computation that supports simple types, a direct\nand intuitive operational semantics, and a confluent reduction semantics.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u65b9\u6cd5\u2014\u2014\u529f\u80fd\u673a\u5668\u6f14\u7b97\uff0c\u7528\u4e8e\u7edf\u4e00\u547d\u4ee4\u5f0f\u548c\u51fd\u6570\u5f0f\u7f16\u7a0b\u8303\u5f0f\uff0c\u6269\u5c55\u4e86lambda\u6f14\u7b97\u4ee5\u5d4c\u5165\u8ba1\u7b97\u6548\u679c\u548c\u63a7\u5236\u6d41\u64cd\u4f5c\u3002", "motivation": "\u65e8\u5728\u7edf\u4e00\u547d\u4ee4\u5f0f\u548c\u51fd\u6570\u5f0f\u7f16\u7a0b\u8303\u5f0f\uff0c\u5e76\u63d0\u4f9b\u4e00\u79cd\u80fd\u591f\u5d4c\u5165\u8ba1\u7b97\u6548\u679c\u548c\u63a7\u5236\u6d41\u64cd\u4f5c\u7684\u6a21\u578b\u3002", "method": "\u901a\u8fc7\u6269\u5c55lambda\u6f14\u7b97\uff0c\u5f15\u5165\u591a\u64cd\u4f5c\u6570\u6808\u548c\u7ee7\u7eed\u6808\u6765\u5efa\u6a21\u6548\u679c\u548c\u63a7\u5236\u6d41\uff0c\u4f7f\u7528\u7b80\u5316\u7684Krivine\u673a\u5668\u5b9a\u4e49\u64cd\u4f5c\u8bed\u4e49\u3002", "result": "\u8be5\u6f14\u7b97\u652f\u6301\u878d\u5408\u5f52\u7ea6\u5173\u7cfb\u548c\u7b80\u5355\u7c7b\u578b\u7cfb\u7edf\uff0c\u786e\u4fdd\u673a\u5668\u7ec8\u6b62\u548c\u5f3a\u89c4\u8303\u5316\u5f52\u7ea6\uff0c\u5e76\u5c06\u8fd9\u4e9b\u7279\u6027\u4f20\u9012\u7ed9\u5d4c\u5165\u7684\u547d\u4ee4\u5f0f\u8bed\u8a00\u3002", "conclusion": "\u529f\u80fd\u673a\u5668\u6f14\u7b97\u63d0\u4f9b\u4e86\u4e00\u4e2a\u7edf\u4e00\u7684\u51fd\u6570-\u547d\u4ee4\u5f0f\u8ba1\u7b97\u6a21\u578b\uff0c\u652f\u6301\u7b80\u5355\u7c7b\u578b\u3001\u76f4\u89c2\u7684\u64cd\u4f5c\u8bed\u4e49\u548c\u878d\u5408\u7684\u5f52\u7ea6\u8bed\u4e49\u3002"}}
{"id": "2510.07430", "categories": ["cs.GT"], "pdf": "https://arxiv.org/pdf/2510.07430", "abs": "https://arxiv.org/abs/2510.07430", "authors": ["Yang Jiao", "Guanpu Chen", "Yiguang Hong"], "title": "BG-FlipIn: A Bayesian game framework for FlipIt-insider models in advanced persistent threats", "comment": null, "summary": "In this paper, we study advanced persistent threats (APT) with an insider who\nhas different preferences. To address the uncertainty of the insider's\npreference, we propose the BG-FlipIn: a Bayesian game framework for\nFlipIt-insider models with an investigation on malicious, inadvertent, or\ncorrupt insiders. We calculate the closed-form Bayesian Nash Equilibrium\nexpression and further obtain three edge cases with deterministic insiders\ncorresponding to their Nash Equilibrium expressions. On this basis, we further\ndiscover several phenomena in APT related to the defender's move rate and cost,\nas well as the insider's preferences. We then provide decision-making guidance\nfor the defender, given different parametric conditions. Two applications\nvalidate that our BG-FlipIn framework enables the defender to make decisions\nconsistently, avoiding detecting the insider's concrete preference or adjusting\nits strategy frequently.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aBG-FlipIn\u7684\u8d1d\u53f6\u65af\u6e38\u620f\u6846\u67b6\uff0c\u7528\u4e8e\u7814\u7a76\u5177\u6709\u4e0d\u540c\u504f\u597d\u7684\u5185\u90e8\u4eba\u5458\u7684APT\u95ee\u9898\uff0c\u5e76\u5206\u6790\u4e86\u9632\u5fa1\u8005\u7684\u51b3\u7b56\u6307\u5bfc\u3002", "motivation": "\u7814\u7a76\u9ad8\u7ea7\u6301\u7eed\u6027\u5a01\u80c1\uff08APT\uff09\u4e2d\u5185\u90e8\u4eba\u5458\u4e0d\u540c\u504f\u597d\u7684\u4e0d\u786e\u5b9a\u6027\uff0c\u5e76\u63d0\u4f9b\u4e00\u4e2a\u6846\u67b6\u6765\u5e94\u5bf9\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4e86BG-FlipIn\u6846\u67b6\uff0c\u4e00\u79cd\u8d1d\u53f6\u65af\u6e38\u620f\u6a21\u578b\uff0c\u7528\u4e8e\u5206\u6790FlipIt-\u5185\u90e8\u4eba\u5458\u6a21\u578b\uff0c\u5e76\u8ba1\u7b97\u4e86\u95ed\u5f0f\u8d1d\u53f6\u65af\u7eb3\u4ec0\u5747\u8861\u8868\u8fbe\u5f0f\u3002", "result": "\u53d1\u73b0\u4e86\u4e0e\u9632\u5fa1\u8005\u79fb\u52a8\u7387\u548c\u6210\u672c\u4ee5\u53ca\u5185\u90e8\u4eba\u5458\u504f\u597d\u76f8\u5173\u7684APT\u73b0\u8c61\uff0c\u5e76\u5728\u4e0d\u540c\u53c2\u6570\u6761\u4ef6\u4e0b\u63d0\u4f9b\u4e86\u9632\u5fa1\u8005\u7684\u51b3\u7b56\u6307\u5bfc\u3002", "conclusion": "BG-FlipIn\u6846\u67b6\u4f7f\u9632\u5fa1\u8005\u80fd\u591f\u4e00\u81f4\u5730\u505a\u51fa\u51b3\u7b56\uff0c\u800c\u65e0\u9700\u9891\u7e41\u68c0\u6d4b\u5185\u90e8\u4eba\u5458\u5177\u4f53\u504f\u597d\u6216\u8c03\u6574\u7b56\u7565\u3002"}}
{"id": "2510.07340", "categories": ["cs.GR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.07340", "abs": "https://arxiv.org/abs/2510.07340", "authors": ["Yongzhi Li", "Saining Zhang", "Yibing Chen", "Boying Li", "Yanxin Zhang", "Xiaoyu Du"], "title": "SpotDiff: Spotting and Disentangling Interference in Feature Space for Subject-Preserving Image Generation", "comment": null, "summary": "Personalized image generation aims to faithfully preserve a reference\nsubject's identity while adapting to diverse text prompts. Existing\noptimization-based methods ensure high fidelity but are computationally\nexpensive, while learning-based approaches offer efficiency at the cost of\nentangled representations influenced by nuisance factors. We introduce\nSpotDiff, a novel learning-based method that extracts subject-specific features\nby spotting and disentangling interference. Leveraging a pre-trained CLIP image\nencoder and specialized expert networks for pose and background, SpotDiff\nisolates subject identity through orthogonality constraints in the feature\nspace. To enable principled training, we introduce SpotDiff10k, a curated\ndataset with consistent pose and background variations. Experiments demonstrate\nthat SpotDiff achieves more robust subject preservation and controllable\nediting than prior methods, while attaining competitive performance with only\n10k training samples.", "AI": {"tldr": "SpotDiff\u662f\u4e00\u79cd\u65b0\u578b\u5b66\u4e60\u578b\u65b9\u6cd5\uff0c\u901a\u8fc7\u63d0\u53d6\u7279\u5b9a\u4e3b\u9898\u7279\u5f81\u5e76\u6d88\u9664\u5e72\u6270\u56e0\u7d20\uff0c\u5b9e\u73b0\u9ad8\u6548\u7684\u4e2a\u6027\u5316\u56fe\u50cf\u751f\u6210\uff0c\u540c\u65f6\u4fdd\u6301\u9ad8\u4fdd\u771f\u5ea6\u548c\u53ef\u63a7\u7f16\u8f91\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u4f18\u5316\u7684\u65b9\u6cd5\u8ba1\u7b97\u6210\u672c\u9ad8\uff0c\u800c\u57fa\u4e8e\u5b66\u4e60\u7684\u65b9\u6cd5\u867d\u9ad8\u6548\u4f46\u6613\u53d7\u5e72\u6270\u56e0\u7d20\u5f71\u54cd\uff0c\u5bfc\u81f4\u7279\u5f81\u7ea0\u7f20\u3002SpotDiff\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "SpotDiff\u5229\u7528\u9884\u8bad\u7ec3\u7684CLIP\u56fe\u50cf\u7f16\u7801\u5668\u548c\u4e13\u7528\u4e13\u5bb6\u7f51\u7edc\uff0c\u901a\u8fc7\u7279\u5f81\u7a7a\u95f4\u7684\u6b63\u4ea4\u7ea6\u675f\u9694\u79bb\u4e3b\u9898\u8eab\u4efd\uff0c\u5e76\u7ed3\u5408SpotDiff10k\u6570\u636e\u96c6\u8fdb\u884c\u8bad\u7ec3\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cSpotDiff\u5728\u4ec510k\u8bad\u7ec3\u6837\u672c\u7684\u60c5\u51b5\u4e0b\uff0c\u5b9e\u73b0\u4e86\u6bd4\u73b0\u6709\u65b9\u6cd5\u66f4\u5177\u9c81\u68d2\u6027\u7684\u4e3b\u9898\u4fdd\u7559\u548c\u53ef\u63a7\u7f16\u8f91\u3002", "conclusion": "SpotDiff\u5728\u9ad8\u6548\u6027\u548c\u4fdd\u771f\u5ea6\u4e4b\u95f4\u53d6\u5f97\u4e86\u5e73\u8861\uff0c\u662f\u4e00\u79cd\u6709\u524d\u666f\u7684\u4e2a\u6027\u5316\u56fe\u50cf\u751f\u6210\u65b9\u6cd5\u3002"}}
{"id": "2510.07572", "categories": ["cs.GT", "cs.IT", "math.IT", "math.PR", "60D05, 68Q87", "G.3; I.2.4"], "pdf": "https://arxiv.org/pdf/2510.07572", "abs": "https://arxiv.org/abs/2510.07572", "authors": ["Jesse D Wei", "Guo Wei"], "title": "Deterministic algorithms for inhomogeneous Bernoulli trials: Shapley value of network devices", "comment": "27 pages", "summary": "Suppose that $n$ computer devices are to be connected to a network via\ninhomogeneous Bernoulli trials. The Shapley value of a device quantifies how\nmuch the network's value increases due to the participation of that device.\nCharacteristic functions of such games are naturally taken as the belief\nfunction (containment function) and Choquet capacity (hitting probability) of a\nrandom set (random network of devices).\n  Traditionally, the Shapley value is either calculated as the expected\nmarginal contribution over all possible coalitions (subnetworks), which results\nin exponential computational complexity, or approximated by the Monte Carlo\nsampling technique, where the performance is highly dependent on the stochastic\nsampling process.\n  The purpose of this study is to design deterministic algorithms for games\nformulated via inhomogeneous Bernoulli trials that approximate the Shapley\nvalue in linear or quadratic time, with rigorous error analysis (Sections 3 and\n4). Additionally, we provide a review of relevant literature on existing\ncalculation methods in Remark 3.1 and Appendix I.\n  A further goal is to supplement Shapley's original proof by deriving the\nShapley value formula using a rigorous approach based on definite integrals and\ncombinatorial analysis. This method explicitly highlights the roles of the\nBinomial Theorem and the Beta function in the proof, addressing a gap in\nShapley's work (Appendix II).", "AI": {"tldr": "\u672c\u7814\u7a76\u8bbe\u8ba1\u4e86\u786e\u5b9a\u6027\u7b97\u6cd5\uff0c\u7528\u4e8e\u5728\u4e0d\u5747\u5300\u4f2f\u52aa\u5229\u8bd5\u9a8c\u4e2d\u8fd1\u4f3c\u8ba1\u7b97Shapley\u503c\uff0c\u5177\u6709\u7ebf\u6027\u6216\u4e8c\u6b21\u65f6\u95f4\u590d\u6742\u6027\uff0c\u5e76\u63d0\u4f9b\u4e86\u4e25\u683c\u7684\u8bef\u5dee\u5206\u6790\u3002", "motivation": "\u4f20\u7edf\u8ba1\u7b97Shapley\u503c\u7684\u65b9\u6cd5\u5b58\u5728\u6307\u6570\u8ba1\u7b97\u590d\u6742\u6027\u6216\u8499\u7279\u5361\u6d1b\u91c7\u6837\u6027\u80fd\u4e0d\u7a33\u5b9a\u7684\u95ee\u9898\uff0c\u56e0\u6b64\u9700\u8981\u8bbe\u8ba1\u66f4\u9ad8\u6548\u7684\u786e\u5b9a\u6027\u7b97\u6cd5\u3002", "method": "\u7814\u7a76\u91c7\u7528\u4e86\u4e0d\u5747\u5300\u4f2f\u52aa\u5229\u8bd5\u9a8c\u7684\u786e\u5b9a\u6027\u7b97\u6cd5\uff0c\u5e76\u7ed3\u5408\u4e25\u683c\u7684\u8bef\u5dee\u5206\u6790\uff0c\u540c\u65f6\u8865\u5145\u4e86Shapley\u539f\u59cb\u8bc1\u660e\u7684\u65b9\u6cd5\uff0c\u4f7f\u7528\u4e86\u5b9a\u79ef\u5206\u548c\u7ec4\u5408\u5206\u6790\u3002", "result": "\u8bbe\u8ba1\u7684\u7b97\u6cd5\u80fd\u591f\u5728\u7ebf\u6027\u6216\u4e8c\u6b21\u65f6\u95f4\u5185\u8fd1\u4f3c\u8ba1\u7b97Shapley\u503c\uff0c\u5e76\u5728\u9644\u5f55\u4e2d\u5c55\u793a\u4e86\u4e8c\u9879\u5f0f\u5b9a\u7406\u548cBeta\u51fd\u6570\u5728\u8bc1\u660e\u4e2d\u7684\u4f5c\u7528\u3002", "conclusion": "\u672c\u7814\u7a76\u586b\u8865\u4e86Shapley\u503c\u8ba1\u7b97\u7684\u6548\u7387\u548c\u65f6\u95f4\u590d\u6742\u6027\u95ee\u9898\u7684\u7a7a\u767d\uff0c\u5e76\u63d0\u4f9b\u4e86\u4e00\u79cd\u4e25\u683c\u7684\u8bc1\u660e\u65b9\u6cd5\u3002"}}
{"id": "2510.07343", "categories": ["cs.GR", "cs.AI", "eess.IV"], "pdf": "https://arxiv.org/pdf/2510.07343", "abs": "https://arxiv.org/abs/2510.07343", "authors": ["Shaorong Zhang", "Rob Brekelmans", "Greg Ver Steeg"], "title": "Local MAP Sampling for Diffusion Models", "comment": null, "summary": "Diffusion Posterior Sampling (DPS) provides a principled Bayesian approach to\ninverse problems by sampling from $p(x_0 \\mid y)$. However, in practice, the\ngoal of inverse problem solving is not to cover the posterior but to recover\nthe most accurate reconstruction, where optimization-based diffusion solvers\noften excel despite lacking a clear probabilistic foundation. We introduce\nLocal MAP Sampling (LMAPS), a new inference framework that iteratively solving\nlocal MAP subproblems along the diffusion trajectory. This perspective\nclarifies their connection to global MAP estimation and DPS, offering a unified\nprobabilistic interpretation for optimization-based methods. Building on this\nfoundation, we develop practical algorithms with a probabilistically\ninterpretable covariance approximation, a reformulated objective for stability\nand interpretability, and a gradient approximation for non-differentiable\noperators. Across a broad set of image restoration and scientific tasks, LMAPS\nachieves state-of-the-art performance, including $\\geq 2$ dB gains on motion\ndeblurring, JPEG restoration, and quantization, and $>1.5$ dB improvements on\ninverse scattering benchmarks.", "AI": {"tldr": "LMAPS\u662f\u4e00\u79cd\u65b0\u7684\u63a8\u7406\u6846\u67b6\uff0c\u901a\u8fc7\u5c40\u90e8MAP\u5b50\u95ee\u9898\u89e3\u51b3\u4f18\u5316\u95ee\u9898\uff0c\u63d0\u4f9b\u4e86\u6982\u7387\u89e3\u91ca\uff0c\u5e76\u5728\u56fe\u50cf\u6062\u590d\u548c\u79d1\u5b66\u4efb\u52a1\u4e2d\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "motivation": "\u9006\u95ee\u9898\u7684\u76ee\u6807\u662f\u6700\u51c6\u786e\u7684\u6062\u590d\uff0c\u800c\u975e\u8986\u76d6\u540e\u9a8c\uff0c\u800c\u73b0\u6709\u4f18\u5316\u65b9\u6cd5\u7f3a\u4e4f\u660e\u786e\u7684\u6982\u7387\u57fa\u7840\u3002LMAPS\u65e8\u5728\u63d0\u4f9b\u4e00\u4e2a\u7edf\u4e00\u7684\u6982\u7387\u89e3\u91ca\u6846\u67b6\u3002", "method": "LMAPS\u63d0\u51fa\u5c40\u90e8MAP\u5b50\u95ee\u9898\u8fed\u4ee3\u89e3\u51b3\u7684\u65b9\u6cd5\uff0c\u7ed3\u5408\u6982\u7387\u89e3\u91ca\u7684\u534f\u65b9\u5dee\u8fd1\u4f3c\u3001\u7a33\u5b9a\u6027\u548c\u53ef\u89e3\u91ca\u6027\u7684\u76ee\u6807\u91cd\u6784\u4ee5\u53ca\u5bf9\u975e\u53ef\u5fae\u5206\u7b97\u5b50\u7684\u68af\u5ea6\u8fd1\u4f3c\u3002", "result": "LMAPS\u5728\u591a\u79cd\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u5982\u8fd0\u52a8\u53bb\u6a21\u7cca\u3001JPEG\u6062\u590d\u548c\u91cf\u5316\u4e2d\u63d0\u5347\u22652 dB\uff0c\u9006\u6563\u5c04\u57fa\u51c6\u4e2d\u63d0\u5347>1.5 dB\u3002", "conclusion": "LMAPS\u4e3a\u4f18\u5316\u65b9\u6cd5\u63d0\u4f9b\u4e86\u7edf\u4e00\u7684\u6982\u7387\u89e3\u91ca\uff0c\u5e76\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u8868\u73b0\u5353\u8d8a\uff0c\u6210\u4e3a\u89e3\u51b3\u9006\u95ee\u9898\u7684\u6709\u529b\u5de5\u5177\u3002"}}
{"id": "2510.08453", "categories": ["cs.GT", "91A44, 91A20, 28E05"], "pdf": "https://arxiv.org/pdf/2510.08453", "abs": "https://arxiv.org/abs/2510.08453", "authors": ["Kiri Sakahara", "Takashi Sato"], "title": "Extending Games beyond the Finite Horizon", "comment": "34 pages, 11 figures", "summary": "This paper argues that the finite horizon paradox, where game theory\ncontradicts intuition, stems from the limitations of standard number systems in\nmodelling the cognitive perception of infinity. To address this issue, we\npropose a new framework based on Alternative Set Theory (AST). This framework\nrepresents different cognitive perspectives on a long history of events using\ndistinct topologies. These topologies define an indiscernibility equivalence\nthat formally treats huge, indistinguishable quantities as equivalent. This\noffers criterion-dependent resolutions to long-standing paradoxes, such as\nSelten's chain store paradox and Rosenthal's centipede game. Our framework\nreveals new intuitive subgame perfect equilibria, the characteristics of which\ndepend on the chosen temporal perspective and payoff evaluation. Ultimately, by\ngrounding its mathematical foundation in different modes of human cognition,\nour work expands the explanatory power of game theory for long-horizon\nscenarios.", "AI": {"tldr": "\u672c\u6587\u8ba4\u4e3a\u6709\u9650\u89c6\u754c\u6096\u8bba\u6e90\u4e8e\u6807\u51c6\u6570\u7cfb\u5728\u5efa\u6a21\u65e0\u9650\u8ba4\u77e5\u65f6\u7684\u5c40\u9650\u6027\uff0c\u63d0\u51fa\u57fa\u4e8e\u66ff\u4ee3\u96c6\u5408\u8bba\u7684\u65b0\u6846\u67b6\uff0c\u901a\u8fc7\u4e0d\u540c\u62d3\u6251\u89e3\u51b3\u6096\u8bba\u3002", "motivation": "\u6709\u9650\u89c6\u754c\u6096\u8bba\u663e\u793a\u6807\u51c6\u6570\u7cfb\u65e0\u6cd5\u6a21\u62df\u4eba\u7c7b\u5bf9\u65e0\u9650\u7684\u8ba4\u77e5\uff0c\u9700\u65b0\u65b9\u6cd5\u89e3\u51b3\u6b64\u7c7b\u6096\u8bba\u3002", "method": "\u63d0\u51fa\u57fa\u4e8e\u66ff\u4ee3\u96c6\u5408\u8bba\u7684\u6846\u67b6\uff0c\u5229\u7528\u4e0d\u540c\u62d3\u6251\u8868\u793a\u8ba4\u77e5\u89c6\u89d2\uff0c\u5b9a\u4e49\u4e0d\u53ef\u533a\u5206\u7b49\u4ef7\u5173\u7cfb\u3002", "result": "\u6846\u67b6\u4e3a\u65b0\u76f4\u89c9\u5b50\u535a\u5f08\u5b8c\u7f8e\u5747\u8861\u63d0\u4f9b\u4f9d\u636e\uff0c\u5e76\u89e3\u51b3\u957f\u671f\u5b58\u5728\u7684\u6096\u8bba\uff0c\u5982Selten\u8fde\u9501\u5e97\u6096\u8bba\u3002", "conclusion": "\u901a\u8fc7\u57fa\u4e8e\u4eba\u7c7b\u8ba4\u77e5\u7684\u6570\u5b66\u57fa\u7840\uff0c\u6269\u5c55\u4e86\u535a\u5f08\u8bba\u5728\u957f\u671f\u573a\u666f\u4e2d\u7684\u89e3\u91ca\u529b\u3002"}}
{"id": "2510.07638", "categories": ["cs.GR"], "pdf": "https://arxiv.org/pdf/2510.07638", "abs": "https://arxiv.org/abs/2510.07638", "authors": ["Kinjal Parikh", "Danny M. Kaufman", "David I. W. Levin", "Alec Jacobson"], "title": "Differentiable Variable Fonts", "comment": null, "summary": "Editing and animating text appearance for graphic designs, commercials, etc.\nremain highly skilled tasks requiring detailed, hands on efforts from artists.\nAutomating these manual workflows requires balancing the competing goals of\nmaintaining legibility and aesthetics of text, while enabling creative\nexpression. Variable fonts, recent parametric extensions to traditional fonts,\noffer the promise of new ways to ease and automate typographic design and\nanimation. Variable fonts provide custom constructed parameters along which\nfonts can be smoothly varied. These parameterizations could then potentially\nserve as high value continuous design spaces, opening the door to automated\ndesign optimization tools. However, currently variable fonts are underutilized\nin creative applications, because artists so far still need to manually tune\nfont parameters. Our work opens the door to intuitive and automated font design\nand animation workflows with differentiable variable fonts. To do so we distill\nthe current variable font specification to a compact mathematical formulation\nthat differentiably connects the highly non linear, non invertible mapping of\nvariable font parameters to the underlying vector graphics representing the\ntext. This enables us to construct a differentiable framework, with respect to\nvariable font parameters, allowing us to perform gradient based optimization of\nenergies defined on vector graphics control points, and on target rasterized\nimages. We demonstrate the utility of this framework with four applications:\ndirect shape manipulation, overlap aware modeling, physics based text\nanimation, and automated font design optimization. Our work now enables\nleveraging the carefully designed affordances of variable fonts with\ndifferentiability to use modern design optimization technologies, opening new\npossibilities for easy and intuitive typographic design workflows.", "AI": {"tldr": "\u901a\u8fc7\u53ef\u5fae\u5206\u53d8\u91cf\u5b57\u4f53\u6280\u672f\uff0c\u5b9e\u73b0\u4e86\u81ea\u52a8\u5316\u5b57\u4f53\u8bbe\u8ba1\u548c\u52a8\u753b\u5de5\u4f5c\u6d41\uff0c\u63d0\u5347\u4e86\u521b\u610f\u8868\u8fbe\u7684\u6548\u7387\u548c\u8d28\u91cf\u3002", "motivation": "\u4f20\u7edf\u5b57\u4f53\u8bbe\u8ba1\u548c\u52a8\u753b\u9700\u8981\u827a\u672f\u5bb6\u624b\u52a8\u8c03\u6574\uff0c\u53d8\u91cf\u5b57\u4f53\u867d\u63d0\u4f9b\u4e86\u53c2\u6570\u5316\u6269\u5c55\uff0c\u4f46\u4ecd\u672a\u88ab\u5145\u5206\u5229\u7528\u3002\u672c\u6587\u65e8\u5728\u901a\u8fc7\u53ef\u5fae\u5206\u65b9\u6cd5\u7b80\u5316\u8fd9\u4e00\u8fc7\u7a0b\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7d27\u51d1\u7684\u6570\u5b66\u516c\u5f0f\uff0c\u5c06\u53d8\u91cf\u5b57\u4f53\u53c2\u6570\u4e0e\u5e95\u5c42\u77e2\u91cf\u56fe\u5f62\u975e\u7ebf\u6027\u6620\u5c04\u8fde\u63a5\u8d77\u6765\uff0c\u6784\u5efa\u4e86\u53ef\u5fae\u5206\u6846\u67b6\uff0c\u652f\u6301\u68af\u5ea6\u4f18\u5316\u3002", "result": "\u5c55\u793a\u4e86\u56db\u79cd\u5e94\u7528\uff1a\u76f4\u63a5\u5f62\u72b6\u64cd\u7eb5\u3001\u91cd\u53e0\u611f\u77e5\u5efa\u6a21\u3001\u57fa\u4e8e\u7269\u7406\u7684\u6587\u672c\u52a8\u753b\u548c\u81ea\u52a8\u5316\u5b57\u4f53\u8bbe\u8ba1\u4f18\u5316\u3002", "conclusion": "\u53ef\u5fae\u5206\u53d8\u91cf\u5b57\u4f53\u6280\u672f\u4e3a\u73b0\u4ee3\u8bbe\u8ba1\u4f18\u5316\u63d0\u4f9b\u4e86\u65b0\u5de5\u5177\uff0c\u5f00\u542f\u4e86\u76f4\u89c2\u3001\u9ad8\u6548\u7684\u5b57\u4f53\u8bbe\u8ba1\u548c\u52a8\u753b\u5de5\u4f5c\u6d41\u3002"}}
{"id": "2510.07868", "categories": ["cs.GR"], "pdf": "https://arxiv.org/pdf/2510.07868", "abs": "https://arxiv.org/abs/2510.07868", "authors": ["Haojie Jin", "Jierui Ren", "Yisong Chen", "Guoping Wang", "Sheng Li"], "title": "NRRS: Neural Russian Roulette and Splitting", "comment": "15 pages", "summary": "We propose a novel framework for Russian Roulette and Splitting (RRS)\ntailored to wavefront path tracing, a highly parallel rendering architecture\nthat processes path states in batched, stage-wise execution for efficient GPU\nutilization. Traditional RRS methods, with unpredictable path counts, are\nfundamentally incompatible with wavefront's preallocated memory and scheduling\nrequirements. To resolve this, we introduce a normalized RRS formulation with a\nbounded path count, enabling stable and memory-efficient execution.\n  Furthermore, we pioneer the use of neural networks to learn RRS factors,\npresenting two models: NRRS and AID-NRRS. At a high level, both feature a\ncarefully designed RRSNet that explicitly incorporates RRS normalization, with\nonly subtle differences in their implementation. To balance computational cost\nand inference accuracy, we introduce Mix-Depth, a path-depth-aware mechanism\nthat adaptively regulates neural evaluation, further improving efficiency.\n  Extensive experiments demonstrate that our method outperforms traditional\nheuristics and recent RRS techniques in both rendering quality and performance\nacross a variety of complex scenes.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u4e13\u4e3a\u6ce2\u524d\u8def\u5f84\u8ffd\u8e2a\u8bbe\u8ba1\u7684\u4fc4\u7f57\u65af\u8f6e\u76d8\u8d4c\u4e0e\u5206\u5272\uff08RRS\uff09\u65b0\u6846\u67b6\uff0c\u89e3\u51b3\u4e86\u4f20\u7edfRRS\u65b9\u6cd5\u4e0e\u6ce2\u524d\u67b6\u6784\u5185\u5b58\u9884\u5206\u914d\u548c\u8c03\u5ea6\u9700\u6c42\u4e0d\u517c\u5bb9\u7684\u95ee\u9898\u3002\u901a\u8fc7\u5f15\u5165\u5f52\u4e00\u5316RRS\u516c\u5f0f\u548c\u6709\u754c\u8def\u5f84\u8ba1\u6570\uff0c\u63d0\u9ad8\u4e86\u6267\u884c\u7684\u7a33\u5b9a\u6027\u548c\u5185\u5b58\u6548\u7387\u3002\u6b64\u5916\uff0c\u8fd8\u9996\u6b21\u4f7f\u7528\u795e\u7ecf\u7f51\u7edc\u5b66\u4e60RRS\u56e0\u5b50\uff0c\u63d0\u51fa\u4e86NRRS\u548cAID-NRRS\u4e24\u79cd\u6a21\u578b\uff0c\u5e76\u901a\u8fc7Mix-Depth\u673a\u5236\u5e73\u8861\u8ba1\u7b97\u6210\u672c\u4e0e\u63a8\u65ad\u7cbe\u5ea6\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6e32\u67d3\u8d28\u91cf\u548c\u6027\u80fd\u3002", "motivation": "\u4f20\u7edf\u7684\u4fc4\u7f57\u65af\u8f6e\u76d8\u8d4c\u4e0e\u5206\u5272\uff08RRS\uff09\u65b9\u6cd5\u56e0\u8def\u5f84\u8ba1\u6570\u4e0d\u53ef\u9884\u6d4b\uff0c\u65e0\u6cd5\u517c\u5bb9\u6ce2\u524d\u8def\u5f84\u8ffd\u8e2a\u8fd9\u4e00\u9ad8\u5ea6\u5e76\u884c\u7684\u6e32\u67d3\u67b6\u6784\u7684\u5185\u5b58\u9884\u5206\u914d\u548c\u8c03\u5ea6\u9700\u6c42\u3002\u672c\u6587\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\uff0c\u5e76\u63d0\u51fa\u66f4\u9ad8\u6548\u7684RRS\u5b9e\u73b0\u65b9\u6848\u3002", "method": "1. \u63d0\u51fa\u5f52\u4e00\u5316RRS\u516c\u5f0f\uff0c\u9650\u5b9a\u8def\u5f84\u8ba1\u6570\u4ee5\u5b9e\u73b0\u7a33\u5b9a\u548c\u5185\u5b58\u9ad8\u6548\u7684\u6267\u884c\uff1b2. \u521b\u65b0\u6027\u5730\u4f7f\u7528\u795e\u7ecf\u7f51\u7edc\u5b66\u4e60RRS\u56e0\u5b50\uff0c\u8bbe\u8ba1NRRS\u548cAID-NRRS\u4e24\u79cd\u6a21\u578b\uff1b3. \u5f15\u5165Mix-Depth\u673a\u5236\uff0c\u6839\u636e\u8def\u5f84\u6df1\u5ea6\u81ea\u9002\u5e94\u8c03\u8282\u795e\u7ecf\u7f51\u7edc\u8bc4\u4f30\uff0c\u5e73\u8861\u8ba1\u7b97\u6210\u672c\u548c\u63a8\u65ad\u7cbe\u5ea6\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u5404\u79cd\u590d\u6742\u573a\u666f\u4e2d\uff0c\u65e0\u8bba\u662f\u6e32\u67d3\u8d28\u91cf\u8fd8\u662f\u6027\u80fd\uff0c\u5747\u4f18\u4e8e\u4f20\u7edf\u542f\u53d1\u5f0f\u65b9\u6cd5\u548c\u8fd1\u671f\u7684RRS\u6280\u672f\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u7684RRS\u65b0\u6846\u67b6\u53ca\u5176\u795e\u7ecf\u7f51\u7edc\u5b9e\u73b0\uff0c\u901a\u8fc7\u5f52\u4e00\u5316\u8bbe\u8ba1\u548c\u81ea\u9002\u5e94\u673a\u5236\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u6ce2\u524d\u8def\u5f84\u8ffd\u8e2a\u7684\u6548\u7387\u548c\u8d28\u91cf\uff0c\u4e3a\u5e76\u884c\u6e32\u67d3\u67b6\u6784\u4e2d\u7684RRS\u5e94\u7528\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2510.08166", "categories": ["cs.GR"], "pdf": "https://arxiv.org/pdf/2510.08166", "abs": "https://arxiv.org/abs/2510.08166", "authors": ["Elias Kristmann", "Markus Sch\u00fctz", "Michael Wimmer"], "title": "Variable-Rate Texture Compression: Real-Time Rendering with JPEG", "comment": null, "summary": "Although variable-rate compressed image formats such as JPEG are widely used\nto efficiently encode images, they have not found their way into real-time\nrendering due to special requirements such as random access to individual\ntexels. In this paper, we investigate the feasibility of variable-rate texture\ncompression on modern GPUs using the JPEG format, and how it compares to the\nGPU-friendly fixed-rate compression approaches BC1 and ASTC. Using a deferred\nrendering pipeline, we are able to identify the subset of blocks that are\nneeded for a given frame, decode these, and colorize the framebuffer's pixels.\nDespite the additional $\\sim$0.17 bit per pixel that we require for our\napproach, JPEG maintains significantly better quality and compression rates\ncompared to BC1, and depending on the type of image, outperforms or competes\nwith ASTC. The JPEG rendering pipeline increases rendering duration by less\nthan 0.3 ms on an RTX 4090, demonstrating that sophisticated variable-rate\ncompression schemes are feasible on modern GPUs, even in VR. Source code and\ndata sets are available at: https://github.com/elias1518693/jpeg_textures", "AI": {"tldr": "\u8be5\u8bba\u6587\u7814\u7a76\u4e86\u5728\u73b0\u4ee3GPU\u4e0a\u4f7f\u7528JPEG\u683c\u5f0f\u8fdb\u884c\u53ef\u53d8\u901f\u7387\u7eb9\u7406\u538b\u7f29\u7684\u53ef\u884c\u6027\uff0c\u5e76\u4e0e\u56fa\u5b9a\u901f\u7387\u538b\u7f29\u65b9\u6cd5BC1\u548cASTC\u8fdb\u884c\u4e86\u6bd4\u8f83\u3002\u7ed3\u679c\u8868\u660e\uff0cJPEG\u5728\u8d28\u91cf\u548c\u538b\u7f29\u7387\u4e0a\u4f18\u4e8eBC1\uff0c\u4e0eASTC\u76f8\u5f53\uff0c\u4e14\u6e32\u67d3\u65f6\u95f4\u4ec5\u589e\u52a0\u4e0d\u52300.3\u6beb\u79d2\u3002", "motivation": "\u5c3d\u7ba1\u53ef\u53d8\u901f\u7387\u538b\u7f29\u56fe\u50cf\u683c\u5f0f\uff08\u5982JPEG\uff09\u88ab\u5e7f\u6cdb\u7528\u4e8e\u9ad8\u6548\u7f16\u7801\u56fe\u50cf\uff0c\u4f46\u7531\u4e8e\u5b9e\u65f6\u6e32\u67d3\u7684\u7279\u6b8a\u9700\u6c42\uff08\u5982\u5bf9\u5355\u4e2a\u7eb9\u7406\u5143\u7d20\u7684\u968f\u673a\u8bbf\u95ee\uff09\uff0c\u8fd9\u7c7b\u683c\u5f0f\u5c1a\u672a\u5e94\u7528\u4e8e\u5b9e\u65f6\u6e32\u67d3\u3002\u8bba\u6587\u65e8\u5728\u63a2\u7d22JPEG\u5728\u73b0\u4ee3GPU\u4e0a\u7684\u5e94\u7528\u6f5c\u529b\u3002", "method": "\u8bba\u6587\u91c7\u7528\u5ef6\u8fdf\u6e32\u67d3\u7ba1\u7ebf\uff0c\u8bc6\u522b\u6bcf\u5e27\u6240\u9700\u7684\u7eb9\u7406\u5757\u5b50\u96c6\uff0c\u89e3\u7801\u8fd9\u4e9b\u5757\u5e76\u4e3a\u5e27\u7f13\u51b2\u533a\u50cf\u7d20\u7740\u8272\u3002\u4f7f\u7528JPEG\u683c\u5f0f\u8fdb\u884c\u6bd4\u8f83\uff0c\u5e76\u4e0e\u56fa\u5b9a\u901f\u7387\u538b\u7f29\u65b9\u6cd5BC1\u548cASTC\u8fdb\u884c\u5bf9\u6bd4\u3002", "result": "\u5c3d\u7ba1JPEG\u65b9\u6cd5\u9700\u8981\u989d\u5916\u7684\u7ea60.17\u4f4d\u6bcf\u50cf\u7d20\uff0c\u4f46\u5176\u5728\u8d28\u91cf\u548c\u538b\u7f29\u7387\u4e0a\u663e\u8457\u4f18\u4e8eBC1\uff0c\u5e76\u4e0eASTC\u76f8\u5f53\u6216\u4f18\u4e8eASTC\u3002\u5728RTX 4090\u4e0a\uff0cJPEG\u6e32\u67d3\u7ba1\u7ebf\u4ec5\u589e\u52a0\u4e0d\u52300.3\u6beb\u79d2\u7684\u6e32\u67d3\u65f6\u95f4\u3002", "conclusion": "\u7814\u7a76\u8868\u660e\uff0c\u5373\u4f7f\u5728VR\u7b49\u9ad8\u6027\u80fd\u9700\u6c42\u573a\u666f\u4e2d\uff0c\u73b0\u4ee3GPU\u4e0a\u91c7\u7528\u590d\u6742\u7684\u53ef\u53d8\u901f\u7387\u538b\u7f29\u65b9\u6848\u662f\u53ef\u884c\u7684\u3002JPEG\u683c\u5f0f\u5728\u8d28\u91cf\u548c\u6548\u7387\u4e0a\u7684\u8868\u73b0\u4f7f\u5176\u6210\u4e3a\u5b9e\u65f6\u6e32\u67d3\u7684\u53ef\u884c\u9009\u62e9\u3002"}}
{"id": "2510.08271", "categories": ["cs.GR", "cs.CV"], "pdf": "https://arxiv.org/pdf/2510.08271", "abs": "https://arxiv.org/abs/2510.08271", "authors": ["Andreas Engelhardt", "Mark Boss", "Vikram Voletti", "Chun-Han Yao", "Hendrik P. A. Lensch", "Varun Jampani"], "title": "SViM3D: Stable Video Material Diffusion for Single Image 3D Generation", "comment": "Accepted by International Conference on Computer Vision (ICCV 2025).\n  Project page: http://svim3d.aengelhardt.com", "summary": "We present Stable Video Materials 3D (SViM3D), a framework to predict\nmulti-view consistent physically based rendering (PBR) materials, given a\nsingle image. Recently, video diffusion models have been successfully used to\nreconstruct 3D objects from a single image efficiently. However, reflectance is\nstill represented by simple material models or needs to be estimated in\nadditional steps to enable relighting and controlled appearance edits. We\nextend a latent video diffusion model to output spatially varying PBR\nparameters and surface normals jointly with each generated view based on\nexplicit camera control. This unique setup allows for relighting and generating\na 3D asset using our model as neural prior. We introduce various mechanisms to\nthis pipeline that improve quality in this ill-posed setting. We show\nstate-of-the-art relighting and novel view synthesis performance on multiple\nobject-centric datasets. Our method generalizes to diverse inputs, enabling the\ngeneration of relightable 3D assets useful in AR/VR, movies, games and other\nvisual media.", "AI": {"tldr": "SViM3D\u662f\u4e00\u4e2a\u901a\u8fc7\u5355\u5f20\u56fe\u50cf\u9884\u6d4b\u591a\u89c6\u89d2\u4e00\u81f4\u7684\u57fa\u4e8e\u7269\u7406\u7684\u6e32\u67d3\uff08PBR\uff09\u6750\u6599\u7684\u6846\u67b6\u3002", "motivation": "\u73b0\u6709\u7684\u89c6\u9891\u6269\u6563\u6a21\u578b\u867d\u80fd\u4ece\u5355\u5f20\u56fe\u50cf\u9ad8\u6548\u91cd\u5efa3D\u7269\u4f53\uff0c\u4f46\u53cd\u5c04\u5c5e\u6027\u4ecd\u7531\u7b80\u5355\u6750\u6599\u6a21\u578b\u8868\u793a\u6216\u9700\u989d\u5916\u6b65\u9aa4\u4f30\u7b97\uff0c\u9650\u5236\u4e86\u91cd\u65b0\u7167\u660e\u548c\u5916\u89c2\u7f16\u8f91\u7684\u63a7\u5236\u3002", "method": "\u6269\u5c55\u4e86\u6f5c\u5728\u89c6\u9891\u6269\u6563\u6a21\u578b\uff0c\u57fa\u4e8e\u663e\u5f0f\u76f8\u673a\u63a7\u5236\u8054\u5408\u8f93\u51fa\u7a7a\u95f4\u53d8\u5316\u7684PBR\u53c2\u6570\u548c\u8868\u9762\u6cd5\u7ebf\uff0c\u5e76\u5f15\u5165\u591a\u79cd\u673a\u5236\u63d0\u5347\u8d28\u91cf\u3002", "result": "\u5728\u591a\u4e2a\u4ee5\u7269\u4f53\u4e3a\u4e2d\u5fc3\u7684\u6570\u636e\u96c6\u4e0a\uff0c\u5c55\u793a\u4e86\u6700\u5148\u8fdb\u7684\u91cd\u65b0\u7167\u660e\u548c\u65b0\u89c6\u89d2\u5408\u6210\u6027\u80fd\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u9002\u7528\u4e8e\u591a\u6837\u5316\u8f93\u5165\uff0c\u80fd\u751f\u6210\u53ef\u7528\u4e8eAR/VR\u3001\u7535\u5f71\u3001\u6e38\u620f\u7b49\u89c6\u89c9\u5a92\u4f53\u7684\u53ef\u91cd\u65b0\u7167\u660e3D\u8d44\u4ea7\u3002"}}
{"id": "2510.08394", "categories": ["cs.GR", "cs.CV"], "pdf": "https://arxiv.org/pdf/2510.08394", "abs": "https://arxiv.org/abs/2510.08394", "authors": ["Mustafa B. Yaldiz", "Ishit Mehta", "Nithin Raghavan", "Andreas Meuleman", "Tzu-Mao Li", "Ravi Ramamoorthi"], "title": "Spectral Prefiltering of Neural Fields", "comment": "16 pages, 10 figures, to be published in Siggraph Asia 2025, Website:\n  https://myaldiz.info/assets/spnf", "summary": "Neural fields excel at representing continuous visual signals but typically\noperate at a single, fixed resolution. We present a simple yet powerful method\nto optimize neural fields that can be prefiltered in a single forward pass. Key\ninnovations and features include: (1) We perform convolutional filtering in the\ninput domain by analytically scaling Fourier feature embeddings with the\nfilter's frequency response. (2) This closed-form modulation generalizes beyond\nGaussian filtering and supports other parametric filters (Box and Lanczos) that\nare unseen at training time. (3) We train the neural field using single-sample\nMonte Carlo estimates of the filtered signal. Our method is fast during both\ntraining and inference, and imposes no additional constraints on the network\narchitecture. We show quantitative and qualitative improvements over existing\nmethods for neural-field filtering.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7b80\u5355\u800c\u5f3a\u5927\u7684\u65b9\u6cd5\u6765\u4f18\u5316\u795e\u7ecf\u7f51\u7edc\u573a\uff0c\u652f\u6301\u5355\u6b21\u524d\u5411\u4f20\u9012\u4e2d\u7684\u9884\u6ee4\u6ce2\uff0c\u63d0\u9ad8\u4e86\u6548\u7387\u548c\u7075\u6d3b\u6027\u3002", "motivation": "\u4f20\u7edf\u795e\u7ecf\u7f51\u7edc\u573a\u901a\u5e38\u5728\u5355\u4e00\u56fa\u5b9a\u5206\u8fa8\u7387\u4e0b\u8fd0\u884c\uff0c\u9650\u5236\u4e86\u5176\u5e94\u7528\u7075\u6d3b\u6027\u3002\u672c\u6587\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\uff0c\u901a\u8fc7\u521b\u65b0\u7684\u65b9\u6cd5\u5b9e\u73b0\u591a\u5206\u8fa8\u7387\u6ee4\u6ce2\u3002", "method": "\u65b9\u6cd5\u5305\u62ec\uff1a(1)\u5728\u8f93\u5165\u57df\u8fdb\u884c\u5377\u79ef\u6ee4\u6ce2\uff0c\u901a\u8fc7\u5085\u91cc\u53f6\u7279\u5f81\u5d4c\u5165\u7684\u9891\u7387\u54cd\u5e94\u5206\u6790\u7f29\u653e\uff1b(2)\u652f\u6301\u9ad8\u65af\u6ee4\u6ce2\u4e4b\u5916\u7684\u53c2\u6570\u5316\u6ee4\u6ce2\uff1b(3)\u4f7f\u7528\u5355\u6837\u672c\u8499\u7279\u5361\u7f57\u4f30\u8ba1\u8bad\u7ec3\u795e\u7ecf\u7f51\u7edc\u573a\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u8bad\u7ec3\u548c\u63a8\u7406\u9636\u6bb5\u5747\u5feb\u901f\u6709\u6548\uff0c\u4e14\u5728\u795e\u7ecf\u7f51\u7edc\u573a\u6ee4\u6ce2\u65b9\u9762\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u63d0\u4f9b\u4e86\u5b9a\u91cf\u548c\u5b9a\u6027\u7684\u6539\u8fdb\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u7684\u65b9\u6cd5\u4e0d\u4ec5\u63d0\u9ad8\u4e86\u795e\u7ecf\u7f51\u7edc\u573a\u7684\u7075\u6d3b\u6027\uff0c\u8fd8\u4e3a\u591a\u5206\u8fa8\u7387\u6ee4\u6ce2\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2510.08491", "categories": ["cs.GR", "cs.CV"], "pdf": "https://arxiv.org/pdf/2510.08491", "abs": "https://arxiv.org/abs/2510.08491", "authors": ["Xilong Zhou", "Bao-Huy Nguyen", "Lo\u00efc Magne", "Vladislav Golyanik", "Thomas Leimk\u00fchler", "Christian Theobalt"], "title": "Splat the Net: Radiance Fields with Splattable Neural Primitives", "comment": null, "summary": "Radiance fields have emerged as a predominant representation for modeling 3D\nscene appearance. Neural formulations such as Neural Radiance Fields provide\nhigh expressivity but require costly ray marching for rendering, whereas\nprimitive-based methods such as 3D Gaussian Splatting offer real-time\nefficiency through splatting, yet at the expense of representational power.\nInspired by advances in both these directions, we introduce splattable neural\nprimitives, a new volumetric representation that reconciles the expressivity of\nneural models with the efficiency of primitive-based splatting. Each primitive\nencodes a bounded neural density field parameterized by a shallow neural\nnetwork. Our formulation admits an exact analytical solution for line\nintegrals, enabling efficient computation of perspectively accurate splatting\nkernels. As a result, our representation supports integration along view rays\nwithout the need for costly ray marching. The primitives flexibly adapt to\nscene geometry and, being larger than prior analytic primitives, reduce the\nnumber required per scene. On novel-view synthesis benchmarks, our approach\nmatches the quality and speed of 3D Gaussian Splatting while using $10\\times$\nfewer primitives and $6\\times$ fewer parameters. These advantages arise\ndirectly from the representation itself, without reliance on complex control or\nadaptation frameworks. The project page is\nhttps://vcai.mpi-inf.mpg.de/projects/SplatNet/.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u4f53\u79ef\u8868\u793a\u65b9\u6cd5\uff0c\u7ed3\u5408\u4e86\u795e\u7ecf\u6a21\u578b\u7684\u8868\u73b0\u529b\u548c\u57fa\u4e8e\u56fe\u5143\u7684\u5feb\u901f\u6e32\u67d3\u6548\u7387\uff0c\u79f0\u4e3a\u53ef\u6492\u5e03\u795e\u7ecf\u56fe\u5143\u3002", "motivation": "\u5f53\u524d\u76843D\u573a\u666f\u5916\u89c2\u5efa\u6a21\u65b9\u6cd5\u4e2d\uff0c\u795e\u7ecf\u8f90\u5c04\u573a\u8868\u73b0\u529b\u5f3a\u4f46\u6e32\u67d3\u6210\u672c\u9ad8\uff0c\u800c\u57fa\u4e8e\u56fe\u5143\u7684\u65b9\u6cd5\uff08\u59823D\u9ad8\u65af\u6cfc\u6e85\uff09\u867d\u7136\u5b9e\u65f6\u6027\u5f3a\u4f46\u8868\u73b0\u529b\u4e0d\u8db3\u3002\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u8868\u793a\u65b9\u6cd5\uff0c\u65e8\u5728\u517c\u987e\u4e24\u8005\u4f18\u52bf\u3002", "method": "\u901a\u8fc7\u5f15\u5165\u53ef\u6492\u5e03\u795e\u7ecf\u56fe\u5143\uff0c\u6bcf\u4e2a\u56fe\u5143\u7f16\u7801\u4e00\u4e2a\u6709\u754c\u7684\u795e\u7ecf\u5bc6\u5ea6\u573a\uff0c\u5e76\u7531\u6d45\u5c42\u795e\u7ecf\u7f51\u7edc\u53c2\u6570\u5316\u3002\u8be5\u65b9\u6cd5\u652f\u6301\u7cbe\u786e\u7684\u89e3\u6790\u89e3\u8ba1\u7b97\u89c6\u7ebf\u79ef\u5206\uff0c\u65e0\u9700\u6602\u8d35\u7684\u5149\u7ebf\u884c\u8fdb\u3002", "result": "\u5728\u65b0\u89c6\u89d2\u5408\u6210\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u8be5\u65b9\u6cd5\u5728\u8d28\u91cf\u548c\u901f\u5ea6\u4e0a\u4e0e3D\u9ad8\u65af\u6cfc\u6e85\u76f8\u5f53\uff0c\u4f46\u4f7f\u7528\u7684\u56fe\u5143\u6570\u91cf\u51cf\u5c11\u4e8610\u500d\uff0c\u53c2\u6570\u51cf\u5c11\u4e866\u500d\u3002", "conclusion": "\u63d0\u51fa\u7684\u8868\u793a\u65b9\u6cd5\u76f4\u63a5\u5b9e\u73b0\u4e86\u9ad8\u6548\u548c\u9ad8\u8868\u73b0\u529b\u7684\u5e73\u8861\uff0c\u65e0\u9700\u4f9d\u8d56\u590d\u6742\u7684\u63a7\u5236\u6216\u9002\u914d\u6846\u67b6\u3002"}}
{"id": "2510.08530", "categories": ["cs.GR", "cs.CV", "68U05", "I.3.3; I.3.6"], "pdf": "https://arxiv.org/pdf/2510.08530", "abs": "https://arxiv.org/abs/2510.08530", "authors": ["Zhitong Huang", "Mohan Zhang", "Renhan Wang", "Rui Tang", "Hao Zhu", "Jing Liao"], "title": "X2Video: Adapting Diffusion Models for Multimodal Controllable Neural Video Rendering", "comment": "Code, model, and dataset will be released at project page soon:\n  https://luckyhzt.github.io/x2video", "summary": "We present X2Video, the first diffusion model for rendering photorealistic\nvideos guided by intrinsic channels including albedo, normal, roughness,\nmetallicity, and irradiance, while supporting intuitive multi-modal controls\nwith reference images and text prompts for both global and local regions. The\nintrinsic guidance allows accurate manipulation of color, material, geometry,\nand lighting, while reference images and text prompts provide intuitive\nadjustments in the absence of intrinsic information. To enable these\nfunctionalities, we extend the intrinsic-guided image generation model XRGB to\nvideo generation by employing a novel and efficient Hybrid Self-Attention,\nwhich ensures temporal consistency across video frames and also enhances\nfidelity to reference images. We further develop a Masked Cross-Attention to\ndisentangle global and local text prompts, applying them effectively onto\nrespective local and global regions. For generating long videos, our novel\nRecursive Sampling method incorporates progressive frame sampling, combining\nkeyframe prediction and frame interpolation to maintain long-range temporal\nconsistency while preventing error accumulation. To support the training of\nX2Video, we assembled a video dataset named InteriorVideo, featuring 1,154\nrooms from 295 interior scenes, complete with reliable ground-truth intrinsic\nchannel sequences and smooth camera trajectories. Both qualitative and\nquantitative evaluations demonstrate that X2Video can produce long, temporally\nconsistent, and photorealistic videos guided by intrinsic conditions.\nAdditionally, X2Video effectively accommodates multi-modal controls with\nreference images, global and local text prompts, and simultaneously supports\nediting on color, material, geometry, and lighting through parametric tuning.\nProject page: https://luckyhzt.github.io/x2video", "AI": {"tldr": "X2Video\u662f\u9996\u500b\u57fa\u65bcRGB\u64f4\u6563\u6a21\u578b\u751f\u6210\u903c\u771f\u8996\u983b\u7684\u7cfb\u7d71\uff0c\u652f\u6301\u591a\u6a21\u614b\u63a7\u5236\uff08\u53c3\u8003\u5716\u50cf\u548c\u6587\u672c\u63d0\u793a\uff09\u548c\u5167\u5728\u901a\u9053\uff08\u5982\u53cd\u7167\u7387\u3001\u6cd5\u7dda\u7b49\uff09\u5f15\u5c0e\u3002", "motivation": "\u73fe\u6709\u8996\u983b\u751f\u6210\u6280\u8853\u96e3\u4ee5\u7cbe\u78ba\u63a7\u5236\u984f\u8272\u3001\u6750\u6599\u3001\u5e7e\u4f55\u548c\u5149\u7167\u7b49\u5167\u5728\u5c6c\u6027\uff0c\u4e14\u7f3a\u4e4f\u591a\u6a21\u614b\u76f4\u89c0\u63a7\u5236\u3002X2Video\u65e8\u5728\u89e3\u6c7a\u9019\u4e9b\u554f\u984c\u3002", "method": "X2Video\u64f4\u5c55\u4e86XRGB\u6a21\u578b\uff0c\u5f15\u5165\u6df7\u5408\u81ea\u6ce8\u610f\u529b\u78ba\u4fdd\u6642\u9593\u4e00\u81f4\u6027\uff0c\u4e26\u958b\u767c\u906e\u853d\u4ea4\u53c9\u6ce8\u610f\u529b\u8655\u7406\u5168\u5c40\u548c\u5c40\u90e8\u6587\u672c\u63d0\u793a\u3002\u9084\u63a1\u7528\u905e\u6b78\u63a1\u6a23\u751f\u6210\u9577\u8996\u983b\u3002", "result": "X2Video\u80fd\u751f\u6210\u9577\u6642\u9593\u4e00\u81f4\u7684\u903c\u771f\u8996\u983b\uff0c\u4e26\u652f\u6301\u901a\u904e\u53c3\u6578\u8abf\u6574\u7de8\u8f2f\u984f\u8272\u3001\u6750\u6599\u3001\u5e7e\u4f55\u548c\u5149\u7167\uff0c\u6548\u679c\u5728\u5b9a\u6027\u548c\u5b9a\u91cf\u8a55\u4f30\u4e2d\u8868\u73fe\u512a\u7570\u3002", "conclusion": "X2Video\u901a\u904e\u591a\u6a21\u614b\u63a7\u5236\u548c\u5167\u5728\u901a\u9053\u5f15\u5c0e\uff0c\u5be6\u73fe\u4e86\u9ad8\u6548\u4e14\u7cbe\u78ba\u7684\u8996\u983b\u751f\u6210\u548c\u7de8\u8f2f\uff0c\u70ba\u8996\u983b\u751f\u6210\u9818\u57df\u63d0\u4f9b\u4e86\u65b0\u7684\u89e3\u6c7a\u65b9\u6848\u3002"}}
