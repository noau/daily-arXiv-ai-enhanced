<div id=toc></div>

# Table of Contents

- [cs.GR](#cs.GR) [Total: 4]
- [cs.PL](#cs.PL) [Total: 4]
- [cs.GT](#cs.GT) [Total: 3]


<div id='cs.GR'></div>

# cs.GR [[Back]](#toc)

### [1] [PartMotionEdit: Fine-Grained Text-Driven 3D Human Motion Editing via Part-Level Modulation](https://arxiv.org/abs/2512.24200)
*Yujie Yang,Zhichao Zhang,Jiazhou Chen,Zichao Wu*

Main category: cs.GR

TL;DR: PartMotionEdit是一种新颖的细粒度运动编辑框架，通过部分级语义调制实现精确控制，优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有文本驱动的3D人体运动编辑方法在全局建模上取得了进展，但难以精确控制详细的部分特定运动。

Method: PartMotionEdit框架包含Part-aware Motion Modulation模块和Bidirectional Motion Interaction模块，通过部分级语义调制和双向跨模态注意力实现精确编辑。

Result: 在知名基准测试上的定量和定性评估表明，PartMotionEdit优于现有方法。

Conclusion: PartMotionEdit通过部分级语义调制实现了更精确的运动编辑，展现了显著的性能提升。

Abstract: Existing text-driven 3D human motion editing methods have demonstrated significant progress, but are still difficult to precisely control over detailed, part-specific motions due to their global modeling nature. In this paper, we propose PartMotionEdit, a novel fine-grained motion editing framework that operates via part-level semantic modulation. The core of PartMotionEdit is a Part-aware Motion Modulation (PMM) module, which builds upon a predefined five-part body decomposition. PMM dynamically predicts time-varying modulation weights for each body part, enabling precise and interpretable editing of local motions. To guide the training of PMM, we also introduce a part-level similarity curve supervision mechanism enhanced with dual-layer normalization. This mechanism assists PMM in learning semantically consistent and editable distributions across all body parts. Furthermore, we design a Bidirectional Motion Interaction (BMI) module. It leverages bidirectional cross-modal attention to achieve more accurate semantic alignment between textual instructions and motion semantics. Extensive quantitative and qualitative evaluations on a well-known benchmark demonstrate that PartMotionEdit outperforms the state-of-the-art methods.

</details>


### [2] [BATISNet: Instance Segmentation of Tooth Point Clouds with Boundary Awareness](https://arxiv.org/abs/2512.24201)
*Yating Cai,Yanghui Xu,Zehua Hu,Jiazhou Chen,Jing Huang*

Main category: cs.GR

TL;DR: 本文提出了一种边界感知实例网络BATISNet，用于解决牙科点云分割中的复杂病例问题，如缺失牙和错位牙。


<details>
  <summary>Details</summary>
Motivation: 现有的语义分割方法由于牙齿紧密排列、边界模糊及复杂病例多样性，难以在复杂牙科病例中取得满意效果。

Method: BATISNet由特征提取主干和实例分割模块组成，结合语义特征和实例特征学习，并设计了边界感知损失函数以提高边界分割的完整性和准确性。

Result: 实验结果表明，BATISNet在牙齿完整性分割上优于现有方法，为临床实践提供了更可靠和详细的数据支持。

Conclusion: BATISNet通过结合语义和实例特征学习及边界感知损失函数，有效解决了牙齿分割中的粘连和边界模糊问题，提升了分割的鲁棒性和准确性。

Abstract: Accurate segmentation of the tooth point cloud is of great significance for diagnosis clinical assisting and treatment planning. Existing methods mostly employ semantic segmentation, focusing on the semantic feature between different types of teeth. However, due to the tightly packed structure of teeth, unclear boundaries, and the diversity of complex cases such as missing teeth, malposed teeth, semantic segmentation often struggles to achieve satisfactory results when dealing with complex dental cases. To address these issues, this paper propose BATISNet, a boundary-aware instance network for tooth point cloud segmentation. This network model consists of a feature extraction backbone and an instance segmentation module. It not only focuses on extracting the semantic features of different types of teeth but also learns the instance features of individual teeth. It helps achieve more robust and accurate tooth instance segmentation in complex clinical scenarios such as missing teeth and malposed teeth. Additionally, to further enhance the completeness and accuracy of tooth boundary segmentation, a boundary-aware loss function is designed to specifically supervise the boundary segmentation between instances. It mitigates effectively tooth adhesion and boundary ambiguity issues. Extensive experimental results show that BATISNet outperforms existing methods in tooth integrity segmentation, providing more reliable and detailed data support for practical clinical applications.

</details>


### [3] [The Uncanny Valley in medical simulation-based training: a visual summary](https://arxiv.org/abs/2512.24240)
*Eleni Grigoriou,Manos Kamarianakis,George Papagiannakis*

Main category: cs.GR

TL;DR: 本文回顾了“恐怖谷”现象（UV）对医学虚拟现实模拟训练的影响，强调了理解这一现象的重要性，以提升医学训练的沉浸感和效果。


<details>
  <summary>Details</summary>
Motivation: 研究旨在探讨“恐怖谷”现象如何通过微小的不完美引发不适感，从而影响医学虚拟现实训练的效果，这对提高沉浸式学习体验至关重要。

Method: 研究团队汇集了计算机图形学、虚拟现实和医学教育领域的专家，采用多学科视角，结合先进的计算机图形系统和VR角色模拟技术进行研究。

Result: 研究强调了“恐怖谷”现象在医学VR训练中的关键影响，并提出需要解决这一问题以优化训练效果。

Conclusion: 理解并应对“恐怖谷”现象对于提升医学虚拟现实训练的逼真度和沉浸感具有重要意义，未来研究需进一步探索解决方法。

Abstract: The purpose of this review article is to provide a bibliographical as well as evidence-based visual guide regarding the effect of ``Uncanny Valley'' (UV) and how it profoundly influences medical virtual reality simulation-based training. The phenomenon, where increasingly realistic virtual humans elicit discomfort due to subtle imperfections, is crucial to understand and address in the context of medical training, where realism and immersion are key to effective learning.
  Our research team, consisting of experts in computer graphics, virtual reality, and medical education, brings a diverse and multidisciplinary perspective to this subject. Our collective experience spans developing advanced computer graphics systems, VR character simulation, and innovative educational technologies. We have collaborated across institutions and industries to push the boundaries of VR applications in medical training.

</details>


### [4] [PhysTalk: Language-driven Real-time Physics in 3D Gaussian Scenes](https://arxiv.org/abs/2512.24986)
*Luca Collorone,Mert Kiray,Indro Spinelli,Fabio Galasso,Benjamin Busam*

Main category: cs.GR

TL;DR: PhysTalk是一个基于3D高斯抛光的框架，通过语言输入生成实时、物理基础的4D动画，无需耗时训练或网格提取。


<details>
  <summary>Details</summary>
Motivation: 现有视觉效果生成方案缺乏物理真实性和高效语言接口，PhysTalk旨在解决这些问题，实现开放词汇、交互式的物理动画生成。

Method: PhysTalk利用大语言模型生成可执行代码，直接通过轻量级代理和粒子动力学修改3D高斯抛光参数，绕过耗时优化和网格提取。

Result: 该系统实现了实时、碰撞感知的物理动画生成，支持多材质对象的交互操作，计算轻量且无需训练。

Conclusion: PhysTalk为4D动画提供了高效、交互式的解决方案，改变了传统“渲染并等待”的工作模式。

Abstract: Realistic visual simulations are omnipresent, yet their creation requires computing time, rendering, and expert animation knowledge. Open-vocabulary visual effects generation from text inputs emerges as a promising solution that can unlock immense creative potential. However, current pipelines lack both physical realism and effective language interfaces, requiring slow offline optimization. In contrast, PhysTalk takes a 3D Gaussian Splatting (3DGS) scene as input and translates arbitrary user prompts into real time, physics based, interactive 4D animations. A large language model (LLM) generates executable code that directly modifies 3DGS parameters through lightweight proxies and particle dynamics. Notably, PhysTalk is the first framework to couple 3DGS directly with a physics simulator without relying on time consuming mesh extraction. While remaining open vocabulary, this design enables interactive 3D Gaussian animation via collision aware, physics based manipulation of arbitrary, multi material objects. Finally, PhysTalk is train-free and computationally lightweight: this makes 4D animation broadly accessible and shifts these workflows from a "render and wait" paradigm toward an interactive dialogue with a modern, physics-informed pipeline.

</details>


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [5] [Enforcing Temporal Constraints for LLM Agents](https://arxiv.org/abs/2512.23738)
*Adharsh Kamath,Sishen Zhang,Calvin Xu,Shubham Ugare,Gagandeep Singh,Sasa Misailovic*

Main category: cs.PL

TL;DR: 论文提出了Agent-C框架，通过运行时保证确保LLM代理遵守形式化的时间安全策略，显著提高了安全性和任务效用。


<details>
  <summary>Details</summary>
Motivation: 当前的安全护栏系统无法防止LLM代理违反时间安全策略，例如在未经身份验证的情况下访问敏感数据。现有方法缺乏形式化保证，导致安全问题。

Method: Agent-C引入了一种领域专用语言，用于表达时间属性，并将规范转换为一阶逻辑，同时利用SMT求解在令牌生成期间检测不合规行为，并通过约束生成技术确保合规性。

Result: 在零售客服和机票预订系统的评估中，Agent-C实现了100%的安全合规性，同时提升了任务效用，在SoTA闭源模型上显著提高了性能和安全性。

Conclusion: Agent-C为LLM代理的可靠性推理设立了新的SoTA标准，有效解决了时间安全策略的违反问题，同时提高了任务完成效率。

Abstract: LLM-based agents are deployed in safety-critical applications, yet current guardrail systems fail to prevent violations of temporal safety policies, requirements that govern the ordering and sequencing of agent actions. For instance, agents may access sensitive data before authenticating users or process refunds to unauthorized payment methods, violations that require reasoning about sequences of action rather than an individual action. Existing guardrails rely on imprecise natural language instructions or post-hoc monitoring, and provide no formal guarantees that agents will satisfy temporal constraints. We present Agent-C, a novel framework that provides run-time guarantees ensuring LLM agents adhere to formal temporal safety properties. Agent-C introduces a domain-specific language for expressing temporal properties (e.g., authenticate before accessing data), translates specifications to first-order logic, and uses SMT solving to detect non-compliant agent actions during token generation. When the LLM attempts to generate a non-compliant tool call, Agent-C leverages constrained generation techniques to ensure that every action generated by the LLM complies with the specification, and to generate a compliant alternative to a non-compliant agent action. We evaluate Agent-C across two real-world applications: retail customer service and airline ticket reservation system, and multiple language models (open and closed-source). Our results demonstrate that Agent-C achieves perfect safety (100% conformance, 0% harm), while improving task utility compared to state-of-the-art guardrails and unrestricted agents. On SoTA closed-source models, Agent-C improves conformance (77.4% to 100% for Claude Sonnet 4.5 and 83.7% to 100% for GPT-5), while simultaneously increasing utility (71.8% to 75.2% and 66.1% to 70.6%, respectively), representing a new SoTA frontier for reliable agentic reasoning.

</details>


### [6] [Towards representation agnostic probabilistic programming](https://arxiv.org/abs/2512.23740)
*Ole Fenske,Maximilian Popko,Sebastian Bader,Thomas Kirste*

Main category: cs.PL

TL;DR: 论文提出了一种因子抽象方法，通过五种基本操作作为通用接口，使概率编程语言能够自由混合不同表示形式（如离散表、高斯分布、基于样本的方法），克服了现有工具在混合离散-连续模型上的限制。


<details>
  <summary>Details</summary>
Motivation: 当前的概率编程语言和工具将模型表示与特定推断算法紧密耦合，限制了用户尝试新表示形式或混合离散-连续模型的自由度。

Method: 论文引入了包含五种基本操作的因子抽象，作为独立于底层表示的通用接口，支持混合使用不同表示形式的统一框架。

Result: 该方法实现了表示无关的概率编程，用户可以在单一框架内自由混合多种表示形式，从而在复杂混合模型中实现实际推断，解决了现有工具无法充分表达的问题。

Conclusion: 因子抽象方法为概率编程提供了更大的灵活性和表达能力，填补了当前工具在复杂混合模型推断上的不足。

Abstract: Current probabilistic programming languages and tools tightly couple model representations with specific inference algorithms, preventing experimentation with novel representations or mixed discrete-continuous models. We introduce a factor abstraction with five fundamental operations that serve as a universal interface for manipulating factors regardless of their underlying representation. This enables representation-agnostic probabilistic programming where users can freely mix different representations (e.g. discrete tables, Gaussians distributions, sample-based approaches) within a single unified framework, allowing practical inference in complex hybrid models that current toolkits cannot adequately express.

</details>


### [7] [VGC: A High-Performance Zone-Based Garbage Collector Architecture for Python with Partitioning and Parallel Execution](https://arxiv.org/abs/2512.23768)
*Abdulla M*

Main category: cs.PL

TL;DR: 虚拟垃圾收集器(VGC)提出了一种新颖的双层内存管理框架，通过主动VGC和被动VGC的结合优化了从嵌入式设备到高性能并行系统的性能。


<details>
  <summary>Details</summary>
Motivation: 传统垃圾收集器在高性能并行系统中存在性能瓶颈和内存碎片问题，VGC旨在通过分层设计解决这些问题。

Method: VGC采用双层架构：主动VGC用于运行时并发标记和清除，被动VGC在编译时优化静态内存分配，实现高效内存管理。

Result: VGC在多线程基准测试中将暂停时间减少30%，内存使用减少25%，同时提高了并行应用的可扩展性。

Conclusion: VGC通过结合编译时和运行时优化，为现代并行应用提供了一个高效、可扩展的内存管理解决方案。

Abstract: The Virtual Garbage Collector (VGC) introduces a novel memory management framework designed to optimize performance across diverse systems, ranging from resource constrained embedded devices to high performance parallel architectures. Unlike conventional garbage collectors, VGC employs a dual layer architecture consisting of Active VGC and Passive VGC to enable efficient, low overhead memory management. Active VGC dynamically manages runtime objects using a concurrent mark and sweep strategy tailored for parallel workloads, reducing pause times by up to 30 percent compared to generational collectors in multithreaded benchmarks. Passive VGC operates at compile time and optimizes static object allocation through predictive memory mapping, minimizing fragmentation by aligning objects to cache boundaries. This separation of responsibilities ensures predictable memory access patterns, reduces total memory usage by up to 25 percent, and improves scalability for modern parallel applications. By integrating compile time and runtime optimizations, VGC provides a robust and adaptable solution for memory intensive systems across both low level and high level programming environments.

</details>


### [8] [State Space Estimation for DPOR-based Model Checkers](https://arxiv.org/abs/2512.23996)
*A. R. Balasubramanian,Mohammad Hossein Khoshechin Jorshari,Rupak Majumdar,Umang Mathur,Minjian Zhang*

Main category: cs.PL

TL;DR: 论文研究了并发程序的估计问题，提出了一个蒙特卡洛方法来实现多项式时间无偏估计器，并通过实验验证了其稳定性和高效性。


<details>
  <summary>Details</summary>
Motivation: 解决并发程序中的Mazurkiewicz追踪等价类数量估计问题，为基于枚举的模型检查提供运行时间和搜索空间覆盖率的预估。

Method: 采用蒙特卡洛方法，将无状态最优DPOR算法转化为无偏估计器，并通过控制方差的技术（如Knuth估计器和随机枚举）实现高效估计。

Result: 实验表明，在共享内存基准测试中，该估计器能在少量试验内提供稳定估计（通常在20%误差范围内），即使状态空间包含10^5--10^6个类。

Conclusion: 论文提出的算法是首个可证明的多项式时间无偏估计器，为模型检查资源分配提供了重要工具。

Abstract: We study the estimation problem for concurrent programs: given a bounded program $P$, estimate the number of Mazurkiewicz trace-equivalence classes induced by its interleavings. This quantity informs two practical questions for enumeration-based model checking: how long a model checking run is likely to take, and what fraction of the search space has been covered so far. We first show the counting problem is #P-hard even for restricted programs and, unless $P=NP$, inapproximable within any subexponential factor, ruling out efficient exact or randomized approximation algorithms. We give a Monte Carlo approach to obtain a poly-time unbiased estimator: we convert a stateless optimal DPOR algorithm into an unbiased estimator by viewing its exploration as a bounded-depth, bounded-width tree whose leaves are the maximal Mazurkiewicz traces. A classical estimator by Knuth, when run on this tree, yields an unbiased estimate. To control the variance, we apply stochastic enumeration by maintaining a small population of partial paths per depth whose evolution is coupled. We have implemented our estimator in the JMC model checker and evaluated it on shared-memory benchmarks. With modest budgets, our estimator yields stable estimates, typically within a 20% band, within a few hundred trials, even when the state space has $10^5$--$10^6$ classes. We also show how the same machinery estimates model-checking cost by weighting all explored graphs, not only complete traces. Our algorithms provide the first provable poly-time unbiased estimators for counting traces, a problem of considerable importance when allocating model checking resources.

</details>


<div id='cs.GT'></div>

# cs.GT [[Back]](#toc)

### [9] [Personalized Promotions in Practice: Dynamic Allocation and Reference Effects](https://arxiv.org/abs/2512.23781)
*Jackie Baek,Will Ma,Dmitry Mitrofanov*

Main category: cs.GT

TL;DR: 该论文提出了一个高效的个性化促销策略，成功提升了4.5%的收入，并通过理论模型分析了顾客的跨期状态优化长期收益。


<details>
  <summary>Details</summary>
Motivation: 针对超过2000万顾客的个性化促销问题，如何在满足全局分配约束的情况下，每天为每位顾客选择最优折扣（如10%、12%、15%、17%或20%），以最大化收入。

Method: 设计了一种高效的促销策略，并结合顾客的跨期模式，提出了一个新的组合定价模型，考虑了顾客对过去促销的“参考价值”。

Result: 在A/B测试中，该策略成功实现了4.5%的收入增长，并通过理论模型证明了最优策略需要周期性地在低促销和高促销之间切换。

Conclusion: 通过实际部署和理论分析，论文展示了如何利用顾客的跨期行为和参考效应优化促销策略，从而显著提升收入。

Abstract: Partnering with a large online retailer, we consider the problem of sending daily personalized promotions to a userbase of over 20 million customers. We propose an efficient policy for determining, every day, the promotion that each customer should receive (10%, 12%, 15%, 17%, or 20% off), while respecting global allocation constraints. This policy was successfully deployed to see a 4.5% revenue increase during an A/B test, by better targeting promotion-sensitive customers and also learning intertemporal patterns across customers.
  We also consider theoretically modeling the intertemporal state of the customer. The data suggests a simple new combinatorial model of pricing with reference effects, where the customer remembers the best promotion they saw over the past $\ell$ days as the "reference value", and is more likely to purchase if this value is poor. We tightly characterize the structure of optimal policies for maximizing long-run average revenue under this model -- they cycle between offering poor promotion values $\ell$ times and offering good values once.

</details>


### [10] [Multilevel Fair Allocation](https://arxiv.org/abs/2512.24105)
*Maxime Lucet,Nawal Benabbou,Aurélie Beynier,Nicolas Maudet*

Main category: cs.GT

TL;DR: 该论文提出了多级公平资源分配的概念，针对树状层次结构的代理关系设计了两种算法，分别具有理论保证和实际公平性。


<details>
  <summary>Details</summary>
Motivation: 研究动机是解决树状层次结构中多级资源分配的公平性和效率问题，尤其是在每个中间节点可能有不同本地分配机制的情况下。

Method: 论文提出了两种算法：一种是通用的多项式时间顺序算法，具有高效性和公平性的理论保证；另一种是将General Yankee Swap扩展到多级设置，虽只有效率保证，但实践中表现出色。

Result: 结果表明，第一种算法在理论上保证了公平性和效率，第二种算法在实践中表现出良好的公平性。

Conclusion: 论文得出结论，提出的两种算法能够有效解决树状层次结构中的多级公平资源分配问题，展示了理论保证与实际表现的平衡。

Abstract: We introduce the concept of multilevel fair allocation of resources with tree-structured hierarchical relations among agents. While at each level it is possible to consider the problem locally as an allocation of an agent to its children, the multilevel allocation can be seen as a trace capturing the fact that the process is iterated until the leaves of the tree. In principle, each intermediary node may have its own local allocation mechanism. The main challenge is then to design algorithms which can retain good fairness and efficiency properties. In this paper we propose two original algorithms under the assumption that leaves of the tree have matroid-rank utility functions and the utility of any internal node is the sum of the utilities of its children. The first one is a generic polynomial-time sequential algorithm that comes with theoretical guarantees in terms of efficiency and fairness. It operates in a top-down fashion -- as commonly observed in real-world applications -- and is compatible with various local algorithms. The second one extends the recently proposed General Yankee Swap to the multilevel setting. This extension comes with efficiency guarantees only, but we show that it preserves excellent fairness properties in practice.

</details>


### [11] [On the Difficulty of Measuring Divisiveness of Proposals under Ranked Preferences](https://arxiv.org/abs/2512.24467)
*Ulle Endriss*

Main category: cs.GT

TL;DR: 该论文探讨了如何在数字民主平台中识别最具争议性的政策提案，并揭示了满足基本规范性要求的困难。


<details>
  <summary>Details</summary>
Motivation: 在数字民主平台中，识别并优先讨论最具争议性的政策提案有助于推动公民审议的进展，但目前缺乏明确的方法来实现这一目标。

Method: 论文采用社会选择理论中的公理化方法，试图定义并选择最具争议性的提案，以满足一定的规范性要求。

Result: 研究发现，在选择最具争议性提案时，即使满足看似温和的规范性要求，也会面临一系列根本性困难。

Conclusion: 论文表明，尽管识别争议性提案对数字民主平台至关重要，但方法学上的挑战使得这一任务复杂化。

Abstract: Given the stated preferences of several people over a number of proposals regarding public policy initiatives, some of those proposals might be judged to be more ``divisive'' than others. When designing online participatory platforms to support digital democracy initiatives enabling citizens to deliberate over such proposals, we might wish to equip those platforms with the functionality to retrieve the most divisive proposals currently under discussion. Such a service would be useful for analysing the progress of deliberation and steering discussion towards issues that still require further debate. Guided by this use case, we explore possibilities for providing a clear definition of what it means to select a set of most divisive proposals on the basis of people's stated preferences over proposals. Then, employing the axiomatic method familiar from social choice theory, we show that the task of selecting the most divisive proposals in a manner that satisfies certain seemingly mild normative requirements faces a number of fundamental difficulties.

</details>
