<div id=toc></div>

# Table of Contents

- [cs.GR](#cs.GR) [Total: 1]
- [cs.GT](#cs.GT) [Total: 1]


<div id='cs.GR'></div>

# cs.GR [[Back]](#toc)

### [1] [Fast Texture Transfer for XR Avatars via Barycentric UV Conversion](https://arxiv.org/abs/2508.19518)
*Hail Song,Seokhwan Yang,Woontack Woo*

Main category: cs.GR

TL;DR: 提出了一种快速高效的方法，将面部纹理转移到SMPL-X全身体模型上，速度比基线快7000倍，同时显著提升纹理质量。


<details>
  <summary>Details</summary>
Motivation: 传统基于仿射变换的方法速度慢且容易产生视觉伪影，无法满足沉浸式XR应用中对高效和高质量纹理传输的需求。

Method: 采用重心UV转换技术，将整个UV映射预先计算为单一变换矩阵，实现单步纹理传输。

Result: 定量和定性评估显示，该方法比基线快7000倍，并消除了边界伪影，显著提升了纹理质量。

Conclusion: 该方法为沉浸式XR应用中的个性化需求提供了实用解决方案，代码已公开。

Abstract: We present a fast and efficient method for transferring facial textures onto
SMPL-X-based full-body avatars. Unlike conventional affine-transform methods
that are slow and prone to visual artifacts, our method utilizes a barycentric
UV conversion technique. Our approach precomputes the entire UV mapping into a
single transformation matrix, enabling texture transfer in a single operation.
This results in a speedup of over 7000x compared to the baseline, while also
significantly improving the final texture quality by eliminating boundary
artifacts. Through quantitative and qualitative evaluations, we demonstrate
that our method offers a practical solution for personalization in immersive XR
applications. The code is available online.

</details>


<div id='cs.GT'></div>

# cs.GT [[Back]](#toc)

### [2] [Aggregate Fictitious Play for Learning in Anonymous Polymatrix Games (Extended Version)](https://arxiv.org/abs/2508.19371)
*Semih Kara,Tamer Başar*

Main category: cs.GT

TL;DR: 该论文提出了聚合虚构博弈（agg-FP）算法，通过聚合其他代理的动作频率来减少动作空间，从而在匿名游戏中加速纳什均衡的收敛。


<details>
  <summary>Details</summary>
Motivation: 传统虚构博弈（FP）算法在多代理系统中面临动作空间指数级增长的问题，导致奖励探索缓慢。匿名游戏的结构可以缓解这一问题，因为其奖励仅依赖于动作而非执行者。

Method: 论文提出了聚合虚构博弈（agg-FP）算法，其中每个代理仅跟踪其他代理执行每个动作的频率，而非个体动作。这种方法减少了动作空间，同时保留了收敛性。

Result: 在匿名多矩阵游戏中，agg-FP在传统FP相同的条件下收敛到纳什均衡。仿真实验显示了这种聚合方法如何加速收敛。

Conclusion: 通过聚合动作频率，agg-FP在匿名游戏中显著减少了动作空间的复杂性，同时保持了收敛性，为多代理系统的纳什均衡求解提供了高效方法。

Abstract: Fictitious play (FP) is a well-studied algorithm that enables agents to learn
Nash equilibrium in games with certain reward structures. However, when agents
have no prior knowledge of the reward functions, FP faces a major challenge:
the joint action space grows exponentially with the number of agents, which
slows down reward exploration. Anonymous games offer a structure that mitigates
this issue. In these games, the rewards depend only on the actions taken; not
on who is taking which action. Under such a structure, we introduce aggregate
fictitious play (agg-FP), a variant of FP where each agent tracks the frequency
of the number of other agents playing each action, rather than these agents'
individual actions. We show that in anonymous polymatrix games, agg-FP
converges to a Nash equilibrium under the same conditions as classical FP. In
essence, by aggregating the agents' actions, we reduce the action space without
losing the convergence guarantees. Using simulations, we provide empirical
evidence on how this reduction accelerates convergence.

</details>
