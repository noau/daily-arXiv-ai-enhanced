<div id=toc></div>

# Table of Contents

- [cs.GR](#cs.GR) [Total: 2]
- [cs.PL](#cs.PL) [Total: 3]
- [cs.GT](#cs.GT) [Total: 1]


<div id='cs.GR'></div>

# cs.GR [[Back]](#toc)

### [1] [LATTICE: Democratize High-Fidelity 3D Generation at Scale](https://arxiv.org/abs/2512.03052)
*Zeqiang Lai,Yunfei Zhao,Zibo Zhao,Haolin Liu,Qingxiang Lin,Jingwei Huang,Chunchao Guo,Xiangyu Yue*

Main category: cs.GR

TL;DR: LATTICE是一个新的高保真3D资产生成框架，通过VoxSet半结构化表示解决了3D生成的挑战，采用两阶段流水线实现高质量和可扩展性。


<details>
  <summary>Details</summary>
Motivation: 3D生成在预测空间结构和几何表面方面比2D更具挑战性，且现有3D表示计算复杂且缺乏结构化编码方案。LATTICE旨在缩小3D与2D生成模型之间的质量与可扩展性差距。

Method: 提出VoxSet半结构化表示，将3D资产压缩到粗粒度体素网格的潜向量中，并采用两阶段流水线：先生成稀疏体素锚点，再通过整流流变换器生成详细几何。

Result: LATTICE支持任意分辨率解码、低成本训练和灵活推理，在多个方面达到最先进性能。

Conclusion: LATTICE为高质量和可扩展的3D资产生成提供了重要进展。

Abstract: We present LATTICE, a new framework for high-fidelity 3D asset generation that bridges the quality and scalability gap between 3D and 2D generative models. While 2D image synthesis benefits from fixed spatial grids and well-established transformer architectures, 3D generation remains fundamentally more challenging due to the need to predict both spatial structure and detailed geometric surfaces from scratch. These challenges are exacerbated by the computational complexity of existing 3D representations and the lack of structured and scalable 3D asset encoding schemes. To address this, we propose VoxSet, a semi-structured representation that compresses 3D assets into a compact set of latent vectors anchored to a coarse voxel grid, enabling efficient and position-aware generation. VoxSet retains the simplicity and compression advantages of prior VecSet methods while introducing explicit structure into the latent space, allowing positional embeddings to guide generation and enabling strong token-level test-time scaling. Built upon this representation, LATTICE adopts a two-stage pipeline: first generating a sparse voxelized geometry anchor, then producing detailed geometry using a rectified flow transformer. Our method is simple at its core, but supports arbitrary resolution decoding, low-cost training, and flexible inference schemes, achieving state-of-the-art performance on various aspects, and offering a significant step toward scalable, high-quality 3D asset creation.

</details>


### [2] [Radiance Meshes for Volumetric Reconstruction](https://arxiv.org/abs/2512.04076)
*Alexander Mai,Trevor Hedstrom,George Kopanas,Janne Kontkanen,Falko Kuester,Jonathan T. Barron*

Main category: cs.GR

TL;DR: 介绍了辐射网格，一种利用Delaunay四面体化生成的恒定密度四面体单元来表示辐射场的技术，支持快速体积渲染和多种应用。


<details>
  <summary>Details</summary>
Motivation: 现有辐射场表示在硬件支持和渲染速度上存在局限性，需要一种能够高效利用现有硬件并支持多样应用的解决方案。

Method: 采用Delaunay四面体化生成恒定密度四面体单元，提出新的光栅化方法，并结合Zip-NeRF风格的骨干网络处理拓扑变化。

Result: 实现了比现有辐射场表示更快的渲染速度，支持高质量、实时的视角合成，并适用于多种应用场景。

Conclusion: 辐射网格技术不仅提升了渲染效率和质量，还为多种应用提供了灵活的解决方案，展现了广泛的应用潜力。

Abstract: We introduce radiance meshes, a technique for representing radiance fields with constant density tetrahedral cells produced with a Delaunay tetrahedralization. Unlike a Voronoi diagram, a Delaunay tetrahedralization yields simple triangles that are natively supported by existing hardware. As such, our model is able to perform exact and fast volume rendering using both rasterization and ray-tracing. We introduce a new rasterization method that achieves faster rendering speeds than all prior radiance field representations (assuming an equivalent number of primitives and resolution) across a variety of platforms. Optimizing the positions of Delaunay vertices introduces topological discontinuities (edge flips). To solve this, we use a Zip-NeRF-style backbone which allows us to express a smoothly varying field even when the topology changes. Our rendering method exactly evaluates the volume rendering equation and enables high quality, real-time view synthesis on standard consumer hardware. Our tetrahedral meshes also lend themselves to a variety of exciting applications including fisheye lens distortion, physics-based simulation, editing, and mesh extraction.

</details>


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [3] [Evaluate the Stack Management in Effect Handlers using the libseff C Library](https://arxiv.org/abs/2512.03083)
*ZeHao Yu*

Main category: cs.PL

TL;DR: 本文探讨了使用用户级超额提交技术优化堆栈管理的方法，通过libseff C库实现动态堆栈调整，与传统方法相比提高了内存利用率并减少了浪费。


<details>
  <summary>Details</summary>
Motivation: 现代编程中效应处理器的动态控制流变化给堆栈管理带来挑战，传统方法在内存利用和性能上存在不足。本文旨在通过用户级超额提交技术解决这些问题。

Method: 利用libseff C库结合虚拟内存机制和保护性懒惰分配，提出用户级超额提交实现动态堆栈调整，并对比传统固定大小堆栈、分段堆栈和内核级超额提交的优势。

Result: 实验结果表明，内核级超额提交在性能和灵活性上表现均衡，而用户级实现虽灵活但存在额外开销，需进一步优化。

Conclusion: 本文详细比较了多种堆栈管理策略，提供了针对特定应用需求的实用建议，并指出未来研究方向包括改进用户级机制和扩展基准测试框架。

Abstract: Effect handlers are increasingly prominent in modern programming for managing complex computational effects, including concurrency, asynchronous operations, and exception handling, in a modular and flexible manner. Efficient stack management remains a significant challenge for effect handlers due to the dynamic control flow changes they introduce. This paper explores a novel stack management approach using user-level overcommitting within the libseff C library, which leverages virtual memory mechanisms and protection-based lazy allocation combined with signal-driven memory commitment. Our user-level overcommitting implementation dynamically resizes stacks on-demand, improving memory utilization and reducing waste compared to traditional methods. We rigorously benchmark and evaluate this novel strategy against conventional fixed- size stacks, segmented stacks, and kernel-based overcommitting, using metrics such as context-switch latency, stack expansion efficiency, multi-threaded performance, and robustness under rapid stack growth conditions. Experimental results demonstrate that kernel-based overcommitting achieves an effective balance between performance and flexibility, whereas our user-level implementation, while flexible, incurs additional overheads, highlighting areas for optimization. This study provides a detailed comparative analysis of various stack management strate- gies, offering practical recommendations tailored to specific application requirements and operational constraints. Future work will focus on refining user-level overcommit- ting mechanisms, mitigating non-deterministic behaviors, and expanding benchmark frameworks to include real-world scenarios.

</details>


### [4] [Beyond Code Pairs: Dialogue-Based Data Generation for LLM Code Translation](https://arxiv.org/abs/2512.03086)
*Le Chen,Nuo Xu,Winson Chen,Bin Lei,Pei-Hung Lin,Dunzhi Zhou,Rajeev Thakur,Caiwen Ding,Ali Jannesari,Chunhua Liao*

Main category: cs.PL

TL;DR: 论文提出了一种自动化数据集生成方法，通过双LLM问答设计结合外部知识，显著提升了低资源编程语言（如Fortran和CUDA）的代码翻译质量。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在代码翻译中表现优异，但在低资源编程领域（如Fortran和CUDA）性能下降，原因是高质量并行数据稀缺。

Method: 采用自动化数据集生成管道，设计了双LLM问答机制（Questioner-Solver），并结合编译器和运行时反馈的外部知识，生成带有单元测试和多轮对话的数据集。

Result: 在Fortran -> C++和C++ -> CUDA任务中，生成了3.64k和3.93k的对话数据，微调后功能正确性提升明显，C++-to-CUDA任务单元测试成功率提高了56\%。

Conclusion: 此方法不仅提升了翻译质量，还使7B开源模型在关键指标（如编译成功率）上显著优于更大的专有系统。

Abstract: Large language models (LLMs) have shown remarkable capabilities in code translation, yet their performance deteriorates in low-resource programming domains such as Fortran and emerging frameworks like CUDA, where high-quality parallel data are scarce. We present an automated dataset generation pipeline featuring a dual-LLM Questioner-Solver design that incorporates external knowledge from compilers and runtime feedback. Beyond traditional source-target code pair datasets, our approach additionally generates (1) verified translations with unit tests for assessing functional consistency, and (2) multi-turn dialogues that capture the reasoning process behind translation refinement. Applied to Fortran -> C++ and C++ -> CUDA, the pipeline yields 3.64k and 3.93k dialogues, respectively. Fine-tuning on this data yields dramatic improvements in functional correctness, boosting unit test success rates by over 56% on the challenging C++-to-CUDA task. We show this data enables a 7B open-weight model to significantly outperform larger proprietary systems on key metrics like compilation success.

</details>


### [5] [OOPredictor: Predicting Object-Oriented Accesses using Static Analysis](https://arxiv.org/abs/2512.03972)
*Hassan Arafat,David Bremner,Kenneth B. Kent,Julian Wang*

Main category: cs.PL

TL;DR: 本文提出一种编译时静态分析方法，用于预测面向对象编程中指针追踪的访问模式，以减少缓存性能问题。


<details>
  <summary>Details</summary>
Motivation: 面向对象编程的间接寻址导致指针追踪频繁，影响了缓存性能且现代硬件预取器难以处理其不可预测性。现有软件方法多依赖运行时分析，开销较大。

Method: 通过静态分析预测程序的常见访问模式，并在OpenJ9 JVM中实现原型。输出为马尔可夫链模型，用于模拟程序行为。

Result: 实验表明，该预测器准确性高，可用于指导最小侵入性的负载停顿缓解策略，如优化垃圾回收的复制顺序。

Conclusion: 提出的静态分析方法能有效预测指针追踪模式，为优化缓存性能提供了一种低开销的解决方案。

Abstract: Object-oriented Programming has become one of the most dominant design paradigms as the separation of concerns and adaptability of design reduce development and maintenance costs. However, the convenience is not without cost. The added indirection inherent in such designs causes excessive pointer chasing, negatively affecting locality, which in turn degrades the performance of cache structures. Furthermore, modern hardware prefetchers are mostly stride prefetchers that are ill-equipped to handle the unpredictability of access patterns generated by pointer chasing. Most software approaches that seek to address this problem resort to profiling the program as it runs, which comes with a significant run-time overhead or requires data from previous runs. In this paper, we propose the use of compile-time static analysis to predict the most common access patterns displayed by a program during run time. Since Java is one of the most popular object-oriented languages, we implement our prototype within the OpenJ9 JVM, inside the OMR optimizer infrastructure. The outputs of our proposed predictor are Markov chains that model the expected behavior of the program. The effectiveness of the proposed predictor is evaluated by comparing the model with the actual run-time behavior of the program measured using an instrumented interpreter. Our experiments show that the proposed predictor exhibits good accuracy and can be used to inform minimally intrusive load stall mitigation strategies, e.g. informing copying GCs on more locality-friendly copying orders

</details>


<div id='cs.GT'></div>

# cs.GT [[Back]](#toc)

### [6] [Sponsored Questions and How to Auction Them](https://arxiv.org/abs/2512.03975)
*Kshipra Bhawalkar,Alexandros Psomas,Di Wang*

Main category: cs.GT

TL;DR: 本文探讨了在线平台如何通过大型语言模型（LLM）主动提供澄清提示来解决用户查询的模糊性，并研究如何将这些提示位分配给广告商，以及如何与传统广告拍卖机制互动。


<details>
  <summary>Details</summary>
Motivation: 用户的搜索查询通常存在意图模糊的问题，传统的被动预测方法效果有限。本文探索如何利用LLM主动提供澄清提示，并将其中的部分提示位（"赞助提示"）分配给广告商，同时研究这种新机制与传统广告拍卖的互动。

Method: 本文提出了一种形式化模型，用于设计和分析此类交互式平台。研究了两种工程选择：一种是联合优化用户交互和最终广告拍卖的端到端管道；另一种是将提示位和后续广告位机制分开。

Result: 研究表明，可以采用VCG机制联合优化赞助提示和后续广告，虽然复杂但实现了高效和真实的分配。而简单实现的模块化方法存在策略性低效问题，其"无政府状态价格"无界。

Conclusion: 本文提出了一种新的交互式平台机制设计方法，证明了联合优化机制的优势，并指出模块化方法在实践中可能导致效率低下。

Abstract: Online platforms connect users with relevant products and services using ads. A key challenge is that a user's search query often leaves their true intent ambiguous. Typically, platforms passively predict relevance based on available signals and in some cases offer query refinements. The shift from traditional search to conversational AI provides a new approach. When a user's query is ambiguous, a Large Language Model (LLM) can proactively offer several clarifying follow-up prompts. In this paper we consider the following: what if some of these follow-up prompts can be ``sponsored,'' i.e., selected for their advertising potential. How should these ``suggestion slots'' be allocated? And, how does this new mechanism interact with the traditional ad auction that might follow?
  This paper introduces a formal model for designing and analyzing these interactive platforms. We use this model to investigate a critical engineering choice: whether it is better to build an end-to-end pipeline that jointly optimizes the user interaction and the final ad auction, or to decouple them into separate mechanisms for the suggestion slots and another for the subsequent ad slot. We show that the VCG mechanism can be adopted to jointly optimize the sponsored suggestion and the ads that follow; while this mechanism is more complex, it achieves outcomes that are efficient and truthful. On the other hand, we prove that the simple-to-implement modular approach suffers from strategic inefficiency: its Price of Anarchy is unbounded.

</details>
