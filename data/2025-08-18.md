<div id=toc></div>

# Table of Contents

- [cs.GR](#cs.GR) [Total: 3]
- [cs.PL](#cs.PL) [Total: 2]
- [cs.GT](#cs.GT) [Total: 1]


<div id='cs.GR'></div>

# cs.GR [[Back]](#toc)

### [1] [LayoutRectifier: An Optimization-based Post-processing for Graphic Design Layout Generation](https://arxiv.org/abs/2508.11177)
*I-Chao Shen,Ariel Shamir,Takeo Igarashi*

Main category: cs.GR

TL;DR: 论文提出了一种基于优化的方法LayoutRectifier，用于纠正自动生成的图形设计布局中的错误（如不对齐、重叠和不满意的包含），并通过两阶段优化（离散搜索和包含函数调整）提高布局质量。


<details>
  <summary>Details</summary>
Motivation: 当前的深度学习生成图形设计布局方法常存在不对齐、重叠和包含问题，影响了布局的实用性和美观性，因此需要一个优化方法来纠正这些错误。

Method: LayoutRectifier采用两阶段优化：第一阶段通过离散搜索利用网格系统解决不对齐问题；第二阶段通过新的包含函数调整元素位置和大小，防止重叠并提升包含效果。

Result: 实验表明，该方法在内容和内容无关的布局生成任务中均能生成更高质量的布局，且无需额外训练。

Conclusion: LayoutRectifier有效补充了基于学习的布局生成方法，显著提升了布局的实用性和美观性。

Abstract: Recent deep learning methods can generate diverse graphic design layouts
efficiently. However, these methods often create layouts with flaws, such as
misalignment, unwanted overlaps, and unsatisfied containment. To tackle this
issue, we propose an optimization-based method called LayoutRectifier, which
gracefully rectifies auto-generated graphic design layouts to reduce these
flaws while minimizing deviation from the generated layout. The core of our
method is a two-stage optimization. First, we utilize grid systems, which
professional designers commonly use to organize elements, to mitigate
misalignments through discrete search. Second, we introduce a novel box
containment function designed to adjust the positions and sizes of the layout
elements, preventing unwanted overlapping and promoting desired containment. We
evaluate our method on content-agnostic and content-aware layout generation
tasks and achieve better-quality layouts that are more suitable for downstream
graphic design tasks. Our method complements learning-based layout generation
methods and does not require additional training.

</details>


### [2] [StyleMM: Stylized 3D Morphable Face Model via Text-Driven Aligned Image Translation](https://arxiv.org/abs/2508.11203)
*Seungmi Lee,Kwan Yun,Junyong Noh*

Main category: cs.GR

TL;DR: StyleMM是一个新框架，能够根据用户定义的文本描述构建风格化的3D可变形模型（3DMM），通过文本引导的图像翻译和扩散模型生成风格化目标，保持面部属性的同时实现一致的3D风格迁移。


<details>
  <summary>Details</summary>
Motivation: 现有的3D可变形模型在风格化方面存在局限性，StyleMM旨在通过文本引导的方式生成风格化的3D人脸模型，同时保持身份、对齐和表情等关键属性。

Method: 基于预训练的网格变形网络和纹理生成器，结合文本引导的图像翻译和扩散模型生成风格化目标，通过图像训练实现一致的3D风格迁移。

Result: 定量和定性评估表明，StyleMM在身份级面部多样性和风格化能力方面优于现有方法。

Conclusion: StyleMM成功实现了基于文本描述的3D风格迁移，生成的面部网格具有一致的顶点连接性和可动画性，推动了3D可变形模型的发展。

Abstract: We introduce StyleMM, a novel framework that can construct a stylized 3D
Morphable Model (3DMM) based on user-defined text descriptions specifying a
target style. Building upon a pre-trained mesh deformation network and a
texture generator for original 3DMM-based realistic human faces, our approach
fine-tunes these models using stylized facial images generated via text-guided
image-to-image (i2i) translation with a diffusion model, which serve as
stylization targets for the rendered mesh. To prevent undesired changes in
identity, facial alignment, or expressions during i2i translation, we introduce
a stylization method that explicitly preserves the facial attributes of the
source image. By maintaining these critical attributes during image
stylization, the proposed approach ensures consistent 3D style transfer across
the 3DMM parameter space through image-based training. Once trained, StyleMM
enables feed-forward generation of stylized face meshes with explicit control
over shape, expression, and texture parameters, producing meshes with
consistent vertex connectivity and animatability. Quantitative and qualitative
evaluations demonstrate that our approach outperforms state-of-the-art methods
in terms of identity-level facial diversity and stylization capability. The
code and videos are available at
[kwanyun.github.io/stylemm_page](kwanyun.github.io/stylemm_page).

</details>


### [3] [SPG: Style-Prompting Guidance for Style-Specific Content Creation](https://arxiv.org/abs/2508.11476)
*Qian Liang,Zichong Chen,Yang Zhou,Hui Huang*

Main category: cs.GR

TL;DR: 提出了一种名为Style-Prompting Guidance (SPG)的新采样策略，用于生成具有特定风格的图像，同时保持语义一致性。


<details>
  <summary>Details</summary>
Motivation: 尽管现有的文本到图像（T2I）扩散模型在将生成图像与文本提示对齐方面表现出色，但控制输出图像的视觉风格仍然是一个挑战。

Method: SPG构建风格噪声向量，并利用其与无条件噪声的方向偏差来引导扩散过程朝向目标风格分布。该方法还与Classifier-Free Guidance (CFG)集成，以实现语义保真和风格一致性。

Result: 实验表明，SPG方法在语义和风格控制方面优于现有技术，并且与ControlNet和IPAdapter等可控框架兼容。

Conclusion: SPG是一种简单、鲁棒且广泛适用的方法，能够有效生成具有特定风格的图像，同时保持语义准确性。

Abstract: Although recent text-to-image (T2I) diffusion models excel at aligning
generated images with textual prompts, controlling the visual style of the
output remains a challenging task. In this work, we propose Style-Prompting
Guidance (SPG), a novel sampling strategy for style-specific image generation.
SPG constructs a style noise vector and leverages its directional deviation
from unconditional noise to guide the diffusion process toward the target style
distribution. By integrating SPG with Classifier-Free Guidance (CFG), our
method achieves both semantic fidelity and style consistency. SPG is simple,
robust, and compatible with controllable frameworks like ControlNet and
IPAdapter, making it practical and widely applicable. Extensive experiments
demonstrate the effectiveness and generality of our approach compared to
state-of-the-art methods. Code is available at
https://github.com/Rumbling281441/SPG.

</details>


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [4] [Generic Reduction-Based Interpreters (Extended Version)](https://arxiv.org/abs/2508.11297)
*Casper Bach*

Main category: cs.PL

TL;DR: 本文探讨如何通过泛型编程技术减少基于归约的解释器中的样板代码。


<details>
  <summary>Details</summary>
Motivation: 传统基于归约的解释器编写需要大量重复代码，希望通过泛型编程减少此类问题。

Method: 结合泛型编程技术，优化解释器的归约过程设计。

Result: 实现了减少样板代码的目标，提升了开发效率。

Conclusion: 泛型编程可有效简化基于归约的解释器实现，减少重复工作。

Abstract: Reduction-based interpreters are traditionally defined in terms of a one-step
reduction function which systematically decomposes a term into a potential
redex and context, contracts the redex, and recomposes it to construct the new
term to be further reduced. While implementing such interpreters follows a
systematic recipe, they often require interpreter engineers to write a
substantial amount of code -- much of it boilerplate. In this paper, we apply
well-known techniques from generic programming to reduce boilerplate code in
reduction-based interpreters.

</details>


### [5] [Towards Efficient Hash Maps in Functional Array Languages](https://arxiv.org/abs/2508.11443)
*William Henrich Due,Martin Elsman,Troels Henriksen*

Main category: cs.PL

TL;DR: 论文提出了一种数据并行实现的两层静态无冲突哈希映射，通过功能化Fredman等人构造并将其扁平化。在功能数组语言中提供灵活、多态和抽象的哈希映射接口面临挑战，特别是动态大小键的问题。算法在Futhark中实现，GPU执行性能在基准测试中优于传统的树/搜索方法。与cuCollections库相比，构造速度更快，但查找稍慢。分析了性能差异的原因，并探讨了功能数组语言模型的扩展可能性。


<details>
  <summary>Details</summary>
Motivation: 研究旨在解决在功能数组语言中实现高效且灵活的哈希映射接口的挑战，特别是处理动态大小键的问题。同时，探索数据并行编程模型的局限性及其改进潜力。

Method: 通过功能化Fredman等人的构造方法，并将其扁平化，实现了数据并行的两层静态无冲突哈希映射。算法在Futhark中实现，并通过基准测试比较性能。

Result: 在GPU上的执行性能优于传统的树/搜索方法。与cuCollections库相比，哈希映射构造速度显著更快，但查找性能略低。性能差异部分归因于Futhark编译器的代码生成限制和数据并行模型的局限性。

Conclusion: 论文分析了功能数组语言模型的局限性，并探讨了是否及如何扩展该模型以解决性能问题。未来研究可能集中在改进编译器或扩展编程模型的功能性。

Abstract: We present a systematic derivation of a data-parallel implementation of
two-level, static and collision-free hash maps, by giving a functional
formulation of the Fredman et al. construction, and then flattening it. We
discuss the challenges of providing a flexible, polymorphic, and abstract
interface to hash maps in a functional array language, with particular
attention paid to the problem of dynamically sized keys, which we address by
associating each hash map with an arbitrary context. The algorithm is
implemented in Futhark, and the achieved GPU execution performance is compared
on simple benchmark problems. We find that our hash maps outperform
conventional tree/search-based approaches. Furthermore, our implementation is
compared against the state-of-the-art cuCollections library, which is
significantly faster for hash map construction, and to a lesser degree for
lookups. We explain to which extent the performance difference is due to
low-level code generation limitation in the Futhark compiler, and to which
extent it can be attributed to the data-parallel programming vocabulary not
providing the constructs necessary to express the equivalent of the algorithms
used by cuCollections. We end by reflecting to which extent the functional
array language programming model could, or should, be extended to address these
weaknesses.

</details>


<div id='cs.GT'></div>

# cs.GT [[Back]](#toc)

### [6] [Can We Tell if ChatGPT is a Parasite? Studying Human-AI Symbiosis with Game Theory](https://arxiv.org/abs/2508.11359)
*Jiejun Hu-Bolz,James Stovold*

Main category: cs.GT

TL;DR: 该研究探讨人类与生成AI系统是否可以通过迭代的信息驱动交互融合为一个整体，并通过信息论工具展示了这种融合的可能性。


<details>
  <summary>Details</summary>
Motivation: 研究动机是探索人类与生成AI系统之间的交互是否能形成共生关系，以及这种关系是否会演变为类似寄生的行为。

Method: 研究采用信息论工具（熵、互信息和传递熵）对三者之间的交互建模，并通过三玩家随机博弈模型分析这种交互。

Result: 结果显示，模型中的生成AI与人类能够形成一个聚合个体，并揭示了AI系统是否会从人类提供的信息中受益甚至寄生的可能性。

Conclusion: 结论表明，生成AI与人类之间的交互具有共生潜力，但也可能演变为单向的信息利用关系，需要进一步研究其伦理意义。

Abstract: This work asks whether a human interacting with a generative AI system can
merge into a single individual through iterative, information-driven
interactions. We model the interactions between a human, a generative AI
system, and the human's wider environment as a three-player stochastic game. We
use information-theoretic measures (entropy, mutual information, and transfer
entropy) to show that our modelled human and generative AI are able to form an
aggregate individual in the sense of Krakauer et al. (2020). The model we
present is able to answer interesting questions around the symbiotic nature of
humans and AI systems, including whether LLM-driven chatbots are acting as
parasites, feeding on the information provided by humans.

</details>
