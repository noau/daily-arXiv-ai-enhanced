<div id=toc></div>

# Table of Contents

- [cs.GR](#cs.GR) [Total: 3]
- [cs.PL](#cs.PL) [Total: 4]
- [cs.GT](#cs.GT) [Total: 1]


<div id='cs.GR'></div>

# cs.GR [[Back]](#toc)

### [1] [Generating real-time detailed ground visualisations from sparse aerial point clouds](https://arxiv.org/abs/2507.18664)
*Aidan Murray,Eddie Waite,Caleb Ross,Scarlet Mitchell,Alexander Bradley,Joanna Jamrozy,Kenny Mitchell*

Main category: cs.GR

TL;DR: 提出了一种自动放大真实世界扫描数据并在3D动画中实时渲染的方法，用于高质量的训练、模拟、游戏和可视化应用。


<details>
  <summary>Details</summary>
Motivation: 传统的户外3D内容制作需要大量艺术家团队，成本高昂且难以精确还原真实世界的多样性。

Method: 通过自动化放大扫描数据，并在3D动画中实时渲染，实现高质量的内容生成。

Result: 能够生成高质量的3D内容，适用于近距离探索的各种应用场景。

Conclusion: 该方法解决了传统3D内容制作的成本和精确性问题，为相关应用提供了高效解决方案。

Abstract: Building realistic wide scale outdoor 3D content with sufficient visual
quality to observe at walking eye level or from driven vehicles is often
carried out by large teams of artists skilled in modelling, texturing, material
shading and lighting, which typically leads to both prohibitive costs and
reduced accuracy honoring the variety of real world ground truth landscapes. In
our proposed method, we define a process to automatically amplify real-world
scanned data and render real-time in animated 3D to explore at close range with
high quality for training, simulation, video game and visualisation
applications.

</details>


### [2] [Procedural city modeling](https://arxiv.org/abs/2507.18899)
*Thomas Lechner,Ben Watson,Uri Wilensky,Martin Felsen*

Main category: cs.GR

TL;DR: 提出了一种通过程序生成逼真且复杂的城市模型的方法，重点关注土地利用和建筑分布，以实现从村庄到大都市的平滑过渡。


<details>
  <summary>Details</summary>
Motivation: 目标是生成人工城市，而非复制现有城市，通过捕捉发展行为创造令人信服的城市环境。同时，希望模型能自我自动化，仅需地形描述作为输入。

Method: 采用基于代理的模拟方法，通过简单的行为规则集让代理在模拟环境中互动，从而涌现出复杂行为。模型支持扩展，可涵盖社会和文化影响。

Result: 生成的系统能够根据地形描述和其他参数自动产生具有说服力的城市模型，支持艺术化调整。

Conclusion: 通过简单规则集的代理互动，能够产生复杂的城市发展行为，模型具有扩展性，可应用于不同类型城市结构的生成。

Abstract: We propose a method to procedurally generate a familiar yet complex human
artifact: the city. We are not trying to reproduce existing cities, but to
generate artificial cities that are convincing and plausible by capturing
developmental behavior. In addition, our results are meant to build upon
themselves, such that they ought to look compelling at any point along the
transition from village to metropolis. Our approach largely focuses upon land
usage and building distribution for creating realistic city environments,
whereas previous attempts at city modeling have mainly focused on populating
road networks. Finally, we want our model to be self automated to the point
that the only necessary input is a terrain description, but other high-level
and low-level parameters can be specified to support artistic contributions.
With the aid of agent based simulation we are generating a system of agents and
behaviors that interact with one another through their effects upon a simulated
environment. Our philosophy is that as each agent follows a simple behavioral
rule set, a more complex behavior will tend to emerge out of the interactions
between the agents and their differing rule sets. By confining our model to a
set of simple rules for each class of agents, we hope to make our model
extendible not only in regard to the types of structures that are produced, but
also in describing the social and cultural influences prevalent in all cities

</details>


### [3] [TiVy: Time Series Visual Summary for Scalable Visualization](https://arxiv.org/abs/2507.18972)
*Gromit Yeuk-Yin Chan,Luis Gustavo Nonato,Themis Palpanas,Cláudio T. Silva,Juliana Freire*

Main category: cs.GR

TL;DR: TiVy算法通过动态时间规整(DTW)和频繁序列模式提取，解决了多时间序列可视化中的可扩展性和视觉清晰度问题，显著提升了速度和准确性。


<details>
  <summary>Details</summary>
Motivation: 现有的多时间序列可视化方法在处理长时间跨度时会导致视觉混乱，难以比较趋势和模式，需要一种更高效的方法。

Method: TiVy算法将时间序列转换为基于DTW视觉相似性的符号序列，并通过频繁序列模式构建相似子序列的分组，生成无视觉混乱的总结。

Result: 实验表明，TiVy能够清晰且准确地提取时间序列模式，速度比传统DTW聚类快1000倍，并能有效探索大规模时间序列数据的隐藏结构。

Conclusion: TiVy提供了一种高效、可扩展的多时间序列可视化方法，显著提升了用户体验和分析效率。

Abstract: Visualizing multiple time series presents fundamental tradeoffs between
scalability and visual clarity. Time series capture the behavior of many
large-scale real-world processes, from stock market trends to urban activities.
Users often gain insights by visualizing them as line charts, juxtaposing or
superposing multiple time series to compare them and identify trends and
patterns. However, existing representations struggle with scalability: when
covering long time spans, leading to visual clutter from too many small
multiples or overlapping lines. We propose TiVy, a new algorithm that
summarizes time series using sequential patterns. It transforms the series into
a set of symbolic sequences based on subsequence visual similarity using
Dynamic Time Warping (DTW), then constructs a disjoint grouping of similar
subsequences based on the frequent sequential patterns. The grouping result, a
visual summary of time series, provides uncluttered superposition with fewer
small multiples. Unlike common clustering techniques, TiVy extracts similar
subsequences (of varying lengths) aligned in time. We also present an
interactive time series visualization that renders large-scale time series in
real-time. Our experimental evaluation shows that our algorithm (1) extracts
clear and accurate patterns when visualizing time series data, (2) achieves a
significant speed-up (1000X) compared to a straightforward DTW clustering. We
also demonstrate the efficiency of our approach to explore hidden structures in
massive time series data in two usage scenarios.

</details>


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [4] [Decompiling Rust: An Empirical Study of Compiler Optimizations and Reverse Engineering Challenges](https://arxiv.org/abs/2507.18792)
*Zixu Zhou*

Main category: cs.PL

TL;DR: 该论文通过基准测试评估了Rust二进制文件反编译的质量，发现泛型、trait方法和错误处理结构显著降低了反编译质量，尤其是在发布版本中。


<details>
  <summary>Details</summary>
Motivation: 由于Rust语言的丰富类型系统、激进的编译器优化和广泛使用的高级抽象，反编译Rust二进制文件具有挑战性。论文旨在评估核心Rust特性及编译器构建模式对反编译质量的影响。

Method: 论文采用基准测试驱动的评估方法，通过自动化评分框架分析不同语言特性对反编译质量的影响，并通过案例研究具体分析控制流、变量命名和类型信息恢复。

Result: 研究发现，泛型、trait方法和错误处理结构显著降低了反编译质量，尤其是在发布版本中。

Conclusion: 研究结果为工具开发者提供了实用建议，并强调了需要采用针对Rust优化的反编译策略。

Abstract: Decompiling Rust binaries is challenging due to the language's rich type
system, aggressive compiler optimizations, and widespread use of high-level
abstractions. In this work, we conduct a benchmark-driven evaluation of
decompilation quality across core Rust features and compiler build modes. Our
automated scoring framework shows that generic types, trait methods, and error
handling constructs significantly reduce decompilation quality, especially in
release builds. Through representative case studies, we analyze how specific
language constructs affect control flow, variable naming, and type information
recovery. Our findings provide actionable insights for tool developers and
highlight the need for Rust-aware decompilation strategies.

</details>


### [5] [IsaMini: Redesigned Isabelle Proof Lanugage for Machine Learning](https://arxiv.org/abs/2507.18885)
*Qiyuan Xu,Renxi Wang,Haonan Li,David Sanan,Conrad Watt*

Main category: cs.PL

TL;DR: 论文探讨了通过重新设计证明语言来改进神经定理证明（NTP），引入MiniLang语言并结合改进版Sledgehammer，显著提升了LLM在PISA基准上的成功率。


<details>
  <summary>Details</summary>
Motivation: 传统证明工程需要高昂的人工或计算成本，研究旨在通过优化LLM的表现形式来降低这些成本，推动形式化验证及其他软件工程方法的发展。

Method: 论文提出并测试了MiniLang这一重新设计的证明语言，结合改进版本的Sledgehammer，用于Isabelle/HOL环境中的神经定理证明。

Result: 实验结果显示，MiniLang显著提升了两种微调LLM在PISA基准上的表现，pass@1成功率高达69.1%，超过之前的65.7%（Baldur's pass@64），pass@8达到79.2%，优于当前最佳Magnushammer的71.0%。

Conclusion: 通过优化证明语言的表示形式，NTP在LLMs上的性能得到了显著提升，为形式化验证和软件工程领域提供了新的工具和方法。

Abstract: Neural Theorem Proving (NTP) employs deep learning methods, particularly
Large Language Models (LLMs), to automate formal proofs in proof assistants.
This approach holds promise for reducing the dramatic labor costs or
computation costs required in proof engineering, which is fundamental to formal
verification and other software engineering methods. The paper explores the
potential of improving NTP by redesigning the proof language, given that LLMs'
capabilities depend highly on representations. We introduce \emph{MiniLang}, a
redesigned proof language for Isabelle/HOL incorporating an improved version of
Sledgehammer. Experiments show MiniLang benefits two fine-tuned LLMs by
improving the success rate on the PISA benchmark by up to 29\% in comparison to
generation of Isar proof script. The success rate under one attempt (so-called
\emph{pass@1}) reaches 69.1\%, exceeding the previous Baldur's pass@64
(65.7\%); The pass@8 reaches 79.2\%, exceeding the state-of-the-art on PISA
(71.0\%) achieved by Magnushammer.

</details>


### [6] [An Enumerative Embedding of the Python Type System in ACL2s](https://arxiv.org/abs/2507.19015)
*Samuel Xifaras,Panagiotis Manolios,Andrew T. Walter,William Robertson*

Main category: cs.PL

TL;DR: 该论文提出了一种使用ACL2s对Python代码进行推理的方法，通过嵌入Python类型系统的子集来生成输入并模糊测试Python程序，从而发现未被现有类型检查器检测到的错误。


<details>
  <summary>Details</summary>
Motivation: Python作为一种广泛使用的高级解释语言，其代码的质量和安全性日益重要。论文旨在利用ACL2s工具对Python代码进行更深入的静态分析和测试。

Method: 通过将Python类型系统的子集嵌入到ACL2s中，定义类型并为这些类型生成实例，用于模糊测试Python程序。

Result: 在四个开源仓库中测试，代码覆盖率在68%至80%以上，并识别了影响覆盖率的代码模式（如复杂分支条件和外部文件系统依赖）。

Conclusion: 论文展示了使用ACL2s进行Python代码分析和测试的潜力，并提出了未来改进的方向。

Abstract: Python is a high-level interpreted language that has become an industry
standard in a wide variety of applications. In this paper, we take a first step
towards using ACL2s to reason about Python code by developing an embedding of a
subset of the Python type system in ACL2s. The subset of Python types we
support includes many of the most commonly used type annotations as well as
user-defined types comprised of supported types. We provide ACL2s definitions
of these types, as well as defdata enumerators that are customized to provide
code coverage and identify errors in Python programs. Using the ACL2s
embedding, we can generate instances of types that can then be used as inputs
to fuzz Python programs, which allows us to identify bugs in Python code that
are not detected by state-of-the-art Python type checkers. We evaluate our work
against four open-source repositories, extracting their type information and
generating inputs for fuzzing functions with type signatures that are in the
supported subset of Python types. Note that we only use the type signatures of
functions to generate inputs and treat the bodies of functions as black boxes.
We measure code coverage, which ranges from about 68% to more than 80%, and
identify code patterns that hinder coverage such as complex branch conditions
and external file system dependencies. We conclude with a discussion of the
results and recommendations for future work.

</details>


### [7] [A Programming Language for Feasible Solutions](https://arxiv.org/abs/2507.19176)
*Weijun Chen,Yuxi Fu,Huan Long*

Main category: cs.PL

TL;DR: 本文介绍了一种新的命令式编程语言，其设计基于静态类型系统，确保所有可定义的程序在多项式时间内运行，并且所有多项式时间可解问题都可通过该语言解决。


<details>
  <summary>Details</summary>
Motivation: 程序验证中的运行效率和终止性是关键问题，本文旨在开发一个稳健的框架，确保这些属性在设计时得到保证。

Method: 设计了一种新的命令式编程语言，基于静态类型系统，确保程序的运行时间在多项式范围内。

Result: 理论方面证实了基本等价性，非平凡地证明了等价定理；实践方面实现了该语言的解释器，验证了方法的可行性。

Conclusion: 该框架为程序分析和验证提供了高效且可靠的方法，同时在理论和实践中均取得了显著成果。

Abstract: Runtime efficiency and termination are crucial properties in the studies of
program verification. Instead of dealing with these issues in an ad hoc manner,
it would be useful to develop a robust framework in which such properties are
guaranteed by design. This paper introduces a new imperative programming
language whose design is grounded in a static type system that ensures the
following equivalence property: All definable programs are guaranteed to run in
polynomial time; Conversely, all problems solvable in polynomial time can be
solved by some programs of the language. The contribution of this work is
twofold. On the theoretical side, the foundational equivalence property is
established, and the proof of the equivalence theorem is non-trivial. On the
practical side, a programming approach is proposed that can streamline program
analysis and verification for feasible computations. An interpreter for the
language has been implemented, demonstrating the feasibility of the approach in
practice.

</details>


<div id='cs.GT'></div>

# cs.GT [[Back]](#toc)

### [8] [Existence of 2-EFX Allocations of Chores](https://arxiv.org/abs/2507.19461)
*Jugal Garg,Aniket Murhekar*

Main category: cs.GT

TL;DR: 该论文研究了不可分割杂务的公平分配问题，证明了在所有具有加法效用的实例中存在2-EFX分配，改进了之前的结果，并提供了一个通用的框架用于实现近似-EFX分配。


<details>
  <summary>Details</summary>
Motivation: 研究不可分割杂务的公平分配问题，特别是满足嫉妒自由性（EFX）及其乘法近似性，以解决实际分配中的公平性问题。

Method: 通过提供一个通用框架，从满足EF1和帕累托最优（PO）的初始分配出发，通过局部交换来达到近似-EFX分配。

Result: 证明了在所有具有加法效用的实例中存在2-EFX分配，并统一证明了多项现有结果的简单证明。

Conclusion: 该框架因其简单性和通用性，有望在近似-EFX领域有更广泛的应用。

Abstract: We study the fair division of indivisible chores among agents with additive
disutility functions. We investigate the existence of allocations satisfying
the popular fairness notion of envy-freeness up to any chore (EFX), and its
multiplicative approximations. The existence of $4$-EFX allocations was
recently established by Garg, Murhekar, and Qin (2025). We improve this
guarantee by proving the existence of $2$-EFX allocations for all instances
with additive disutilities. This approximation was previously known only for
restricted instances such as bivalued disutilities (Lin, Wu, and Zhou (2025))
or three agents (Afshinmehr, Ansaripour, Danaei, and Mehlhorn (2024)).
  We obtain our result by providing a general framework for achieving
approximate-EFX allocations. The approach begins with a suitable initial
allocation and performs a sequence of local swaps between the bundles of
envious and envied agents. For our main result, we begin with an initial
allocation that satisfies envy-freeness up to one chore (EF1) and
Pareto-optimality (PO); the existence of such an allocation was recently
established in a major breakthrough by Mahara (2025). We further demonstrate
the strength and generality of our framework by giving simple and unified
proofs of existing results, namely (i) $2$-EFX for bivalued instances, (ii)
2-EFX for three agents, (iii) EFX when the number of chores is at most twice
the number of agents, and (iv) $4$-EFX for all instances. We expect this
framework to have broader applications in approximate-EFX due to its simplicity
and generality.

</details>
