{"id": "2601.17670", "categories": ["cs.PL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.17670", "abs": "https://arxiv.org/abs/2601.17670", "authors": ["Roberto Rossi", "Steven D. Prestwich"], "title": "Grammar-Aware Literate Generative Mathematical Programming with Compiler-in-the-Loop", "comment": "18 pages, 10 figures", "summary": "This work investigates generative mathematical programming through the lens of Algebraic Modelling Languages (AMLs) and compiler-guided model synthesis. By leveraging PyOPL, an OPL-like AML compiler that provides detailed syntax diagnostics, we introduce SyntAGM, an end-to-end system that translates natural language problem descriptions into PyOPL models via a generate--compile--assess--revise loop. SyntAGM is grammar-aware thanks to in-context exposure to the PyOPL BNF grammar, and benefits from few-shot retrieval of literate PyOPL model exemplars. To obtain a valid PyOPL model that matches the problem description, SyntAGM mobilises compiler feedback and an LLM-based alignment judge. In a comparative study against established prompting baselines SyntAGM achieves competitive accuracy with superior token, cost, and latency profiles."}
{"id": "2601.17957", "categories": ["cs.PL", "cs.DC", "cs.FL", "cs.LO", "cs.MA"], "pdf": "https://arxiv.org/pdf/2601.17957", "abs": "https://arxiv.org/abs/2601.17957", "authors": ["Ehud Shapiro"], "title": "Types for Grassroots Logic Programs", "comment": null, "summary": "Grassroots Logic Programs (GLP) is a concurrent logic programming language in which logic variables are partitioned into paired readers and writers. An assignment is produced at most once via a writer and consumed at most once via its paired reader, and may contain additional readers and/or writers. This enables the concise expression of rich multidirectional communication modalities.\n  ``Logic Programs as Types for Logic Programs'' (LICS'91) defined types as regular sets of paths over derivable ground atoms. Here, we define types to be regular sets of moded paths, where a mode captures directionality of communication -- whether a subterm is consumed from or produced to the environment -- enabling the typing of interactive partial computations including those that eventually deadlock or fail, or never terminate. We provide a syntactic definition of well-typing and prove that a program is well-typed iff the path abstraction of its moded-atom semantics satisfies covariance and contravariance conditions with respect to its type.\n  The GLP type system was implemented in Dart by AI, starting from a mathematical specification of Typed GLP (this paper), deriving from it an English spec (written by AI), and from the spec deriving Dart code (by AI). While GLP is naturally untyped, the motivation for Typed GLP comes from programming with AI: Asking AI to program complex communication modalities in GLP (and in general) and hoping for the best is a tenuous strategy. The emerging discipline we advocate and employ is for the human designer and AI to jointly develop and agree upon (1)~GLP types; (2)~GLP procedure type declarations; (3)~informal (English) descriptions of the procedures; and only then let AI attempt to write (4)~GLP code based on those."}
{"id": "2601.18793", "categories": ["cs.PL"], "pdf": "https://arxiv.org/pdf/2601.18793", "abs": "https://arxiv.org/abs/2601.18793", "authors": ["Michael Lee", "Ningning Xie", "Oleg Kiselyov", "Jeremy Yallop"], "title": "Handling Scope Checks (Extended Version)", "comment": "Extended version of Handling Scope Checks (POPL'26): includes appendices, fixes minor typos, and tweaks phrasing for readability", "summary": "Metaprogramming and effect handlers interact in unexpected, and sometimes undesirable, ways. One example is scope extrusion: the generation of ill-scoped code. Scope extrusion can either be preemptively prevented, via static type systems, or retroactively detected, via dynamic checks. Static type systems exist in theory, but struggle with a range of implementation and usability problems in practice. In contrast, dynamic checks exist in practice (e.g. in MetaOCaml), but are understudied in theory. Designers of metalanguages are thus given little guidance regarding the design and implementation of checks. We present the first formal study of dynamic scope extrusion checks, introducing a calculus ($λ_{\\langle\\langle\\text{op}\\rangle\\rangle}$) for describing and evaluating checks. Further, we introduce a novel dynamic check $\\unicode{x2014}$ the \"Cause-for-Concern\" check $\\unicode{x2014}$ which we prove correct, characterise without reference to its implementation, and argue combines the advantages of existing dynamic checks. Finally, we extend our framework with refined environment classifiers, which statically prevent scope extrusion, and compare their expressivity with the dynamic checks."}
{"id": "2601.18475", "categories": ["cs.GR", "cs.CV"], "pdf": "https://arxiv.org/pdf/2601.18475", "abs": "https://arxiv.org/abs/2601.18475", "authors": ["Xinhui Liu", "Can Wang", "Lei Liu", "Zhenghao Chen", "Wei Jiang", "Wei Wang", "Dong Xu"], "title": "LoD-Structured 3D Gaussian Splatting for Streaming Video Reconstruction", "comment": null, "summary": "Free-Viewpoint Video (FVV) reconstruction enables photorealistic and interactive 3D scene visualization; however, real-time streaming is often bottlenecked by sparse-view inputs, prohibitive training costs, and bandwidth constraints. While recent 3D Gaussian Splatting (3DGS) has advanced FVV due to its superior rendering speed, Streaming Free-Viewpoint Video (SFVV) introduces additional demands for rapid optimization, high-fidelity reconstruction under sparse constraints, and minimal storage footprints. To bridge this gap, we propose StreamLoD-GS, an LoD-based Gaussian Splatting framework designed specifically for SFVV. Our approach integrates three core innovations: 1) an Anchor- and Octree-based LoD-structured 3DGS with a hierarchical Gaussian dropout technique to ensure efficient and stable optimization while maintaining high-quality rendering; 2) a GMM-based motion partitioning mechanism that separates dynamic and static content, refining dynamic regions while preserving background stability; and 3) a quantized residual refinement framework that significantly reduces storage requirements without compromising visual fidelity. Extensive experiments demonstrate that StreamLoD-GS achieves competitive or state-of-the-art performance in terms of quality, efficiency, and storage."}
{"id": "2601.17131", "categories": ["cs.GT"], "pdf": "https://arxiv.org/pdf/2601.17131", "abs": "https://arxiv.org/abs/2601.17131", "authors": ["Ondrej Kubicek", "Viliam Lisy", "Tuomas Sandholm"], "title": "Equilibrium Refinements Improve Subgame Solving in Imperfect-Information Games", "comment": null, "summary": "Subgame solving is a technique for scaling algorithms to large games by locally refining a precomputed blueprint strategy during gameplay. While straightforward in perfect-information games where search starts from the current state, subgame solving in imperfect-information games must account for hidden states and uncertainty about the opponent's past strategy. Gadget games were developed to ensure that the improved subgame strategy is robust against any possible opponent's strategy in a zero-sum game. Gadget games typically contain infinitely many Nash equilibria. We demonstrate that while these equilibria are equivalent in the gadget game, they yield vastly different performance in the full game, even when facing a rational opponent. We propose gadget game sequential equilibria as the preferred solution concept. We introduce modifications to the sequence-form linear program and counterfactual regret minimization that converge to these refined solutions with only mild additional computational cost. Additionally, we provide several new insights into the surprising superiority of the resolving gadget game over the max-margin gadget game. Our experiments compare different Nash equilibria of gadget games in several standard benchmark games, showing that our refined equilibria consistently outperform unrefined Nash equilibria, and can reduce the exploitability of the overall strategy by more than 50%"}
{"id": "2601.17263", "categories": ["cs.GT"], "pdf": "https://arxiv.org/pdf/2601.17263", "abs": "https://arxiv.org/abs/2601.17263", "authors": ["Sanyukta Deshpande", "Sheldon H. Jacobson"], "title": "Strategic AI in Cournot Markets", "comment": null, "summary": "As artificial intelligence increasingly automates decision-making in competitive markets, understanding the resulting dynamics and ensuring fair market mechanisms is essential. We investigate the multi-faceted decision-making of large language models (LLMs) in oligopolistic Cournot markets, showing that LLMs not only grasp complex market dynamics--demonstrating their potential as effective economic planning agents--but also engage in sustained tacit collusion, driving prices up to 200% above Nash equilibrium levels. Our analysis examines LLM behavior across three dimensions-(1) decision type, (2) opponent strategies, and (3) market composition--revealing how these factors may shape the competitiveness of LLM-based decision-makers. Furthermore, we show that regulating a few dominant agents by enforcing best-response strategies effectively disrupts collusion and helps restore competitive pricing. Our findings identify potential concerns associated with AI integration in competitive market environments and provide regulatory policy recommendations for the era of automation."}
{"id": "2601.17538", "categories": ["cs.GT"], "pdf": "https://arxiv.org/pdf/2601.17538", "abs": "https://arxiv.org/abs/2601.17538", "authors": ["Qishen Han", "Artem Ivaniuk", "Edith Elkind", "Lirong Xia"], "title": "Truth-Revealing Participatory Budgeting", "comment": null, "summary": "Participatory Budgeting (PB) is commonly studied from an axiomatic perspective, where the aim is to design procedurally fair and economically efficient rules for voters with full information regarding their preferences. In contrast, we take an epistemic perspective and consider a framework where PB projects have different levels of underlying quality, indicating how well the project will take effect, which cannot be directly observed before implementation. Agents with noisy information cast votes to aggregate their information, and aim to elect a high-quality set of projects. We evaluate the performance of common PB rules by measuring the expected utility of their outcomes, compared to the optimal set of projects. We find that the quality of approximation improves as the range of project costs shrinks. When projects have unit cost, these common rules can identify the ``best'' set with probability converging to 1. We also study whether strategic agents have incentives to honestly convey their information in the vote. We find that it happens only under very restrictive conditions. We also run numerical experiments to examine the performance of different rules empirically and support our theoretical findings."}
{"id": "2601.17931", "categories": ["cs.GT"], "pdf": "https://arxiv.org/pdf/2601.17931", "abs": "https://arxiv.org/abs/2601.17931", "authors": ["Piotr Faliszewski", "Jitka Mertlová", "Pierre Nunn", "Stanisław Szufa", "Tomasz Wąs"], "title": "Distances Between Top-Truncated Elections of Different Sizes", "comment": "24 pages, 13 figures, AAAI 2025 Conference", "summary": "The map of elections framework is a methodology for visualizing and analyzing election datasets. So far, the framework was restricted to elections that have equal numbers of candidates, equal numbers of voters, and where all the (ordinal) votes rank all the candidates. We extend it to the case of elections of different sizes, where the votes can be top-truncated. We use our results to present a visualization of a large fragment of the Preflib database."}
{"id": "2601.17944", "categories": ["cs.GT", "cs.AI", "cs.OS"], "pdf": "https://arxiv.org/pdf/2601.17944", "abs": "https://arxiv.org/abs/2601.17944", "authors": ["Seyed Majid Zahedi", "Rupert Freeman"], "title": "Credit Fairness: Online Fairness In Shared Resource Pools", "comment": null, "summary": "We consider a setting in which a group of agents share resources that must be allocated among them in each discrete time period. Agents have time-varying demands and derive constant marginal utility from each unit of resource received up to their demand, with zero utility for any additional resources. In this setting, it is known that independently maximizing the minimum utility in each round satisfies sharing incentives (agents weakly prefer participating in the mechanism to not participating), strategyproofness (agents have no incentive to misreport their demands), and Pareto efficiency (Freeman et al. 2018). However, recent work (Vuppalapati et al. 2023) has shown that this max-min mechanism can lead to large disparities in the total resources received by agents, even when they have the same average demand. In this paper, we introduce credit fairness, a strengthening of sharing incentives that ensures agents who lend resources in early rounds are able to recoup them in later rounds. Credit fairness can be achieved in conjunction with either Pareto efficiency or strategyproofness, but not both. We propose a mechanism that is credit fair and Pareto efficient, and we evaluate its performance in a computational resource-sharing setting."}
{"id": "2601.18117", "categories": ["cs.GT", "math.OC"], "pdf": "https://arxiv.org/pdf/2601.18117", "abs": "https://arxiv.org/abs/2601.18117", "authors": ["Boxiao Chen", "Jiashuo Jiang", "Stefanus Jasin"], "title": "Decentralized Multi-product Pricing: Diagonal Dominance, Nash Equilibrium, and Price of Anarchy", "comment": null, "summary": "Decentralized decision making in multi--product firms can lead to efficiency losses when autonomous decision makers fail to internalize cross--product demand interactions. This paper quantifies the magnitude of such losses by analyzing the Price of Anarchy in a pricing game in which each decision maker independently sets prices to maximize its own product--level revenue. We model demand using a linear system that captures both substitution and complementarity effects across products. We first establish existence and uniqueness of a pure--strategy Nash equilibrium under economically standard diagonal dominance conditions. Our main contribution is the derivation of a tight worst--case lower bound on the ratio between decentralized revenue and the optimal centralized revenue. We show that this efficiency loss is governed by a single scalar parameter, denoted by $μ$, which measures the aggregate strength of cross--price effects relative to own--price sensitivities. In particular, we prove that the revenue ratio is bounded below by $4(1-μ)/(2-μ)^2$, and we demonstrate the tightness of this bound by constructing a symmetric market topology in which the bound is exactly attained. We further refine the analysis by providing an instance--exact characterization of efficiency loss based on the spectral properties of the demand interaction matrix. Together, these results offer a quantitative framework for assessing the trade--off between centralized pricing and decentralized autonomy in multi--product firms."}
{"id": "2601.18303", "categories": ["cs.GT", "cs.LO", "cs.MA"], "pdf": "https://arxiv.org/pdf/2601.18303", "abs": "https://arxiv.org/abs/2601.18303", "authors": ["Léonard Brice", "Thomas A. Henzinger", "K. S. Thejaswini"], "title": "Dicey Games: Shared Sources of Randomness in Distributed Systems", "comment": "16 pages, 9 figures", "summary": "Consider a 4-player version of Matching Pennies where a team of three players competes against the Devil. Each player simultaneously says \"Heads\" or \"Tails\". The team wins if all four choices match; otherwise the Devil wins. If all team players randomise independently, they win with probability 1/8; if all players share a common source of randomness, they win with probability 1/2. What happens when each pair of team players shares a source of randomness? Can the team do better than win with probability 1/4? The surprising (and nontrivial) answer is yes! We introduce Dicey Games, a formal framework motivated by the study of distributed systems with shared sources of randomness (of which the above example is a specific instance). We characterise the existence, representation and computational complexity of optimal strategies in Dicey Games, and we study the problem of allocating limited sources of randomness optimally within a team."}
{"id": "2601.18348", "categories": ["cs.GT"], "pdf": "https://arxiv.org/pdf/2601.18348", "abs": "https://arxiv.org/abs/2601.18348", "authors": ["Filip Nikolow", "Piotr Faliszewski", "Stanisław Szufa"], "title": "Maps of Tournaments: Distances, Experiments, and Data", "comment": null, "summary": "We form a \"map of tournaments\" by adapting the map framework from the world of elections. By a tournament we mean a complete directed graph where the nodes are the players and an edge points from a winner of a game to the loser (with no ties allowed). A map is a set of tournaments represented as points on a 2D plane, so that their Euclidean distances resemble the distances computed according to a given measure. We identify useful distance measures, discuss ways of generating random tournaments (and compare them to several real-life ones), and show how the maps are helpful in visualizing experimental results (also for knockout tournaments)."}
{"id": "2601.18573", "categories": ["cs.GT", "cs.DS"], "pdf": "https://arxiv.org/pdf/2601.18573", "abs": "https://arxiv.org/abs/2601.18573", "authors": ["Frederik Glitzner", "David Manlove"], "title": "Stable Matching with Deviators and Conformists", "comment": "Preliminary version to appear at AAMAS 2026", "summary": "In the fundamental Stable Marriage and Stable Roommates problems, there are inherent trade-offs between the size and stability of solutions. While in the former problem, a stable matching always exists and can be found efficiently using the celebrated Gale-Shapley algorithm, the existence of a stable matching is not guaranteed in the latter problem, but can be determined efficiently using Irving's algorithm. However, the computation of matchings that minimise the instability, either due to the presence of additional constraints on the size of the matching or due to restrictive preference cycles, gives rise to a collection of infamously intractable almost-stable matching problems. In practice, however, not every agent is able or likely to initiate deviations caused by blocking pairs. Suppose we knew, for example, due to a set of requirements or estimates based on historical data, which agents are likely to initiate deviations - the deviators - and which are likely to comply with whatever matching they are presented with - the conformists. Can we decide efficiently whether a matching exists in which no deviator is blocking, i.e., in which no deviator has an incentive to initiate a deviation? Furthermore, can we find matchings in which only a few deviators are blocking? We characterise the computational complexity of this question in bipartite and non-bipartite preference settings. Surprisingly, these problems prove computationally intractable in strong ways: for example, unlike in the classical setting, where every agent is considered a deviator, in this extension, we prove that it is NP-complete to decide whether a matching exists where no deviator is blocking. On the positive side, we identify polynomial-time and fixed-parameter tractable cases, providing novel algorithmics for multi-agent systems where stability cannot be fully guaranteed."}
{"id": "2601.18651", "categories": ["cs.GT"], "pdf": "https://arxiv.org/pdf/2601.18651", "abs": "https://arxiv.org/abs/2601.18651", "authors": ["Piotr Faliszewski", "Łukasz Janeczko", "Andrzej Kaczmarczyk", "Marcin Kurdziel", "Grzegorz Pierczyński", "Stanisław Szufa"], "title": "Learning Real-Life Approval Elections", "comment": null, "summary": "We study the independent approval model (IAM) for approval elections, where each candidate has its own approval probability and is approved independently of the other ones. This model generalizes, e.g., the impartial culture, the Hamming noise model, and the resampling model. We propose algorithms for learning IAMs and their mixtures from data, using either maximum likelihood estimation or Bayesian learning. We then apply these algorithms to a large set of elections from the Pabulib database. In particular, we find that single-component models are rarely sufficient to capture the complexity of real-life data, whereas their mixtures perform well."}
