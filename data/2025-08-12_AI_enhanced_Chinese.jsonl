{"id": "2508.07240", "categories": ["cs.GR"], "pdf": "https://arxiv.org/pdf/2508.07240", "abs": "https://arxiv.org/abs/2508.07240", "authors": ["Zixuan Li", "Zixiong Wang", "Jian Yang", "Milos Hasan", "Beibei Wang"], "title": "PureSample: Neural Materials Learned by Sampling Microgeometry", "comment": null, "summary": "Traditional physically-based material models rely on analytically derived\nbidirectional reflectance distribution functions (BRDFs), typically by\nconsidering statistics of micro-primitives such as facets, flakes, or spheres,\nsometimes combined with multi-bounce interactions such as layering and multiple\nscattering. These derivations are often complex and model-specific, and\ntypically consider a statistical aggregate of a large surface area, ignoring\nspatial variation. Once an analytic BRDF's evaluation is defined, one still\nneeds to design an importance sampling method for it, and a way to evaluate the\npdf of that sampling distribution, requiring further model-specific\nderivations.\n  We present PureSample: a novel neural BRDF representation that allows\nlearning a material's behavior purely by sampling forward random walks on the\nmicrogeometry, which is usually straightforward to implement. Our\nrepresentation allows for efficient importance sampling, pdf evaluation, and\nBRDF evaluation, for homogeneous as well as spatially varying materials.\n  We achieve this by two learnable components: first, the sampling distribution\nis modeled using a flow matching neural network, which allows both importance\nsampling and pdf evaluation; second, we introduce a view-dependent albedo term,\ncaptured by a lightweight neural network, which allows for converting a scalar\npdf value to a colored BRDF value for any pair of view and light directions.\n  We demonstrate PureSample on challenging materials, including multi-layered\nmaterials, multiple-scattering microfacet materials, and various other\nmicrostructures.", "AI": {"tldr": "PureSample\u662f\u4e00\u79cd\u65b0\u9896\u7684\u795e\u7ecfBRDF\u8868\u793a\u65b9\u6cd5\uff0c\u901a\u8fc7\u968f\u673a\u884c\u8d70\u91c7\u6837\u5b66\u4e60\u6750\u6599\u884c\u4e3a\uff0c\u7b80\u5316\u4e86\u4f20\u7edf\u590d\u6742\u4e14\u6a21\u578b\u7279\u5b9a\u7684BRDF\u5206\u6790\u65b9\u6cd5\u3002", "motivation": "\u4f20\u7edf\u7684\u57fa\u4e8e\u7269\u7406\u7684\u6750\u6599\u6a21\u578b\u4f9d\u8d56\u4e8e\u5206\u6790\u63a8\u5bfc\u7684BRDF\uff0c\u8fc7\u7a0b\u590d\u6742\u4e14\u5ffd\u7565\u7a7a\u95f4\u53d8\u5316\u3002PureSample\u65e8\u5728\u7b80\u5316\u8fd9\u4e00\u8fc7\u7a0b\uff0c\u901a\u8fc7\u5b66\u4e60\u5fae\u89c2\u51e0\u4f55\u4e0a\u7684\u968f\u673a\u884c\u8d70\u5b9e\u73b0\u5bf9\u6750\u6599\u884c\u4e3a\u7684\u76f4\u63a5\u5efa\u6a21\u3002", "method": "\u63d0\u51faPureSample\uff0c\u901a\u8fc7\u4e24\u90e8\u5206\u53ef\u5b66\u4e60\u7ec4\u4ef6\u5b9e\u73b0\uff1a1\uff09\u4f7f\u7528\u6d41\u5339\u914d\u795e\u7ecf\u7f51\u7edc\u5efa\u6a21\u91c7\u6837\u5206\u5e03\uff0c\u652f\u6301\u91cd\u8981\u6027\u91c7\u6837\u548cPDF\u8bc4\u4f30\uff1b2\uff09\u5f15\u5165\u89c6\u70b9\u76f8\u5173\u53cd\u7167\u7387\u9879\uff0c\u901a\u8fc7\u8f7b\u91cf\u7ea7\u795e\u7ecf\u7f51\u7edc\u5c06\u6807\u91cfPDF\u503c\u8f6c\u6362\u4e3a\u5f69\u8272BRDF\u503c\u3002", "result": "\u5728\u591a\u5c42\u6750\u6599\u3001\u591a\u6563\u5c04\u5fae\u9762\u6750\u6599\u7b49\u591a\u79cd\u590d\u6742\u6750\u6599\u4e0a\u9a8c\u8bc1\u4e86PureSample\u7684\u6709\u6548\u6027\uff0c\u5c55\u793a\u4e86\u5176\u5728\u9ad8\u6548\u7387\u91cd\u8981\u6027\u91c7\u6837\u3001PDF\u8bc4\u4f30\u548cBRDF\u8bc4\u4f30\u65b9\u9762\u7684\u4f18\u52bf\u3002", "conclusion": "PureSample\u63d0\u4f9b\u4e86\u4e00\u79cd\u901a\u7528\u7684\u795e\u7ecfBRDF\u8868\u793a\u65b9\u6cd5\uff0c\u5728\u590d\u6742\u6750\u6599\u7684\u5efa\u6a21\u548c\u6e32\u67d3\u4e2d\u8868\u73b0\u51fa\u9ad8\u6548\u6027\u548c\u7075\u6d3b\u6027\u3002"}}
{"id": "2508.07615", "categories": ["cs.GR"], "pdf": "https://arxiv.org/pdf/2508.07615", "abs": "https://arxiv.org/abs/2508.07615", "authors": ["Chuanfu Hu", "Aimin Hou"], "title": "Verification Method for Graph Isomorphism Criteria", "comment": "17 pages, 5 figures, 2 tables", "summary": "The criteria for determining graph isomorphism are crucial for solving graph\nisomorphism problems. The necessary condition is that two isomorphic graphs\npossess invariants, but their function can only be used to filtrate and\nsubdivide candidate spaces. The sufficient conditions are used to rebuild the\nisomorphic reconstruction of special graphs, but their drawback is that the\nisomorphic functions of subgraphs may not form part of the isomorphic functions\nof the parent graph. The use of sufficient or necessary conditions generally\nresults in backtracking to ensure the correctness of the decision algorithm.\nThe sufficient and necessary conditions can ensure that the determination of\ngraph isomorphism does not require backtracking, but the correctness of its\nproof process is difficult to guarantee. This article proposes a verification\nmethod that can correctly determine whether the judgment conditions proposed by\nprevious researchers are sufficient and necessary conditions. A subdivision\nmethod has also been proposed in this article, which can obtain more\nsubdivisions for necessary conditions and effectively reduce the size of\nbacktracking space.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u9a8c\u8bc1\u56fe\u540c\u6784\u5224\u5b9a\u7684\u5145\u5206\u5fc5\u8981\u6761\u4ef6\u7684\u65b9\u6cd5\uff0c\u5e76\u5f15\u5165\u4e86\u4e00\u79cd\u7ec6\u5206\u65b9\u6cd5\u4ee5\u51cf\u5c11\u56de\u6eaf\u7a7a\u95f4\u7684\u5927\u5c0f\u3002", "motivation": "\u56fe\u540c\u6784\u5224\u5b9a\u7684\u5145\u5206\u5fc5\u8981\u6761\u4ef6\u5bf9\u89e3\u51b3\u56fe\u540c\u6784\u95ee\u9898\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u7684\u6b63\u786e\u6027\u96be\u4ee5\u4fdd\u8bc1\uff0c\u4e14\u5b58\u5728\u56de\u6eaf\u7a7a\u95f4\u8fc7\u5927\u7684\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u9a8c\u8bc1\u65b9\u6cd5\u6765\u5224\u5b9a\u524d\u4eba\u63d0\u51fa\u7684\u6761\u4ef6\u662f\u5426\u5145\u5206\u5fc5\u8981\uff0c\u5e76\u5f15\u5165\u4e00\u79cd\u7ec6\u5206\u65b9\u6cd5\u4ee5\u4f18\u5316\u5fc5\u8981\u6761\u4ef6\u7684\u5e94\u7528\u3002", "result": "\u8be5\u65b9\u6cd5\u80fd\u591f\u6b63\u786e\u9a8c\u8bc1\u5145\u5206\u5fc5\u8981\u6761\u4ef6\uff0c\u5e76\u901a\u8fc7\u7ec6\u5206\u65b9\u6cd5\u663e\u8457\u51cf\u5c11\u56de\u6eaf\u7a7a\u95f4\u7684\u5927\u5c0f\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u7684\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86\u56fe\u540c\u6784\u5224\u5b9a\u4e2d\u6761\u4ef6\u9a8c\u8bc1\u548c\u56de\u6eaf\u7a7a\u95f4\u7684\u95ee\u9898\uff0c\u63d0\u5347\u4e86\u7b97\u6cd5\u7684\u6548\u7387\u548c\u53ef\u9760\u6027\u3002"}}
{"id": "2508.07852", "categories": ["cs.GR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.07852", "abs": "https://arxiv.org/abs/2508.07852", "authors": ["Rui Su", "Honghao Dong", "Haojie Jin", "Yisong Chen", "Guoping Wang", "Sheng Li"], "title": "Vertex Features for Neural Global Illumination", "comment": "Accepted by ACM SIGGRAPH Asia'2025", "summary": "Recent research on learnable neural representations has been widely adopted\nin the field of 3D scene reconstruction and neural rendering applications.\nHowever, traditional feature grid representations often suffer from substantial\nmemory footprint, posing a significant bottleneck for modern parallel computing\nhardware. In this paper, we present neural vertex features, a generalized\nformulation of learnable representation for neural rendering tasks involving\nexplicit mesh surfaces. Instead of uniformly distributing neural features\nthroughout 3D space, our method stores learnable features directly at mesh\nvertices, leveraging the underlying geometry as a compact and structured\nrepresentation for neural processing. This not only optimizes memory\nefficiency, but also improves feature representation by aligning compactly with\nthe surface using task-specific geometric priors. We validate our neural\nrepresentation across diverse neural rendering tasks, with a specific emphasis\non neural radiosity. Experimental results demonstrate that our method reduces\nmemory consumption to only one-fifth (or even less) of grid-based\nrepresentations, while maintaining comparable rendering quality and lowering\ninference overhead.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u79f0\u4e3a\u795e\u7ecf\u9876\u70b9\u7279\u5f81\u7684\u8868\u793a\u65b9\u6cd5\uff0c\u7528\u4e8e\u4f18\u5316\u795e\u7ecf\u6e32\u67d3\u4efb\u52a1\u4e2d\u7684\u5185\u5b58\u6548\u7387\u548c\u7279\u5f81\u5bf9\u9f50\u3002", "motivation": "\u4f20\u7edf\u7684\u7279\u5f81\u7f51\u683c\u8868\u793a\u5728\u795e\u7ecf\u6e32\u67d3\u4efb\u52a1\u4e2d\u5b58\u5728\u5185\u5b58\u5360\u7528\u8fc7\u9ad8\u7684\u95ee\u9898\uff0c\u9650\u5236\u4e86\u5e76\u884c\u8ba1\u7b97\u786c\u4ef6\u7684\u6548\u7387\u3002", "method": "\u91c7\u7528\u795e\u7ecf\u9876\u70b9\u7279\u5f81\uff0c\u5c06\u53ef\u5b66\u4e60\u7279\u5f81\u76f4\u63a5\u5b58\u50a8\u5728\u7f51\u683c\u9876\u70b9\u4e0a\uff0c\u5229\u7528\u5e95\u5c42\u51e0\u4f55\u7ed3\u6784\u4f5c\u4e3a\u7d27\u51d1\u4e14\u7ed3\u6784\u5316\u7684\u8868\u793a\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5c06\u5185\u5b58\u6d88\u8017\u964d\u81f3\u57fa\u4e8e\u7f51\u683c\u8868\u793a\u7684\u4e94\u5206\u4e4b\u4e00\uff08\u751a\u81f3\u66f4\u4f4e\uff09\uff0c\u540c\u65f6\u4fdd\u6301\u53ef\u6bd4\u7684\u6e32\u67d3\u8d28\u91cf\u5e76\u51cf\u5c11\u63a8\u7406\u5f00\u9500\u3002", "conclusion": "\u795e\u7ecf\u9876\u70b9\u7279\u5f81\u662f\u4e00\u79cd\u6709\u6548\u7684\u66ff\u4ee3\u65b9\u6848\uff0c\u80fd\u591f\u663e\u8457\u4f18\u5316\u5185\u5b58\u6548\u7387\u5e76\u6539\u8fdb\u7279\u5f81\u8868\u793a\u3002"}}
{"id": "2508.08198", "categories": ["cs.GR", "cs.RO"], "pdf": "https://arxiv.org/pdf/2508.08198", "abs": "https://arxiv.org/abs/2508.08198", "authors": ["Yupeng Zhang", "Adam Alon", "M. Khalid Jawed"], "title": "Emergent morphogenesis via planar fabrication enabled by a reduced model of composites", "comment": "GitHub repository:\n  https://github.com/StructuresComp/discrete-shells-shrinky-dink/", "summary": "The ability to engineer complex three-dimensional shapes from planar sheets\nwith precise, programmable control underpins emerging technologies in soft\nrobotics, reconfigurable devices, and functional materials. Here, we present a\nreduced-order numerical and experimental framework for a bilayer system\nconsisting of a stimuli-responsive thermoplastic sheet (Shrinky Dink) bonded to\na kirigami-patterned, inert plastic layer. Upon uniform heating, the active\nlayer contracts while the patterned layer constrains in-plane stretch but\nallows out-of-plane bending, yielding programmable 3D morphologies from simple\nplanar precursors. Our approach enables efficient computational design and\nscalable manufacturing of 3D forms with a single-layer reduced model that\ncaptures the coupled mechanics of stretching and bending. Unlike traditional\nbilayer modeling, our framework collapses the multilayer composite into a\nsingle layer of nodes and elements, reducing the degrees of freedom and\nenabling simulation on a 2D geometry. This is achieved by introducing a novel\nenergy formulation that captures the coupling between in-plane stretch mismatch\nand out-of-plane bending - extending beyond simple isotropic linear elastic\nmodels. Experimentally, we establish a fully planar, repeatable fabrication\nprotocol using a stimuli-responsive thermoplastic and a laser-cut inert plastic\nlayer. The programmed strain mismatch drives an array of 3D morphologies, such\nas bowls, canoes, and flower petals, all verified by both simulation and\nphysical prototypes.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u53cc\u5c42\u7cfb\u7edf\u7684\u6570\u503c\u548c\u5b9e\u9a8c\u6846\u67b6\uff0c\u901a\u8fc7\u523a\u6fc0\u54cd\u5e94\u70ed\u5851\u6027\u6750\u6599\u548c\u60f0\u6027\u5851\u6599\u5c42\u7684\u7ec4\u5408\uff0c\u5b9e\u73b0\u4e86\u4ece\u5e73\u9762\u5230\u590d\u6742\u4e09\u7ef4\u5f62\u72b6\u7684\u53ef\u7f16\u7a0b\u63a7\u5236\u3002", "motivation": "\u4e3a\u4e86\u5728\u8f6f\u4f53\u673a\u5668\u4eba\u3001\u53ef\u91cd\u6784\u8bbe\u5907\u548c\u529f\u80fd\u6750\u6599\u7b49\u65b0\u5174\u6280\u672f\u4e2d\u5b9e\u73b0\u590d\u6742\u4e09\u7ef4\u5f62\u72b6\u7684\u53ef\u7f16\u7a0b\u63a7\u5236\u3002", "method": "\u91c7\u7528\u53cc\u5c42\u7cfb\u7edf\uff08\u523a\u6fc0\u54cd\u5e94\u70ed\u5851\u6027\u6750\u6599\u4e0e\u60f0\u6027\u5851\u6599\u5c42\u7ed3\u5408\uff09\u7684\u5355\u5c42\u964d\u9636\u6a21\u578b\uff0c\u901a\u8fc7\u65b0\u7684\u80fd\u91cf\u65b9\u7a0b\u6355\u6349\u62c9\u4f38\u548c\u5f2f\u66f2\u7684\u8026\u5408\u6548\u5e94\u3002", "result": "\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u4ece\u5e73\u9762\u5230\u4e09\u7ef4\u5f62\u6001\uff08\u5982\u7897\u3001\u72ec\u6728\u821f\u548c\u82b1\u74e3\uff09\u7684\u53ef\u7f16\u7a0b\u8f6c\u6362\uff0c\u4eff\u771f\u4e0e\u7269\u7406\u539f\u578b\u4e00\u81f4\u3002", "conclusion": "\u8be5\u6846\u67b6\u901a\u8fc7\u7b80\u5316\u6a21\u578b\u548c\u9ad8\u6548\u5236\u9020\u65b9\u6cd5\uff0c\u4e3a\u590d\u6742\u4e09\u7ef4\u5f62\u6001\u7684\u8bbe\u8ba1\u548c\u5236\u9020\u63d0\u4f9b\u4e86\u65b0\u9014\u5f84\u3002"}}
{"id": "2508.06550", "categories": ["cs.GT", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.06550", "abs": "https://arxiv.org/abs/2508.06550", "authors": ["Yinqiu Huang", "Hao Ma", "Wenshuai Chen", "Shuli Wang", "Yongqiang Zhang", "Xue Wei", "Yinhua Zhu", "Haitao Wang", "Xingxing Wang"], "title": "Generative Bid Shading in Real-Time Bidding Advertising", "comment": null, "summary": "Bid shading plays a crucial role in Real-Time Bidding~(RTB) by adaptively\nadjusting the bid to avoid advertisers overspending. Existing mainstream\ntwo-stage methods, which first model bid landscapes and then optimize surplus\nusing operations research techniques, are constrained by unimodal assumptions\nthat fail to adapt for non-convex surplus curves and are vulnerable to\ncascading errors in sequential workflows. Additionally, existing discretization\nmodels of continuous values ignore the dependence between discrete intervals,\nreducing the model's error correction ability, while sample selection bias in\nbidding scenarios presents further challenges for prediction. To address these\nissues, this paper introduces Generative Bid Shading~(GBS), which comprises two\nprimary components: (1) an end-to-end generative model that utilizes an\nautoregressive approach to generate shading ratios by stepwise residuals,\ncapturing complex value dependencies without relying on predefined priors; and\n(2) a reward preference alignment system, which incorporates a channel-aware\nhierarchical dynamic network~(CHNet) as the reward model to extract\nfine-grained features, along with modules for surplus optimization and\nexploration utility reward alignment, ultimately optimizing both short-term and\nlong-term surplus using group relative policy optimization~(GRPO). Extensive\nexperiments on both offline and online A/B tests validate GBS's effectiveness.\nMoreover, GBS has been deployed on the Meituan DSP platform, serving billions\nof bid requests daily.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a\u751f\u6210\u5f0f\u7ade\u4ef7\u9634\u5f71\uff08GBS\uff09\u7684\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7\u7aef\u5230\u7aef\u751f\u6210\u6a21\u578b\u548c\u5956\u52b1\u504f\u597d\u5bf9\u9f50\u7cfb\u7edf\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u7ade\u4ef7\u65b9\u6cd5\u4e2d\u7684\u975e\u51f8\u95ee\u9898\u548c\u7ea7\u8054\u9519\u8bef\uff0c\u5e76\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002", "motivation": "\u73b0\u6709\u7684\u4e24\u9636\u6bb5\u7ade\u4ef7\u9634\u5f71\u65b9\u6cd5\u53d7\u9650\u4e8e\u5355\u6a21\u6001\u5047\u8bbe\uff0c\u65e0\u6cd5\u9002\u5e94\u975e\u51f8\u7684\u5269\u4f59\u66f2\u7ebf\uff0c\u4e14\u5b58\u5728\u7ea7\u8054\u9519\u8bef\u548c\u79bb\u6563\u5316\u6a21\u578b\u7684\u4f9d\u8d56\u95ee\u9898\uff0c\u5bfc\u81f4\u9884\u6d4b\u80fd\u529b\u4e0b\u964d\u3002", "method": "GBS\u5305\u62ec\u4e24\u90e8\u5206\uff1a(1) \u7aef\u5230\u7aef\u751f\u6210\u6a21\u578b\uff0c\u901a\u8fc7\u9010\u6b65\u6b8b\u5dee\u751f\u6210\u9634\u5f71\u6bd4\u7387\uff1b(2) \u5956\u52b1\u504f\u597d\u5bf9\u9f50\u7cfb\u7edf\uff0c\u5229\u7528CHNet\u63d0\u53d6\u7ec6\u7c92\u5ea6\u7279\u5f81\uff0c\u5e76\u901a\u8fc7GRPO\u4f18\u5316\u77ed\u671f\u548c\u957f\u671f\u5269\u4f59\u3002", "result": "\u79bb\u7ebf\u548c\u5728\u7ebfA/B\u6d4b\u8bd5\u9a8c\u8bc1\u4e86GBS\u7684\u6709\u6548\u6027\uff0c\u5e76\u4e14\u8be5\u65b9\u6cd5\u5df2\u5728\u7f8e\u56e2DSP\u5e73\u53f0\u4e0a\u6bcf\u65e5\u5904\u7406\u6570\u5341\u4ebf\u6b21\u7ade\u4ef7\u8bf7\u6c42\u3002", "conclusion": "GBS\u901a\u8fc7\u521b\u65b0\u7684\u751f\u6210\u6a21\u578b\u548c\u5956\u52b1\u5bf9\u9f50\u7cfb\u7edf\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u7ade\u4ef7\u9634\u5f71\u65b9\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u5e76\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u5c55\u73b0\u4e86\u663e\u8457\u7684\u4f18\u52bf\u3002"}}
{"id": "2508.07855", "categories": ["cs.PL"], "pdf": "https://arxiv.org/pdf/2508.07855", "abs": "https://arxiv.org/abs/2508.07855", "authors": ["Parosh Aziz Abdulla", "Mohamed Faouzi Atig", "R. Govind", "Samuel Grahn", "Ramanathan S. Thinniyam"], "title": "Checking Consistency of Event-driven Traces", "comment": null, "summary": "Event-driven programming is a popular paradigm where the flow of execution is\ncontrolled by two features: (1) shared memory and (2) sending and receiving of\nmessages between multiple handler threads (just called handler). Each handler\nhas a mailbox (modelled as a queue) for receiving messages, with the constraint\nthat the handler processes its messages sequentially. Executions of messages by\ndifferent handlers may be interleaved. A central problem in this setting is\nchecking whether a candidate execution is consistent with the semantics of\nevent-driven programs. In this paper, we propose an axiomatic semantics for\neventdriven programs based on the standard notion of traces (also known as\nexecution graphs). We prove the equivalence of axiomatic and operational\nsemantics. This allows us to rephrase the consistency problem axiomatically,\nresulting in the event-driven consistency problem: checking whether a given\ntrace is consistent. We analyze the computational complexity of this problem\nand show that it is NP-complete, even when the number of handler threads is\nbounded. We then identify a tractable fragment: in the absence of nested\nposting, where handlers do not post new messages while processing a message,\nconsistency checking can be performed in polynomial time. Finally, we implement\nour approach in a prototype tool and report on experimental results on a wide\nrange of benchmarks.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u8ff9\uff08\u6267\u884c\u56fe\uff09\u7684\u4e8b\u4ef6\u9a71\u52a8\u7a0b\u5e8f\u7684\u516c\u7406\u5316\u8bed\u4e49\uff0c\u8bc1\u660e\u4e86\u5176\u4e0e\u64cd\u4f5c\u8bed\u4e49\u7684\u7b49\u4ef7\u6027\uff0c\u5e76\u5c06\u4e00\u81f4\u6027\u95ee\u9898\u8f6c\u5316\u4e3a\u4e8b\u4ef6\u9a71\u52a8\u4e00\u81f4\u6027\u95ee\u9898\uff0c\u5206\u6790\u4e86\u5176\u8ba1\u7b97\u590d\u6742\u6027\uff0c\u5e76\u5b9e\u73b0\u4e86\u4e00\u4e2a\u539f\u578b\u5de5\u5177\u8fdb\u884c\u5b9e\u9a8c\u9a8c\u8bc1\u3002", "motivation": "\u4e8b\u4ef6\u9a71\u52a8\u7f16\u7a0b\u4e2d\uff0c\u68c0\u67e5\u5019\u9009\u6267\u884c\u662f\u5426\u7b26\u5408\u8bed\u4e49\u662f\u4e00\u4e2a\u6838\u5fc3\u95ee\u9898\u3002\u672c\u6587\u65e8\u5728\u901a\u8fc7\u516c\u7406\u5316\u8bed\u4e49\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u57fa\u4e8e\u8ff9\u7684\u516c\u7406\u5316\u8bed\u4e49\uff0c\u8bc1\u660e\u5176\u4e0e\u64cd\u4f5c\u8bed\u4e49\u7684\u7b49\u4ef7\u6027\uff0c\u5c06\u4e00\u81f4\u6027\u95ee\u9898\u91cd\u65b0\u8868\u8ff0\u4e3a\u4e8b\u4ef6\u9a71\u52a8\u4e00\u81f4\u6027\u95ee\u9898\uff0c\u5206\u6790\u5176\u8ba1\u7b97\u590d\u6742\u6027\uff0c\u5e76\u8bc6\u522b\u51fa\u4e00\u4e2a\u53ef\u9ad8\u6548\u8ba1\u7b97\u7684\u7247\u6bb5\u3002", "result": "\u8bc1\u660e\u4e8b\u4ef6\u9a71\u52a8\u4e00\u81f4\u6027\u95ee\u9898\u4e3aNP\u5b8c\u5168\u95ee\u9898\uff0c\u4f46\u5728\u65e0\u5d4c\u5957\u53d1\u5e03\u65f6\u53ef\u5728\u591a\u9879\u5f0f\u65f6\u95f4\u5185\u89e3\u51b3\u3002\u901a\u8fc7\u539f\u578b\u5de5\u5177\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u672c\u6587\u4e3a\u4e8b\u4ef6\u9a71\u52a8\u7a0b\u5e8f\u63d0\u4f9b\u4e86\u4e00\u79cd\u516c\u7406\u5316\u8bed\u4e49\u65b9\u6cd5\uff0c\u89e3\u51b3\u4e86\u590d\u6742\u6027\u5e76\u8bc6\u522b\u51fa\u53ef\u884c\u7247\u6bb5\uff0c\u5b9e\u9a8c\u7ed3\u679c\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u5b9e\u7528\u6027\u3002"}}
{"id": "2508.08228", "categories": ["cs.GR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.08228", "abs": "https://arxiv.org/abs/2508.08228", "authors": ["Sining Lu", "Guan Chen", "Nam Anh Dinh", "Itai Lang", "Ari Holtzman", "Rana Hanocka"], "title": "LL3M: Large Language 3D Modelers", "comment": "Our project page is at https://threedle.github.io/ll3m", "summary": "We present LL3M, a multi-agent system that leverages pretrained large\nlanguage models (LLMs) to generate 3D assets by writing interpretable Python\ncode in Blender. We break away from the typical generative approach that learns\nfrom a collection of 3D data. Instead, we reformulate shape generation as a\ncode-writing task, enabling greater modularity, editability, and integration\nwith artist workflows. Given a text prompt, LL3M coordinates a team of\nspecialized LLM agents to plan, retrieve, write, debug, and refine Blender\nscripts that generate and edit geometry and appearance. The generated code\nworks as a high-level, interpretable, human-readable, well-documented\nrepresentation of scenes and objects, making full use of sophisticated Blender\nconstructs (e.g. B-meshes, geometry modifiers, shader nodes) for diverse,\nunconstrained shapes, materials, and scenes. This code presents many avenues\nfor further agent and human editing and experimentation via code tweaks or\nprocedural parameters. This medium naturally enables a co-creative loop in our\nsystem: agents can automatically self-critique using code and visuals, while\niterative user instructions provide an intuitive way to refine assets. A shared\ncode context across agents enables awareness of previous attempts, and a\nretrieval-augmented generation knowledge base built from Blender API\ndocumentation, BlenderRAG, equips agents with examples, types, and functions\nempowering advanced modeling operations and code correctness. We demonstrate\nthe effectiveness of LL3M across diverse shape categories, style and material\nedits, and user-driven refinements. Our experiments showcase the power of code\nas a generative and interpretable medium for 3D asset creation. Our project\npage is at https://threedle.github.io/ll3m.", "AI": {"tldr": "LL3M\u662f\u4e00\u4e2a\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\uff0c\u5229\u7528\u9884\u8bad\u7ec3\u7684\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u901a\u8fc7\u7f16\u5199\u53ef\u89e3\u91ca\u7684Python\u4ee3\u7801\u5728Blender\u4e2d\u751f\u62103D\u8d44\u4ea7\uff0c\u6539\u8fdb\u4e86\u4f20\u7edf\u57fa\u4e8e\u6570\u636e\u5b66\u4e60\u7684\u751f\u6210\u65b9\u6cd5\uff0c\u63d0\u4f9b\u66f4\u9ad8\u7684\u6a21\u5757\u5316\u3001\u53ef\u7f16\u8f91\u6027\u548c\u827a\u672f\u5de5\u4f5c\u6d41\u96c6\u6210\u3002", "motivation": "\u4f20\u7edf\u76843D\u8d44\u4ea7\u751f\u6210\u65b9\u6cd5\u901a\u5e38\u4f9d\u8d56\u5927\u91cf3D\u6570\u636e\u5b66\u4e60\uff0c\u7f3a\u4e4f\u6a21\u5757\u5316\u548c\u7f16\u8f91\u6027\u3002LL3M\u65e8\u5728\u901a\u8fc7\u5c06\u5f62\u72b6\u751f\u6210\u91cd\u65b0\u5b9a\u4e49\u4e3a\u4ee3\u7801\u7f16\u5199\u4efb\u52a1\uff0c\u63d0\u4f9b\u66f4\u7075\u6d3b\u3001\u53ef\u89e3\u91ca\u4e14\u6613\u4e8e\u96c6\u6210\u7684\u5de5\u4f5c\u6d41\u3002", "method": "LL3M\u901a\u8fc7\u534f\u8c03\u591a\u4e2a\u4e13\u95e8\u7684LLM\u667a\u80fd\u4f53\u6765\u89c4\u5212\u3001\u68c0\u7d22\u3001\u7f16\u5199\u3001\u8c03\u8bd5\u548c\u4f18\u5316Blender\u811a\u672c\uff0c\u751f\u6210\u548c\u7f16\u8f91\u51e0\u4f55\u4e0e\u5916\u89c2\u3002\u7ed3\u5408Blender API\u6587\u6863\u77e5\u8bc6\u5e93\uff08BlenderRAG\uff09\uff0c\u667a\u80fd\u4f53\u80fd\u591f\u6267\u884c\u9ad8\u7ea7\u5efa\u6a21\u64cd\u4f5c\u5e76\u786e\u4fdd\u4ee3\u7801\u6b63\u786e\u6027\u3002", "result": "LL3M\u5728\u591a\u79cd\u5f62\u72b6\u7c7b\u522b\u3001\u98ce\u683c\u548c\u6750\u8d28\u7f16\u8f91\u4ee5\u53ca\u7528\u6237\u9a71\u52a8\u7684\u7ec6\u5316\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u5c55\u793a\u4e86\u4ee3\u7801\u4f5c\u4e3a\u751f\u6210\u548c\u89e3\u91ca3D\u8d44\u4ea7\u5a92\u4ecb\u7684\u5f3a\u5927\u80fd\u529b\u3002", "conclusion": "LL3M\u901a\u8fc7\u4ee3\u7801\u751f\u6210\u7684\u65b9\u5f0f\uff0c\u4e3a3D\u8d44\u4ea7\u521b\u4f5c\u63d0\u4f9b\u4e86\u4e00\u79cd\u6a21\u5757\u5316\u3001\u53ef\u7f16\u8f91\u4e14\u53ef\u89e3\u91ca\u7684\u65b0\u65b9\u6cd5\uff0c\u652f\u6301\u667a\u80fd\u4f53\u548c\u7528\u6237\u7684\u534f\u540c\u521b\u4f5c\uff0c\u63a8\u52a8\u4e863D\u751f\u6210\u6280\u672f\u7684\u53d1\u5c55\u3002"}}
{"id": "2508.06562", "categories": ["cs.GT", "cs.DS", "econ.TH"], "pdf": "https://arxiv.org/pdf/2508.06562", "abs": "https://arxiv.org/abs/2508.06562", "authors": ["Mohammad T. Hajiaghayi", "Suho Shin"], "title": "Algorithmic Delegated Choice: An Annotated Reading List", "comment": "SIGecom Exchanges, Vol 23, No. 1, July 2025, Pages 80-85", "summary": "The problem of delegated choice has been of long interest in economics and\nrecently on computer science. We overview a list of papers on delegated choice\nproblem, from classic works to recent papers with algorithmic perspectives.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7efc\u8ff0\u4e86\u59d4\u6258\u9009\u62e9\u95ee\u9898\u7684\u7ecf\u5178\u7ecf\u6d4e\u5b66\u7814\u7a76\u53ca\u8fd1\u671f\u8ba1\u7b97\u673a\u79d1\u5b66\u7684\u7b97\u6cd5\u89c6\u89d2\u3002", "motivation": "\u59d4\u6258\u9009\u62e9\u95ee\u9898\u957f\u671f\u4ee5\u6765\u5728\u7ecf\u6d4e\u5b66\u4e2d\u5177\u6709\u91cd\u8981\u610f\u4e49\uff0c\u6700\u8fd1\u5728\u8ba1\u7b97\u673a\u79d1\u5b66\u9886\u57df\u4e5f\u53d7\u5230\u5173\u6ce8\uff0c\u9700\u8981\u5bf9\u5176\u7814\u7a76\u8fdb\u884c\u5168\u9762\u56de\u987e\u3002", "method": "\u8bba\u6587\u901a\u8fc7\u7efc\u8ff0\u7ecf\u5178\u5de5\u4f5c\u548c\u8fd1\u671f\u7b97\u6cd5\u89c6\u89d2\u7684\u6587\u732e\uff0c\u5bf9\u59d4\u6258\u9009\u62e9\u95ee\u9898\u8fdb\u884c\u4e86\u7cfb\u7edf\u6027\u7684\u68b3\u7406\u3002", "result": "\u8bba\u6587\u6982\u8ff0\u4e86\u59d4\u6258\u9009\u62e9\u95ee\u9898\u7684\u7814\u7a76\u8fdb\u5c55\uff0c\u6db5\u76d6\u4e86\u4ece\u7ecf\u6d4e\u5b66\u5230\u8ba1\u7b97\u673a\u79d1\u5b66\u7684\u8de8\u5b66\u79d1\u89c6\u89d2\u3002", "conclusion": "\u59d4\u6258\u9009\u62e9\u95ee\u9898\u662f\u4e00\u4e2a\u8de8\u5b66\u79d1\u7684\u7814\u7a76\u4e3b\u9898\uff0c\u672a\u6765\u7684\u7814\u7a76\u53ef\u4ee5\u8fdb\u4e00\u6b65\u7ed3\u5408\u7ecf\u6d4e\u5b66\u548c\u8ba1\u7b97\u673a\u79d1\u5b66\u7684\u4f18\u52bf\u3002"}}
{"id": "2508.06619", "categories": ["cs.GT", "cs.MA", "cs.SI", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2508.06619", "abs": "https://arxiv.org/abs/2508.06619", "authors": ["Kiran Rokade", "Adit Jain", "Francesca Parise", "Vikram Krishnamurthy", "Eva Tardos"], "title": "Asymmetric Network Games: $\u03b1$-Potential Function and Learning", "comment": null, "summary": "In a network game, players interact over a network and the utility of each\nplayer depends on his own action and on an aggregate of his neighbours'\nactions. Many real world networks of interest are asymmetric and involve a\nlarge number of heterogeneous players. This paper analyzes static network games\nusing the framework of $\\alpha$-potential games. Under mild assumptions on the\naction sets (compact intervals) and the utility functions (twice continuously\ndifferentiable) of the players, we derive an expression for an inexact\npotential function of the game, called the $\\alpha$-potential function. Using\nsuch a function, we show that modified versions of the sequential best-response\nalgorithm and the simultaneous gradient play algorithm achieve convergence of\nplayers' actions to a $2\\alpha$-Nash equilibrium. For linear-quadratic network\ngames, we show that $\\alpha$ depends on the maximum asymmetry in the network\nand is well-behaved for a wide range of networks of practical interest.\nFurther, we derive bounds on the social welfare of the $\\alpha$-Nash\nequilibrium corresponding to the maximum of the $\\alpha$-potential function,\nunder suitable assumptions. We numerically illustrate the convergence of the\nproposed algorithms and properties of the learned $2\\alpha$-Nash equilibria.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7814\u7a76\u4e86\u7f51\u7edc\u6e38\u620f\u4e2d\u7684\u9759\u6001\u535a\u5f08\uff0c\u5229\u7528\u03b1-\u52bf\u51fd\u6570\u6846\u67b6\uff0c\u63d0\u51fa\u4e86\u73a9\u5bb6\u52a8\u4f5c\u6536\u655b\u52302\u03b1-\u7eb3\u4ec0\u5747\u8861\u7684\u7b97\u6cd5\uff0c\u5e76\u5206\u6790\u4e86\u7f51\u7edc\u4e0d\u5bf9\u79f0\u6027\u5bf9\u03b1\u7684\u5f71\u54cd\u53ca\u793e\u4f1a\u798f\u7949\u7684\u754c\u9650\u3002", "motivation": "\u73b0\u5b9e\u4e16\u754c\u4e2d\u7684\u7f51\u7edc\u535a\u5f08\u901a\u5e38\u662f\u4e0d\u5bf9\u79f0\u7684\u4e14\u6d89\u53ca\u5927\u91cf\u5f02\u8d28\u73a9\u5bb6\uff0c\u9700\u8981\u4e00\u79cd\u80fd\u591f\u5904\u7406\u8fd9\u4e9b\u7279\u6027\u7684\u5206\u6790\u6846\u67b6\u3002", "method": "\u4f7f\u7528\u03b1-\u52bf\u51fd\u6570\u6846\u67b6\uff0c\u5047\u8bbe\u884c\u52a8\u96c6\u4e3a\u7d27\u533a\u95f4\u4e14\u6548\u7528\u51fd\u6570\u4e8c\u6b21\u8fde\u7eed\u53ef\u5fae\uff0c\u63a8\u5bfc\u4e86\u03b1-\u52bf\u51fd\u6570\u7684\u8868\u8fbe\u5f0f\uff0c\u5e76\u63d0\u51fa\u4e86\u987a\u5e8f\u6700\u4f73\u54cd\u5e94\u7b97\u6cd5\u548c\u540c\u6b65\u68af\u5ea6\u7b97\u6cd5\u7684\u6539\u8fdb\u7248\u672c\u3002", "result": "\u5728\u4e00\u5b9a\u7684\u5047\u8bbe\u4e0b\uff0c\u6539\u8fdb\u7684\u7b97\u6cd5\u80fd\u591f\u4f7f\u73a9\u5bb6\u52a8\u4f5c\u6536\u655b\u52302\u03b1-\u7eb3\u4ec0\u5747\u8861\uff0c\u4e14\u03b1\u4e0e\u7f51\u7edc\u7684\u6700\u5927\u4e0d\u5bf9\u79f0\u6027\u76f8\u5173\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u7f51\u7edc\u535a\u5f08\u4e2d\u73a9\u5bb6\u884c\u4e3a\u6536\u655b\u548c\u793e\u4f1a\u798f\u7949\u63d0\u4f9b\u4e86\u7406\u8bba\u652f\u6301\uff0c\u5e76\u901a\u8fc7\u6570\u503c\u6a21\u62df\u9a8c\u8bc1\u4e86\u7b97\u6cd5\u7684\u6709\u6548\u6027\u548c\u5747\u8861\u6027\u8d28\u3002"}}
{"id": "2508.06661", "categories": ["cs.GT"], "pdf": "https://arxiv.org/pdf/2508.06661", "abs": "https://arxiv.org/abs/2508.06661", "authors": ["Keith Badger", "Marek Petrik", "Jefferson Huang"], "title": "Convergence of Fast Policy Iteration in Markov Games and Robust MDPs", "comment": null, "summary": "Markov games and robust MDPs are closely related models that involve\ncomputing a pair of saddle point policies. As part of the long-standing effort\nto develop efficient algorithms for these models, the Filar-Tolwinski (FT)\nalgorithm has shown considerable promise. As our first contribution, we\ndemonstrate that FT may fail to converge to a saddle point and may loop\nindefinitely, even in small games. This observation contradicts the proof of\nFT's convergence to a saddle point in the original paper. As our second\ncontribution, we propose Residual Conditioned Policy Iteration (RCPI). RCPI\nbuilds on FT, but is guaranteed to converge to a saddle point. Our numerical\nresults show that RCPI outperforms other convergent algorithms by several\norders of magnitude.", "AI": {"tldr": "\u7814\u7a76\u4e86\u9a6c\u5c14\u79d1\u592b\u6e38\u620f\u548c\u9c81\u68d2MDP\u4e2d\u7684Filar-Tolwinski\uff08FT\uff09\u7b97\u6cd5\uff0c\u53d1\u73b0\u5176\u53ef\u80fd\u65e0\u6cd5\u6536\u655b\u5230\u978d\u70b9\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u7b97\u6cd5RCPI\uff0c\u4fdd\u8bc1\u6536\u655b\u4e14\u6027\u80fd\u4f18\u8d8a\u3002", "motivation": "\u4e3a\u4e86\u89e3\u51b3\u9a6c\u5c14\u79d1\u592b\u6e38\u620f\u548c\u9c81\u68d2MDP\u4e2dFT\u7b97\u6cd5\u53ef\u80fd\u65e0\u6cd5\u6536\u655b\u7684\u95ee\u9898\uff0c\u63d0\u51fa\u4e86\u66f4\u53ef\u9760\u7684\u7b97\u6cd5RCPI\u3002", "method": "\u63d0\u51fa\u4e86Residual Conditioned Policy Iteration\uff08RCPI\uff09\u7b97\u6cd5\uff0c\u57fa\u4e8eFT\u7b97\u6cd5\u6539\u8fdb\uff0c\u5e76\u4fdd\u8bc1\u6536\u655b\u5230\u978d\u70b9\u3002", "result": "\u6570\u503c\u5b9e\u9a8c\u8868\u660e\uff0cRCPI\u7684\u6027\u80fd\u663e\u8457\u4f18\u4e8e\u5176\u4ed6\u6536\u655b\u7b97\u6cd5\uff0c\u5feb\u4e86\u51e0\u4e2a\u6570\u91cf\u7ea7\u3002", "conclusion": "RCPI\u4f5c\u4e3a\u4e00\u79cd\u6539\u8fdb\u7b97\u6cd5\uff0c\u89e3\u51b3\u4e86FT\u7684\u4e0d\u6536\u655b\u95ee\u9898\uff0c\u5e76\u5728\u6027\u80fd\u4e0a\u53d6\u5f97\u4e86\u663e\u8457\u63d0\u5347\u3002"}}
{"id": "2508.06702", "categories": ["cs.GT"], "pdf": "https://arxiv.org/pdf/2508.06702", "abs": "https://arxiv.org/abs/2508.06702", "authors": ["Zhao Song", "The Anh Han"], "title": "Emergence of Cooperation and Commitment in Optional Prisoner's Dilemma", "comment": null, "summary": "Commitment is a well-established mechanism for fostering cooperation in human\nsociety and multi-agent systems. However, existing research has predominantly\nfocused on the commitment that neglects the freedom of players to abstain from\nan interaction, limiting their applicability to many real-world scenarios where\nparticipation is often voluntary. In this paper, we present a two-stage game\nmodel to investigate the evolution of commitment-based behaviours and\ncooperation within the framework of the optional Prisoner's Dilemma game. In\nthe pre-game stage, players decide whether to accept a mutual commitment. Once\nin the game, they choose among cooperation, defection, or exiting, depending on\nthe formation of a pre-game commitment. We find that optional participation\nboosts commitment acceptance but fails to foster cooperation, leading instead\nto widespread exit behaviour. To address this, we then introduce and compare\ntwo institutional incentive approaches: i) a strict one (STRICT-COM) that\nrewards only committed players who cooperate in the game, and ii) a flexible\none (FLEXIBLE-COM) that rewards any committed players who do not defect in the\ngame. The results reveal that, while the strict approach is demonstrably better\nfor promoting cooperation as the flexible rule creates a loophole for an\nopportunistic exit after committing, the flexible rule offers an efficient\nalternative for enhancing social welfare when such opportunistic behaviour\nresults in a high gain. This study highlights the limitations of relying solely\non voluntary participation and commitment to resolving social dilemmas,\nemphasising the importance of well-designed institutional incentives to promote\ncooperation and social welfare effectively.", "AI": {"tldr": "\u8be5\u8bba\u6587\u901a\u8fc7\u4e24\u9636\u6bb5\u535a\u5f08\u6a21\u578b\u7814\u7a76\u4e86\u5728\u53ef\u9009\u56da\u5f92\u56f0\u5883\u6e38\u620f\u4e2d\u57fa\u4e8e\u627f\u8bfa\u7684\u884c\u4e3a\u4e0e\u5408\u4f5c\u7684\u6f14\u53d8\uff0c\u53d1\u73b0\u81ea\u613f\u53c2\u4e0e\u63d0\u5347\u4e86\u627f\u8bfa\u63a5\u53d7\u5ea6\u4f46\u672a\u80fd\u4fc3\u8fdb\u5408\u4f5c\uff0c\u4e14\u4e25\u683c\u7684\u5236\u5ea6\u6fc0\u52b1\u66f4\u6709\u6548\u3002", "motivation": "\u73b0\u6709\u7814\u7a76\u5ffd\u89c6\u4e86\u53c2\u4e0e\u8005\u81ea\u613f\u9009\u62e9\u4e0d\u53c2\u4e0e\u7684\u81ea\u7531\uff0c\u9650\u5236\u4e86\u5176\u5728\u73b0\u5b9e\u573a\u666f\u4e2d\u7684\u5e94\u7528\u3002\u8bba\u6587\u65e8\u5728\u63a2\u8ba8\u81ea\u613f\u53c2\u4e0e\u4e0b\u627f\u8bfa\u884c\u4e3a\u5bf9\u5408\u4f5c\u7684\u5f71\u54cd\u3002", "method": "\u91c7\u7528\u4e86\u4e24\u9636\u6bb5\u535a\u5f08\u6a21\u578b\uff0c\u7b2c\u4e00\u9636\u6bb5\u51b3\u5b9a\u662f\u5426\u63a5\u53d7\u627f\u8bfa\uff0c\u7b2c\u4e8c\u9636\u6bb5\u6839\u636e\u627f\u8bfa\u60c5\u51b5\u9009\u62e9\u5408\u4f5c\u3001\u80cc\u53db\u6216\u9000\u51fa\uff0c\u5e76\u6bd4\u8f83\u4e86\u4e24\u79cd\u5236\u5ea6\u6fc0\u52b1\u65b9\u6cd5\uff08STRICT-COM\u548cFLEXIBLE-COM\uff09\u7684\u6548\u679c\u3002", "result": "\u7814\u7a76\u53d1\u73b0\u81ea\u613f\u53c2\u4e0e\u63d0\u9ad8\u4e86\u627f\u8bfa\u63a5\u53d7\u5ea6\u4f46\u672a\u4fc3\u8fdb\u5408\u4f5c\uff0c\u4e25\u683c\u7684\u5236\u5ea6\u6fc0\u52b1\u66f4\u6709\u5229\u4e8e\u5408\u4f5c\uff0c\u800c\u7075\u6d3b\u7684\u6fc0\u52b1\u5728\u7279\u5b9a\u573a\u666f\u4e0b\u80fd\u63d0\u5347\u793e\u4f1a\u798f\u5229\u3002", "conclusion": "\u7814\u7a76\u63ed\u793a\u4e86\u5355\u7eaf\u4f9d\u8d56\u81ea\u613f\u53c2\u4e0e\u548c\u627f\u8bfa\u65e0\u6cd5\u6709\u6548\u89e3\u51b3\u793e\u4f1a\u56f0\u5883\uff0c\u5f3a\u8c03\u4e86\u8bbe\u8ba1\u826f\u597d\u7684\u5236\u5ea6\u6fc0\u52b1\u5bf9\u4fc3\u8fdb\u5408\u4f5c\u548c\u793e\u4f1a\u798f\u5229\u7684\u91cd\u8981\u6027\u3002"}}
{"id": "2508.07145", "categories": ["cs.GT"], "pdf": "https://arxiv.org/pdf/2508.07145", "abs": "https://arxiv.org/abs/2508.07145", "authors": ["Ivan Geffner", "Erez Karpas", "Moshe Tennenholtz"], "title": "When Competition Helps: Achieving Optimal Traffic Flow with Multiple Autonomous Planners", "comment": null, "summary": "The inefficiency of selfish routing in congested networks is a classical\nproblem in algorithmic game theory, often captured by the Price of Anarchy\n(i.e., the ratio between the social cost of decentralized decisions and that of\na centrally optimized solution.) With the advent of autonomous vehicles,\ncapable of receiving and executing centrally assigned routes, it is natural to\nask whether their deployment can eliminate this inefficiency. At first glance,\na central authority could simply compute an optimal traffic assignment and\ninstruct each vehicle to follow its assigned path. However, this vision\noverlooks critical challenges: routes must be individually rational (no vehicle\nhas an incentive to deviate), and in practice, multiple planning agents (e.g.,\ndifferent companies) may coexist and compete. Surprisingly, we show that such\ncompetition is not merely an obstacle but a necessary ingredient for achieving\noptimal outcomes. In this work, we design a routing mechanism that embraces\ncompetition and converges to an optimal assignment, starting from the classical\nPigou network as a foundational case.", "AI": {"tldr": "\u81ea\u79c1\u8def\u7531\u5728\u62e5\u5835\u7f51\u7edc\u4e2d\u7684\u4f4e\u6548\u662f\u7b97\u6cd5\u535a\u5f08\u8bba\u4e2d\u7684\u7ecf\u5178\u95ee\u9898\uff0c\u672c\u6587\u7814\u7a76\u901a\u8fc7\u81ea\u52a8\u9a7e\u9a76\u8f66\u8f86\u7684\u96c6\u4e2d\u5206\u914d\u8def\u7ebf\u662f\u5426\u89e3\u51b3\u8fd9\u4e00\u4f4e\u6548\uff0c\u5e76\u53d1\u73b0\u7ade\u4e89\u662f\u5b9e\u73b0\u6700\u4f18\u7ed3\u679c\u7684\u5fc5\u8981\u56e0\u7d20\u3002", "motivation": "\u7814\u7a76\u81ea\u52a8\u9a7e\u9a76\u8f66\u8f86\u80fd\u5426\u901a\u8fc7\u96c6\u4e2d\u5206\u914d\u8def\u7ebf\u6d88\u9664\u81ea\u79c1\u8def\u7531\u7684\u4f4e\u6548\uff0c\u540c\u65f6\u8003\u8651\u5230\u4e2a\u4f53\u6fc0\u52b1\u548c\u5b9e\u8df5\u4e2d\u7684\u591a\u4ee3\u7406\u7ade\u4e89\u3002", "method": "\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u63a5\u53d7\u7ade\u4e89\u7684\u8def\u7531\u673a\u5236\uff0c\u4ece\u7ecf\u5178\u7684Pigou\u7f51\u7edc\u4e3a\u57fa\u7840\u6848\u4f8b\u51fa\u53d1\uff0c\u5f15\u5bfc\u8def\u7ebf\u5206\u914d\u6536\u655b\u5230\u6700\u4f18\u72b6\u6001\u3002", "result": "\u7814\u7a76\u53d1\u73b0\u7ade\u4e89\u662f\u5b9e\u73b0\u6700\u4f18\u4ea4\u901a\u5206\u914d\u7684\u5fc5\u8981\u6761\u4ef6\uff0c\u8bbe\u8ba1\u7684\u673a\u5236\u80fd\u591f\u6536\u655b\u5230\u6700\u4f18\u5206\u914d\u72b6\u6001\u3002", "conclusion": "\u7ade\u4e89\u4e0d\u4ec5\u4e0d\u662f\u969c\u788d\uff0c\u53cd\u800c\u662f\u5b9e\u73b0\u4ea4\u901a\u5206\u914d\u6700\u4f18\u5316\u7684\u5173\u952e\u56e0\u7d20\uff1b\u901a\u8fc7\u8bbe\u8ba1\u5408\u9002\u7684\u673a\u5236\uff0c\u53ef\u4ee5\u514b\u670d\u81ea\u79c1\u8def\u7531\u7684\u4f4e\u6548\u95ee\u9898\u3002"}}
{"id": "2508.07147", "categories": ["cs.GT", "econ.TH"], "pdf": "https://arxiv.org/pdf/2508.07147", "abs": "https://arxiv.org/abs/2508.07147", "authors": ["Ivan Geffner", "Caspar Oesterheld", "Vincent Conitzer"], "title": "Maximizing Social Welfare with Side Payments", "comment": null, "summary": "We examine normal-form games in which players may \\emph{pre-commit} to\noutcome-contingent transfers before choosing their actions. In the one-shot\nversion of this model, Jackson and Wilkie showed that side contracting can\nbackfire: even a game with a Pareto-optimal Nash equilibrium can devolve into\ninefficient equilibria once unbounded, simultaneous commitments are allowed.\nThe root cause is a prisoner's dilemma effect, where each player can exploit\nher commitment power to reshape the equilibrium in her favor, harming overall\nwelfare.\n  To circumvent this problem we introduce a \\emph{staged-commitment} protocol.\nPlayers may pledge transfers only in small, capped increments over multiple\nrounds, and the phase continues only with unanimous consent. We prove that,\nstarting from any finite game $\\Gamma$ with a non-degenerate Nash equilibrium\n$\\vec{\\sigma}$, this protocol implements every welfare-maximizing payoff\nprofile that \\emph{strictly} Pareto-improves $\\vec{\\sigma}$. Thus, gradual and\nbounded commitments restore the full efficiency potential of side payments\nwhile avoiding the inefficiencies identified by Jackson and Wilkie.", "AI": {"tldr": "\u7814\u7a76\u4e86\u4e00\u79cd\u5206\u9636\u6bb5\u627f\u8bfa\u534f\u8bae\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u4e00\u6b21\u6027\u627f\u8bfa\u535a\u5f08\u4e2d\u56e0\u56da\u5f92\u56f0\u5883\u6548\u5e94\u5bfc\u81f4\u7684\u6548\u7387\u4f4e\u4e0b\u95ee\u9898\uff0c\u6062\u590d\u4e86\u4fa7\u652f\u4ed8\u7684\u6548\u7387\u6f5c\u529b\u3002", "motivation": "\u4f20\u7edf\u7684\u535a\u5f08\u6a21\u578b\u4e2d\uff0c\u4e00\u6b21\u6027\u65e0\u9650\u5236\u7684\u627f\u8bfa\u53ef\u80fd\u5bfc\u81f4\u6548\u7387\u4f4e\u4e0b\u7684\u5747\u8861\uff0c\u5373\u4f7f\u5b58\u5728\u5e15\u7d2f\u6258\u6700\u4f18\u7684\u7eb3\u4ec0\u5747\u8861\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u5206\u9636\u6bb5\u627f\u8bfa\u534f\u8bae\uff0c\u5141\u8bb8\u73a9\u5bb6\u5728\u591a\u8f6e\u4e2d\u4ee5\u5c0f\u989d\u3001\u6709\u9650\u7684\u589e\u91cf\u627f\u8bfa\u8f6c\u79fb\uff0c\u4e14\u6bcf\u8f6e\u9700\u4e00\u81f4\u540c\u610f\u624d\u80fd\u7ee7\u7eed\u3002", "result": "\u8be5\u534f\u8bae\u5b9e\u73b0\u4e86\u4ece\u4efb\u4f55\u6709\u9650\u535a\u5f08\u51fa\u53d1\uff0c\u4e25\u683c\u5e15\u7d2f\u6258\u6539\u8fdb\u975e\u9000\u5316\u7eb3\u4ec0\u5747\u8861\u7684\u798f\u5229\u6700\u5927\u5316\u652f\u4ed8\u914d\u7f6e\u3002", "conclusion": "\u5206\u9636\u6bb5\u4e14\u6709\u9650\u7684\u627f\u8bfa\u80fd\u591f\u907f\u514d\u4f20\u7edf\u6a21\u578b\u4e2d\u7684\u6548\u7387\u4f4e\u4e0b\u95ee\u9898\uff0c\u6062\u590d\u4e86\u4fa7\u652f\u4ed8\u7684\u6548\u7387\u6f5c\u529b\u3002"}}
{"id": "2508.07699", "categories": ["cs.GT"], "pdf": "https://arxiv.org/pdf/2508.07699", "abs": "https://arxiv.org/abs/2508.07699", "authors": ["Hang Ren", "Xiaozhen Sun", "Tianzi Ma", "Jiajia Zhang", "Xuan Wang"], "title": "Last-Iterate Convergence in Adaptive Regret Minimization for Approximate Extensive-Form Perfect Equilibrium", "comment": null, "summary": "The Nash Equilibrium (NE) assumes rational play in imperfect-information\nExtensive-Form Games (EFGs) but fails to ensure optimal strategies for\noff-equilibrium branches of the game tree, potentially leading to suboptimal\noutcomes in practical settings. To address this, the Extensive-Form Perfect\nEquilibrium (EFPE), a refinement of NE, introduces controlled perturbations to\nmodel potential player errors. However, existing EFPE-finding algorithms, which\ntypically rely on average strategy convergence and fixed perturbations, face\nsignificant limitations: computing average strategies incurs high computational\ncosts and approximation errors, while fixed perturbations create a trade-off\nbetween NE approximation accuracy and the convergence rate of NE refinements.\n  To tackle these challenges, we propose an efficient adaptive regret\nminimization algorithm for computing approximate EFPE, achieving last-iterate\nconvergence in two-player zero-sum EFGs. Our approach introduces Reward\nTransformation Counterfactual Regret Minimization (RTCFR) to solve perturbed\ngames and defines a novel metric, the Information Set Nash Equilibrium (ISNE),\nto dynamically adjust perturbations. Theoretical analysis confirms convergence\nto EFPE, and experimental results demonstrate that our method significantly\noutperforms state-of-the-art algorithms in both NE and EFPE-finding tasks.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u9ad8\u6548\u7684\u9002\u5e94\u6027\u9057\u61be\u6700\u5c0f\u5316\u7b97\u6cd5\uff0c\u7528\u4e8e\u8ba1\u7b97\u8fd1\u4f3cEFPE\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u5728\u8ba1\u7b97\u6210\u672c\u3001\u8fd1\u4f3c\u8bef\u5dee\u548c\u6536\u655b\u901f\u5ea6\u4e0a\u7684\u95ee\u9898\u3002", "motivation": "\u73b0\u6709EFPE\u7b97\u6cd5\u5b58\u5728\u8ba1\u7b97\u6210\u672c\u9ad8\u3001\u8fd1\u4f3c\u8bef\u5dee\u5927\u548c\u56fa\u5b9a\u6270\u52a8\u5bfc\u81f4\u7684\u6536\u655b\u95ee\u9898\uff0c\u9700\u8981\u4e00\u79cd\u66f4\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u63d0\u51fa\u4e86Reward Transformation Counterfactual Regret Minimization (RTCFR)\u7b97\u6cd5\u548c\u52a8\u6001\u8c03\u6574\u6270\u52a8\u7684\u65b0\u5ea6\u91cfISNE\u3002", "result": "\u7406\u8bba\u5206\u6790\u786e\u8ba4\u4e86EFPE\u7684\u6536\u655b\u6027\uff0c\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\u8be5\u65b9\u6cd5\u5728NE\u548cEFPE\u67e5\u627e\u4efb\u52a1\u4e2d\u4f18\u4e8e\u73b0\u6709\u7b97\u6cd5\u3002", "conclusion": "RTCFR\u7b97\u6cd5\u548cISNE\u5ea6\u91cf\u7684\u7ed3\u5408\u663e\u8457\u63d0\u5347\u4e86EFPE\u7684\u8ba1\u7b97\u6548\u7387\u548c\u7cbe\u5ea6\u3002"}}
{"id": "2508.08036", "categories": ["cs.GT"], "pdf": "https://arxiv.org/pdf/2508.08036", "abs": "https://arxiv.org/abs/2508.08036", "authors": ["Xiaojia Han", "Wenjing Liu", "Qizhi Fang"], "title": "Truthful Two-Obnoxious-Facility Location Games with Optional Preferences and Minimum Distance Constraint", "comment": null, "summary": "In this paper, we study a truthful two-obnoxious-facility location problem,\nin which each agent has a private location in [0, 1] and a public optional\npreference over two obnoxious facilities, and there is a minimum distance\nconstraint d between the two facilities. Each agent wants to be as far away as\npossible from the facilities that affect her, and the utility of each agent is\nthe total distance from her to these facilities. The goal is to decide how to\nplace the facilities in [0, 1] so as to incentivize agents to report their\nprivate locations truthfully as well as maximize the social utility. First, we\nconsider the special setting where d = 0, that is, the two facilities can be\nlocated at any point in [0, 1]. We propose a deterministic strategyproof\nmechanism with approximation ratio of at most 4 and a randomized strategyproof\nmechanism with approximation ratio of at most 2, respectively. Then we study\nthe general setting. We propose a deterministic strategyproof mechanism with\napproximation ratio of at most 8 and a randomized strategyproof mechanism with\napproximation ratio of at most 4, respectively. Furthermore, we provide lower\nbounds of 2 and 14/13 on the approximation ratio for any deterministic and any\nrandomized strategyproof mechanism, respectively.", "AI": {"tldr": "\u7814\u7a76\u4e86\u771f\u5b9e\u7684\u4e24\u8ba8\u538c\u8bbe\u65bd\u9009\u5740\u95ee\u9898\uff0c\u63d0\u51fa\u4e86\u4e24\u79cd\u673a\u5236\uff08\u786e\u5b9a\u6027\u548c\u968f\u673a\u6027\uff09\u4ee5\u6fc0\u52b1\u4ee3\u7406\u4eba\u771f\u5b9e\u62a5\u544a\u4f4d\u7f6e\u5e76\u6700\u5927\u5316\u793e\u4f1a\u6548\u7528\u3002", "motivation": "\u89e3\u51b3\u4ee3\u7406\u4eba\u9700\u8981\u5c3d\u53ef\u80fd\u8fdc\u79bb\u5f71\u54cd\u5176\u7684\u8bbe\u65bd\u7684\u95ee\u9898\uff0c\u5e76\u786e\u4fdd\u4ee3\u7406\u4eba\u771f\u5b9e\u62a5\u544a\u5176\u4f4d\u7f6e\u3002", "method": "\u5206\u522b\u7814\u7a76\u4e86\u65e0\u8ddd\u79bb\u7ea6\u675f\uff08d=0\uff09\u548c\u4e00\u822c\u60c5\u51b5\uff08d>0\uff09\uff0c\u63d0\u51fa\u4e86\u786e\u5b9a\u6027\u548c\u968f\u673a\u6027\u7b56\u7565\u673a\u5236\u3002", "result": "\u5728d=0\u60c5\u51b5\u4e0b\uff0c\u786e\u5b9a\u6027\u673a\u5236\u8fd1\u4f3c\u6bd4\u4e3a4\uff0c\u968f\u673a\u6027\u4e3a2\uff1b\u5728\u4e00\u822c\u60c5\u51b5\u4e0b\uff0c\u786e\u5b9a\u6027\u4e3a8\uff0c\u968f\u673a\u6027\u4e3a4\u3002\u5e76\u7ed9\u51fa\u4e86\u4e0b\u754c\u3002", "conclusion": "\u63d0\u51fa\u4e86\u9ad8\u6548\u4e14\u6fc0\u52b1\u76f8\u5bb9\u7684\u673a\u5236\u6765\u89e3\u51b3\u4e24\u8ba8\u538c\u8bbe\u65bd\u9009\u5740\u95ee\u9898\u3002"}}
{"id": "2508.08045", "categories": ["cs.GT"], "pdf": "https://arxiv.org/pdf/2508.08045", "abs": "https://arxiv.org/abs/2508.08045", "authors": ["Xinru Xu", "Wenjing Liu", "Qizhi Fang"], "title": "Constrained Distributed Heterogeneous Two-Facility Location Problems with Max-Variant Cost", "comment": null, "summary": "We study a constrained distributed heterogeneous two-facility location\nproblem, where a set of agents with private locations on the real line are\ndivided into disjoint groups. The constraint means that the facilities can only\nbe built in a given multiset of candidate locations and at most one facility\ncan be built at each candidate location. Given the locations of the two\nfacilities, the cost of an agent is the distance from her location to the\nfarthest facility (referred to as max-variant). Our goal is to design\nstrategyproof distributed mechanisms that can incentivize all agents to\ntruthfully report their locations and approximately optimize some social\nobjective. A distributed mechanism consists of two steps: for each group, the\nmechanism chooses two candidate locations as the representatives of the group\nbased only on the locations reported by agents therein; then, it outputs two\nfacility locations among all the representatives. We focus on a class of\ndeterministic strategyproof distributed mechanisms and analyze upper and lower\nbounds on the distortion under the Average-of-Average cost (average of the\naverage individual cost of agents in each group), the Max-of-Max cost (maximum\nindividual cost among all agents), the Average-of-Max cost (average of the\nmaximum individual cost among all agents in each group) and the Max-of-Average\ncost (maximum of the average individual cost of all agents in each group).\nUnder four social objectives, we obtain constant upper and lower distortion\nbounds.", "AI": {"tldr": "\u7814\u7a76\u53d7\u9650\u5206\u5e03\u5f0f\u5f02\u6784\u53cc\u8bbe\u65bd\u9009\u5740\u95ee\u9898\uff0c\u8bbe\u8ba1\u7b56\u7565\u8bc1\u660e\u7684\u5206\u5e03\u5f0f\u673a\u5236\u4ee5\u6fc0\u52b1\u4ee3\u7406\u771f\u5b9e\u62a5\u544a\u4f4d\u7f6e\uff0c\u5e76\u8fd1\u4f3c\u4f18\u5316\u591a\u79cd\u793e\u4f1a\u76ee\u6807\uff0c\u83b7\u5f97\u6052\u5b9a\u7684\u5931\u771f\u4e0a\u4e0b\u754c\u3002", "motivation": "\u5728\u5206\u5e03\u5f0f\u5f02\u6784\u73af\u5883\u4e2d\uff0c\u4ee3\u7406\u4f4d\u7f6e\u4fe1\u606f\u79c1\u5bc6\u4e14\u5206\u7ec4\u7684\u60c5\u51b5\u4e0b\uff0c\u5982\u4f55\u8bbe\u8ba1\u7b56\u7565\u8bc1\u660e\u673a\u5236\u4ee5\u771f\u5b9e\u83b7\u53d6\u4ee3\u7406\u4f4d\u7f6e\u5e76\u4f18\u5316\u793e\u4f1a\u76ee\u6807\u662f\u8be5\u7814\u7a76\u7684\u6838\u5fc3\u52a8\u673a\u3002", "method": "\u91c7\u7528\u5206\u5e03\u5f0f\u673a\u5236\u8bbe\u8ba1\uff0c\u5206\u4e3a\u4e24\u6b65\uff1a\u6bcf\u7ec4\u57fa\u4e8e\u4ee3\u7406\u62a5\u544a\u9009\u62e9\u4e24\u4e2a\u5019\u9009\u4f4d\u7f6e\u4f5c\u4e3a\u4ee3\u8868\uff1b\u7136\u540e\u4ece\u6240\u6709\u4ee3\u8868\u4e2d\u9009\u62e9\u4e24\u4e2a\u8bbe\u65bd\u4f4d\u7f6e\u3002\u7814\u7a76\u786e\u5b9a\u6027\u7b56\u7565\u8bc1\u660e\u673a\u5236\uff0c\u5206\u6790\u56db\u79cd\u793e\u4f1a\u76ee\u6807\u4e0b\u7684\u5931\u771f\u4e0a\u4e0b\u754c\u3002", "result": "\u5728\u56db\u79cd\u793e\u4f1a\u76ee\u6807\uff08Average-of-Average\u3001Max-of-Max\u3001Average-of-Max \u548c Max-of-Average\uff09\u4e0b\uff0c\u83b7\u5f97\u4e86\u6052\u5b9a\u7684\u5931\u771f\u4e0a\u4e0b\u754c\u3002", "conclusion": "\u7814\u7a76\u8868\u660e\uff0c\u786e\u5b9a\u6027\u7b56\u7565\u8bc1\u660e\u5206\u5e03\u5f0f\u673a\u5236\u80fd\u5728\u53d7\u9650\u8bbe\u65bd\u9009\u5740\u95ee\u9898\u4e2d\u6709\u6548\u6fc0\u52b1\u4ee3\u7406\u771f\u5b9e\u62a5\u544a\uff0c\u5e76\u5728\u591a\u79cd\u793e\u4f1a\u76ee\u6807\u4e0b\u5b9e\u73b0\u8fd1\u4f3c\u4f18\u5316\u3002"}}
