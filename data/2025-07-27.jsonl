{"id": "2507.17981", "categories": ["cs.GT"], "pdf": "https://arxiv.org/pdf/2507.17981", "abs": "https://arxiv.org/abs/2507.17981", "authors": ["Fatih Erdem Kizilkaya", "David Kempe"], "title": "$k$-Approval Veto: A Spectrum of Voting Rules Balancing Metric Distortion and Minority Protection", "comment": "22 pages, AAMAS'25", "summary": "In the context of single-winner ranked-choice elections between $m$\ncandidates, we explore the tradeoff between two competing goals in every\ndemocratic system: the majority principle (maximizing the social welfare) and\nthe minority principle (safeguarding minority groups from overly bad\noutcomes).To measure the social welfare, we use the well-established framework\nof metric distortion subject to various objectives: utilitarian (i.e., total\ncost), $\\alpha$-percentile (e.g., median cost for $\\alpha = 1/2$), and\negalitarian (i.e., max cost). To measure the protection of minorities, we\nintroduce the $\\ell$-mutual minority criterion, which requires that if a\nsufficiently large (parametrized by $\\ell$) coalition $T$ of voters ranks all\ncandidates in $S$ lower than all other candidates, then none of the candidates\nin $S$ should win. The highest $\\ell$ for which the criterion is satisfied\nprovides a well-defined measure of mutual minority protection (ranging from 1\nto $m$).\n  Our main contribution is the analysis of a recently proposed class of voting\nrules called $k$-Approval Veto, offering a comprehensive range of trade-offs\nbetween the two principles. This class spans between Plurality Veto (for $k=1$)\n- a simple voting rule achieving optimal metric distortion - and Vote By Veto\n(for $k=m$) which picks a candidate from the proportional veto core. We show\nthat $k$-Approval Veto has minority protection at least $k$, and thus, it\naccommodates any desired level of minority protection. However, this comes at\nthe price of lower social welfare. For the utilitarian objective, the metric\ndistortion increases linearly in $k$. For the $\\alpha$-percentile objective,\nthe metric distortion is the optimal value of 5 for $\\alpha \\ge k/(k+1)$ and\nunbounded for $\\alpha < k/(k+1)$. For the egalitarian objective, the metric\ndistortion is the optimal value of 3 for all values of $k$."}
{"id": "2507.18251", "categories": ["cs.GT"], "pdf": "https://arxiv.org/pdf/2507.18251", "abs": "https://arxiv.org/abs/2507.18251", "authors": ["Jiarong Jin", "Biaoshuai Tao"], "title": "On Pareto-Optimal and Fair Allocations with Personalized Bi-Valued Utilities", "comment": null, "summary": "We study the fair division problem of allocating $m$ indivisible goods to $n$\nagents with additive personalized bi-valued utilities. Specifically, each agent\n$i$ assigns one of two positive values $a_i > b_i > 0$ to each good, indicating\nthat agent $i$'s valuation of any good is either $a_i$ or $b_i$. For\nconvenience, we denote the value ratio of agent $i$ as $r_i = a_i / b_i$.\n  We give a characterization to all the Pareto-optimal allocations. Our\ncharacterization implies a polynomial-time algorithm to decide if a given\nallocation is Pareto-optimal in the case each $r_i$ is an integer. For the\ngeneral case (where $r_i$ may be fractional), we show that this decision\nproblem is coNP-complete. Our result complements the existing results: this\ndecision problem is coNP-complete for tri-valued utilities (where each agent's\nvalue for each good belongs to $\\{a,b,c\\}$ for some prescribed $a>b>c\\geq0$),\nand this decision problem belongs to P for bi-valued utilities (where $r_i$ in\nour model is the same for each agent).\n  We further show that an EFX allocation always exists and can be computed in\npolynomial time under the personalized bi-valued utilities setting, which\nextends the previous result on bi-valued utilities. We propose the open problem\nof whether an EFX and Pareto-optimal allocation always exists (and can be\ncomputed in polynomial time)."}
{"id": "2507.18509", "categories": ["cs.PL"], "pdf": "https://arxiv.org/pdf/2507.18509", "abs": "https://arxiv.org/abs/2507.18509", "authors": ["Henning Urbat"], "title": "Higher-Order Behavioural Conformances via Fibrations", "comment": null, "summary": "Coinduction is a widely used technique for establishing behavioural\nequivalence of programs in higher-order languages. In recent years, the rise of\nlanguages with quantitative (e.g.~probabilistic) features has led to extensions\nof coinductive methods to more refined types of behavioural conformances, most\nnotably notions of behavioural distance. To guarantee soundness of coinductive\nreasoning, one needs to show that the behavioural conformance at hand forms a\nprogram congruence, i.e. it is suitably compatible with the operations of the\nlanguage. This is usually achieved by a complex proof technique known as\n\\emph{Howe's method}, which needs to be carefully adapted to both the specific\nlanguage and the targeted notion of behavioural conformance. We develop a\nuniform categorical approach to Howe's method that features two orthogonal\ndimensions of abstraction: (1) the underlying higher-order language is modelled\nby an \\emph{abstract higher-order specification} (AHOS), a novel and very\ngeneral categorical account of operational semantics, and (2) notions of\nbehavioural conformance (such as relations or metrics) are modelled via\nfibrations over the base category of an AHOS. Our main result is a fundamental\ncongruence theorem at this level of generality: Under natural conditions on the\ncategorical ingredients and the operational rules of a language modelled by an\nAHOS, the greatest behavioural (bi)conformance on its operational model forms a\ncongruence. We illustrate our theory by deriving congruence of bisimilarity and\nbehavioural pseudometrics for probabilistic higher-order languages."}
{"id": "2507.17963", "categories": ["cs.GR", "cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.17963", "abs": "https://arxiv.org/abs/2507.17963", "authors": ["Rameen Abdal", "Or Patashnik", "Ekaterina Deyneka", "Hao Chen", "Aliaksandr Siarohin", "Sergey Tulyakov", "Daniel Cohen-Or", "Kfir Aberman"], "title": "Zero-Shot Dynamic Concept Personalization with Grid-Based LoRA", "comment": "Project Page and Video :\n  https://snap-research.github.io/zero-shot-dynamic-concepts/", "summary": "Recent advances in text-to-video generation have enabled high-quality\nsynthesis from text and image prompts. While the personalization of dynamic\nconcepts, which capture subject-specific appearance and motion from a single\nvideo, is now feasible, most existing methods require per-instance fine-tuning,\nlimiting scalability. We introduce a fully zero-shot framework for dynamic\nconcept personalization in text-to-video models. Our method leverages\nstructured 2x2 video grids that spatially organize input and output pairs,\nenabling the training of lightweight Grid-LoRA adapters for editing and\ncomposition within these grids. At inference, a dedicated Grid Fill module\ncompletes partially observed layouts, producing temporally coherent and\nidentity preserving outputs. Once trained, the entire system operates in a\nsingle forward pass, generalizing to previously unseen dynamic concepts without\nany test-time optimization. Extensive experiments demonstrate high-quality and\nconsistent results across a wide range of subjects beyond trained concepts and\nediting scenarios."}
{"id": "2507.18052", "categories": ["cs.GR", "I.3.2; C.2.1"], "pdf": "https://arxiv.org/pdf/2507.18052", "abs": "https://arxiv.org/abs/2507.18052", "authors": ["David Sinclair", "Ademyemi Ademola", "Babis Koniaris", "Kenny Mitchell"], "title": "DanceGraph: A Complementary Architecture for Synchronous Dancing Online", "comment": "36th International Conference on Computer Animation and Social Agents", "summary": "DanceGraph is an architecture for synchronized online dancing overcoming the\nlatency of networked body pose sharing. We break down this challenge by\ndeveloping a real-time bandwidth-efficient architecture to minimize lag and\nreduce the timeframe of required motion prediction for synchronization with the\nmusic's rhythm. In addition, we show an interactive method for the\nparameterized stylization of dance motions for rhythmic dance using online\ndance correctives."}
{"id": "2507.18155", "categories": ["cs.GR", "cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.18155", "abs": "https://arxiv.org/abs/2507.18155", "authors": ["SeungJun Moon", "Hah Min Lew", "Seungeun Lee", "Ji-Su Kang", "Gyeong-Moon Park"], "title": "GeoAvatar: Adaptive Geometrical Gaussian Splatting for 3D Head Avatar", "comment": "ICCV 2025, Project page: https://hahminlew.github.io/geoavatar/", "summary": "Despite recent progress in 3D head avatar generation, balancing identity\npreservation, i.e., reconstruction, with novel poses and expressions, i.e.,\nanimation, remains a challenge. Existing methods struggle to adapt Gaussians to\nvarying geometrical deviations across facial regions, resulting in suboptimal\nquality. To address this, we propose GeoAvatar, a framework for adaptive\ngeometrical Gaussian Splatting. GeoAvatar leverages Adaptive Pre-allocation\nStage (APS), an unsupervised method that segments Gaussians into rigid and\nflexible sets for adaptive offset regularization. Then, based on mouth anatomy\nand dynamics, we introduce a novel mouth structure and the part-wise\ndeformation strategy to enhance the animation fidelity of the mouth. Finally,\nwe propose a regularization loss for precise rigging between Gaussians and 3DMM\nfaces. Moreover, we release DynamicFace, a video dataset with highly expressive\nfacial motions. Extensive experiments show the superiority of GeoAvatar\ncompared to state-of-the-art methods in reconstruction and novel animation\nscenarios."}
{"id": "2507.18231", "categories": ["cs.GR", "cs.CV"], "pdf": "https://arxiv.org/pdf/2507.18231", "abs": "https://arxiv.org/abs/2507.18231", "authors": ["Yixiao Chen", "Bin Liang", "Hanzhi Guo", "Yongqing Cheng", "Jiayi Zhao", "Dongdong Weng"], "title": "PS-GS: Gaussian Splatting for Multi-View Photometric Stereo", "comment": null, "summary": "Integrating inverse rendering with multi-view photometric stereo (MVPS)\nyields more accurate 3D reconstructions than the inverse rendering approaches\nthat rely on fixed environment illumination. However, efficient inverse\nrendering with MVPS remains challenging. To fill this gap, we introduce the\nGaussian Splatting for Multi-view Photometric Stereo (PS-GS), which efficiently\nand jointly estimates the geometry, materials, and lighting of the object that\nis illuminated by diverse directional lights (multi-light). Our method first\nreconstructs a standard 2D Gaussian splatting model as the initial geometry.\nBased on the initialization model, it then proceeds with the deferred inverse\nrendering by the full rendering equation containing a lighting-computing\nmulti-layer perceptron. During the whole optimization, we regularize the\nrendered normal maps by the uncalibrated photometric stereo estimated normals.\nWe also propose the 2D Gaussian ray-tracing for single directional light to\nrefine the incident lighting. The regularizations and the use of multi-view and\nmulti-light images mitigate the ill-posed problem of inverse rendering. After\noptimization, the reconstructed object can be used for novel-view synthesis,\nrelighting, and material and shape editing. Experiments on both synthetic and\nreal datasets demonstrate that our method outperforms prior works in terms of\nreconstruction accuracy and computational efficiency."}
{"id": "2507.18352", "categories": ["cs.GR", "cs.LG", "cs.MM", "cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2507.18352", "abs": "https://arxiv.org/abs/2507.18352", "authors": ["Zhen Han", "Mattias Teye", "Derek Yadgaroff", "Judith Bütepage"], "title": "Tiny is not small enough: High-quality, low-resource facial animation models through hybrid knowledge distillation", "comment": "Accepted to ACM Transactions on Graphics 2025 (SIGGRAPH journal\n  track)", "summary": "The training of high-quality, robust machine learning models for\nspeech-driven 3D facial animation requires a large, diverse dataset of\nhigh-quality audio-animation pairs. To overcome the lack of such a dataset,\nrecent work has introduced large pre-trained speech encoders that are robust to\nvariations in the input audio and, therefore, enable the facial animation model\nto generalize across speakers, audio quality, and languages. However, the\nresulting facial animation models are prohibitively large and lend themselves\nonly to offline inference on a dedicated machine. In this work, we explore\non-device, real-time facial animation models in the context of game\ndevelopment. We overcome the lack of large datasets by using hybrid knowledge\ndistillation with pseudo-labeling. Given a large audio dataset, we employ a\nhigh-performing teacher model to train very small student models. In contrast\nto the pre-trained speech encoders, our student models only consist of\nconvolutional and fully-connected layers, removing the need for attention\ncontext or recurrent updates. In our experiments, we demonstrate that we can\nreduce the memory footprint to up to 3.4 MB and required future audio context\nto up to 81 ms while maintaining high-quality animations. This paves the way\nfor on-device inference, an important step towards realistic, model-driven\ndigital characters."}
