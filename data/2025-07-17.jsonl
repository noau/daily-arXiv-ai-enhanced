{"id": "2507.11676", "categories": ["cs.PL", "cs.LO", "quant-ph"], "pdf": "https://arxiv.org/pdf/2507.11676", "abs": "https://arxiv.org/abs/2507.11676", "authors": ["Chris Heunen", "Louis Lemonnier", "Christopher McNally", "Alex Rice"], "title": "Quantum circuits are just a phase", "comment": "43 pages, 5 figures", "summary": "Quantum programs today are written at a low level of abstraction - quantum\ncircuits akin to assembly languages - and even advanced quantum programming\nlanguages essentially function as circuit description languages. This state of\naffairs impedes scalability, clarity, and support for higher-level reasoning.\nMore abstract and expressive quantum programming constructs are needed.\n  To this end, we introduce a novel yet simple quantum programming language for\ngenerating unitaries from \"just a phase\"; we combine a (global) phase operation\nthat captures phase shifts with a quantum analogue of the \"if let\" construct\nthat captures subspace selection via pattern matching. This minimal language\nlifts the focus from quantum gates to eigendecomposition, conjugation, and\ncontrolled unitaries; common building blocks in quantum algorithm design.\n  We demonstrate several aspects of the expressive power of our language in\nseveral ways. Firstly, we establish that our representation is universal by\nderiving a universal quantum gate set. Secondly, we show that important quantum\nalgorithms can be expressed naturally and concisely, including Grover's search\nalgorithm, Hamiltonian simulation, Quantum Fourier Transform, Quantum Signal\nProcessing, and the Quantum Eigenvalue Transformation. Furthermore, we give\nclean denotational semantics grounded in categorical quantum mechanics.\nFinally, we implement a prototype compiler that efficiently translates terms of\nour language to quantum circuits, and prove that it is sound with respect to\nthese semantics. Collectively, these contributions show that this construct\noffers a principled and practical step toward more abstract and structured\nquantum programming."}
{"id": "2507.11731", "categories": ["cs.PL", "68N15", "D.3.2"], "pdf": "https://arxiv.org/pdf/2507.11731", "abs": "https://arxiv.org/abs/2507.11731", "authors": ["Neng-Fa Zhou", "Cristian Grozea", "Håkan Kjellerstrand", "Oisín Mac Fhearaí"], "title": "Picat Through the Lens of Advent of Code", "comment": "14 pages", "summary": "Picat is a logic-based, multi-paradigm programming language that integrates\nfeatures from logic, functional, constraint, and imperative programming\nparadigms. This paper presents solutions to several problems from the 2024\nAdvent of Code (AoC). While AoC problems are not designed for any specific\nprogramming language, certain problem types, such as reverse engineering and\npath-finding, are particularly well-suited to Picat due to its built-in\nconstraint solving, pattern matching, backtracking, and dynamic programming\nwith tabling. This paper demonstrates that Picat's features, especially its\nSAT-based constraint solving and tabling, enable concise, declarative, and\nhighly efficient implementations of problems that would require significantly\nmore effort in imperative languages."}
{"id": "2507.11827", "categories": ["cs.PL"], "pdf": "https://arxiv.org/pdf/2507.11827", "abs": "https://arxiv.org/abs/2507.11827", "authors": ["Shaurya Gomber", "Debangshu Banerjee", "Gagandeep Singh"], "title": "Universal Synthesis of Differentiably Tunable Numerical Abstract Transformers", "comment": "42 pages, 8 figures", "summary": "Numerical abstract interpretation is a widely used framework for the static\nanalysis of numerical programs. However, existing numerical abstract\ninterpreters rely on hand-crafted, instruction-specific transformers tailored\nto each domain, with no general algorithm for handling common operations across\ndomains. This limits extensibility, prevents precise compositional reasoning\nover instruction sequences, and forces all downstream tasks to use the same\nfixed transformer regardless of their precision, efficiency, or task-specific\nrequirements. To address these limitations, we propose a universal transformer\nsynthesis algorithm that constructs a parametric family of sound abstract\ntransformers for any given polyhedral numerical domain and a concrete operator\nfrom the class of Quadratic-Bounded Guarded Operators (QGO), which includes\nboth individual instructions and structured sequences. Each instantiation in\nthis family is sound by construction, enabling downstream analyses to adapt the\ntransformer to their particular needs. The space of transformers is\ndifferentiable but complex. To efficiently explore this space of transformers,\nwe introduce the Adaptive Gradient Guidance (AGG) procedure, a gradient-guided\nsearch strategy that steers the search process based on downstream analysis\nobjectives and runtime constraints. We implement these ideas in the USTAD\nframework and evaluate their effectiveness across three numerical abstract\ndomains: Zones, Octagons, and Polyhedra. Our results demonstrate that the\nuniversal synthesis algorithm successfully constructs sound families of\ntransformers across domains, and that USTAD achieves significant, tunable\nprecision gains over baselines by leveraging compositional reasoning and\nefficient gradient-guided traversal of the transformer space."}
{"id": "2507.11897", "categories": ["cs.PL", "cs.DB"], "pdf": "https://arxiv.org/pdf/2507.11897", "abs": "https://arxiv.org/abs/2507.11897", "authors": ["Tyler Hou", "Shadaj Laddad", "Joseph M. Hellerstein"], "title": "Towards Relational Contextual Equality Saturation", "comment": "Appeared at EGRAPHS 2024", "summary": "Equality saturation is a powerful technique for program optimization.\nContextual equality saturation extends this to support rewrite rules that are\nconditioned on where a term appears in an expression. Existing work has brought\ncontextual reasoning to egg; in this paper, we share our ongoing work to extend\nthis to relational equality saturation in egglog. We summarize the existing\napproaches to contextual equality saturation, outline its main applications,\nand identify key challenges in combining this approach with relational models."}
{"id": "2507.11808", "categories": ["cs.GT", "cs.DM", "Primary 68R10, Secondary 90B06"], "pdf": "https://arxiv.org/pdf/2507.11808", "abs": "https://arxiv.org/abs/2507.11808", "authors": ["Taiki Yamada", "Taisuke Matsubae", "Tomoya Akamatsu"], "title": "New allocation rule based on graph structures and their application to economic phenomena", "comment": "15 pages, 8 figures", "summary": "This study introduces the \\emph{edge-based Shapley value}, a novel allocation\nrule within cooperative game theory, specifically tailored for networked\nsystems, where value is generated through interactions represented by edges.\nTraditional allocation rules, such as the Shapley and Myerson values, evaluate\nplayer contributions based on node-level characteristics, or connected\ncomponents. However, these approaches often fail to adequately capture the\nfunctional role of edges, which are crucial in systems such as supply chains\nand digital platforms, where interactions, rather than individual agents, are\nthe primary drivers of value. Our edge-based Shapley value shifts the\ncharacteristic function from node sets to edge sets, thereby enabling a more\ngranular and context-sensitive evaluation of the contributions. We establish\nits theoretical foundations, demonstrate its relationship to classical\nallocation rules, and show that it retains key properties such as fairness and\nsymmetry. To illustrate its applicability, we present two use cases: content\nplatform networks and supply chain logistics (SCL). In both cases, our method\nproduces intuitive and structurally consistent allocations, particularly in\nscenarios with overlapping routes, exclusive contracts or cost-sensitive paths.\nThis framework offers a new perspective on value attribution in cooperative\nsettings with complex interaction structures and provides practical tools for\nanalyzing real-world economic and logistical networks."}
{"id": "2507.11794", "categories": ["cs.GR"], "pdf": "https://arxiv.org/pdf/2507.11794", "abs": "https://arxiv.org/abs/2507.11794", "authors": ["Nak-Jun Sung", "Jun Ma", "TaeHeon Kim", "Yoo-joo Choi", "Min-Hyung Choi", "Min Hong"], "title": "Real-Time Cloth Simulation Using WebGPU: Evaluating Limits of High-Resolution", "comment": null, "summary": "This study explores the capabilities of WebGPU, an emerging web graphics\nparadigm, for real-time cloth simulation. Traditional WebGL-based methods have\nbeen in handling complex physical simulations due to their emphasis on graphics\nrendering rather than general-purpose GPU (GPGPU) operations. WebGPU, designed\nto provide modern 3D graphics and computational capabilities, offers\nsignificant improvements through parallel processing and support for\ncomputational shaders. In this work, we implemented a cloth simulation system\nusing the Mass-Spring Method within the WebGPU framework, integrating collision\ndetection and response handling with the 3D surface model. First, comparative\nperformance evaluations demonstrate that WebGPU substantially outperforms\nWebGL, particularly in high-resolution simulations, maintaining 60 frames per\nsecond (fps) even with up to 640K nodes. The second experiment aimed to\ndetermine the real-time limitations of WebGPU and confirmed that WebGPU can\nhandle real-time collisions between 4K and 100k cloth node models and a 100K\ntriangle surface model in real-time. These experiments also highlight the\nimportance of balancing real-time performance with realistic rendering when\nhandling collisions between cloth models and complex 3D objects. Our source\ncode is available at https://github.com/nakjun/Cloth-Simulation-WebGPU"}
{"id": "2507.11883", "categories": ["cs.GT"], "pdf": "https://arxiv.org/pdf/2507.11883", "abs": "https://arxiv.org/abs/2507.11883", "authors": ["Yao Zhang", "Indrajit Saha", "Zhaohong Sun", "Makoto Yokoo"], "title": "Coalitions on the Fly in Cooperative Games", "comment": "Full version with all detailed proofs of ECAI 2025 accepted paper\n  with the same title", "summary": "In this work, we examine a sequential setting of a cooperative game in which\nplayers arrive dynamically to form coalitions and complete tasks either\ntogether or individually, depending on the value created. Upon arrival, a new\nplayer as a decision maker faces two options: forming a new coalition or\njoining an existing one. We assume that players are greedy, i.e., they aim to\nmaximize their rewards based on the information available at their arrival. The\nobjective is to design an online value distribution policy that incentivizes\nplayers to form a coalition structure that maximizes social welfare. We focus\non monotone and bounded cooperative games. Our main result establishes an upper\nbound of $\\frac{3\\mathsf{min}}{\\mathsf{max}}$ on the competitive ratio for any\nirrevocable policy (i.e., one without redistribution), and proposes a policy\nthat achieves a near-optimal competitive ratio of $\\min\\left\\{\\frac{1}{2},\n\\frac{3\\mathsf{min}}{\\mathsf{max}}\\right\\}$, where $\\mathsf{min}$ and\n$\\mathsf{max}$ denote the smallest and largest marginal contribution of any\nsub-coalition of players respectively. Finally, we also consider\nnon-irrevocable policies, with alternative bounds only when the number of\nplayers is limited."}
{"id": "2507.11857", "categories": ["cs.GR", "cs.HC"], "pdf": "https://arxiv.org/pdf/2507.11857", "abs": "https://arxiv.org/abs/2507.11857", "authors": ["Benjamin Watson", "Alinda Friedman", "Aaron McGaffey"], "title": "Measuring and predicting visual fidelity", "comment": null, "summary": "This paper is a study of techniques for measuring and predicting visual\nfidelity. As visual stimuli we use polygonal models, and vary their fidelity\nwith two different model simplification algorithms. We also group the stimuli\ninto two object types: animals and man made artifacts. We examine three\ndifferent experimental techniques for measuring these fidelity changes: naming\ntimes, ratings, and preferences. All the measures were sensitive to the type of\nsimplification and level of simplification. However, the measures differed from\none another in their response to object type. We also examine several automatic\ntechniques for predicting these experimental measures, including techniques\nbased on images and on the models themselves. Automatic measures of fidelity\nwere successful at predicting experimental ratings, less successful at\npredicting preferences, and largely failures at predicting naming times. We\nconclude with suggestions for use and improvement of the experimental and\nautomatic measures of visual fidelity."}
{"id": "2507.12054", "categories": ["cs.GT"], "pdf": "https://arxiv.org/pdf/2507.12054", "abs": "https://arxiv.org/abs/2507.12054", "authors": ["Tian Bai", "Yiding Feng", "Yaohao Liu", "Mengfan Ma", "Mingyu Xiao"], "title": "Contracting with a Mechanism Designer", "comment": null, "summary": "This paper explores the economic interactions within modern crowdsourcing\nmarkets. In these markets, employers issue requests for tasks, platforms\nfacilitate the recruitment of crowd workers, and workers complete tasks for\nmonetary rewards. Recognizing that these roles serve distinct functions within\nthe ecosystem, we introduce a three-party model that distinguishes among the\nprincipal (the requester), the intermediary (the platform), and the pool of\nagents (the workers). The principal, unable to directly engage with agents,\nrelies on the intermediary to recruit and incentivize them. This interaction\nunfolds in two stages: first, the principal designs a profit-sharing contract\nwith the intermediary; second, the intermediary implements a mechanism to\nselect an agent to complete the delegated task.\n  We analyze the proposed model as an extensive-form Stackelberg game. Our\ncontributions are fourfold: (1) We fully characterize the subgame perfect\nequilibrium. In particular, we reduce the principal's contract design problem\nto a novel auction-theoretic formulation we term virtual value pricing, and\nreveals that linear contracts are optimal even when the task have multiple\noutcomes and agents' cost distributions are asymmetric. (2) To quantify the\nprincipal's utility loss from delegation and information asymmetry, we\nintroduce the price of double marginalization (PoDM) and the classical price of\nanarchy (PoA), and derive tight or nearly tight bounds on both ratios under\nregular and monotone hazard rate (MHR) distributions. (3) We further examine\nthese two ratios in a natural setting where the intermediary is restricted to\nanonymous pricing mechanisms, and show that similar qualitative insights\ncontinue to hold. (4) Finally, we extend our results on both ratios to a robust\nframework that accommodates scenarios in which the principal lacks precise\ninformation about the market size."}
{"id": "2507.11949", "categories": ["cs.GR", "cs.CV", "cs.RO"], "pdf": "https://arxiv.org/pdf/2507.11949", "abs": "https://arxiv.org/abs/2507.11949", "authors": ["Shuyang Xu", "Zhiyang Dou", "Mingyi Shi", "Liang Pan", "Leo Ho", "Jingbo Wang", "Yuan Liu", "Cheng Lin", "Yuexin Ma", "Wenping Wang", "Taku Komura"], "title": "MOSPA: Human Motion Generation Driven by Spatial Audio", "comment": null, "summary": "Enabling virtual humans to dynamically and realistically respond to diverse\nauditory stimuli remains a key challenge in character animation, demanding the\nintegration of perceptual modeling and motion synthesis. Despite its\nsignificance, this task remains largely unexplored. Most previous works have\nprimarily focused on mapping modalities like speech, audio, and music to\ngenerate human motion. As of yet, these models typically overlook the impact of\nspatial features encoded in spatial audio signals on human motion. To bridge\nthis gap and enable high-quality modeling of human movements in response to\nspatial audio, we introduce the first comprehensive Spatial Audio-Driven Human\nMotion (SAM) dataset, which contains diverse and high-quality spatial audio and\nmotion data. For benchmarking, we develop a simple yet effective\ndiffusion-based generative framework for human MOtion generation driven by\nSPatial Audio, termed MOSPA, which faithfully captures the relationship between\nbody motion and spatial audio through an effective fusion mechanism. Once\ntrained, MOSPA could generate diverse realistic human motions conditioned on\nvarying spatial audio inputs. We perform a thorough investigation of the\nproposed dataset and conduct extensive experiments for benchmarking, where our\nmethod achieves state-of-the-art performance on this task. Our model and\ndataset will be open-sourced upon acceptance. Please refer to our supplementary\nvideo for more details."}
{"id": "2507.11971", "categories": ["cs.GR", "cs.CV"], "pdf": "https://arxiv.org/pdf/2507.11971", "abs": "https://arxiv.org/abs/2507.11971", "authors": ["Tielong Wang", "Yuxuan Xiong", "Jinfan Liu", "Zhifan Zhang", "Ye Chen", "Yue Shi", "Bingbing Ni"], "title": "HPR3D: Hierarchical Proxy Representation for High-Fidelity 3D Reconstruction and Controllable Editing", "comment": null, "summary": "Current 3D representations like meshes, voxels, point clouds, and NeRF-based\nneural implicit fields exhibit significant limitations: they are often\ntask-specific, lacking universal applicability across reconstruction,\ngeneration, editing, and driving. While meshes offer high precision, their\ndense vertex data complicates editing; NeRFs deliver excellent rendering but\nsuffer from structural ambiguity, hindering animation and manipulation; all\nrepresentations inherently struggle with the trade-off between data complexity\nand fidelity. To overcome these issues, we introduce a novel 3D Hierarchical\nProxy Node representation. Its core innovation lies in representing an object's\nshape and texture via a sparse set of hierarchically organized\n(tree-structured) proxy nodes distributed on its surface and interior. Each\nnode stores local shape and texture information (implicitly encoded by a small\nMLP) within its neighborhood. Querying any 3D coordinate's properties involves\nefficient neural interpolation and lightweight decoding from relevant nearby\nand parent nodes. This framework yields a highly compact representation where\nnodes align with local semantics, enabling direct drag-and-edit manipulation,\nand offers scalable quality-complexity control. Extensive experiments across 3D\nreconstruction and editing demonstrate our method's expressive efficiency,\nhigh-fidelity rendering quality, and superior editability."}
{"id": "2507.12156", "categories": ["cs.GR"], "pdf": "https://arxiv.org/pdf/2507.12156", "abs": "https://arxiv.org/abs/2507.12156", "authors": ["Chen Li", "Shanshan Dong", "Sheng Qiu", "Jianmin Han", "Zan Gao", "Kemeng Huang", "Taku Komura"], "title": "SmokeSVD: Smoke Reconstruction from A Single View via Progressive Novel View Synthesis and Refinement with Diffusion Models", "comment": null, "summary": "Reconstructing dynamic fluids from sparse views is a long-standing and\nchallenging problem, due to the severe lack of 3D information from insufficient\nview coverage. While several pioneering approaches have attempted to address\nthis issue using differentiable rendering or novel view synthesis, they are\noften limited by time-consuming optimization and refinement processes under\nill-posed conditions. To tackle above challenges, we propose SmokeSVD, an\nefficient and effective framework to progressively generate and reconstruct\ndynamic smoke from a single video by integrating both the powerful generative\ncapabilities from diffusion models and physically guided consistency\noptimization towards realistic appearance and dynamic evolution. Specifically,\nwe first propose a physically guided side-view synthesizer based on diffusion\nmodels, which explicitly incorporates divergence and gradient guidance of\nvelocity fields to generate visually realistic and spatio-temporally consistent\nside-view images frame by frame, significantly alleviating the ill-posedness of\nsingle-view reconstruction without imposing additional constraints.\nSubsequently, we determine a rough estimation of density field from the pair of\nfront-view input and side-view synthetic image, and further refine 2D blurry\nnovel-view images and 3D coarse-grained density field through an iterative\nprocess that progressively renders and enhances the images from increasing\nnovel viewing angles, generating high-quality multi-view image sequences.\nFinally, we reconstruct and estimate the fine-grained density field, velocity\nfield, and smoke source via differentiable advection by leveraging the\nNavier-Stokes equations. Extensive quantitative and qualitative experiments\nshow that our approach achieves high-quality reconstruction and outperforms\nprevious state-of-the-art techniques."}
{"id": "2507.12168", "categories": ["cs.GR"], "pdf": "https://arxiv.org/pdf/2507.12168", "abs": "https://arxiv.org/abs/2507.12168", "authors": ["Lu Yu", "Zhong Ren", "Youyi Zheng", "Xiang Chen", "Kun Zhou"], "title": "Shape Adaptation for 3D Hairstyle Retargeting", "comment": null, "summary": "It is demanding to author an existing hairstyle for novel characters in games\nand VR applications. However, it is a non-trivial task for artists due to the\ncomplicated hair geometries and spatial interactions to preserve. In this\npaper, we present an automatic shape adaptation method to retarget 3D\nhairstyles. We formulate the adaptation process as a constrained optimization\nproblem, where all the shape properties and spatial relationships are converted\ninto individual objectives and constraints. To make such an optimization on\nhigh-resolution hairstyles tractable, we adopt a multi-scale strategy to\ncompute the target positions of the hair strands in a coarse-to-fine manner.\nThe global solving for the inter-strands coupling is restricted to the coarse\nlevel, and the solving for fine details is made local and parallel. In\naddition, we present a novel hairline edit tool to allow for user customization\nduring retargeting. We achieve it by solving physics-based deformations of an\nembedded membrane to redistribute the hair roots with minimal distortion. We\ndemonstrate the efficacy of our method through quantitative and qualitative\nexperiments on various hairstyles and characters."}
