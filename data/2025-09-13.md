<div id=toc></div>

# Table of Contents

- [cs.GR](#cs.GR) [Total: 2]
- [cs.PL](#cs.PL) [Total: 2]
- [cs.GT](#cs.GT) [Total: 4]


<div id='cs.GR'></div>

# cs.GR [[Back]](#toc)

### [1] [Morphology-Preserving Remeshing Approach to Particulate Microstructures via Harmonic Decomposition](https://arxiv.org/abs/2509.08855)
*Mahmoud Shaqfa*

Main category: cs.GR

TL;DR: 论文提出了一种基于分层扩散的方法，用于重新参数化曲面以生成高质量的三角网格，解决了传统谐波分解方法在局部离散化不均匀的问题，并在形态保持和计算效率上取得了显著改进。


<details>
  <summary>Details</summary>
Motivation: 传统谐波分解方法在重建或生成工程微结构的高质量网格时，由于未考虑基础函数雅可比矩阵的局部变化，导致离散化不均匀，影响数值模拟的精度和时间步长，因此需要一种更有效的重新参数化方法。

Method: 采用基于分层非线性扩散的重新参数化方法，通过在分析域上重新采样曲线坐标，实现曲面三角网格的均衡化，类似于热问题的解决方案，同时测试了各向同性和各向异性扩散方案。

Result: 结果表明，该方法显著提高了曲面三角网格的质量指标，同时保持了表面形态、面积和体积，适用于大型2D和3D微结构的数字孪生应用。

Conclusion: 提出的方法不仅解决了传统谐波分解方法的局限性，还在计算效率和形态保持方面表现出色，为微结构的高质量离散化提供了新的解决方案，具有广阔的应用前景。

Abstract: Harmonic decomposition of surfaces, such as spherical and spheroidal
harmonics, is used to analyze morphology, reconstruct, and generate surface
inclusions of particulate microstructures. However, obtaining high-quality
meshes of engineering microstructures using these approaches remains an open
question. In harmonic approaches, we usually reconstruct surfaces by evaluating
the harmonic bases on equidistantly sampled simplicial complexes of the base
domains (e.g., triangular spheroids and disks). However, this traditional
sampling does not account for local changes in the Jacobian of the basis
functions, resulting in nonuniform discretization after reconstruction or
generation. As it impacts the accuracy and time step, high-quality
discretization of microstructures is crucial for efficient numerical
simulations (e.g., finite element and discrete element methods). To circumvent
this issue, we propose an efficient hierarchical diffusion-based approach for
resampling the surface-i.e., performing a reparameterization-to yield an
equalized mesh triangulation. Analogous to heat problems, we use nonlinear
diffusion to resample the curvilinear coordinates of the analysis domain,
thereby enlarging small triangles at the expense of large triangles on
surfaces. We tested isotropic and anisotropic diffusion schemes on the recent
spheroidal and hemispheroidal harmonics methods. The results show a substantial
improvement in the quality metrics for surface triangulation. Unlike
traditional surface reconstruction and meshing techniques, this approach
preserves surface morphology, along with the areas and volumes of surfaces. We
discuss the results and the associated computational costs for large 2D and 3D
microstructures, such as digital twins of concrete and stone masonry, and their
future applications.

</details>


### [2] [CameraVDP: Perceptual Display Assessment with Uncertainty Estimation via Camera and Visual Difference Prediction](https://arxiv.org/abs/2509.08947)
*Yancheng Cai,Robert Wanat,Rafal Mantiuk*

Main category: cs.GR

TL;DR: 本文提出了一种结合相机重建管道和视觉差异预测器的方法，用于感知评估电子显示屏的缺陷。


<details>
  <summary>Details</summary>
Motivation: 传统显示测量方法无法捕捉高频和像素级失真，而相机测量又存在光学、采样和光度失真。需要结合视觉系统模型评估失真是否可见。

Method: 使用HDR图像堆叠、MTF反转、渐晕校正、几何校正、单应变换和色彩校正的相机重建管道，并结合视觉差异预测器（VDP）模型。

Result: 提出的CameraVDP框架通过缺陷像素检测、色彩边缘感知和显示不均匀性评估三种应用进行了验证。

Conclusion: CameraVDP框架能有效评估显示质量，并提供缺陷检测性能的理论上限和VDP质量评分的置信区间。

Abstract: Accurate measurement of images produced by electronic displays is critical
for the evaluation of both traditional and computational displays. Traditional
display measurement methods based on sparse radiometric sampling and fitting a
model are inadequate for capturing spatially varying display artifacts, as they
fail to capture high-frequency and pixel-level distortions. While cameras offer
sufficient spatial resolution, they introduce optical, sampling, and
photometric distortions. Furthermore, the physical measurement must be combined
with a model of a visual system to assess whether the distortions are going to
be visible. To enable perceptual assessment of displays, we propose a
combination of a camera-based reconstruction pipeline with a visual difference
predictor, which account for both the inaccuracy of camera measurements and
visual difference prediction. The reconstruction pipeline combines HDR image
stacking, MTF inversion, vignetting correction, geometric undistortion,
homography transformation, and color correction, enabling cameras to function
as precise display measurement instruments. By incorporating a Visual
Difference Predictor (VDP), our system models the visibility of various stimuli
under different viewing conditions for the human visual system. We validate the
proposed CameraVDP framework through three applications: defective pixel
detection, color fringing awareness, and display non-uniformity evaluation. Our
uncertainty analysis framework enables the estimation of the theoretical upper
bound for defect pixel detection performance and provides confidence intervals
for VDP quality scores.

</details>


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [3] [Towards Verified Compilation of Floating-point Optimization in Scientific Computing Programs](https://arxiv.org/abs/2509.09019)
*Mohit Tekriwal,John Sarracino*

Main category: cs.PL

TL;DR: 本文探讨了在LLVM IR级别上验证浮点数优化（尤其是快速数学优化）的正确性，重点关注Fused-Multiply-Add (FMA)优化的证明，并提出扩展方法。


<details>
  <summary>Details</summary>
Motivation: 科学计算程序需要高性能和高效资源利用，但必须确保编译器优化的正确性，特别是浮点数优化。

Method: 利用Rocq定理证明器中的Verified LLVM框架，验证了基本块中实现算术表达式$a * b + c$的FMA优化的正确性。

Result: 初步证明了FMA优化的正确性，并提出了扩展方法以支持更多程序特性和浮点数优化。

Conclusion: 通过理论验证和扩展方法，为科学计算程序中的浮点数优化提供了一个可靠的基础。

Abstract: Scientific computing programs often undergo aggressive compiler optimization
to achieve high performance and efficient resource utilization. While
performance is critical, we also need to ensure that these optimizations are
correct. In this paper, we focus on a specific class of optimizations,
floating-point optimizations, notably due to fast math, at the LLVM IR level.
We present a preliminary work, which leverages the Verified LLVM framework in
the Rocq theorem prover, to prove the correctness of Fused-Multiply-Add (FMA)
optimization for a basic block implementing the arithmetic expression $a * b +
c$ . We then propose ways to extend this preliminary results by adding more
program features and fast math floating-point optimizations.

</details>


### [4] [Dependent-Type-Preserving Memory Allocation](https://arxiv.org/abs/2509.09059)
*Paulette Koronkevich,William J. Bowman*

Main category: cs.PL

TL;DR: 依赖类型编程语言（如Coq、Agda、Idris和F*）允许程序员编写详细的程序规范并证明其符合性，但在编译过程中规范可能被违反。本文提出了一种类型保留编译方法，以防止链接时违反规范。


<details>
  <summary>Details</summary>
Motivation: 在依赖类型编程语言中，程序规范在编译后被擦除可能导致外部程序违反规范，影响编译后程序的行为。动机在于解决这一问题，确保类型安全在链接时仍然有效。

Method: 采用类型保留编译方法，开发支持依赖内存分配的中间语言，并设计依赖类型保留的编译器传递来处理内存分配问题。

Result: 提出了一种能够检查并阻止链接时类型错误的方法，确保程序在链接后仍符合原始依赖类型规范。

Conclusion: 通过类型保留编译和依赖中间语言的设计，可以有效防止外部程序违反依赖类型规范，增强程序的安全性和可靠性。

Abstract: Dependently typed programming languages such as Coq, Agda, Idris, and F*,
allow programmers to write detailed specifications of their programs and prove
their programs meet these specifications. However, these specifications can be
violated during compilation since they are erased after type checking. External
programs linked with the compiled program can violate the specifications of the
original program and change the behavior of the compiled program -- even when
compiled with a verified compiler. For example, since Coq does not allow
explicitly allocating memory, a programmer might link their Coq program with a
C program that can allocate memory. Even if the Coq program is compiled with a
verified compiler, the external C program can still violate the memory-safe
specification of the Coq program by providing an uninitialized pointer to
memory. This error could be ruled out by type checking in a language expressive
enough to indicate whether memory is initialized versus uninitialized. Linking
with a program with an uninitialized pointer could be considered ill-typed, and
our linking process could prevent linking with ill-typed programs. To
facilitate type checking during linking, we can use type-preserving
compilation, which preserves the types through the compilation process. In this
ongoing work, we develop a typed intermediate language that supports dependent
memory allocation, as well as a dependent-type-preserving compiler pass for
memory allocation.

</details>


<div id='cs.GT'></div>

# cs.GT [[Back]](#toc)

### [5] [Toward a Multi-Echelon Cyber Warfare Theory: A Meta-Game-Theoretic Paradigm for Defense and Dominance](https://arxiv.org/abs/2509.08976)
*Ya-Ting Yang,Quanyan Zhu*

Main category: cs.GT

TL;DR: 该论文探讨了网络战在多域作战中的重要性，并提出了一种基于博弈论和AI技术的综合框架来设计和优化网络战策略。


<details>
  <summary>Details</summary>
Motivation: 网络战已成为现代冲突的核心领域，但现有研究多关注孤立战术或碎片化技术，缺乏整体理解。博弈论为此提供了统一的框架。

Method: 论文结合博弈论和现代AI技术，建立了网络战的量化模型，用于分析攻防互动、风险评估和战略推理，并通过案例研究（RedCyber）验证方法的有效性。

Result: 研究表明，博弈论模型能够捕捉网络战中的非线性动态和资源分配的悖论逻辑，为策略设计和优化提供了量化工具。

Conclusion: 论文总结了博弈论在网络战中的应用前景，并提出了未来研究方向，包括弹性策略、跨层级规划和AI在网络战中的演进作用。

Abstract: Cyber warfare has become a central element of modern conflict, especially
within multi-domain operations. As both a distinct and critical domain, cyber
warfare requires integrating defensive and offensive technologies into coherent
strategies. While prior research has emphasized isolated tactics or fragmented
technologies, a holistic understanding is essential for effective resource
deployment and risk mitigation. Game theory offers a unifying framework for
this purpose. It not only models attacker-defender interactions but also
provides quantitative tools for equilibrium analysis, risk assessment, and
strategic reasoning. Integrated with modern AI techniques, game-theoretic
models enable the design and optimization of strategies across multiple levels
of cyber warfare, from policy and strategy to operations, tactics, and
technical implementations. These models capture the paradoxical logic of
conflict, where more resources do not always translate into greater advantage,
and where nonlinear dynamics govern outcomes. To illustrate the approach, this
chapter examines RedCyber, a synthetic cyber conflict, demonstrating how
game-theoretic methods capture the interdependencies of cyber operations. The
chapter concludes with directions for future research on resilience,
cros-echelon planning, and the evolving role of AI in cyber warfare.

</details>


### [6] [Persuasion Gains and Losses from Peer Communication](https://arxiv.org/abs/2509.09099)
*Toygar T. Kerman,Anastas P. Tenev,Konstantin Zabarnyi*

Main category: cs.GT

TL;DR: 研究了贝叶斯说服情境，分析了发送者如何通过部分信息说服接收者，并探讨了网络结构对发送者效用的影响。


<details>
  <summary>Details</summary>
Motivation: 探讨在网络通信中，发送者如何通过部分信息说服接收者，并分析网络密度对发送者效用的影响。

Method: 通过贝叶斯说服模型，研究发送者在不同网络结构中的效用变化，特别关注网络扩展的影响。

Result: 研究发现，网络扩展可以严格提升发送者的效用，但其增益不随网络密度单调增加；某些网络结构可以达到空网络的效用上限。

Conclusion: 研究表明，更多通信未必带来更好的集体结果，网络结构的微小变化可能显著影响发送者的收益。

Abstract: We study a Bayesian persuasion setting in which a sender wants to persuade a
critical mass of receivers by revealing partial information about the state to
them. The homogeneous binary-action receivers are located on a communication
network, and each observes the private messages sent to them and their
immediate neighbors. We examine how the sender's expected utility varies with
increased communication among receivers. We show that for general families of
networks, extending the network can strictly benefit the sender. Thus, the
sender's gain from persuasion is not monotonic in network density. Moreover,
many network extensions can achieve the upper bound on the sender's expected
utility among all networks, which corresponds to the payoff in an empty
network. This is the case in networks reflecting a clear informational
hierarchy (e.g., in global corporations), as well as in decentralized networks
in which information originates from multiple sources (e.g., influencers in
social media). Finally, we show that a slight modification to the structure of
some of these networks precludes the possibility of such beneficial extensions.
Overall, our results caution against presuming that more communication
necessarily leads to better collective outcomes.

</details>


### [7] [Mechanism Design with Outliers and Predictions](https://arxiv.org/abs/2509.09561)
*Argyrios Deligkas,Eduard Eiben,Sophie Klumper,Guido Schäfer,Artem Tsikiridis*

Main category: cs.GT

TL;DR: 本文研究了机制设计中排除离群代理的影响，特别关注了在线上设施定位问题中，通过排除最远的z个代理来优化社会成本函数。研究发现，排除离群代理并不总能提高效率，甚至可能导致相反的结果。


<details>
  <summary>Details</summary>
Motivation: 研究动机在于探讨在机制设计中，通过排除离群代理（z个最远的代理）是否能够优化社会成本函数。这在代理表现出极端或非典型偏好时尤其重要。

Method: 研究方法包括对确定性策略证明机制在功利主义和平均主义社会成本目标下的性能进行严格分析，并推导出紧界。此外，还研究了随机机制在期望真实情况下的性能。

Result: 结果显示，当z≥n/2时，策略证明机制无法为任一目标实现有界的近似比。对于平均主义成本，选择第(z+1)阶统计量是策略证明且2-近似的，且这是最佳可能。对于功利主义成本，策略证明机制无法有效利用离群代理，且近似比随着离群代理数量的增加而恶化。但通过预测，可以实现最优的一致性和鲁棒性权衡。

Conclusion: 结论表明，排除离群代理并非总能提升效率，其影响因目标函数而异。平均主义成本下2-近似是最佳结果，而功利主义成本下需要通过预测来实现最优机制设计。

Abstract: We initiate the study of mechanism design with outliers, where the designer
can discard $z$ agents from the social cost objective. This setting is
particularly relevant when some agents exhibit extreme or atypical preferences.
As a natural case study, we consider facility location on the line: $n$
strategic agents report their preferred locations, and a mechanism places a
facility to minimize a social cost function. In our setting, the $z$ agents
farthest from the chosen facility are excluded from the social cost. While it
may seem intuitive that discarding outliers improves efficiency, our results
reveal that the opposite can hold.
  We derive tight bounds for deterministic strategyproof mechanisms under the
two most-studied objectives: utilitarian and egalitarian social cost. Our
results offer a comprehensive view of the impact of outliers. We first show
that when $z \ge n/2$, no strategyproof mechanism can achieve a bounded
approximation for either objective. For egalitarian cost, selecting the $(z +
1)$-th order statistic is strategyproof and 2-approximate. In fact, we show
that this is best possible by providing a matching lower bound. Notably, this
lower bound of 2 persists even when the mechanism has access to a prediction of
the optimal location, in stark contrast to the setting without outliers. For
utilitarian cost, we show that strategyproof mechanisms cannot effectively
exploit outliers, leading to the counterintuitive outcome that approximation
guarantees worsen as the number of outliers increases. However, in this case,
access to a prediction allows us to design a strategyproof mechanism achieving
the best possible trade-off between consistency and robustness. Finally, we
also establish lower bounds for randomized mechanisms that are truthful in
expectation.

</details>


### [8] [Maximizing social welfare among EF1 allocations at the presence of two types of agents](https://arxiv.org/abs/2509.09641)
*Jiaxuan Ma,Yong Chen,Guangting Chen,Mingyang Gong,Guohui Lin,An Zhang*

Main category: cs.GT

TL;DR: 研究了在公平分配不可分物品以最大化社会福利时，提出了改进的近似算法，优化了之前的结果。


<details>
  <summary>Details</summary>
Motivation: 探索在特定的公平标准（EF1）和有限的效用函数下，如何更高效地分配物品以最大化社会福利。

Method: 提出了针对两种标准化效用函数的2-近似算法，并在三代理情况下提出了改进的5/3-近似和2-近似算法。

Result: 在两种标准化效用函数下，2-近似算法优于之前的16√n；在三代理情况下，5/3-近似和2-近似算法优于之前的3-近似。

Conclusion: 这些改进的近似算法验证了APX-完全性，并为特定情况下的分配问题提供了更优解。

Abstract: We study the fair allocation of indivisible items to $n$ agents to maximize
the utilitarian social welfare, where the fairness criterion is envy-free up to
one item and there are only two different utility functions shared by the
agents. We present a $2$-approximation algorithm when the two utility functions
are normalized, improving the previous best ratio of $16 \sqrt{n}$ shown for
general normalized utility functions; thus this constant ratio approximation
algorithm confirms the APX-completeness in this special case previously shown
APX-hard. When there are only three agents, i.e., $n = 3$, the previous best
ratio is $3$ shown for general utility functions, and we present an improved
and tight $\frac 53$-approximation algorithm when the two utility functions are
normalized, and a best possible and tight $2$-approximation algorithm when the
two utility functions are unnormalized.

</details>
