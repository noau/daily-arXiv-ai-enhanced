<div id=toc></div>

# Table of Contents

- [cs.GR](#cs.GR) [Total: 5]
- [cs.PL](#cs.PL) [Total: 3]
- [cs.GT](#cs.GT) [Total: 2]


<div id='cs.GR'></div>

# cs.GR [[Back]](#toc)

### [1] [EngravingGNN: A Hybrid Graph Neural Network for End-to-End Piano Score Engraving](https://arxiv.org/abs/2509.19412)
*Emmanouil Karystinaios,Francesco Foscarin,Gerhard Widmer*

Main category: cs.GR

TL;DR: 本文提出了一种基于图神经网络（GNN）的统一框架，用于自动音乐雕刻，能够同时处理多个子任务，并在钢琴音乐和量化符号输入上表现出色。


<details>
  <summary>Details</summary>
Motivation: 音乐雕刻是从音乐内容生成人类可读乐谱的步骤，虽对涉及人类演奏者的应用至关重要，但在符号音乐处理领域仍鲜有研究。本文旨在填补这一空白。

Method: 该方法采用多任务图神经网络（GNN），联合预测声部连接、五线谱分配、音高拼写、调号、符干方向、八度移位和谱号标志，并通过后处理流程生成可打印的MusicXML/MEI输出。

Result: 在J-Pop和DCML Romantic两种钢琴数据集上的综合评估表明，统一模型在所有子任务上均表现出较高的准确率，优于仅专注于特定子任务的现有系统。

Conclusion: 多任务GNN框架为自动音乐雕刻提供了可扩展且高效的解决方案，表明共享编码器与轻量级任务特定解码器的组合具有显著优势。

Abstract: This paper focuses on automatic music engraving, i.e., the creation of a
humanly-readable musical score from musical content. This step is fundamental
for all applications that include a human player, but it remains a mostly
unexplored topic in symbolic music processing. In this work, we formalize the
problem as a collection of interdependent subtasks, and propose a unified graph
neural network (GNN) framework that targets the case of piano music and
quantized symbolic input. Our method employs a multi-task GNN to jointly
predict voice connections, staff assignments, pitch spelling, key signature,
stem direction, octave shifts, and clef signs. A dedicated postprocessing
pipeline generates print-ready MusicXML/MEI outputs. Comprehensive evaluation
on two diverse piano corpora (J-Pop and DCML Romantic) demonstrates that our
unified model achieves good accuracy across all subtasks, compared to existing
systems that only specialize in specific subtasks. These results indicate that
a shared GNN encoder with lightweight task-specific decoders in a multi-task
setting offers a scalable and effective solution for automatic music engraving.

</details>


### [2] [AJAHR: Amputated Joint Aware 3D Human Mesh Recovery](https://arxiv.org/abs/2509.19939)
*Hyunjin Cho,Giyun Choi,Jongwon Choi*

Main category: cs.GR

TL;DR: 论文提出了一种名为AJAHR的自适应姿势估计框架，专注于改进截肢者的三维人体网格重建，并引入了合成数据集A3D以支持训练。


<details>
  <summary>Details</summary>
Motivation: 现有的人体网格恢复方法假设标准人体结构，忽视了截肢等多样性解剖条件，导致对截肢者的应用存在偏差。数据集的稀缺性进一步加剧了这一问题。

Method: AJAHR框架结合了身体部位截肢分类器，与网格恢复网络联合训练以检测潜在截肢情况。同时引入了合成数据集A3D。

Result: 在非截肢者上保持竞争力的同时，该方法在截肢者的网格重建上达到了最先进的性能。

Conclusion: AJAHR框架有效解决了截肢者的三维人体网格恢复问题，并为相关领域提供了实用的数据集和方法。

Abstract: Existing human mesh recovery methods assume a standard human body structure,
overlooking diverse anatomical conditions such as limb loss. This assumption
introduces bias when applied to individuals with amputations - a limitation
further exacerbated by the scarcity of suitable datasets. To address this gap,
we propose Amputated Joint Aware 3D Human Mesh Recovery (AJAHR), which is an
adaptive pose estimation framework that improves mesh reconstruction for
individuals with limb loss. Our model integrates a body-part amputation
classifier, jointly trained with the mesh recovery network, to detect potential
amputations. We also introduce Amputee 3D (A3D), which is a synthetic dataset
offering a wide range of amputee poses for robust training. While maintaining
competitive performance on non-amputees, our approach achieves state-of-the-art
results for amputated individuals. Additional materials can be found at the
project webpage.

</details>


### [3] [MeshMosaic: Scaling Artist Mesh Generation via Local-to-Global Assembly](https://arxiv.org/abs/2509.19995)
*Rui Xu,Tianyang Xue,Qiujie Dong,Le Wan,Zhe Zhu,Peng Li,Zhiyang Dou,Cheng Lin,Shiqing Xin,Yuan Liu,Wenping Wang,Taku Komura*

Main category: cs.GR

TL;DR: MeshMosaic是一种新颖的局部到全局框架，用于艺术家设计的网格生成，能够扩展到超过10万个三角形，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有的自回归生成模型在高三角形数量的艺术家设计网格生成中存在长序列瓶颈和量化分辨率限制的问题，无法忠实再现精细几何细节和结构化密度模式。

Method: MeshMosaic首先将形状分割为补丁，自回归生成每个补丁，并通过共享边界条件促进邻近区域的连贯性、对称性和无缝连接。

Result: 在多个公开数据集上的实验表明，MeshMosaic在几何保真度和用户偏好方面显著优于最先进的方法。

Conclusion: MeshMosaic支持卓越的细节表示和实际应用中的网格生成，为高分辨率网格的可扩展性提供了有效解决方案。

Abstract: Scaling artist-designed meshes to high triangle numbers remains challenging
for autoregressive generative models. Existing transformer-based methods suffer
from long-sequence bottlenecks and limited quantization resolution, primarily
due to the large number of tokens required and constrained quantization
granularity. These issues prevent faithful reproduction of fine geometric
details and structured density patterns. We introduce MeshMosaic, a novel
local-to-global framework for artist mesh generation that scales to over 100K
triangles--substantially surpassing prior methods, which typically handle only
around 8K faces. MeshMosaic first segments shapes into patches, generating each
patch autoregressively and leveraging shared boundary conditions to promote
coherence, symmetry, and seamless connectivity between neighboring regions.
This strategy enhances scalability to high-resolution meshes by quantizing
patches individually, resulting in more symmetrical and organized mesh density
and structure. Extensive experiments across multiple public datasets
demonstrate that MeshMosaic significantly outperforms state-of-the-art methods
in both geometric fidelity and user preference, supporting superior detail
representation and practical mesh generation for real-world applications.

</details>


### [4] [KSDiff: Keyframe-Augmented Speech-Aware Dual-Path Diffusion for Facial Animation](https://arxiv.org/abs/2509.20128)
*Tianle Lyu,Junchuan Zhao,Ye Wang*

Main category: cs.GR

TL;DR: KSDiff是一种基于关键帧增强和语音感知的双路径扩散框架，通过分离语音特征中的表情和头部姿态信息，并结合关键帧预测，显著提升了说话头部动画的同步性和自然性。


<details>
  <summary>Details</summary>
Motivation: 现有方法通常将语音特征作为一个整体处理，未能捕捉到细粒度驱动的面部运动差异，同时忽略了动态关键帧的重要性。

Method: KSDiff框架包含双路径语音编码器（DPSE）和自回归关键帧学习模块（KEL），分别用于分离语音特征和预测关键帧，并通过双路径运动生成器合成面部动画。

Result: 在HDTF和VoxCeleb数据集上的实验表明，KSDiff在唇部同步准确性和头部姿态自然性方面均达到最优性能。

Conclusion: KSDiff通过结合语音特征分离和关键帧感知扩散模型，显著提升了说话头部动画的生成质量。

Abstract: Audio-driven facial animation has made significant progress in multimedia
applications, with diffusion models showing strong potential for talking-face
synthesis. However, most existing works treat speech features as a monolithic
representation and fail to capture their fine-grained roles in driving
different facial motions, while also overlooking the importance of modeling
keyframes with intense dynamics. To address these limitations, we propose
KSDiff, a Keyframe-Augmented Speech-Aware Dual-Path Diffusion framework.
Specifically, the raw audio and transcript are processed by a Dual-Path Speech
Encoder (DPSE) to disentangle expression-related and head-pose-related
features, while an autoregressive Keyframe Establishment Learning (KEL) module
predicts the most salient motion frames. These components are integrated into a
Dual-path Motion generator to synthesize coherent and realistic facial motions.
Extensive experiments on HDTF and VoxCeleb demonstrate that KSDiff achieves
state-of-the-art performance, with improvements in both lip synchronization
accuracy and head-pose naturalness. Our results highlight the effectiveness of
combining speech disentanglement with keyframe-aware diffusion for talking-head
generation.

</details>


### [5] [LidarScout: Direct Out-of-Core Rendering of Massive Point Clouds](https://arxiv.org/abs/2509.20198)
*Philipp Erler,Lukas Herzberger,Michael Wimmer,Markus Schütz*

Main category: cs.GR

TL;DR: 该论文提出了一种无需预处理的方法，能够即时可视化包含数百亿点的大规模地形扫描数据，通过加载稀疏子样本、表面重建和动态加载全分辨率数据实现实时交互。


<details>
  <summary>Details</summary>
Motivation: 大规模地形扫描数据集的点云数据体积庞大，即使是基本的查看任务也需要数小时到数天的预处理，导致无法实时交互。论文旨在解决这一瓶颈。

Method: 论文的方法包括初始加载稀疏子样本点，生成概览；随后进行表面重建以生成高质量无空洞的高度图；根据用户视角动态加载和卸载全分辨率点云数据。

Result: 该方法实现了对海量点云数据集的直接外核渲染，无需预处理或额外磁盘空间，支持实时交互和高质量可视化。

Conclusion: 该方法为大规模点云数据的实时可视化提供了一种高效解决方案，克服了传统预处理方法的瓶颈，适用于多种实际应用场景。

Abstract: Large-scale terrain scans are the basis for many important tasks, such as
topographic mapping, forestry, agriculture, and infrastructure planning. The
resulting point cloud data sets are so massive in size that even basic tasks
like viewing take hours to days of pre-processing in order to create
level-of-detail structures that allow inspecting the data set in their entirety
in real time. In this paper, we propose a method that is capable of instantly
visualizing massive country-sized scans with hundreds of billions of points.
Upon opening the data set, we first load a sparse subsample of points and
initialize an overview of the entire point cloud, immediately followed by a
surface reconstruction process to generate higher-quality, hole-free
heightmaps. As users start navigating towards a region of interest, we continue
to prioritize the heightmap construction process to the user's viewpoint. Once
a user zooms in closely, we load the full-resolution point cloud data for that
region and update the corresponding height map textures with the
full-resolution data. As users navigate elsewhere, full-resolution point data
that is no longer needed is unloaded, but the updated heightmap textures are
retained as a form of medium level of detail. Overall, our method constitutes a
form of direct out-of-core rendering for massive point cloud data sets
(terabytes, compressed) that requires no preprocessing and no additional disk
space. Source code, executable, pre-trained model, and dataset are available
at: https://github.com/cg-tuwien/lidarscout

</details>


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [6] [Macro-embedding Compiler Intermediate Languages in Racket](https://arxiv.org/abs/2509.19607)
*William J. Bowman*

Main category: cs.PL

TL;DR: 本文介绍了一种在Racket中嵌入从类Scheme语言到x86-64的编译器中间语言家族的设计与实现，用于编译器课程的测试框架。


<details>
  <summary>Details</summary>
Motivation: 探讨语言导向技术和抽象，以实现（1）大规模语言家族和（2）高低级语言间的互操作性。

Method: 通过宏扩展将开放语言特征模块化和组合化嵌入单一宿主语言（Racket），支持功能性和不安全特征的交互。

Result: 实现了高代码复用和互操作性，简化了中间语言语义开发，并支持特征扩展和多接口暴露。

Conclusion: 该方法通过宿主语言和中间语言的复用与互操作，显著提升了开发效率和灵活性。

Abstract: We present the design and implementation of a macro-embedding of a family of
compiler intermediate languages, from a Scheme-like language to x86-64, into
Racket. This embedding is used as part of a testing framework for a compilers
course to derive interpreters for all the intermediate languages. The embedding
implements features including safe, functional abstractions as well as unsafe
assembly features, and the interactions between the two at various intermediate
stages.
  This paper aims to demonstrate language-oriented techniques and abstractions
for implementing (1) a large family of languages and (2) interoperability
between low- and high-level languages. The primary strength of this approach is
the high degree of code reuse and interoperability compared to implementing
each interpreter separately. The design emphasizes modularity and
compositionality of an open set of language features by local macro expansion
into a single host language, rather than implementing a language pre-defined by
a closed set of features. This enables reuse from both the host language
(Racket) and between intermediate languages, and enables interoperability
between high- and low-level features, simplifying development of the
intermediate language semantics. It also facilitates extending or redefining
individual language features in intermediate languages, and exposing multiple
interfaces to the embedded languages.

</details>


### [7] [Compilation as Multi-Language Semantics](https://arxiv.org/abs/2509.19613)
*William J. Bowman*

Main category: cs.PL

TL;DR: 该论文提出了一种统一建模编译器的方法，将其视为多语言语义中的开放项归约系统，减少了传统方法中的重复定义，并为编译和互操作性语义提供了有趣的见解。


<details>
  <summary>Details</summary>
Motivation: 传统多语言语义中，编译器建模需要为每个编译通道定义两个变体：编译的语法翻译和运行时翻译。这种方法存在重复定义的缺点。

Method: 论文提出了一种新的方法，将编译器统一建模为多语言语义中的开放项归约系统，而非语法翻译。这一方法通过归一化跨语言归约来表示AOT编译，通过多语言评估来表示JIT编译。

Result: 该方法减少了传统方法中的重复定义，同时提供了编译器正确性和安全性证明的思路，并通过主题归约确保编译器保持类型不变性。

Conclusion: 统一建模方法不仅简化了编译器的定义，还为编译器的正确性和安全性提供了新的证明路径。

Abstract: Modeling interoperability between programs in different languages is a key
problem when modeling verified and secure compilation, which has been
successfully addressed using multi-language semantics. Unfortunately, existing
models of compilation using multi-language semantics define two variants of
each compiler pass: a syntactic translation on open terms to model compilation,
and a run-time translation of closed terms at multi-language boundaries to
model interoperability.
  In this talk, I discuss work-in-progress approach to uniformly model a
compiler entirely as a reduction system on open term in a multi-language
semantics, rather than as a syntactic translation. This simultaneously defines
the compiler and the interoperability semantics, reducing duplication. It also
provides interesting semantic insights. Normalization of the cross-language
redexes performs ahead-of-time (AOT) compilation. Evaluation in the
multi-language models just-in-time (JIT) compilation. Confluence of
multi-language reduction implies compiler correctness, and part of the secure
compilation proof (full abstraction), enabling focus on the difficult part of
the proof. Subject reduction of the multi-language reduction implies
type-preservation of the compiler.

</details>


### [8] [The Syntax and Semantics of einsum](https://arxiv.org/abs/2509.20020)
*Maurice Wenig,Paul G. Rump,Mark Blacher,Joachim Giesen*

Main category: cs.PL

TL;DR: 本文探讨了einsum符号的理论基础，提出了一个正式的einsum语言定义，并证明了张量表达式的重要等价规则及其实际应用价值。


<details>
  <summary>Details</summary>
Motivation: 尽管einsum符号在实践中广泛应用于机器学习、量子电路模拟等领域，但其缺乏坚实的理论基础，且在不同框架中未统一，限制了形式化推理和系统优化的机会。

Method: 本文讨论了张量表达式的术语学，并提出了einsum语言的正式定义。在此基础上，形式化并证明了张量表达式的重要等价规则。

Result: 通过形式化einsum语言，证明了张量表达式的重要等价规则，并揭示了这些规则在实际应用中的价值。

Conclusion: 本文填补了einsum符号的理论空白，为其形式化推理和优化提供了基础，对实际应用具有重要意义。

Abstract: In 2011, einsum was introduced to NumPy as a practical and convenient
notation for tensor expressions in machine learning, quantum circuit
simulation, and other fields. It has since been implemented in additional
Python frameworks such as PyTorch and TensorFlow, as well as in other
programming languages such as Julia. Despite its practical success, the einsum
notation still lacks a solid theoretical basis, and is not unified across the
different frameworks, limiting opportunities for formal reasoning and
systematic optimization. In this work, we discuss the terminology of tensor
expressions and provide a formal definition of the einsum language. Based on
this definition, we formalize and prove important equivalence rules for tensor
expressions and highlight their relevance in practical applications.

</details>


<div id='cs.GT'></div>

# cs.GT [[Back]](#toc)

### [9] [Choose Your Battles: Distributed Learning Over Multiple Tug of War Games](https://arxiv.org/abs/2509.20147)
*Siddharth Chandak,Ilai Bistritz,Nicholas Bambos*

Main category: cs.GT

TL;DR: 本文提出了一种称为“Meta Tug-of-Peace”的分布式算法，用于解决多玩家参与的多个游戏中的动态决策问题，确保玩家达到目标服务质量奖励。


<details>
  <summary>Details</summary>
Motivation: 研究动机是为多玩家同时参与多个竞争性游戏（如电力控制、传感器网络中的任务分配等）提供一种分布式解决方案，确保玩家在动态环境中达到均衡状态。

Method: 方法是通过一种简单的随机逼近算法更新玩家动作，并采用低频1位通信机制决定游戏切换，设计了Meta Tug-of-Peace算法。

Result: 结果表明，该算法能够在Meta-ToW游戏中收敛到一个满足目标服务质量奖励向量的均衡状态，并通过仿真验证了其有效性。

Conclusion: 结论是Meta Tug-of-Peace算法在多玩家动态竞争游戏中具有高效性和实用性，适用于多种现实场景。

Abstract: Consider N players and K games taking place simultaneously. Each of these
games is modeled as a Tug-of-War (ToW) game where increasing the action of one
player decreases the reward for all other players. Each player participates in
only one game at any given time. At each time step, a player decides the game
in which they wish to participate in and the action they take in that game.
Their reward depends on the actions of all players that are in the same game.
This system of K games is termed `Meta Tug-of-War' (Meta-ToW) game. These games
can model scenarios such as power control, distributed task allocation, and
activation in sensor networks. We propose the Meta Tug-of-Peace algorithm, a
distributed algorithm where the action updates are done using a simple
stochastic approximation algorithm, and the decision to switch games is made
using an infrequent 1-bit communication between the players. We prove that in
Meta-ToW games, our algorithm converges to an equilibrium that satisfies a
target Quality of Service reward vector for the players. We then demonstrate
the efficacy of our algorithm through simulations for the scenarios mentioned
above.

</details>


### [10] [A Novel Framework for Honey-X Deception in Zero-Sum Games](https://arxiv.org/abs/2509.20329)
*Brendan Gould,Kyriakos Vamvoudakis*

Main category: cs.GT

TL;DR: 本文提出了一种新颖的博弈论模型，用于研究双人零和博弈中的欺骗行为。


<details>
  <summary>Details</summary>
Motivation: 研究动机在于探讨信息不对称情况下，欺骗者如何通过策略性地修改收益信息来最大化自身利益，同时约束欺骗的幅度。

Method: 方法包括将最优欺骗策略建模为一个双层优化问题，并提供精确解和高效计算高质量可行解的方法。

Result: 结果表明，该方法在受蜜罐欺骗启发的数值示例中表现出高效性。

Conclusion: 结论指出该框架为博弈论中的欺骗行为提供了新的理论分析和实用工具。

Abstract: In this paper, we present a novel, game-theoretic model of deception in
two-player, zero-sum games. Our framework leverages an information asymmetry:
one player (the deceiver) has access to accurate payoff information, while the
other (the victim) observes a modified version of these payoffs due to the
deception strategy employed. The deceiver's objective is to choose a
deception-action pair that optimally exploits the victim's best response to the
altered payoffs, subject to a constraint on the deception's magnitude. We
characterize the optimal deceptive strategy as the solution to a bi-level
optimization problem, and we provide both an exact solution and an efficient
method for computing a high-quality feasible point. Finally, we demonstrate the
effectiveness of our approach on numerical examples inspired by honeypot
deception.

</details>
