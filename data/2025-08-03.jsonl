{"id": "2507.23002", "categories": ["cs.GR", "cs.CR", "cs.CV"], "pdf": "https://arxiv.org/pdf/2507.23002", "abs": "https://arxiv.org/abs/2507.23002", "authors": ["Peter F. Michael", "Zekun Hao", "Serge Belongie", "Abe Davis"], "title": "Noise-Coded Illumination for Forensic and Photometric Video Analysis", "comment": "ACM Transactions on Graphics (2025), presented at SIGGRAPH 2025", "summary": "The proliferation of advanced tools for manipulating video has led to an arms\nrace, pitting those who wish to sow disinformation against those who want to\ndetect and expose it. Unfortunately, time favors the ill-intentioned in this\nrace, with fake videos growing increasingly difficult to distinguish from real\nones. At the root of this trend is a fundamental advantage held by those\nmanipulating media: equal access to a distribution of what we consider\nauthentic (i.e., \"natural\") video. In this paper, we show how coding very\nsubtle, noise-like modulations into the illumination of a scene can help combat\nthis advantage by creating an information asymmetry that favors verification.\nOur approach effectively adds a temporal watermark to any video recorded under\ncoded illumination. However, rather than encoding a specific message, this\nwatermark encodes an image of the unmanipulated scene as it would appear lit\nonly by the coded illumination. We show that even when an adversary knows that\nour technique is being used, creating a plausible coded fake video amounts to\nsolving a second, more difficult version of the original adversarial content\ncreation problem at an information disadvantage. This is a promising avenue for\nprotecting high-stakes settings like public events and interviews, where the\ncontent on display is a likely target for manipulation, and while the\nillumination can be controlled, the cameras capturing video cannot."}
{"id": "2507.23777", "categories": ["cs.GR", "cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.23777", "abs": "https://arxiv.org/abs/2507.23777", "authors": ["Dian Chen", "Yansong Qu", "Xinyang Li", "Ming Li", "Shengchuan Zhang"], "title": "XSpecMesh: Quality-Preserving Auto-Regressive Mesh Generation Acceleration via Multi-Head Speculative Decoding", "comment": null, "summary": "Current auto-regressive models can generate high-quality, topologically\nprecise meshes; however, they necessitate thousands-or even tens of\nthousands-of next-token predictions during inference, resulting in substantial\nlatency. We introduce XSpecMesh, a quality-preserving acceleration method for\nauto-regressive mesh generation models. XSpecMesh employs a lightweight,\nmulti-head speculative decoding scheme to predict multiple tokens in parallel\nwithin a single forward pass, thereby accelerating inference. We further\npropose a verification and resampling strategy: the backbone model verifies\neach predicted token and resamples any tokens that do not meet the quality\ncriteria. In addition, we propose a distillation strategy that trains the\nlightweight decoding heads by distilling from the backbone model, encouraging\ntheir prediction distributions to align and improving the success rate of\nspeculative predictions. Extensive experiments demonstrate that our method\nachieves a 1.7x speedup without sacrificing generation quality. Our code will\nbe released."}
{"id": "2507.23151", "categories": ["cs.PL", "cs.LO"], "pdf": "https://arxiv.org/pdf/2507.23151", "abs": "https://arxiv.org/abs/2507.23151", "authors": ["Louis Rustenholz", "Pedro Lopez-Garcia", "Manuel V. Hermenegildo"], "title": "Abstractions of Sequences, Functions and Operators", "comment": "Under consideration for publication in STTT", "summary": "We present theoretical and practical results on the order theory of lattices\nof functions, focusing on Galois connections that abstract (sets of) functions\n- a topic known as higher-order abstract interpretation.\n  We are motivated by the challenge of inferring closed-form bounds on\nfunctions which are defined recursively, i.e. as the fixed point of an operator\nor, equivalently, as the solution to a functional equation. This has multiple\napplications in program analysis (e.g. cost analysis, loop acceleration,\ndeclarative language analysis) and in hybrid systems governed by differential\nequations.\n  Our main contribution is a new family of constraint-based abstract domains\nfor abstracting numerical functions, B-bound domains, which abstract a function\nf by a conjunction of bounds from a preselected set of boundary functions. They\nallow inferring highly non-linear numerical invariants, which classical\nnumerical abstract domains struggle with. We uncover a convexity property in\nthe constraint space that simplifies, and, in some cases, fully automates,\ntransfer function design.\n  We also introduce domain abstraction, a functor that lifts arbitrary mappings\nin value space to Galois connections in function space. This supports\nabstraction from symbolic to numerical functions (i.e. size abstraction), and\nenables dimensionality reduction of equations.\n  We base our constructions of transfer functions on a simple operator\nlanguage, starting with sequences, and extending to more general functions,\nincluding multivariate, piecewise, and non-discrete domains."}
{"id": "2507.23149", "categories": ["cs.GT", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2507.23149", "abs": "https://arxiv.org/abs/2507.23149", "authors": ["Ruifan Yang", "Manxi Wu"], "title": "Learning with Episodic Hypothesis Testing in General Games: A Framework for Equilibrium Selection", "comment": null, "summary": "We introduce a new hypothesis testing-based learning dynamics in which\nplayers update their strategies by combining hypothesis testing with\nutility-driven exploration. In this dynamics, each player forms beliefs about\nopponents' strategies and episodically tests these beliefs using empirical\nobservations. Beliefs are resampled either when the hypothesis test is rejected\nor through exploration, where the probability of exploration decreases with the\nplayer's (transformed) utility. In general finite normal-form games, we show\nthat the learning process converges to a set of approximate Nash equilibria\nand, more importantly, to a refinement that selects equilibria maximizing the\nminimum (transformed) utility across all players. Our result establishes\nconvergence to equilibrium in general finite games and reveals a novel\nmechanism for equilibrium selection induced by the structure of the learning\ndynamics."}
{"id": "2507.23205", "categories": ["cs.PL", "cs.SE"], "pdf": "https://arxiv.org/pdf/2507.23205", "abs": "https://arxiv.org/abs/2507.23205", "authors": ["Hebi Li", "Forrest Sheng Bao", "Qi Xiao", "Jin Tian"], "title": "Kernel-FFI: Transparent Foreign Function Interfaces for Interactive Notebooks", "comment": null, "summary": "Foreign Function Interfaces (FFIs) are essential for enabling\ninteroperability between programming languages, yet existing FFI solutions are\nill-suited for the dynamic, interactive workflows prevalent in modern notebook\nenvironments such as Jupyter. Current approaches require extensive manual\nconfiguration, introduce significant boilerplate, and often lack support for\nrecursive calls and object-oriented programming (OOP) constructs-features\ncritical for productive, multi-language development.\n  We present Kernel-FFI, a transparent, language-agnostic framework that\nenables seamless cross-language function calls and object manipulation within\ninteractive notebooks. Kernel-FFI employs source-level transformation to\nautomatically rewrite cross-language invocations, eliminating the need for\nmanual bindings or boilerplate. Kernel-FFI provides robust support for OOP by\nenabling foreign object referencing and automatic resource management across\nlanguage boundaries. Furthermore, to address the blocking nature of Jupyter\nkernels and support recursive and asynchronous foreign calls, we introduce a\nnovel side-channel communication mechanism. Our tool will be open-sourced and\navailable at https://codepod.io/docs/kernel-ffi"}
{"id": "2507.23500", "categories": ["cs.GT", "cs.DS"], "pdf": "https://arxiv.org/pdf/2507.23500", "abs": "https://arxiv.org/abs/2507.23500", "authors": ["Michal Feldman", "Simon Mauras", "Divyarthi Mohan", "Rebecca Reiffenh√§user"], "title": "Online Combinatorial Allocation with Interdependent Values", "comment": null, "summary": "We study online combinatorial allocation problems in the secretary setting,\nunder interdependent values. In the interdependent model, introduced by Milgrom\nand Weber (1982), each agent possesses a private signal that captures her\ninformation about an item for sale, and the value of every agent depends on the\nsignals held by all agents. Mauras, Mohan, and Reiffenh\\\"auser (2024) were the\nfirst to study interdependent values in online settings, providing\nconstant-approximation guarantees for secretary settings, where agents arrive\nonline along with their signals and values, and the goal is to select the agent\nwith the highest value.\n  In this work, we extend this framework to {\\em combinatorial} secretary\nproblems, where agents have interdependent valuations over {\\em bundles} of\nitems, introducing additional challenges due to both combinatorial structure\nand interdependence. We provide $2e$-competitive algorithms for a broad class\nof valuation functions, including submodular and XOS functions, matching the\napproximation guarantees in the single-choice secretary setting. Furthermore,\nour results cover the same range of valuation classes for which constant-factor\nalgorithms exist in classical (non-interdependent) secretary settings, while\nincurring only an additional factor of $2$ due to interdependence. Finally, we\nextend our study to strategic settings, and provide a $4e$-competitive truthful\nmechanism for online bipartite matching with interdependent valuations, again\nmeeting the frontier of what is known, even without interdependence."}
