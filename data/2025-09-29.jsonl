{"id": "2509.21629", "categories": ["cs.PL", "cs.AI", "cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.21629", "abs": "https://arxiv.org/abs/2509.21629", "authors": ["Anjiang Wei", "Tarun Suresh", "Tianran Sun", "Haoze Wu", "Ke Wang", "Alex Aiken"], "title": "InvBench: Can LLMs Accelerate Program Verification with Invariant Synthesis?", "comment": null, "summary": "Program verification relies on loop invariants, yet automatically discovering\nstrong invariants remains a long-standing challenge. We introduce a principled\nframework for evaluating LLMs on invariant synthesis. Our approach uses a\nverifier-based decision procedure with a formal soundness guarantee and\nassesses not only correctness but also the speedup that invariants provide in\nverification. We evaluate 7 state-of-the-art LLMs, and existing LLM-based\nverifiers against the traditional solver UAutomizer. While LLM-based verifiers\nrepresent a promising direction, they do not yet offer a significant advantage\nover UAutomizer. Model capability also proves critical, as shown by sharp\ndifferences in speedups across models, and our benchmark remains an open\nchallenge for current LLMs. Finally, we show that supervised fine-tuning and\nBest-of-N sampling can improve performance: fine-tuning on 3589 instances\nraises the percentage of speedup cases for Qwen3-Coder-480B from 8% to 29.2%,\nand Best-of-N sampling with N=16 improves Claude-sonnet-4 from 8.8% to 22.1%."}
{"id": "2509.21793", "categories": ["cs.PL", "cs.CL"], "pdf": "https://arxiv.org/pdf/2509.21793", "abs": "https://arxiv.org/abs/2509.21793", "authors": ["Jianhong Zhao", "Everett Hildenbrandt", "Juan Conejero", "Yongwang Zhao"], "title": "Compiling by Proving: Language-Agnostic Automatic Optimization from Formal Semantics", "comment": null, "summary": "Verification proofs encode complete program behavior, yet we discard them\nafter checking correctness. We present compiling by proving, a paradigm that\ntransforms these proofs into optimized execution rules. By constructing\nAll-Path Reachability Proofs through symbolic execution and compiling their\ngraph structure, we consolidate many semantic rewrites into single rules while\npreserving correctness by construction. We implement this as a\nlanguage-agnostic extension to the K framework. Evaluation demonstrates\nperformance improvements across different compilation scopes: opcode-level\noptimizations show consistent speedups, while whole-program compilation\nachieves orders of magnitude greater performance gains."}
{"id": "2509.22614", "categories": ["cs.PL", "D.3.1; F.3.2; D.3.2; D.3.3"], "pdf": "https://arxiv.org/pdf/2509.22614", "abs": "https://arxiv.org/abs/2509.22614", "authors": ["Dmitri Volkov", "Yafei Yang", "Chung-chieh Shan"], "title": "Committing to the bit: Relational programming with semiring arrays and SAT solving", "comment": "12 pages, for associated repo see\n  https://github.com/sporkl/semiringkanren", "summary": "We propose semiringKanren, a relational programming language where each\nrelation expression denotes a semiring array. We formalize a type system that\nrestricts the arrays to finite size. We then define a semantics that is\nparameterized by the semiring that the arrays draw their elements from. We\ncompile semiringKanren types to bitstring representations. For the Boolean\nsemiring, this compilation enables us to use an SAT solver to run\nsemiringKanren programs efficiently. We compare the performance of\nsemiringKanren and faster miniKanren for solving Sudoku puzzles. Our experiment\nshows that semiringKanren can be a more efficient variant of miniKanren."}
{"id": "2509.21570", "categories": ["cs.GT", "quant-ph", "91A05, 81Q93 (Primary) 68Q32, 91A26, 37N40 (Secondary)"], "pdf": "https://arxiv.org/pdf/2509.21570", "abs": "https://arxiv.org/abs/2509.21570", "authors": ["Yiheng Su", "Emmanouil-Vasileios Vlatakis-Gkaragkounis", "Pucheng Xiong"], "title": "Breaking $1/Îµ$ Barrier in Quantum Zero-Sum Games: Generalizing Metric Subregularity for Spectraplexes", "comment": "29 pages", "summary": "Long studied as a toy model, quantum zero-sum games have recently resurfaced\nas a canonical playground for modern areas such as non-local games, quantum\ninteractive proofs, and quantum machine learning. In this simple yet\nfundamental setting, two competing quantum players send iteratively mixed\nquantum states to a referee, who performs a joint measurement to determine\ntheir payoffs. In 2025, Vasconcelos et al. [arXiv:2311.10859] connected quantum\ncommunication channels with a hierarchy of quantum optimization algorithms that\ngeneralize Matrix Multiplicative Weights Update ($\\texttt{MMWU}$) through\nextra-gradient mechanisms, establishing an average-iterate convergence rate of\n$\\mathcal{O}(1/\\epsilon)$ iterations to $\\epsilon$-Nash equilibria. While a\nlong line of work has shown that bilinear games over polyhedral domains admit\ngradient methods with linear last-iterate convergence rates of\n$\\mathcal{O}(\\log(1/\\epsilon))$, it has been conjectured that a fundamental\nperformance gap must persist between quantum feasible sets (spectraplexes) and\nclassical polyhedral sets (simplices). We resolve this conjecture in the\nnegative. We prove that matrix variants of $\\textit{Nesterov's iterative\nsmoothing}$ ($\\texttt{IterSmooth}$) and $\\textit{Optimistic Gradient\nDescent-Ascent}$ ($\\texttt{OGDA}$) achieve last-iterate convergence at a linear\nrate in quantum zero-sum games, thereby matching the classical polyhedral case.\nOur analysis relies on a new generalization of error bounds in semidefinite\nprogramming geometry, establishing that (SP-MS) holds for monotone operators\nover spectrahedra, despite their uncountably many extreme points. Finally, as a\nbyproduct, we obtain an exponential speed-up over the classical Jain-Watrous\n[arXiv:0808.2775] method for parallel approximation of strictly positive\nsemidefinite programs."}
{"id": "2509.21541", "categories": ["cs.GR", "cs.CV", "I.3; I.2; I.4"], "pdf": "https://arxiv.org/pdf/2509.21541", "abs": "https://arxiv.org/abs/2509.21541", "authors": ["Weikai Lin", "Haoxiang Li", "Yuhao Zhu"], "title": "ControlHair: Physically-based Video Diffusion for Controllable Dynamic Hair Rendering", "comment": "9 pages,Project website: https://ctrlhair-arxiv.netlify.app/", "summary": "Hair simulation and rendering are challenging due to complex strand dynamics,\ndiverse material properties, and intricate light-hair interactions. Recent\nvideo diffusion models can generate high-quality videos, but they lack\nfine-grained control over hair dynamics. We present ControlHair, a hybrid\nframework that integrates a physics simulator with conditional video diffusion\nto enable controllable dynamic hair rendering. ControlHair adopts a three-stage\npipeline: it first encodes physics parameters (e.g., hair stiffness, wind) into\nper-frame geometry using a simulator, then extracts per-frame control signals,\nand finally feeds control signals into a video diffusion model to generate\nvideos with desired hair dynamics. This cascaded design decouples physics\nreasoning from video generation, supports diverse physics, and makes training\nthe video diffusion model easy. Trained on a curated 10K video dataset,\nControlHair outperforms text- and pose-conditioned baselines, delivering\nprecisely controlled hair dynamics. We further demonstrate three use cases of\nControlHair: dynamic hairstyle try-on, bullet-time effects, and cinemagraphic.\nControlHair introduces the first physics-informed video diffusion framework for\ncontrollable dynamics. We provide a teaser video and experimental results on\nour website."}
{"id": "2509.21612", "categories": ["cs.GT"], "pdf": "https://arxiv.org/pdf/2509.21612", "abs": "https://arxiv.org/abs/2509.21612", "authors": ["Ariel D. Procaccia", "Han Shao", "Itai Shapira"], "title": "Incentives in Federated Learning with Heterogeneous Agents", "comment": null, "summary": "Federated learning promises significant sample-efficiency gains by pooling\ndata across multiple agents, yet incentive misalignment is an obstacle: each\nupdate is costly to the contributor but boosts every participant. We introduce\na game-theoretic framework that captures heterogeneous data: an agent's utility\ndepends on who supplies each sample, not just how many. Agents aim to meet a\nPAC-style accuracy threshold at minimal personal cost. We show that\nuncoordinated play yields pathologies: pure equilibria may not exist, and the\nbest equilibrium can be arbitrarily more costly than cooperation. To steer\ncollaboration, we analyze the cost-minimizing contribution vector, prove that\ncomputing it is NP-hard, and derive a polynomial-time linear program that\nachieves a logarithmic approximation. Finally, pairing the LP with a simple\npay-what-you-contribute rule - each agent receives a payment equal to its\nsample cost - yields a mechanism that is strategyproof and, within the class of\ncontribution-based transfers, is unique."}
{"id": "2509.21702", "categories": ["cs.GR", "I.3; I.4"], "pdf": "https://arxiv.org/pdf/2509.21702", "abs": "https://arxiv.org/abs/2509.21702", "authors": ["Weikai Lin", "Sushant Kondguli", "Carl Marshall", "Yuhao Zhu"], "title": "PowerGS: Display-Rendering Power Co-Optimization for Neural Rendering in Power-Constrained XR Systems", "comment": "10 pages, Accepted to Siggraph Asia 2025", "summary": "3D Gaussian Splatting (3DGS) combines classic image-based rendering,\npointbased graphics, and modern differentiable techniques, and offers an\ninteresting alternative to traditional physically-based rendering. 3DGS-family\nmodels are far from efficient for power-constrained Extended Reality (XR)\ndevices, which need to operate at a Watt-level. This paper introduces PowerGS,\nthe first framework to jointly minimize the rendering and display power in 3DGS\nunder a quality constraint. We present a general problem formulation and show\nthat solving the problem amounts to 1) identifying the iso-quality curve(s) in\nthe landscape subtended by the display and rendering power and 2) identifying\nthe power-minimal point on a given curve, which has a closed-form solution\ngiven a proper parameterization of the curves. PowerGS also readily supports\nfoveated rendering for further power savings. Extensive experiments and user\nstudies show that PowerGS achieves up to 86% total power reduction compared to\nstate-of-the-art 3DGS models, with minimal loss in both subjective and\nobjective quality. Code is available at\nhttps://github.com/horizon-research/PowerGS."}
{"id": "2509.22563", "categories": ["cs.GT", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.22563", "abs": "https://arxiv.org/abs/2509.22563", "authors": ["Simone Di Gregorio", "Paul DÃ¼tting", "Federico Fusco", "Chris Schwiegelshohn"], "title": "Nearly Tight Regret Bounds for Profit Maximization in Bilateral Trade", "comment": "Accept at FOCS '25", "summary": "Bilateral trade models the task of intermediating between two strategic\nagents, a seller and a buyer, willing to trade a good for which they hold\nprivate valuations. We study this problem from the perspective of a broker, in\na regret minimization framework. At each time step, a new seller and buyer\narrive, and the broker has to propose a mechanism that is incentive-compatible\nand individually rational, with the goal of maximizing profit.\n  We propose a learning algorithm that guarantees a nearly tight\n$\\tilde{O}(\\sqrt{T})$ regret in the stochastic setting when seller and buyer\nvaluations are drawn i.i.d. from a fixed and possibly correlated unknown\ndistribution. We further show that it is impossible to achieve sublinear regret\nin the non-stationary scenario where valuations are generated upfront by an\nadversary. Our ambitious benchmark for these results is the best\nincentive-compatible and individually rational mechanism. This separates us\nfrom previous works on efficiency maximization in bilateral trade, where the\nbenchmark is a single number: the best fixed price in hindsight.\n  A particular challenge we face is that uniform convergence for all\nmechanisms' profits is impossible. We overcome this difficulty via a careful\nchaining analysis that proves convergence for a provably near-optimal mechanism\nat (essentially) optimal rate. We further showcase the broader applicability of\nour techniques by providing nearly optimal results for the joint ads problem."}
{"id": "2509.22222", "categories": ["cs.GR", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2509.22222", "abs": "https://arxiv.org/abs/2509.22222", "authors": ["Jinhyeok Kim", "Jaehun Bang", "Seunghyun Seo", "Kyungdon Joo"], "title": "Rigidity-Aware 3D Gaussian Deformation from a Single Image", "comment": "10 pages, 11 figures, conference", "summary": "Reconstructing object deformation from a single image remains a significant\nchallenge in computer vision and graphics. Existing methods typically rely on\nmulti-view video to recover deformation, limiting their applicability under\nconstrained scenarios. To address this, we propose DeformSplat, a novel\nframework that effectively guides 3D Gaussian deformation from only a single\nimage. Our method introduces two main technical contributions. First, we\npresent Gaussian-to-Pixel Matching which bridges the domain gap between 3D\nGaussian representations and 2D pixel observations. This enables robust\ndeformation guidance from sparse visual cues. Second, we propose Rigid Part\nSegmentation consisting of initialization and refinement. This segmentation\nexplicitly identifies rigid regions, crucial for maintaining geometric\ncoherence during deformation. By combining these two techniques, our approach\ncan reconstruct consistent deformations from a single image. Extensive\nexperiments demonstrate that our approach significantly outperforms existing\nmethods and naturally extends to various applications,such as frame\ninterpolation and interactive object manipulation."}
{"id": "2509.22227", "categories": ["cs.GR", "cs.CV"], "pdf": "https://arxiv.org/pdf/2509.22227", "abs": "https://arxiv.org/abs/2509.22227", "authors": ["Weidan Xiong", "Bochuan Zeng", "Ziyu Hu", "Jianwei Guo", "Ke Xie", "Hui Huang"], "title": "Aerial Path Planning for Urban Geometry and Texture Co-Capture", "comment": "ACM TOG and SIGGRAPH Asia 2025 (Patent Protected); Project page:\n  https://vcc.tech/research/2025/DroneTex", "summary": "Recent advances in image acquisition and scene reconstruction have enabled\nthe generation of high-quality structural urban scene geometry, given\nsufficient site information. However, current capture techniques often overlook\nthe crucial importance of texture quality, resulting in noticeable visual\nartifacts in the textured models. In this work, we introduce the urban geometry\nand texture co-capture problem under limited prior knowledge before a site\nvisit. The only inputs are a 2D building contour map of the target area and a\nsafe flying altitude above the buildings. We propose an innovative aerial path\nplanning framework designed to co-capture images for reconstructing both\nstructured geometry and high-fidelity textures. To evaluate and guide view\nplanning, we introduce a comprehensive texture quality assessment system,\nincluding two novel metrics tailored for building facades. Firstly, our method\ngenerates high-quality vertical dipping views and horizontal planar views to\neffectively capture both geometric and textural details. A multi-objective\noptimization strategy is then proposed to jointly maximize texture fidelity,\nimprove geometric accuracy, and minimize the cost associated with aerial views.\nFurthermore, we present a sequential path planning algorithm that accounts for\ntexture consistency during image capture. Extensive experiments on large-scale\nsynthetic and real-world urban datasets demonstrate that our approach\neffectively produces image sets suitable for concurrent geometric and texture\nreconstruction, enabling the creation of realistic, textured scene proxies at\nlow operational cost."}
{"id": "2509.22442", "categories": ["cs.GR", "cs.AI", "cs.LG", "cs.RO"], "pdf": "https://arxiv.org/pdf/2509.22442", "abs": "https://arxiv.org/abs/2509.22442", "authors": ["Pei Xu", "Zhen Wu", "Ruocheng Wang", "Vishnu Sarukkai", "Kayvon Fatahalian", "Ioannis Karamouzas", "Victor Zordan", "C. Karen Liu"], "title": "Learning to Ball: Composing Policies for Long-Horizon Basketball Moves", "comment": "ACM Transactions on Graphics (Proceedings of SIGGRAPH Asia 2025).\n  Website: http://pei-xu.github.io/basketball. Video:\n  https://youtu.be/2RBFIjjmR2I. Code: https://github.com/xupei0610/basketball", "summary": "Learning a control policy for a multi-phase, long-horizon task, such as\nbasketball maneuvers, remains challenging for reinforcement learning approaches\ndue to the need for seamless policy composition and transitions between skills.\nA long-horizon task typically consists of distinct subtasks with well-defined\ngoals, separated by transitional subtasks with unclear goals but critical to\nthe success of the entire task. Existing methods like the mixture of experts\nand skill chaining struggle with tasks where individual policies do not share\nsignificant commonly explored states or lack well-defined initial and terminal\nstates between different phases. In this paper, we introduce a novel policy\nintegration framework to enable the composition of drastically different motor\nskills in multi-phase long-horizon tasks with ill-defined intermediate states.\nBased on that, we further introduce a high-level soft router to enable seamless\nand robust transitions between the subtasks. We evaluate our framework on a set\nof fundamental basketball skills and challenging transitions. Policies trained\nby our approach can effectively control the simulated character to interact\nwith the ball and accomplish the long-horizon task specified by real-time user\ncommands, without relying on ball trajectory references."}
