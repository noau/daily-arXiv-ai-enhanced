{"id": "2602.12973", "categories": ["cs.PL"], "pdf": "https://arxiv.org/pdf/2602.12973", "abs": "https://arxiv.org/abs/2602.12973", "authors": ["Federico Bruzzone", "Walter Cazzola"], "title": "Meta-Monomorphizing Specializations", "comment": "31 pages", "summary": "Achieving zero-cost specialization remains a fundamental challenge in programming language and compiler design. It often necessitates trade-offs between expressive power and type system soundness, as the interaction between conditional compilation and static dispatch can easily lead to unforeseen coherence violations and increased complexity in the formal model. This paper introduces meta-monomorphizing specializations, a novel framework that achieves specialization by repurposing monomorphization through compile-time metaprogramming. Instead of modifying the host compiler, our approach generates meta-monomorphized traits and implementations that encode specialization constraints directly into the type structure, enabling deterministic, coherent dispatch without overlapping instances. We formalize this method for first-order, predicate-based, and higher-ranked polymorphic specialization, also in presence of lifetime parameters. Our evaluation, based on a Rust implementation using only existing macro facilities, demonstrates that meta-monomorphization enables expressive specialization patterns -- previously rejected by the compiler -- while maintaining full compatibility with standard optimization pipelines. We show that specialization can be realized as a disciplined metaprogramming layer, offering a practical, language-agnostic path to high-performance abstraction. A comprehensive study of public Rust codebases further validates our approach, revealing numerous workarounds that meta-monomorphization can eliminate, leading to more idiomatic and efficient code.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a\"\u5143\u5355\u6001\u5316\u7279\u5316\"\u7684\u65b0\u6846\u67b6\uff0c\u901a\u8fc7\u7f16\u8bd1\u65f6\u4ee3\u7801\u751f\u6210\u91cd\u65b0\u5229\u7528\u5355\u6001\u5316\uff0c\u4ece\u800c\u5728\u4e0d\u4fee\u6539\u4e3b\u673a\u7f16\u8bd1\u5668\u7684\u60c5\u51b5\u4e0b\u5b9e\u73b0\u96f6\u6210\u672c\u7279\u5316\u3002", "motivation": "\u5728\u7f16\u7a0b\u8bed\u8a00\u548c\u7f16\u8bd1\u5668\u8bbe\u8ba1\u4e2d\uff0c\u5b9e\u73b0\u96f6\u6210\u672c\u7279\u5316\u4e00\u76f4\u662f\u4e00\u4e2a\u57fa\u672c\u6311\u6218\uff0c\u901a\u5e38\u9700\u8981\u5728\u8868\u8fbe\u80fd\u529b\u548c\u7c7b\u578b\u7cfb\u7edf\u5b8c\u5907\u6027\u4e4b\u95f4\u8fdb\u884c\u6743\u8861\u3002\u4f20\u7edf\u65b9\u6cd5\u53ef\u80fd\u5bfc\u81f4\u610f\u5916\u7684\u8fde\u8d2f\u6027\u8fdd\u53cd\u548c\u5f62\u5f0f\u6a21\u578b\u590d\u6742\u6027\u7684\u589e\u52a0\u3002", "method": "\u672c\u6587\u5f15\u5165\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u6846\u67b6\uff0c\u5373\"\u5143\u5355\u6001\u5316\u7279\u5316\"\uff0c\u901a\u8fc7\u7f16\u8bd1\u65f6\u4ee3\u7801\u751f\u6210\u91cd\u65b0\u5229\u7528\u5355\u6001\u5316\u3002\u8be5\u65b9\u6cd5\u751f\u6210\u5143\u5355\u6001\u5316\u7684\u7279\u8d28\u548c\u5b9e\u73b0\uff0c\u5c06\u7279\u5316\u7ea6\u675f\u76f4\u63a5\u7f16\u7801\u5230\u7c7b\u578b\u7ed3\u6784\u4e2d\uff0c\u4ece\u800c\u5b9e\u73b0\u786e\u5b9a\u6027\u3001\u8fde\u8d2f\u7684\u6d3e\u53d1\u3002", "result": "\u57fa\u4e8eRust\u5b9e\u73b0\u7684\u8bc4\u4f30\u8868\u660e\uff0c\u5143\u5355\u6001\u5316\u80fd\u591f\u652f\u6301\u4e4b\u524d\u88ab\u7f16\u8bd1\u5668\u62d2\u7edd\u7684\u8868\u8fbe\u6027\u7279\u5316\u6a21\u5f0f\uff0c\u540c\u65f6\u5b8c\u5168\u517c\u5bb9\u6807\u51c6\u7684\u4f18\u5316\u6d41\u7a0b\u3002\u516c\u5f00Rust\u4ee3\u7801\u5e93\u7684\u7efc\u5408\u7814\u7a76\u8fdb\u4e00\u6b65\u9a8c\u8bc1\u4e86\u8be5\u65b9\u6cd5\uff0c\u63ed\u793a\u4e86\u8bb8\u591a\u53ef\u88ab\u6d88\u9664\u7684\u53d8\u901a\u65b9\u6cd5\u3002", "conclusion": "\u672c\u6587\u8868\u660e\u7279\u5316\u53ef\u4ee5\u901a\u8fc7\u4e00\u79cd\u6709\u7eaa\u5f8b\u7684\u4ee3\u7801\u751f\u6210\u5c42\u5b9e\u73b0\uff0c\u4e3a\u9ad8\u6027\u80fd\u62bd\u8c61\u63d0\u4f9b\u4e86\u4e00\u79cd\u5b9e\u7528\u7684\u3001\u4e0e\u8bed\u8a00\u65e0\u5173\u7684\u8def\u5f84\u3002"}}
{"id": "2602.12349", "categories": ["cs.GR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.12349", "abs": "https://arxiv.org/abs/2602.12349", "authors": ["Joao Teixeira", "Eitan Grinspun", "Otman Benchekroun"], "title": "Variational Green's Functions for Volumetric PDEs", "comment": null, "summary": "Green's functions characterize the fundamental solutions of partial differential equations; they are essential for tasks ranging from shape analysis to physical simulation, yet they remain computationally prohibitive to evaluate on arbitrary geometric discretizations. We present Variational Green's Function (VGF), a method that learns a smooth, differentiable representation of the Green's function for linear self-adjoint PDE operators, including the Poisson, the screened Poisson, and the biharmonic equations. To resolve the sharp singularities characteristic of the Green's functions, our method decomposes the Green's function into an analytic free-space component, and a learned corrector component. Our method leverages a variational foundation to impose Neumann boundary conditions naturally, and imposes Dirichlet boundary conditions via a projective layer on the output of the neural field. The resulting Green's functions are fast to evaluate, differentiable with respect to source application, and can be conditioned on other signals parameterizing our geometry.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aVariational Green's Function (VGF)\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u5b66\u4e60Green\u51fd\u6570\u7684\u5e73\u6ed1\u53ef\u5fae\u5206\u8868\u793a\uff0c\u89e3\u51b3\u4e86\u5176\u5728\u4efb\u610f\u51e0\u4f55\u79bb\u6563\u5316\u4e0a\u8ba1\u7b97\u6210\u672c\u9ad8\u7684\u95ee\u9898\u3002", "motivation": "Green\u51fd\u6570\u5728\u504f\u5fae\u5206\u65b9\u7a0b\u7684\u57fa\u7840\u89e3\u548c\u5e94\u7528\u4e2d\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u5728\u4efb\u610f\u51e0\u4f55\u79bb\u6563\u5316\u4e0a\u7684\u8ba1\u7b97\u6210\u672c\u8fc7\u9ad8\uff0c\u9650\u5236\u4e86\u5176\u5e7f\u6cdb\u5e94\u7528\u3002", "method": "VGF\u65b9\u6cd5\u5c06Green\u51fd\u6570\u5206\u89e3\u4e3a\u89e3\u6790\u7684\u81ea\u7531\u7a7a\u95f4\u5206\u91cf\u548c\u5b66\u4e60\u7684\u6821\u6b63\u5206\u91cf\uff0c\u5229\u7528\u53d8\u5206\u57fa\u7840\u81ea\u7136\u65bd\u52a0Neumann\u8fb9\u754c\u6761\u4ef6\uff0c\u5e76\u901a\u8fc7\u6295\u5f71\u5c42\u5904\u7406Dirichlet\u8fb9\u754c\u6761\u4ef6\u3002", "result": "\u6240\u5f97\u5230\u7684Green\u51fd\u6570\u8ba1\u7b97\u901f\u5ea6\u5feb\uff0c\u5bf9\u6e90\u5e94\u7528\u53ef\u5fae\u5206\uff0c\u5e76\u80fd\u57fa\u4e8e\u51e0\u4f55\u53c2\u6570\u7684\u5176\u4ed6\u4fe1\u53f7\u8fdb\u884c\u6761\u4ef6\u5316\u3002", "conclusion": "VGF\u65b9\u6cd5\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u4e14\u7075\u6d3b\u7684\u65b9\u5f0f\u6765\u8868\u793a\u548c\u8ba1\u7b97Green\u51fd\u6570\uff0c\u9002\u7528\u4e8e\u591a\u79cd\u7ebf\u6027\u81ea\u4f34PDE\u7b97\u5b50\u7684\u6c42\u89e3\u3002"}}
{"id": "2602.12359", "categories": ["cs.GT"], "pdf": "https://arxiv.org/pdf/2602.12359", "abs": "https://arxiv.org/abs/2602.12359", "authors": ["Daniel Halpern", "Alexandros Psomas", "Shirley Zhang"], "title": "Truthful Fair Division under Stochastic Valuations", "comment": null, "summary": "We study no-money mechanisms for allocating indivisible items to strategic agents with additive preferences under a stochastic model. In this model, items' values are drawn from an underlying distribution and mechanisms are evaluated with respect to this draw (e.g., in expectation, or with high probability). Motivated by worst-case impossibilities which show that truthfulness severely restricts fairness and efficiency, we ask whether truthful mechanisms continue to perform poorly on random instances.\n  We first focus on dominant-strategy incentive compatible (DSIC) mechanisms. For two agents, we obtain a tight picture. Specifically, we show that there exists a distribution under which no DSIC mechanism achieves an expected welfare approximation better than $\\frac{2+\\sqrt{2}}{4}\\approx 0.854$, and we give a DSIC mechanism that matches this bound for all distributions simultaneously. We further show that, for every distribution, there exists a DSIC mechanism that is envy-free with high probability and obtains the same welfare. A key ingredient is a new, tight connection between welfare guarantees of a family of DSIC, no-money mechanisms and i.i.d.\\ prophet inequalities. This connection allows us to generalize to $n$ agents; in particular, we obtain a DSIC mechanism that achieves a $\\approx 0.745$ approximation to welfare, and another DSIC mechanism achieving a $1/2$-approximation welfare that is envy-free with high probability.\n  We then turn to Bayesian incentive compatibility (BIC). Under i.i.d.\\ valuations, we show that BIC comes at essentially no cost: we design a prior-independent BIC mechanism that achieves a $(1-\\varepsilon)$-approximation to the optimal welfare, while being envy-free with high probability. Under independent but non-identical priors, we obtain BIC mechanisms that are $(1-\\varepsilon)$-approximately Pareto efficient and envy-free with high probability.", "AI": {"tldr": "\u7814\u7a76\u5728\u968f\u673a\u6a21\u578b\u4e2d\uff0c\u9488\u5bf9\u5177\u6709\u52a0\u6027\u504f\u597d\u7684\u7b56\u7565\u6027\u4ee3\u7406\u4eba\u5206\u914d\u4e0d\u53ef\u5206\u5272\u7269\u54c1\u7684\u65e0\u91d1\u94b1\u673a\u5236\uff0c\u63a2\u8ba8\u5176\u516c\u5e73\u6027\u548c\u6548\u7387\u3002", "motivation": "\u7531\u4e8e\u6700\u574f\u60c5\u51b5\u4e0b\u7684\u4e0d\u53ef\u80fd\u6027\u8868\u660e\u771f\u5b9e\u6027\u4e25\u91cd\u9650\u5236\u4e86\u516c\u5e73\u6027\u548c\u6548\u7387\uff0c\u56e0\u6b64\u7814\u7a76\u771f\u5b9e\u6027\u673a\u5236\u5728\u968f\u673a\u5b9e\u4f8b\u4e2d\u662f\u5426\u8868\u73b0\u4e0d\u4f73\u3002", "method": "\u9996\u5148\u7814\u7a76\u652f\u914d\u7b56\u7565\u6fc0\u52b1\u517c\u5bb9\uff08DSIC\uff09\u673a\u5236\uff0c\u5efa\u7acb\u798f\u5229\u8fd1\u4f3c\u7684\u65b0\u8fde\u63a5\uff0c\u5e76\u5c06\u5176\u63a8\u5e7f\u5230n\u4e2a\u4ee3\u7406\u4eba\uff1b\u7136\u540e\u8f6c\u5411\u8d1d\u53f6\u65af\u6fc0\u52b1\u517c\u5bb9\uff08BIC\uff09\u673a\u5236\u3002", "result": "\u53d1\u73b0DSIC\u673a\u5236\u7684\u9884\u671f\u798f\u5229\u8fd1\u4f3c\u4e0a\u9650\u4e3a$\\\\frac{2+\\\\sqrt{2}}{4}$\uff0c\u5e76\u63d0\u51fa\u5339\u914d\u8be5\u8fb9\u754c\u7684\u673a\u5236\uff1bBIC\u673a\u5236\u5728i.i.d.\u4f30\u503c\u4e0b\u51e0\u4e4e\u65e0\u6210\u672c\u5730\u5b9e\u73b0\u9ad8\u6548\u7387\u548c\u516c\u5e73\u6027\u3002", "conclusion": "\u771f\u5b9e\u6027\u673a\u5236\u5728\u968f\u673a\u6a21\u578b\u4e2d\u53ef\u4ee5\u5b9e\u73b0\u8f83\u9ad8\u7684\u798f\u5229\u548c\u516c\u5e73\u6027\uff0c\u5c24\u5176\u662f\u5728BIC\u6846\u67b6\u4e0b\u8868\u73b0\u66f4\u4e3a\u4f18\u8d8a\u3002"}}
{"id": "2602.12949", "categories": ["cs.GR"], "pdf": "https://arxiv.org/pdf/2602.12949", "abs": "https://arxiv.org/abs/2602.12949", "authors": ["Arno Coomans", "Giacomo Nazzaro", "Edoardo A. Dominici", "Christian D\u00f6ring", "Floor Verhoeven", "Konstantinos Vardis", "Markus Steinberger"], "title": "Real-time Rendering with a Neural Irradiance Volume", "comment": "Accepted at Eurographics 2026", "summary": "Rendering diffuse global illumination in real-time is often approximated by pre-computing and storing irradiance in a 3D grid of probes. As long as most of the scene remains static, probes approximate irradiance for all surfaces immersed in the irradiance volume, including novel dynamic objects. This approach, however, suffers from aliasing artifacts and high memory consumption. We propose Neural Irradiance Volume (NIV), a neural-based technique that allows accurate real-time rendering of diffuse global illumination via a compact pre-computed model, overcoming the limitations of traditional probe-based methods, such as the expensive memory footprint, aliasing artifacts, and scene-specific heuristics. The key insight is that neural compression creates an adaptive and amortized representation of irradiance, circumventing the cubic scaling of grid-based methods. Our superior memory-scaling improves quality by at least 10x at the same memory budget, and enables a straightforward representation of higher-dimensional irradiance fields, allowing rendering of time-varying or dynamic effects without requiring additional computation at runtime. Unlike other neural rendering techniques, our method works within strict real-time constraints, providing fast inference (around 1 ms per frame on consumer GPUs at full HD resolution), reduced memory usage (1-5 MB for medium-sized scenes), and only requires a G-buffer as input, without expensive ray tracing or denoising.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aNIV\u7684\u795e\u7ecf\u6e32\u67d3\u6280\u672f\uff0c\u901a\u8fc7\u795e\u7ecf\u538b\u7f29\u65b9\u6cd5\u5b9e\u65f6\u6e32\u67d3\u6f2b\u53cd\u5c04\u5168\u5c40\u5149\u7167\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u57fa\u4e8e\u63a2\u9488\u65b9\u6cd5\u7684\u5185\u5b58\u6d88\u8017\u548c\u4f2a\u5f71\u95ee\u9898\u3002", "motivation": "\u4f20\u7edf\u7684\u57fa\u4e8e3D\u63a2\u9488\u7684\u6f2b\u53cd\u5c04\u5168\u5c40\u5149\u7167\u5b9e\u65f6\u6e32\u67d3\u65b9\u6cd5\u5b58\u5728\u4f2a\u5f71\u548c\u9ad8\u5185\u5b58\u6d88\u8017\u7684\u95ee\u9898\uff0c\u9700\u8981\u65b0\u7684\u6280\u672f\u6765\u514b\u670d\u8fd9\u4e9b\u9650\u5236\u3002", "method": "\u4f7f\u7528\u795e\u7ecf\u538b\u7f29\u6280\u672f\u521b\u5efa\u81ea\u9002\u5e94\u7684\u8f90\u7167\u5ea6\u8868\u793a\uff0c\u907f\u514d\u4e86\u57fa\u4e8e\u7f51\u683c\u65b9\u6cd5\u7684\u7acb\u65b9\u7f29\u653e\u95ee\u9898\uff0c\u5e76\u5728\u4e25\u683c\u5b9e\u65f6\u7ea6\u675f\u4e0b\u5de5\u4f5c\u3002", "result": "\u5728\u76f8\u540c\u5185\u5b58\u9884\u7b97\u4e0b\uff0cNIV\u65b9\u6cd5\u7684\u6e32\u67d3\u8d28\u91cf\u63d0\u5347\u4e86\u81f3\u5c1110\u500d\uff0c\u4e14\u80fd\u9ad8\u6548\u5904\u7406\u65f6\u95f4\u53d8\u5316\u6216\u52a8\u6001\u6548\u679c\uff0c\u63a8\u7406\u901f\u5ea6\u5feb\uff08\u7ea61\u6beb\u79d2\u6bcf\u5e27\uff09\u3002", "conclusion": "NIV\u6280\u672f\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u3001\u4f4e\u5185\u5b58\u5360\u7528\u4e14\u9ad8\u8d28\u91cf\u7684\u5b9e\u65f6\u5168\u5c40\u5149\u7167\u6e32\u67d3\u65b9\u6848\uff0c\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\u548c\u5176\u4ed6\u795e\u7ecf\u6e32\u67d3\u6280\u672f\u3002"}}
{"id": "2602.12481", "categories": ["cs.GT"], "pdf": "https://arxiv.org/pdf/2602.12481", "abs": "https://arxiv.org/abs/2602.12481", "authors": ["Gagan Aggarwal", "Yifan Wang", "Mingfei Zhao"], "title": "Online Advertising with Spatial Interactions", "comment": null, "summary": "Online advertising platforms must decide how to allocate multiple ads across limited screen real estate, where each ad's effectiveness depends not only on its own placement but also on nearby ads competing for user attention. Such spatial externalities - arising from proximity, clutter, or crowding - can significantly alter welfare and revenue outcomes, yet existing auction and allocation models typically treat ad slots as independent or ordered along a single dimension.\n  We introduce a new framework for spatial externalities in online advertising, in which the value of an ad depends on both its slot and the configuration of surrounding ads. We model ad slots as points in a metric space, and model an advertiser's value as a function of both their bid and a discount factor determined by the configuration of other displayed ads. Within this framework, we analyze two natural models. For the Nearest-Neighbor model, where the value suppression depends only on the closest neighboring ad, we present a polynomial-time algorithm that achieves a constant approximation for the general case. We show that the allocation rule is monotone and can be implemented as a truthful mechanism. For a structured setting of 2D Euclidean space, we provide a PTAS. In contrast, for the Product-Distance model, where interference is aggregated multiplicatively across all neighbors, we establish a strong (and nearly-tight) hardness of approximation - no polynomial-time algorithm can achieve any polynomial-factor approximation unless P=NP, via a reduction from Max-Independent-Set.\n  Our results provide a foundation for reasoning about spatial externalities in ad allocation and for designing efficient, truthful mechanisms under such interactions.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u5728\u7ebf\u5e7f\u544a\u7a7a\u95f4\u5916\u90e8\u6027\u6846\u67b6\uff0c\u8003\u8651\u4e86\u5e7f\u544a\u4f4d\u7f6e\u548c\u5468\u56f4\u5e7f\u544a\u914d\u7f6e\u7684\u5f71\u54cd\uff0c\u5e76\u63d0\u51fa\u4e86\u4e24\u79cd\u6a21\u578b\u7684\u5206\u6790\u548c\u7b97\u6cd5\u3002", "motivation": "\u5728\u7ebf\u5e7f\u544a\u5e73\u53f0\u9700\u8981\u5728\u6709\u9650\u7684\u5c4f\u5e55\u7a7a\u95f4\u5185\u5206\u914d\u591a\u4e2a\u5e7f\u544a\uff0c\u800c\u5e7f\u544a\u7684\u6548\u679c\u4e0d\u4ec5\u53d6\u51b3\u4e8e\u81ea\u8eab\u4f4d\u7f6e\uff0c\u8fd8\u53d7\u5468\u56f4\u5e7f\u544a\u7ade\u4e89\u7684\u5f71\u54cd\u3002\u73b0\u6709\u7684\u62cd\u5356\u548c\u5206\u914d\u6a21\u578b\u901a\u5e38\u5c06\u5e7f\u544a\u4f4d\u89c6\u4e3a\u72ec\u7acb\u6216\u4e00\u7ef4\u6392\u5e8f\uff0c\u5ffd\u7565\u4e86\u7a7a\u95f4\u5916\u90e8\u6027\u7684\u5f71\u54cd\u3002", "method": "\u8bba\u6587\u5c06\u5e7f\u544a\u4f4d\u5efa\u6a21\u4e3a\u5ea6\u91cf\u7a7a\u95f4\u4e2d\u7684\u70b9\uff0c\u5e7f\u544a\u4ef7\u503c\u7531\u5176\u51fa\u4ef7\u548c\u5468\u56f4\u5e7f\u544a\u914d\u7f6e\u51b3\u5b9a\u7684\u6298\u6263\u56e0\u5b50\u5171\u540c\u51b3\u5b9a\u3002\u63d0\u51fa\u4e86\u4e24\u79cd\u6a21\u578b\uff1a\u6700\u8fd1\u90bb\u6a21\u578b\uff08Nearest-Neighbor\uff09\u548c\u4e58\u79ef\u8ddd\u79bb\u6a21\u578b\uff08Product-Distance\uff09\uff0c\u5e76\u5206\u522b\u5206\u6790\u4e86\u5b83\u4eec\u7684\u8fd1\u4f3c\u7b97\u6cd5\u548c\u8ba1\u7b97\u590d\u6742\u6027\u3002", "result": "\u5bf9\u4e8e\u6700\u8fd1\u90bb\u6a21\u578b\uff0c\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u591a\u9879\u5f0f\u65f6\u95f4\u7b97\u6cd5\uff0c\u5b9e\u73b0\u4e86\u6052\u5b9a\u8fd1\u4f3c\u6bd4\uff0c\u5e76\u4e14\u5206\u914d\u89c4\u5219\u5355\u8c03\uff0c\u53ef\u5b9e\u73b0\u771f\u5b9e\u673a\u5236\u3002\u5728\u4e8c\u7ef4\u6b27\u51e0\u91cc\u5f97\u7a7a\u95f4\u7684\u7279\u6b8a\u60c5\u51b5\u4e0b\uff0c\u63d0\u4f9b\u4e86PTAS\u3002\u800c\u5bf9\u4e8e\u4e58\u79ef\u8ddd\u79bb\u6a21\u578b\uff0c\u8bc1\u660e\u4e86\u9664\u975eP=NP\uff0c\u5426\u5219\u4e0d\u5b58\u5728\u591a\u9879\u5f0f\u56e0\u5b50\u7684\u8fd1\u4f3c\u7b97\u6cd5\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u4e3a\u7406\u89e3\u5e7f\u544a\u5206\u914d\u4e2d\u7684\u7a7a\u95f4\u5916\u90e8\u6027\u63d0\u4f9b\u4e86\u7406\u8bba\u57fa\u7840\uff0c\u5e76\u4e3a\u8bbe\u8ba1\u9ad8\u6548\u3001\u771f\u5b9e\u7684\u673a\u5236\u63d0\u4f9b\u4e86\u65b9\u5411\u3002"}}
{"id": "2602.12583", "categories": ["cs.GT"], "pdf": "https://arxiv.org/pdf/2602.12583", "abs": "https://arxiv.org/abs/2602.12583", "authors": ["Yulong He", "Dutao Zhang", "Sergey Kovalchuk", "Pengyi Li", "Artem Sedakov"], "title": "Opinion dynamics and mutual influence with LLM agents through dialog simulation", "comment": null, "summary": "A fundamental challenge in opinion dynamics research is the scarcity of real-world longitudinal opinion data, which complicates the validation of theoretical models. To address this, we propose a novel simulation framework using large language model (LLM) agents in structured multi-round dialogs. Each agent's dialog history is iteratively updated with its own previously stated opinions and those of others analogous to the classical DeGroot model. Furthermore, by retaining each agent's initial opinion throughout the dialog, we simulate anchoring effects consistent with the Friedkin-Johnsen model of opinion dynamics. Our framework thus bridges classical opinion dynamics models and modern multi-agent LLM systems, providing a scalable tool for simulating and analyzing opinion formation when real-world data is limited or inaccessible.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u4f7f\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u4ee3\u7406\u7684\u591a\u8f6e\u5bf9\u8bdd\u6a21\u62df\u6846\u67b6\uff0c\u4ee5\u89e3\u51b3\u610f\u89c1\u52a8\u6001\u7814\u7a76\u4e2d\u771f\u5b9e\u4e16\u754c\u7eb5\u5411\u6570\u636e\u7a00\u7f3a\u7684\u95ee\u9898\u3002", "motivation": "\u610f\u89c1\u52a8\u6001\u7814\u7a76\u9762\u4e34\u7684\u4e00\u4e2a\u57fa\u672c\u6311\u6218\u662f\u7f3a\u4e4f\u771f\u5b9e\u4e16\u754c\u7684\u7eb5\u5411\u610f\u89c1\u6570\u636e\uff0c\u8fd9\u4f7f\u5f97\u7406\u8bba\u6a21\u578b\u7684\u9a8c\u8bc1\u53d8\u5f97\u590d\u6742\u3002", "method": "\u901a\u8fc7\u5728\u591a\u8f6e\u7ed3\u6784\u5316\u5bf9\u8bdd\u4e2d\u4f7f\u7528LLM\u4ee3\u7406\uff0c\u6a21\u62df\u4e86\u610f\u89c1\u52a8\u6001\u7684\u8fed\u4ee3\u66f4\u65b0\u8fc7\u7a0b\uff0c\u5e76\u7ed3\u5408\u4e86DeGroot\u6a21\u578b\u548cFriedkin-Johnsen\u6a21\u578b\u7684\u7279\u6027\u3002", "result": "\u8be5\u6846\u67b6\u5728\u7f3a\u4e4f\u771f\u5b9e\u6570\u636e\u7684\u60c5\u51b5\u4e0b\uff0c\u63d0\u4f9b\u4e86\u4e00\u79cd\u53ef\u6269\u5c55\u7684\u5de5\u5177\u6765\u6a21\u62df\u548c\u5206\u6790\u610f\u89c1\u5f62\u6210\u8fc7\u7a0b\u3002", "conclusion": "\u8fd9\u4e00\u7814\u7a76\u5c06\u7ecf\u5178\u610f\u89c1\u52a8\u6001\u6a21\u578b\u4e0e\u73b0\u4ee3\u591a\u4ee3\u7406LLM\u7cfb\u7edf\u76f8\u7ed3\u5408\uff0c\u4e3a\u89e3\u51b3\u6570\u636e\u7a00\u7f3a\u95ee\u9898\u63d0\u4f9b\u4e86\u65b0\u7684\u601d\u8def\u548c\u65b9\u6cd5\u3002"}}
{"id": "2602.12615", "categories": ["cs.GT"], "pdf": "https://arxiv.org/pdf/2602.12615", "abs": "https://arxiv.org/abs/2602.12615", "authors": ["Yao Zhang", "Makoto Yokoo"], "title": "Feature-based Uncertainty Model for School Choice", "comment": "Full version for the paper with the same title at AAMAS 2026", "summary": "In this work, we consider a school choice scenario where a student does not exactly know which college is better for her. Although it is hard for a student to obtain an exact preference, she can usually compare specific features of colleges, such as reputation, location, and campus facilities. Motivated by this, we propose a feature-based uncertainty model for school choice where a student's preference is based on a linear combination of her utilities over different features, and the coefficients of the combination are treated as random variables. Our main goal is to achieve a higher probability of stability (ProS) and incentive compatibility (IC) for students. Unfortunately, these two goals are incompatible in general. We show that a student-proposing deferred acceptance (DA) that prioritizes colleges with higher expected ranking can achieve a worst-case approximation ratio of $(1/n)^n$ on ProS, while a DA with a carefully defined iterated comparison vector can guarantee the strongest achievable form of IC. Finally, we provide additional results for some specific restrictions on the model.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63a2\u8ba8\u4e86\u4e00\u79cd\u57fa\u4e8e\u7279\u5f81\u7684\u5b66\u6821\u9009\u62e9\u4e0d\u786e\u5b9a\u6a21\u578b\uff0c\u65e8\u5728\u63d0\u9ad8\u5b66\u751f\u7684\u7a33\u5b9a\u6027\u6982\u7387\uff08ProS\uff09\u548c\u6fc0\u52b1\u76f8\u5bb9\u6027\uff08IC\uff09\uff0c\u5e76\u5c55\u793a\u4e86\u4e0d\u540c\u7b56\u7565\u5728\u8fd9\u4e24\u4e2a\u76ee\u6807\u4e0a\u7684\u8868\u73b0\u3002", "motivation": "\u5b66\u751f\u5728\u9009\u62e9\u5b66\u6821\u65f6\u5f80\u5f80\u65e0\u6cd5\u51c6\u786e\u6bd4\u8f83\u504f\u597d\u7684\u4f18\u5148\u7ea7\uff0c\u4f46\u53ef\u4ee5\u901a\u8fc7\u6bd4\u8f83\u5b66\u6821\u7684\u7279\u5b9a\u7279\u5f81\uff08\u5982\u58f0\u8a89\u3001\u4f4d\u7f6e\u548c\u8bbe\u65bd\uff09\u6765\u5f62\u6210\u504f\u597d\u3002\u8fd9\u4fc3\u4f7f\u7814\u7a76\u8005\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u7279\u5f81\u7684\u6a21\u578b\u6765\u5904\u7406\u8fd9\u79cd\u4e0d\u786e\u5b9a\u6027\u3002", "method": "\u7814\u7a76\u8005\u63d0\u51fa\u4e86\u4e00\u4e2a\u57fa\u4e8e\u7ebf\u6027\u7ec4\u5408\u7684\u7279\u5f81\u6a21\u578b\uff0c\u5176\u4e2d\u5b66\u751f\u504f\u597d\u7684\u7ec4\u5408\u7cfb\u6570\u88ab\u89c6\u4e3a\u968f\u673a\u53d8\u91cf\u3002\u4ed6\u4eec\u5206\u6790\u4e86\u5b66\u751f\u63d0\u8bae\u7684\u5ef6\u8fdf\u63a5\u53d7\uff08DA\uff09\u7b97\u6cd5\uff0c\u5e76\u63a2\u8ba8\u4e86\u4e24\u79cd\u4e0d\u540c\u7b56\u7565\uff1a\u4e00\u79cd\u662f\u4f18\u5148\u8003\u8651\u671f\u671b\u6392\u540d\u8f83\u9ad8\u7684\u5b66\u6821\uff0c\u53e6\u4e00\u79cd\u662f\u4f7f\u7528\u7cbe\u5fc3\u5b9a\u4e49\u7684\u8fed\u4ee3\u6bd4\u8f83\u5411\u91cf\u3002", "result": "\u7814\u7a76\u8868\u660e\uff0c\u4f18\u5148\u8003\u8651\u671f\u671b\u6392\u540d\u8f83\u9ad8\u7684DA\u7b97\u6cd5\u5728ProS\u4e0a\u53ef\u4ee5\u5b9e\u73b0$(1/n)^n$\u7684\u6700\u574f\u8fd1\u4f3c\u6bd4\uff1b\u800c\u4f7f\u7528\u8fed\u4ee3\u6bd4\u8f83\u5411\u91cf\u7684DA\u7b97\u6cd5\u53ef\u4ee5\u4fdd\u8bc1\u6700\u5f3a\u5f62\u5f0f\u7684IC\u3002\u6b64\u5916\uff0c\u6a21\u578b\u5728\u67d0\u4e9b\u7279\u5b9a\u9650\u5236\u4e0b\u8fd8\u63d0\u4f9b\u4e86\u989d\u5916\u7ed3\u679c\u3002", "conclusion": "\u7814\u7a76\u5f97\u51fa\u7ed3\u8bba\uff0c\u5728\u4e00\u822c\u60c5\u51b5\u4e0b\uff0cProS\u548cIC\u662f\u76f8\u4e92\u51b2\u7a81\u7684\u76ee\u6807\u3002\u7136\u800c\uff0c\u901a\u8fc7\u4e0d\u540c\u7684DA\u7b56\u7565\uff0c\u53ef\u4ee5\u5728\u4e00\u5b9a\u7a0b\u5ea6\u4e0a\u5e73\u8861\u6216\u4f18\u5316\u8fd9\u4e24\u4e2a\u76ee\u6807\u3002"}}
{"id": "2602.12830", "categories": ["cs.GT", "cs.MA"], "pdf": "https://arxiv.org/pdf/2602.12830", "abs": "https://arxiv.org/abs/2602.12830", "authors": ["Seref Taha Kiremitci", "Ahmed Said Donmez", "Muhammed O. Sayin"], "title": "Decentralized Optimal Equilibrium Learning in Stochastic Games via Single-bit Feedback", "comment": null, "summary": "We study decentralized equilibrium selection in stochastic games under severe information and communication constraints. In such settings, convergence to equilibrium alone is insufficient, as stochastic games typically admit many equilibria with markedly different welfare properties. We address decentralized optimal equilibrium selection, where agents coordinate on equilibria that optimize a designer-specified social welfare objective while allowing heterogeneous tolerance to deviations from strict best responses. Agents observe only the global state trajectory and their realized rewards, and exchange a single randomized bit of feedback per agent per round. This semantic content/discontent signaling mechanism implicitly aligns decentralized learning dynamics with the global welfare objective. We develop explore-and-commit and online variants applicable to general stochastic games, accommodating heterogeneous model-based or model-free methods for solving the induced Markov decision processes, and establish explicit finite-time regret guarantees, showing logarithmic expected regret under mild conditions.", "AI": {"tldr": "\u7814\u7a76\u4e86\u5728\u4e25\u91cd\u4fe1\u606f\u548c\u901a\u4fe1\u7ea6\u675f\u4e0b\u7684\u968f\u673a\u535a\u5f08\u4e2d\u7684\u5206\u6563\u5747\u8861\u9009\u62e9\u95ee\u9898\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u4fe1\u53f7\u673a\u5236\u6765\u4f18\u5316\u793e\u4f1a\u798f\u5229\u76ee\u6807\u3002", "motivation": "\u5728\u968f\u673a\u535a\u5f08\u4e2d\uff0c\u5b58\u5728\u591a\u4e2a\u5747\u8861\u4e14\u5176\u798f\u5229\u6027\u8d28\u5dee\u5f02\u663e\u8457\uff0c\u4ec5\u6536\u655b\u5230\u5747\u8861\u4e0d\u8db3\u4ee5\u89e3\u51b3\u95ee\u9898\u3002\u56e0\u6b64\uff0c\u9700\u8981\u5728\u5206\u6563\u5316\u73af\u5883\u4e2d\u9009\u62e9\u6700\u4f18\u5747\u8861\u3002", "method": "\u91c7\u7528\u5206\u6563\u5316\u7684\u5b66\u4e60\u65b9\u6cd5\uff0c\u4ee3\u7406\u4ec5\u89c2\u5bdf\u5168\u5c40\u72b6\u6001\u8f68\u8ff9\u548c\u81ea\u8eab\u5956\u52b1\uff0c\u5e76\u901a\u8fc7\u6bcf\u8f6e\u4ea4\u6362\u4e00\u4e2a\u968f\u673a\u6bd4\u7279\u7684\u4fe1\u53f7\u6765\u9690\u542b\u5730\u5bf9\u9f50\u793e\u4f1a\u798f\u5229\u76ee\u6807\u3002\u5f00\u53d1\u4e86\u63a2\u7d22-\u63d0\u4ea4\u548c\u5728\u7ebf\u53d8\u4f53\u65b9\u6cd5\u3002", "result": "\u63d0\u51fa\u4e86\u9002\u7528\u4e8e\u4e00\u822c\u968f\u673a\u535a\u5f08\u7684\u6846\u67b6\uff0c\u652f\u6301\u5f02\u6784\u7684\u57fa\u4e8e\u6a21\u578b\u6216\u65e0\u6a21\u578b\u65b9\u6cd5\uff0c\u5e76\u5728\u6e29\u548c\u6761\u4ef6\u4e0b\u5b9e\u73b0\u4e86\u5bf9\u6570\u7ea7\u522b\u7684\u6709\u9650\u65f6\u95f4\u9057\u61be\u4fdd\u969c\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u901a\u8fc7\u7b80\u5355\u7684\u4fe1\u53f7\u673a\u5236\u5b9e\u73b0\u4e86\u5206\u6563\u5316\u7684\u6700\u4f18\u5747\u8861\u9009\u62e9\uff0c\u5e76\u5728\u7406\u8bba\u4e0a\u9a8c\u8bc1\u4e86\u5176\u6027\u80fd\u3002"}}
{"id": "2602.12888", "categories": ["cs.GT"], "pdf": "https://arxiv.org/pdf/2602.12888", "abs": "https://arxiv.org/abs/2602.12888", "authors": ["Bar Light", "Wenyu Wang"], "title": "Experimentation, Biased Learning, and Conjectural Variations in Competitive Dynamic Pricing", "comment": null, "summary": "We study competitive dynamic pricing among multiple sellers, motivated by the rise of large-scale experimentation and algorithmic pricing in retail and online marketplaces. Sellers repeatedly set prices using simple learning rules and observe only their own prices and realized demand, even though demand depends on all sellers' prices and is subject to random shocks. Each seller runs two-point A/B price experiments, in the spirit of switchback-style designs, and updates a baseline price using a linear demand estimate fitted to its own data. Under certain conditions on demand, the resulting dynamics converge to a Conjectural Variations (CV) equilibrium, a classic static equilibrium notion in which each seller best responds under a conjecture that rivals' prices respond systematically to changes in its own price. Unlike standard CV models that treat conjectures as behavioral primitives, we show that these conjectures arise endogenously from the bias in demand learning induced by correlated experimentation (e.g., due to synchronized repricing schedules). This learning bias selects the long-run equilibrium, often leading to supra-competitive prices. Notably, we show that under independent experimentation, this bias vanishes and the learning dynamics converge to the standard Nash equilibrium. We provide simple sufficient conditions on demand for convergence in standard models and establish a finite-sample guarantee: up to logarithmic factors, the squared price error decays on the order of $T^{-1/2}$. Our results imply that in competitive markets, experimentation design can serve as a market design lever, selecting the equilibrium reached by practical learning algorithms.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u591a\u5356\u5bb6\u4e4b\u95f4\u7684\u7ade\u4e89\u6027\u52a8\u6001\u5b9a\u4ef7\u95ee\u9898\uff0c\u63ed\u793a\u4e86\u5b9e\u9a8c\u8bbe\u8ba1\u548c\u5b66\u4e60\u504f\u5dee\u5982\u4f55\u5f71\u54cd\u957f\u671f\u5747\u8861\u7ed3\u679c\u3002", "motivation": "\u7814\u7a76\u52a8\u673a\u6e90\u4e8e\u96f6\u552e\u548c\u5728\u7ebf\u5e02\u573a\u4e2d\u5927\u89c4\u6a21\u5b9e\u9a8c\u548c\u7b97\u6cd5\u5b9a\u4ef7\u7684\u5174\u8d77\u3002\u5356\u5bb6\u901a\u8fc7\u7b80\u5355\u7684\u5b66\u4e60\u89c4\u5219\u53cd\u590d\u5b9a\u4ef7\uff0c\u4f46\u4ec5\u80fd\u89c2\u5bdf\u5230\u81ea\u8eab\u4ef7\u683c\u548c\u9700\u6c42\uff0c\u5c3d\u7ba1\u9700\u6c42\u53d7\u6240\u6709\u5356\u5bb6\u4ef7\u683c\u548c\u968f\u673a\u51b2\u51fb\u7684\u5f71\u54cd\u3002", "method": "\u5356\u5bb6\u91c7\u7528\u4e24\u70b9A/B\u4ef7\u683c\u5b9e\u9a8c\uff0c\u57fa\u4e8e\u81ea\u8eab\u6570\u636e\u62df\u5408\u7ebf\u6027\u9700\u6c42\u4f30\u8ba1\u6765\u66f4\u65b0\u57fa\u51c6\u4ef7\u683c\u3002\u901a\u8fc7\u76f8\u5173\u5b9e\u9a8c\uff08\u5982\u540c\u6b65\u91cd\u65b0\u5b9a\u4ef7\uff09\u5f15\u53d1\u7684\u5b66\u4e60\u504f\u5dee\uff0c\u7814\u7a76\u4e86\u52a8\u6001\u6536\u655b\u5230\u63a8\u6d4b\u53d8\u5316\uff08CV\uff09\u5747\u8861\u7684\u6761\u4ef6\u3002", "result": "\u7ed3\u679c\u8868\u660e\uff0c\u76f8\u5173\u5b9e\u9a8c\u4f1a\u5bfc\u81f4\u5b66\u4e60\u504f\u5dee\u548c\u8d85\u7ade\u4e89\u4ef7\u683c\u7684\u957f\u671f\u5747\u8861\uff1b\u800c\u72ec\u7acb\u5b9e\u9a8c\u5219\u6d88\u9664\u4e86\u504f\u5dee\uff0c\u52a8\u6001\u6536\u655b\u5230\u6807\u51c6\u7eb3\u4ec0\u5747\u8861\u3002\u6b64\u5916\uff0c\u63d0\u4f9b\u4e86\u6536\u655b\u7684\u7b80\u5355\u5145\u5206\u6761\u4ef6\u53ca\u6709\u9650\u6837\u672c\u4fdd\u8bc1\u3002", "conclusion": "\u7ed3\u8bba\u6307\u51fa\uff0c\u5b9e\u9a8c\u8bbe\u8ba1\u53ef\u4f5c\u4e3a\u5e02\u573a\u8bbe\u8ba1\u5de5\u5177\uff0c\u901a\u8fc7\u5b9e\u7528\u7684\u5b66\u4e60\u7b97\u6cd5\u9009\u62e9\u8fbe\u5230\u7684\u5747\u8861\u72b6\u6001\u3002"}}
{"id": "2602.12903", "categories": ["cs.GT", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.12903", "abs": "https://arxiv.org/abs/2602.12903", "authors": ["Romain Cosson", "Federico Fusco", "Anupam Gupta", "Stefano Leonardi", "Renato Paes Leme", "Matteo Russo"], "title": "Contextual Online Bilateral Trade", "comment": null, "summary": "We study repeated bilateral trade when the valuations of the sellers and the buyers are contextual. More precisely, the agents' valuations are given by the inner product of a context vector with two unknown $d$-dimensional vectors -- one for the buyers and one for the sellers.\n  At each time step $t$, the learner receives a context and posts two prices, one for the seller and one for the buyer, and the trade happens if both agents accept their price. We study two objectives for this problem, gain from trade and profit, proving no-regret with respect to a surprisingly strong benchmark: the best omniscient dynamic strategy.\n  In the natural scenario where the learner observes \\emph{separately} whether the agents accept their price -- the so-called \\emph{two-bit} feedback -- we design algorithms that achieve $O(d\\log d)$ regret for gain from trade, and $O(d \\log\\log T + d\\log d)$ regret for profit maximization. Both results are tight, up to the $\\log(d)$ factor, and implement per-step budget balance, meaning that the learner never incurs negative profit.\n  In the less informative \\emph{one-bit} feedback model, the learner only observes whether a trade happens or not. For this scenario, we show that the tight two-bit regret regimes are still attainable, at the cost of allowing the learner to possibly incur a small negative profit of order $O(d\\log d)$, which is notably independent of the time horizon. As a final set of results, we investigate the combination of one-bit feedback and per-step budget balance. There, we design an algorithm for gain from trade that suffers regret independent of the time horizon, but \\emph{exponential} in the dimension $d$. For profit maximization, we maintain this exponential dependence on the dimension, which gets multiplied by a $\\log T$ factor.", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u4e86\u4e0a\u4e0b\u6587\u4f30\u503c\u4e0b\u7684\u91cd\u590d\u53cc\u8fb9\u8d38\u6613\u95ee\u9898\uff0c\u8bbe\u8ba1\u4e86\u5728\u4e0d\u540c\u53cd\u9988\u6a21\u5f0f\u4e0b\u5b9e\u73b0\u4f4e\u540e\u6094\u7684\u7b97\u6cd5\uff0c\u8bc1\u660e\u4e86\u5176\u5728\u6700\u4f18\u52a8\u6001\u7b56\u7565\u4e0b\u7684\u6027\u80fd\u3002", "motivation": "\u7814\u7a76\u52a8\u673a\u662f\u89e3\u51b3\u5728\u52a8\u6001\u4e14\u4e0a\u4e0b\u6587\u76f8\u5173\u7684\u8d38\u6613\u73af\u5883\u4e2d\uff0c\u5982\u4f55\u901a\u8fc7\u6709\u9650\u7684\u53cd\u9988\u4fe1\u606f\uff08\u5982\u4e24\u6bd4\u7279\u6216\u5355\u6bd4\u7279\u53cd\u9988\uff09\u8bbe\u8ba1\u9ad8\u6548\u7684\u5b9a\u4ef7\u7b56\u7565\uff0c\u4ee5\u6700\u5927\u5316\u4ea4\u6613\u589e\u76ca\u6216\u5229\u6da6\u3002", "method": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u4e0a\u4e0b\u6587\u5411\u91cf\u7684\u5b66\u4e60\u65b9\u6cd5\uff0c\u5206\u522b\u8bbe\u8ba1\u4e86\u4e24\u6bd4\u7279\u53cd\u9988\u548c\u5355\u6bd4\u7279\u53cd\u9988\u4e0b\u7684\u7b97\u6cd5\u3002\u4e24\u6bd4\u7279\u53cd\u9988\u4e0b\u7b97\u6cd5\u5b9e\u73b0$O(d\\log d)$\u548c$O(d \\log\\log T + d\\log d)$\u540e\u6094\uff1b\u5355\u6bd4\u7279\u53cd\u9988\u4e0b\u5219\u901a\u8fc7\u5141\u8bb8\u5c11\u91cf\u8d1f\u5229\u6da6\u5b9e\u73b0\u7c7b\u4f3c\u6027\u80fd\u3002", "result": "\u5728\u4e24\u6bd4\u7279\u53cd\u9988\u4e0b\uff0c\u7b97\u6cd5\u5728\u4ea4\u6613\u589e\u76ca\u548c\u5229\u6da6\u6700\u5927\u5316\u4e0a\u5206\u522b\u8fbe\u5230$O(d\\log d)$\u548c$O(d \\log\\log T + d\\log d)$\u540e\u6094\u754c\uff1b\u5355\u6bd4\u7279\u53cd\u9988\u4e0b\u4ecd\u53ef\u8fbe\u5230\u7c7b\u4f3c\u540e\u6094\u754c\uff0c\u4f46\u53ef\u80fd\u4ea7\u751f\u5c11\u91cf\u8d1f\u5229\u6da6\u3002", "conclusion": "\u8bba\u6587\u8868\u660e\uff0c\u5373\u4f7f\u5728\u6709\u9650\u7684\u53cd\u9988\u4fe1\u606f\u4e0b\uff0c\u7b97\u6cd5\u4ecd\u80fd\u63a5\u8fd1\u6700\u4f18\u52a8\u6001\u7b56\u7565\u6027\u80fd\uff0c\u4f46\u5355\u6bd4\u7279\u53cd\u9988\u4e0e\u9884\u7b97\u5e73\u8861\u7ed3\u5408\u65f6\u4f1a\u5bfc\u81f4\u5bf9\u7ef4\u5ea6\u7684\u6307\u6570\u4f9d\u8d56\u3002"}}
{"id": "2602.12904", "categories": ["cs.GT", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.12904", "abs": "https://arxiv.org/abs/2602.12904", "authors": ["Emanuele Coccia", "Martino Bernasconi", "Andrea Celli"], "title": "Nonparametric Contextual Online Bilateral Trade", "comment": null, "summary": "We study the problem of contextual online bilateral trade. At each round, the learner faces a seller-buyer pair and must propose a trade price without observing their private valuations for the item being sold. The goal of the learner is to post prices to facilitate trades between the two parties. Before posting a price, the learner observes a $d$-dimensional context vector that influences the agent's valuations. Prior work in the contextual setting has focused on linear models. In this work, we tackle a general nonparametric setting in which the buyer's and seller's valuations behave according to arbitrary Lipschitz functions of the context. We design an algorithm that leverages contextual information through a hierarchical tree construction and guarantees regret $\\widetilde{O}(T^{{(d-1)}/d})$. Remarkably, our algorithm operates under two stringent features of the setting: (1) one-bit feedback, where the learner only observes whether a trade occurred or not, and (2) strong budget balance, where the learner cannot subsidize or profit from the market participants. We further provide a matching lower bound in the full-feedback setting, demonstrating the tightness of our regret bound.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u60c5\u5883\u5728\u7ebf\u53cc\u8fb9\u8d38\u6613\u95ee\u9898\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u5728\u975e\u53c2\u6570\u8bbe\u7f6e\u4e0b\u8fd0\u884c\u7684\u7b97\u6cd5\uff0c\u5229\u7528\u5206\u5c42\u6811\u7ed3\u6784\u5904\u7406\u60c5\u5883\u4fe1\u606f\uff0c\u5e76\u8bc1\u660e\u4e86\u5176\u9057\u61be\u8fb9\u754c\u7684\u6700\u4f18\u6027\u3002", "motivation": "\u4e3a\u4e86\u89e3\u51b3\u60c5\u5883\u5728\u7ebf\u53cc\u8fb9\u8d38\u6613\u4e2d\uff0c\u4f20\u7edf\u7684\u7ebf\u6027\u6a21\u578b\u65e0\u6cd5\u5904\u7406\u975e\u53c2\u6570\u60c5\u5883\u7684\u95ee\u9898\uff0c\u5e76\u6ee1\u8db3\u5355\u6bd4\u7279\u53cd\u9988\u548c\u5f3a\u9884\u7b97\u5e73\u8861\u7684\u4e25\u683c\u9650\u5236\u3002", "method": "\u8bbe\u8ba1\u4e86\u4e00\u79cd\u57fa\u4e8e\u5206\u5c42\u6811\u6784\u9020\u7684\u7b97\u6cd5\uff0c\u80fd\u591f\u5904\u7406\u4efb\u610fLipschitz\u51fd\u6570\u63cf\u8ff0\u7684\u60c5\u5883\u4f9d\u8d56\u4f30\u503c\uff0c\u5e76\u4fdd\u8bc1\u9057\u61be\u8fb9\u754c\u4e3a$\\widetilde{O}(T^{{(d-1)}/d})$\u3002", "result": "\u7b97\u6cd5\u5728\u5355\u6bd4\u7279\u53cd\u9988\u548c\u5f3a\u9884\u7b97\u5e73\u8861\u7684\u9650\u5236\u4e0b\u8868\u73b0\u4f18\u5f02\uff0c\u5e76\u5728\u5168\u53cd\u9988\u8bbe\u7f6e\u4e2d\u8bc1\u660e\u4e86\u9057\u61be\u8fb9\u754c\u7684\u6700\u4f18\u6027\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u7684\u7b97\u6cd5\u5728\u975e\u53c2\u6570\u60c5\u5883\u4e0b\u5b9e\u73b0\u4e86\u9ad8\u6548\u7684\u8d38\u6613\u5b9a\u4ef7\uff0c\u4e14\u9057\u61be\u8fb9\u754c\u6700\u4f18\uff0c\u4e3a\u5b9e\u9645\u5e94\u7528\u63d0\u4f9b\u4e86\u7406\u8bba\u652f\u6301\u3002"}}
{"id": "2602.12927", "categories": ["cs.GT"], "pdf": "https://arxiv.org/pdf/2602.12927", "abs": "https://arxiv.org/abs/2602.12927", "authors": ["Moritz Graf", "Anthony Lin", "Rupak Majumdar"], "title": "Solving Qualitative Multi-Objective Stochastic Games", "comment": "Accepted as a full paper at AAMAS 2026", "summary": "Many problems in compositional synthesis and verification of multi-agent systems -- such as rational verification and assume-guarantee verification in probabilistic systems -- reduce to reasoning about two-player multi-objective stochastic games. This motivates us to study the problem of characterizing the complexity and memory requirements for two-player stochastic games with Boolean combinations of qualitative reachability and safety objectives. Reachability objectives require that a given set of states is reached; safety requires that a given set is invariant. A qualitative winning condition asks that an objective is satisfied almost surely (AS) or (in negated form) with non-zero (NZ) probability.\n  We study the determinacy and complexity landscape of the problem. We show that games with conjunctions of AS and NZ reachability and safety objectives are determined, and determining the winner is PSPACE-complete. The same holds for positive boolean combinations of AS reachability and safety, as well as for negations thereof. On the other hand, games with full Boolean combinations of qualitative objectives are not determined, and are NEXPTIME-hard. Our hardness results show a connection between stochastic games and logics with partially-ordered quantification. Our results shed light on the relationship between determinacy and complexity, and extend the complexity landscape for stochastic games in the multi-objective setting.", "AI": {"tldr": "\u7814\u7a76\u4e24\u73a9\u5bb6\u968f\u673a\u535a\u5f08\u7684\u590d\u6742\u6027\u548c\u5185\u5b58\u9700\u6c42\uff0c\u7279\u522b\u662f\u5b9a\u6027\u53ef\u8fbe\u6027\u548c\u5b89\u5168\u6027\u76ee\u6807\u7684\u5e03\u5c14\u7ec4\u5408\u3002", "motivation": "\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u7684\u7ec4\u5408\u5408\u6210\u4e0e\u9a8c\u8bc1\u95ee\u9898\uff08\u5982\u7406\u6027\u9a8c\u8bc1\u548c\u6982\u7387\u7cfb\u7edf\u4e2d\u7684\u5047\u8bbe\u4fdd\u8bc1\u9a8c\u8bc1\uff09\u9700\u8981\u7814\u7a76\u4e24\u73a9\u5bb6\u968f\u673a\u535a\u5f08\u7684\u590d\u6742\u6027\u53ca\u5176\u76ee\u6807\u7ec4\u5408\u3002", "method": "\u5206\u6790\u5b9a\u6027\u53ef\u8fbe\u6027\u548c\u5b89\u5168\u6027\u76ee\u6807\u7684\u5e03\u5c14\u7ec4\u5408\u7684\u590d\u6742\u6027\u53ca\u5185\u5b58\u9700\u6c42\uff0c\u63a2\u8ba8\u786e\u5b9a\u6027\u53ca\u5176\u590d\u6742\u5ea6\u3002", "result": "\u8bc1\u660e\u5177\u6709AS\u548cNZ\u53ef\u8fbe\u6027\u53ca\u5b89\u5168\u6027\u76ee\u6807\u7684\u535a\u5f08\u662f\u786e\u5b9a\u7684\uff0c\u4e14\u5224\u5b9a\u80dc\u8d1f\u662fPSPACE\u5b8c\u5168\u7684\uff1b\u800c\u5b8c\u5168\u5e03\u5c14\u7ec4\u5408\u7684\u76ee\u6807\u535a\u5f08\u5219\u4e0d\u5177\u5907\u786e\u5b9a\u6027\u4e14\u662fNEXPTIME\u56f0\u96be\u7684\u3002", "conclusion": "\u7ed3\u679c\u63ed\u793a\u4e86\u968f\u673a\u535a\u5f08\u4e0e\u90e8\u5206\u6709\u5e8f\u91cf\u5316\u903b\u8f91\u7684\u8054\u7cfb\uff0c\u6269\u5c55\u4e86\u591a\u76ee\u6807\u968f\u673a\u535a\u5f08\u7684\u590d\u6742\u6027\u56fe\u666f\u3002"}}
