{"id": "2508.15844", "categories": ["cs.GT", "cs.CR"], "pdf": "https://arxiv.org/pdf/2508.15844", "abs": "https://arxiv.org/abs/2508.15844", "authors": ["Haohui Zhang", "Sirui Shen", "Xinyu Hu", "Chenglu Jin"], "title": "Ransomware Negotiation: Dynamics and Privacy-Preserving Mechanism Design", "comment": null, "summary": "Ransomware attacks have become a pervasive and costly form of cybercrime,\ncausing tens of millions of dollars in losses as organizations increasingly pay\nransoms to mitigate operational disruptions and financial risks. While prior\nresearch has largely focused on proactive defenses, the post-infection\nnegotiation dynamics between attackers and victims remains underexplored. This\npaper presents a formal analysis of attacker-victim interactions in modern\nransomware incidents using a finite-horizon alternating-offers bargaining game\nmodel. Our analysis demonstrates how bargaining alters the optimal strategies\nof both parties. In practice, incomplete information-attackers lacking\nknowledge of victims' data valuations and victims lacking knowledge of\nattackers' reservation ransoms-can prolong negotiations and increase victims'\nbusiness interruption costs. To address this, we design a Bayesian\nincentive-compatible mechanism that facilitates rapid agreement on a fair\nransom without requiring either party to disclose private valuations. We\nfurther implement this mechanism using secure two-party computation based on\ngarbled circuits, thereby eliminating the need for trusted intermediaries and\npreserving the privacy of both parties throughout the negotiation. To the best\nof our knowledge, this is the first automated, privacy-preserving negotiation\nmechanism grounded in a formal analysis of ransomware negotiation dynamics."}
{"id": "2508.16007", "categories": ["cs.GT"], "pdf": "https://arxiv.org/pdf/2508.16007", "abs": "https://arxiv.org/abs/2508.16007", "authors": ["Minbiao Han", "Seyed A. Esmaeili", "Michael Albert", "Haifeng Xu"], "title": "Data Auctions for Retrieval Augmented Generation", "comment": null, "summary": "We study the problem of data selling for Retrieval Augmented Generation (RAG)\ntasks in Generative AI applications. We model each buyer's valuation of a\ndataset with a natural coverage-based valuation function that increases with\nthe inclusion of more relevant data points that would enhance responses to\nanticipated queries. Motivated by issues such as data control and prior-free\nrevenue maximization, we focus on the scenario where each data point can be\nallocated to only one buyer. We show that the problem of welfare maximization\nin this setting is NP-hard even with two bidders, but design a polynomial-time\n$(1-1/e)$ approximation algorithm for any number of bidders. Unfortunately,\nhowever, this efficient allocation algorithm fails to be incentive compatible.\nThe crux of our approach is a carefully tailored post-processing step called\n\\emph{data burning} which retains the $(1-1/e)$ approximation factor but\nachieves incentive compatibility. Our thorough experiments on synthetic and\nreal-world image and text datasets demonstrate the practical effectiveness of\nour algorithm compared to popular baseline algorithms for combinatorial\nauctions."}
{"id": "2508.16177", "categories": ["cs.GT", "econ.TH"], "pdf": "https://arxiv.org/pdf/2508.16177", "abs": "https://arxiv.org/abs/2508.16177", "authors": ["Patrick Lederer"], "title": "Proportional Representation in Rank Aggregation", "comment": null, "summary": "In rank aggregation, the task is to aggregate multiple weighted input\nrankings into a single output ranking. While numerous methods, so-called social\nwelfare functions (SWFs), have been suggested for this problem, all of the\nclassical SWFs tend to be majoritarian and are thus not acceptable when a\nproportional ranking is required. Motivated by this observation, we will design\nSWFs that guarantee that every input ranking is proportionally represented by\nthe output ranking. Specifically, our central fairness condition requires that\nthe number of pairwise comparisons between candidates on which an input ranking\nand the output ranking agree is proportional to the weight of the input\nranking. As our main contribution, we present a simple SWF called the\nProportional Sequential Borda rule, which satisfies this condition. Moreover,\nwe introduce two variants of this rule: the Ranked Method of Equal Shares,\nwhich has a more utilitarian flavor while still satisfying our fairness\ncondition, and the Flow-adjusting Borda rule, which satisfies an even stronger\nfairness condition. Many of our axioms and techniques are inspired by results\non approval-based committee voting and participatory budgeting, where the\nconcept of proportional representation has been studied in depth."}
{"id": "2508.16195", "categories": ["cs.GT", "econ.TH"], "pdf": "https://arxiv.org/pdf/2508.16195", "abs": "https://arxiv.org/abs/2508.16195", "authors": ["Patrick Lederer"], "title": "Strategyproof Randomized Social Choice for Restricted Sets of Utility Functions", "comment": "Accepted at IJCAI 2021", "summary": "Social decision schemes (SDSs) map the voters' preferences over multiple\nalternatives to a probability distribution over these alternatives. In a\nseminal result, Gibbard (1977) has characterized the set of SDSs that are\nstrategyproof with respect to all utility functions and his result implies that\nall such SDSs are either unfair to the voters or alternatives, or they require\na significant amount of randomization. To circumvent this negative result, we\npropose the notion of $U$-strategyproofness which postulates that only voters\nwith a utility function in a predefined set $U$ cannot manipulate. We then\nanalyze the tradeoff between $U$-strategyproofness and various decisiveness\nnotions that restrict the amount of randomization of SDSs. In particular, we\nshow that if the utility functions in the set $U$ value the best alternative\nmuch more than other alternatives, there are $U$-strategyproof SDSs that choose\nan alternative with probability $1$ whenever all but $k$ voters rank it first.\nOn the negative side, we demonstrate that $U$-strategyproofness is incompatible\nwith Condorcet-consistency if the set $U$ satisfies minimal symmetry\nconditions. Finally, we show that no ex post efficient and $U$-strategyproof\nSDS can be significantly more decisive than the uniform random dictatorship if\nthe voters are close to indifferent between their two favorite alternatives."}
{"id": "2508.16024", "categories": ["cs.GR", "cs.CV"], "pdf": "https://arxiv.org/pdf/2508.16024", "abs": "https://arxiv.org/abs/2508.16024", "authors": ["Prateek Poudel", "Prashant Aryal", "Kirtan Kunwar", "Navin Nepal", "Dinesh Bania Kshatri"], "title": "Wavelet-Space Super-Resolution for Real-Time Rendering", "comment": null, "summary": "We investigate the use of wavelet-space feature decomposition in neural\nsuper-resolution for rendering pipelines. Building on the DFASR framework, we\nintroduce a wavelet-domain representation that separates low- and\nhigh-frequency details before reconstruction, enabling the network to better\npreserve fine textures while maintaining structural consistency. Unlike\nRGB-space regression, our approach leverages the stationary wavelet transform\n(SWT) to avoid spatial down-sampling, ensuring alignment across subbands and\npreserving shift invariance. The model predicts wavelet coefficients\nconditioned on spatial G-buffers and temporally warped history frames, which\nare then recombined through inverse wavelet synthesis. We conduct a\ncomprehensive ablation study across wavelet families, transform types, and\narchitectural variants, showing that incorporating SWT improves PSNR by up to\n1.5 dB and reduces LPIPS by 17% on average, at a computational overhead of\nroughly +24 ms compared to out DFASR baseline. While absolute runtimes on our\nRTX 3050 mobile GPU are higher ( 141ms) than the original DFASR report on RTX\n4090( 11ms), the relative overhead remains modest, suggesting that on\nhigher-end GPUs our method would also remain real-time capable. Taken together,\nour results suggest that wavelet-domain representations are a principled and\neffective way to enhance perceptual quality in neural upscaling for graphics\napplications."}
{"id": "2508.15866", "categories": ["cs.PL", "cs.LG", "cs.SE"], "pdf": "https://arxiv.org/pdf/2508.15866", "abs": "https://arxiv.org/abs/2508.15866", "authors": ["Lingxiao Li", "Salar Rahili", "Yiwei Zhao"], "title": "Correctness-Guaranteed Code Generation via Constrained Decoding", "comment": "Published at COLM 2025", "summary": "Language Models (LMs) are increasingly being used for code generation, but\nensuring the correctness of generated programs remains a significant challenge.\nAlthough imperfect code may be acceptable during software development with\nhuman oversight, domains such as video games and robotics require one-shot\ncorrectness for runtime-critical components. We present a constrained decoding\nalgorithm for generating semantically correct programs that incorporates a\ncontext-sensitive parser, which, at each step, outputs a regular expression\nthat satisfies a critical non-extensible property to guide the generation of\nthe next token sequence that can continue to a correct program. To build such a\ncontext-sensitive parser, we propose a framework of a dynamic tree of parsers\n(ToP) during parsing, where each parser corresponds to a modular context-free\ngrammar enriched with contextual information such as variable scopes and type\nconstraints, with tree branches representing ambiguity in the future code\nsegment. We demonstrate our approach through sLua, a strongly typed variant of\nLua, showing that our method can generate semantically correct programs\nconforming to any prescribed scripting API. We further show that, with careful\ndesign, our semantic guarantees extend to runtime correctness, as validated in\nthe application of generating game mechanics for a roguelike video game."}
{"id": "2508.16245", "categories": ["cs.GT", "cs.LG", "cs.MA", "econ.TH"], "pdf": "https://arxiv.org/pdf/2508.16245", "abs": "https://arxiv.org/abs/2508.16245", "authors": ["Cole Wyeth", "Marcus Hutter", "Jan Leike", "Jessica Taylor"], "title": "Limit-Computable Grains of Truth for Arbitrary Computable Extensive-Form (Un)Known Games", "comment": "42 pages; 2 figures; 7 algorithms", "summary": "A Bayesian player acting in an infinite multi-player game learns to predict\nthe other players' strategies if his prior assigns positive probability to\ntheir play (or contains a grain of truth). Kalai and Lehrer's classic grain of\ntruth problem is to find a reasonably large class of strategies that contains\nthe Bayes-optimal policies with respect to this class, allowing\nmutually-consistent beliefs about strategy choice that obey the rules of\nBayesian inference. Only small classes are known to have a grain of truth and\nthe literature contains several related impossibility results. In this paper we\npresent a formal and general solution to the full grain of truth problem: we\nconstruct a class of strategies wide enough to contain all computable\nstrategies as well as Bayes-optimal strategies for every reasonable prior over\nthe class. When the \"environment\" is a known repeated stage game, we show\nconvergence in the sense of [KL93a] and [KL93b]. When the environment is\nunknown, agents using Thompson sampling converge to play $\\varepsilon$-Nash\nequilibria in arbitrary unknown computable multi-agent environments. Finally,\nwe include an application to self-predictive policies that avoid planning.\nWhile these results use computability theory only as a conceptual tool to solve\na classic game theory problem, we show that our solution can naturally be\ncomputationally approximated arbitrarily closely."}
{"id": "2508.16401", "categories": ["cs.GR", "cs.HC", "cs.LG", "cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2508.16401", "abs": "https://arxiv.org/abs/2508.16401", "authors": ["NVIDIA", ":", "Chaeyeon Chung", "Ilya Fedorov", "Michael Huang", "Aleksey Karmanov", "Dmitry Korobchenko", "Roger Ribera", "Yeongho Seol"], "title": "Audio2Face-3D: Audio-driven Realistic Facial Animation For Digital Avatars", "comment": null, "summary": "Audio-driven facial animation presents an effective solution for animating\ndigital avatars. In this paper, we detail the technical aspects of NVIDIA\nAudio2Face-3D, including data acquisition, network architecture, retargeting\nmethodology, evaluation metrics, and use cases. Audio2Face-3D system enables\nreal-time interaction between human users and interactive avatars, facilitating\nfacial animation authoring for game characters. To assist digital avatar\ncreators and game developers in generating realistic facial animations, we have\nopen-sourced Audio2Face-3D networks, SDK, training framework, and example\ndataset."}
{"id": "2508.15898", "categories": ["cs.PL", "cs.CR"], "pdf": "https://arxiv.org/pdf/2508.15898", "abs": "https://arxiv.org/abs/2508.15898", "authors": ["Matthew Sotoudeh", "Zachary Yedidia"], "title": "Automated Formal Verification of a Software Fault Isolation System", "comment": "Short paper to appear at FMCAD 2025, https://fmcad.org/", "summary": "Software fault isolation (SFI) is a popular way to sandbox untrusted\nsoftware. A key component of SFI is the verifier that checks the untrusted code\nis written in a subset of the machine language that guarantees it never reads\nor writes outside of a region of memory dedicated to the sandbox. Soundness\nbugs in the SFI verifier would break the SFI security model and allow the\nsupposedly sandboxed code to read protected memory. In this paper, we address\nthe concern of SFI verifier bugs by performing an automated formal verification\nof a recent SFI system called Lightweight Fault Isolation (LFI). In particular,\nwe formally verify that programs accepted by the LFI verifier never read or\nwrite to memory outside of a designated sandbox region."}
{"id": "2508.16251", "categories": ["cs.GT"], "pdf": "https://arxiv.org/pdf/2508.16251", "abs": "https://arxiv.org/abs/2508.16251", "authors": ["Hongjia Wu", "Minrui Xu", "Zehui Xiong", "Lin Gao", "Haoyuan Pan", "Dusit Niyato", "Tse-Tin Chan"], "title": "A QoE-Driven Personalized Incentive Mechanism Design for AIGC Services in Resource-Constrained Edge Networks", "comment": null, "summary": "With rapid advancements in large language models (LLMs), AI-generated content\n(AIGC) has emerged as a key driver of technological innovation and economic\ntransformation. Personalizing AIGC services to meet individual user demands is\nessential but challenging for AIGC service providers (ASPs) due to the\nsubjective and complex demands of mobile users (MUs), as well as the\ncomputational and communication resource constraints faced by ASPs. To tackle\nthese challenges, we first develop a novel multi-dimensional\nquality-of-experience (QoE) metric. This metric comprehensively evaluates AIGC\nservices by integrating accuracy, token count, and timeliness. We focus on a\nmobile edge computing (MEC)-enabled AIGC network, consisting of multiple ASPs\ndeploying differentiated AIGC models on edge servers and multiple MUs with\nheterogeneous QoE requirements requesting AIGC services from ASPs. To\nincentivize ASPs to provide personalized AIGC services under MEC resource\nconstraints, we propose a QoE-driven incentive mechanism. We formulate the\nproblem as an equilibrium problem with equilibrium constraints (EPEC), where\nMUs as leaders determine rewards, while ASPs as followers optimize resource\nallocation. To solve this, we develop a dual-perturbation reward optimization\nalgorithm, reducing the implementation complexity of adaptive pricing.\nExperimental results demonstrate that our proposed mechanism achieves a\nreduction of approximately $64.9\\%$ in average computational and communication\noverhead, while the average service cost for MUs and the resource consumption\nof ASPs decrease by $66.5\\%$ and $76.8\\%$, respectively, compared to\nstate-of-the-art benchmarks."}
{"id": "2508.16535", "categories": ["cs.GR", "cs.HC", "cs.MM"], "pdf": "https://arxiv.org/pdf/2508.16535", "abs": "https://arxiv.org/abs/2508.16535", "authors": ["Trung Hieu Pham", "Chanh Minh Tran", "Eiji Kamioka", "Xuan Tan Phan"], "title": "Real-time 3D Light-field Viewing with Eye-tracking on Conventional Displays", "comment": null, "summary": "Creating immersive 3D visual experiences typically requires expensive and\nspecialized hardware such as VR headsets, autostereoscopic displays, or active\nshutter glasses. These constraints limit the accessibility and everyday use of\n3D visualization technologies in resource-constrained settings. To address\nthis, we propose a low-cost system that enables real-time 3D light-field\nviewing using only a standard 2D monitor, a conventional RGB webcam, and\nred-cyan anaglyph glasses. The system integrates real-time eye-tracking to\ndynamically adapt the displayed light-field image to the user's head position\nwith a lightweight rendering pipeline that selects and composites stereoscopic\nviews from pre-captured light-field data. The resulting anaglyph image is\nupdated in real-time, creating a more immersive and responsive 3D experience.\nThe system operates entirely on CPU and maintains a stable frame rate of 30\nFPS, confirming its feasibility on typical consumer-grade hardware. All of\nthese highlight the potential of our approach as an accessible platform for\ninteractive 3D applications in education, digital media, and beyond."}
{"id": "2508.16063", "categories": ["cs.PL", "cs.FL"], "pdf": "https://arxiv.org/pdf/2508.16063", "abs": "https://arxiv.org/abs/2508.16063", "authors": ["Paul Krogmeier", "P. Madhusudan"], "title": "Synthesizing DSLs for Few-Shot Learning", "comment": null, "summary": "We study the problem of synthesizing domain-specific languages (DSLs) for\nfew-shot learning in symbolic domains. Given a base language and instances of\nfew-shot learning problems, where each instance is split into training and\ntesting samples, the DSL synthesis problem asks for a grammar over the base\nlanguage that guarantees that small expressions solving training samples also\nsolve corresponding testing samples. We prove that the problem is decidable for\na class of languages whose semantics over fixed structures can be evaluated by\ntree automata and when expression size corresponds to parse tree depth in the\ngrammar, and, furthermore, the grammars solving the problem correspond to a\nregular set of trees. We also prove decidability results for variants of the\nproblem where DSLs are only required to express solutions for input learning\nproblems and where DSLs are defined using macro grammars."}
{"id": "2508.16285", "categories": ["cs.GT"], "pdf": "https://arxiv.org/pdf/2508.16285", "abs": "https://arxiv.org/abs/2508.16285", "authors": ["Eyal Briman", "Nimrod Talmon", "Angela Kreitenweis", "Muhammad Idrees"], "title": "A Social Choice Analysis of Optimism's Retroactive Project Funding", "comment": "Accepted to DAWO25 in Zurich", "summary": "The Optimism Retroactive Project Funding (RetroPGF) is a key initiative\nwithin the blockchain ecosystem that retroactively rewards projects deemed\nvaluable to the Ethereum and Optimism communities. Managed by the Optimism\nCollective, a decentralized autonomous organization (DAO), RetroPGF represents\na large-scale experiment in decentralized governance. Funding rewards are\ndistributed in OP tokens, the native digital currency of the ecosystem. As of\nthis writing, four funding rounds have been completed, collectively allocating\nover 100M dollars, with an additional 1.3B dollars reserved for future rounds.\nHowever, we identify significant shortcomings in the current allocation system,\nunderscoring the need for improved governance mechanisms given the scale of\nfunds involved.\n  Leveraging computational social choice techniques and insights from\nmultiagent systems, we propose improvements to the voting process by\nrecommending the adoption of a utilitarian moving phantoms mechanism. This\nmechanism, originally introduced by Freeman et al. in 2019, is designed to\nenhance social welfare (using the L1 norm) while satisfying strategyproofness\n-- two key properties aligned with the application's governance requirements.\nOur analysis provides a formal framework for designing improved funding\nmechanisms for DAOs, contributing to the broader discourse on decentralized\ngovernance and public goods allocation."}
{"id": "2508.16125", "categories": ["cs.PL", "cs.SE"], "pdf": "https://arxiv.org/pdf/2508.16125", "abs": "https://arxiv.org/abs/2508.16125", "authors": ["Zhenyang Xu", "Hongxu Xu", "Yongqiang Tian", "Xintong Zhou", "Chengnian Sun"], "title": "Leveraging Large Language Models to Detect Missed Peephole Optimizations", "comment": null, "summary": "By replacing small, suboptimal instruction sequences within programs with a\nmore efficient equivalent, peephole optimization can not only directly optimize\ncode size and performance, but also potentially enables further transformations\nin the subsequent optimization pipeline. Although peephole optimization is a\ncritical class of compiler optimizations, discovering new and effective\npeephole optimizations is challenging as the instruction sets can be extremely\ncomplex and diverse. Previous methods either do not scale well or can only\ncapture a limited subset of peephole optimizations. In this work, we leverage\nLarge Language Models (LLMs) to detect missed peephole optimizations. We\npropose Lampo, a novel automated framework that synergistically combines the\ncreative but unreliable code optimization ability of LLMs with rigorous\ncorrectness verification performed by translation validation tools, integrated\nin a feedback-driven iterative process. Through a comprehensive evaluation\nwithin LLVM ecosystems, we show that Lampo can successfully detect up to 17 out\nof 25 previously reported missed optimizations in LLVM on average, and that 22\nout of 25 can potentially be found by Lampo with different LLMs. For\ncomparison, the state-of-the-art superoptimizer for LLVM, Souper, identified 15\nof them. Moreover, within seven months of development and intermittent\nexperiments, Lampo found 26 missed peephole optimizations, 15 of which have\nbeen confirmed and 6 already fixed. These results demonstrate Lampo's strong\npotential in continuously detecting missed peephole optimizations."}
{"id": "2508.16522", "categories": ["cs.PL", "cs.DC"], "pdf": "https://arxiv.org/pdf/2508.16522", "abs": "https://arxiv.org/abs/2508.16522", "authors": ["Rohan Yadav", "Joseph Guman", "Sean Treichler", "Michael Garland", "Alex Aiken", "Fredrik Kjolstad", "Michael Bauer"], "title": "On the Duality of Task and Actor Programming Models", "comment": null, "summary": "Programming models for distributed and heterogeneous machines are rapidly\ngrowing in popularity to meet the demands of modern workloads. Task and actor\nmodels are common choices that offer different trade-offs between development\nproductivity and achieved performance. Task-based models offer better\nproductivity and composition of software, whereas actor-based models routinely\ndeliver better peak performance due to lower overheads. While task-based and\nactor-based models appear to be different superficially, we demonstrate these\nprogramming models are duals of each other. Importantly, we show that this\nduality extends beyond functionality to performance, and elucidate techniques\nthat let task-based systems deliver performance competitive with actor-based\nsystems without compromising productivity. We apply these techniques to both\nRealm, an explicitly parallel task-based runtime, as well as Legion, an\nimplicitly parallel task-based runtime. We show these techniques reduce Realm's\noverheads by between 1.7-5.3x, coming within a factor of two of the overheads\nimposed by heavily optimized actor-based systems like Charm++ and MPI. We\nfurther show that our techniques enable between 1.3-5.0x improved strong\nscaling of unmodified Legion applications."}
