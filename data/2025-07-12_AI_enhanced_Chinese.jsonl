{"id": "2507.07480", "categories": ["cs.PL"], "pdf": "https://arxiv.org/pdf/2507.07480", "abs": "https://arxiv.org/abs/2507.07480", "authors": ["Tobias Kapp\u00e9"], "title": "On Propositional Program Equivalence (extended abstract)", "comment": null, "summary": "General program equivalence is undecidable. However, if we abstract away the\nsemantics of statements, then this problem becomes not just decidable, but\npractically feasible. For instance, a program of the form \"if $b$ then $e$ else\n$f$\" should be equivalent to \"if not $b$ then $f$ else $e$\" - no matter what\n$b$, $e$ and $f$ are. This kind of equivalence is known as propositional\nequivalence. In this extended abstract, we discuss recent developments in\npropositional program equivalence from the perspective of (Guarded) Kleene\nAlgebra with Tests, or (G)KAT.", "AI": {"tldr": "\u8bba\u6587\u8ba8\u8bba\u4e86\u547d\u9898\u7a0b\u5e8f\u7b49\u4ef7\u6027\u7684\u6700\u65b0\u8fdb\u5c55\uff0c\u91cd\u70b9\u662f\u4ece(Guarded) Kleene\u4ee3\u6570\u4e0e\u6d4b\u8bd5\uff08(G)KAT\uff09\u7684\u89d2\u5ea6\u8fdb\u884c\u5206\u6790\u3002", "motivation": "\u52a8\u673a\u5728\u4e8e\u89e3\u51b3\u7a0b\u5e8f\u7b49\u4ef7\u6027\u95ee\u9898\uff0c\u867d\u7136\u901a\u7528\u7a0b\u5e8f\u7b49\u4ef7\u6027\u662f\u4e0d\u53ef\u5224\u5b9a\u7684\uff0c\u4f46\u901a\u8fc7\u62bd\u8c61\u8bed\u53e5\u7684\u8bed\u4e49\uff0c\u8fd9\u4e00\u95ee\u9898\u53d8\u5f97\u53ef\u5224\u5b9a\u4e14\u5b9e\u9645\u53ef\u884c\u3002", "method": "\u65b9\u6cd5\u662f\u901a\u8fc7(Guarded) Kleene\u4ee3\u6570\u4e0e\u6d4b\u8bd5\uff08(G)KAT\uff09\u7684\u7406\u8bba\u6846\u67b6\u6765\u5206\u6790\u547d\u9898\u7a0b\u5e8f\u7b49\u4ef7\u6027\u3002", "result": "\u7ed3\u679c\u8868\u660e\u547d\u9898\u7a0b\u5e8f\u7b49\u4ef7\u6027\u5728\u62bd\u8c61\u8bed\u4e49\u7684\u60c5\u51b5\u4e0b\u662f\u53ef\u5224\u5b9a\u7684\uff0c\u5e76\u4e14\u53ef\u4ee5\u901a\u8fc7(G)KAT\u6846\u67b6\u8fdb\u884c\u6709\u6548\u5206\u6790\u3002", "conclusion": "\u7ed3\u8bba\u6307\u51fa\u547d\u9898\u7a0b\u5e8f\u7b49\u4ef7\u6027\u5728\u62bd\u8c61\u8bed\u4e49\u7684\u60c5\u51b5\u4e0b\u5177\u6709\u53ef\u884c\u6027\uff0c\u4e14(G)KAT\u4e3a\u8fd9\u4e00\u95ee\u9898\u7684\u7814\u7a76\u63d0\u4f9b\u4e86\u6709\u529b\u7684\u7406\u8bba\u5de5\u5177\u3002"}}
{"id": "2507.07418", "categories": ["cs.GT", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.07418", "abs": "https://arxiv.org/abs/2507.07418", "authors": ["Yang Li", "Yuchao Ma", "Qi Qi"], "title": "Optimal Auction Design in the Joint Advertising", "comment": "Accepted by ICML 2025 (International Conference on Machine Learning).\n  17 pages, 4 figures", "summary": "Online advertising is a vital revenue source for major internet platforms.\nRecently, joint advertising, which assigns a bundle of two advertisers in an ad\nslot instead of allocating a single advertiser, has emerged as an effective\nmethod for enhancing allocation efficiency and revenue. However, existing\nmechanisms for joint advertising fail to realize the optimality, as they tend\nto focus on individual advertisers and overlook bundle structures. This paper\nidentifies an optimal mechanism for joint advertising in a single-slot setting.\nFor multi-slot joint advertising, we propose \\textbf{BundleNet}, a novel\nbundle-based neural network approach specifically designed for joint\nadvertising. Our extensive experiments demonstrate that the mechanisms\ngenerated by \\textbf{BundleNet} approximate the theoretical analysis results in\nthe single-slot setting and achieve state-of-the-art performance in the\nmulti-slot setting. This significantly increases platform revenue while\nensuring approximate dominant strategy incentive compatibility and individual\nrationality.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u4f18\u5316\u7684\u8054\u5408\u5e7f\u544a\u673a\u5236\uff0c\u5305\u62ec\u5355\u5e7f\u544a\u4f4d\u7684\u6700\u4f18\u673a\u5236\u548c\u591a\u5e7f\u544a\u4f4d\u7684\u795e\u7ecf\u7f51\u7edc\u65b9\u6cd5BundleNet\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5e73\u53f0\u6536\u5165\u3002", "motivation": "\u73b0\u6709\u8054\u5408\u5e7f\u544a\u673a\u5236\u672a\u80fd\u5b9e\u73b0\u6700\u4f18\u5206\u914d\u548c\u6536\u5165\u6700\u5927\u5316\uff0c\u4e3b\u8981\u56e0\u4e3a\u5b83\u4eec\u5173\u6ce8\u5355\u4e2a\u5e7f\u544a\u4e3b\u800c\u5ffd\u7565\u4e86\u5e7f\u544a\u7ec4\u5408\u7ed3\u6784\u3002", "method": "\u5728\u5355\u5e7f\u544a\u4f4d\u8bbe\u7f6e\u4e2d\u63d0\u51fa\u6700\u4f18\u673a\u5236\uff1b\u5728\u591a\u5e7f\u544a\u4f4d\u8bbe\u7f6e\u4e2d\u8bbe\u8ba1\u4e86\u57fa\u4e8e\u795e\u7ecf\u7f51\u7edc\u7684BundleNet\u65b9\u6cd5\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cBundleNet\u751f\u6210\u7684\u673a\u5236\u5728\u5355\u5e7f\u544a\u4f4d\u4e2d\u8fd1\u4f3c\u7406\u8bba\u6700\u4f18\u7ed3\u679c\uff0c\u5728\u591a\u5e7f\u544a\u4f4d\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6536\u5165\u5e76\u6ee1\u8db3\u6fc0\u52b1\u517c\u5bb9\u6027\u548c\u4e2a\u4f53\u7406\u6027\u3002", "conclusion": "BundleNet\u662f\u4e00\u79cd\u9ad8\u6548\u7684\u8054\u5408\u5e7f\u544a\u673a\u5236\uff0c\u9002\u7528\u4e8e\u591a\u5e7f\u544a\u4f4d\u573a\u666f\uff0c\u4e3a\u5e73\u53f0\u63d0\u4f9b\u4e86\u663e\u8457\u7684\u6536\u5165\u589e\u957f\u548c\u7406\u8bba\u4fdd\u969c\u3002"}}
{"id": "2507.07133", "categories": ["cs.GR", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.07133", "abs": "https://arxiv.org/abs/2507.07133", "authors": ["Mathieu Tuli", "Kaveh Kamali", "David B. Lindell"], "title": "Generative Panoramic Image Stitching", "comment": null, "summary": "We introduce the task of generative panoramic image stitching, which aims to\nsynthesize seamless panoramas that are faithful to the content of multiple\nreference images containing parallax effects and strong variations in lighting,\ncamera capture settings, or style. In this challenging setting, traditional\nimage stitching pipelines fail, producing outputs with ghosting and other\nartifacts. While recent generative models are capable of outpainting content\nconsistent with multiple reference images, they fail when tasked with\nsynthesizing large, coherent regions of a panorama. To address these\nlimitations, we propose a method that fine-tunes a diffusion-based inpainting\nmodel to preserve a scene's content and layout based on multiple reference\nimages. Once fine-tuned, the model outpaints a full panorama from a single\nreference image, producing a seamless and visually coherent result that\nfaithfully integrates content from all reference images. Our approach\nsignificantly outperforms baselines for this task in terms of image quality and\nthe consistency of image structure and scene layout when evaluated on captured\ndatasets.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u751f\u6210\u5168\u666f\u56fe\u50cf\u62fc\u63a5\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u5fae\u8c03\u57fa\u4e8e\u6269\u6563\u7684\u4fee\u590d\u6a21\u578b\uff0c\u5b9e\u73b0\u5728\u591a\u4e2a\u53c2\u8003\u56fe\u50cf\u4e2d\u5408\u6210\u65e0\u7f1d\u4e14\u89c6\u89c9\u4e00\u81f4\u7684\u5168\u666f\u56fe\u50cf\u3002", "motivation": "\u4f20\u7edf\u56fe\u50cf\u62fc\u63a5\u65b9\u6cd5\u5728\u5904\u7406\u89c6\u5dee\u6548\u5e94\u3001\u5149\u7167\u53d8\u5316\u548c\u76f8\u673a\u8bbe\u7f6e\u5dee\u5f02\u65f6\u5bb9\u6613\u4ea7\u751f\u4f2a\u5f71\uff0c\u800c\u73b0\u6709\u751f\u6210\u6a21\u578b\u5728\u5927\u8303\u56f4\u5168\u666f\u5185\u5bb9\u5408\u6210\u4e0a\u8868\u73b0\u4e0d\u4f73\u3002", "method": "\u63d0\u51fa\u4e00\u79cd\u65b9\u6cd5\uff0c\u901a\u8fc7\u5fae\u8c03\u57fa\u4e8e\u6269\u6563\u7684\u4fee\u590d\u6a21\u578b\uff0c\u4ee5\u4fdd\u7559\u573a\u666f\u5185\u5bb9\u548c\u5e03\u5c40\uff0c\u5e76\u57fa\u4e8e\u5355\u4e2a\u53c2\u8003\u56fe\u50cf\u751f\u6210\u5b8c\u6574\u7684\u5168\u666f\u3002", "result": "\u5728\u6355\u83b7\u7684\u6570\u636e\u96c6\u4e0a\u8bc4\u4f30\uff0c\u8be5\u65b9\u6cd5\u5728\u56fe\u50cf\u8d28\u91cf\u548c\u573a\u666f\u4e00\u81f4\u6027\u65b9\u9762\u663e\u8457\u4f18\u4e8e\u57fa\u7ebf\u6a21\u578b\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u80fd\u591f\u751f\u6210\u65e0\u7f1d\u4e14\u89c6\u89c9\u4e00\u81f4\u7684\u5168\u666f\u56fe\u50cf\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u4f20\u7edf\u65b9\u6cd5\u548c\u73b0\u6709\u751f\u6210\u6a21\u578b\u7684\u5c40\u9650\u6027\u3002"}}
{"id": "2507.07688", "categories": ["cs.GT"], "pdf": "https://arxiv.org/pdf/2507.07688", "abs": "https://arxiv.org/abs/2507.07688", "authors": ["Jowa Yangchin", "Ningrinla Marchang"], "title": "Incentive Mechanism for Mobile Crowd Sensing with Assumed Bid Cost Reverse Auction", "comment": null, "summary": "Mobile Crowd Sensing (MCS) is the mechanism wherein people can contribute in\ndata collection process using their own mobile devices which have sensing\ncapabilities. Incentives are rewards that individuals get in exchange for data\nthey submit. Reverse Auction Bidding (RAB) is a framework that allows users to\nplace bids for selling the data they collected. Task providers can select users\nto buy data from by looking at bids. Using the RAB framework, MCS system can be\noptimized for better user utility, task provider utility and platform utility.\nIn this paper, we propose a novel approach called Reverse Auction with Assumed\nBid Cost (RA-ABC) which allows users to place a bid in the system before\ncollecting data. We opine that performing the tasks only after winning helps in\nreducing resource consumption instead of performing the tasks before bidding.\nUser Return on Investment (ROI) is calculated with which they decide to further\nparticipate or not by either increasing or decreasing their bids. We also\npropose an extension of RA-ABC with dynamic recruitment (RA-ABCDR) in which we\nallow new users to join the system at any time during bidding rounds.\nSimulation results demonstrate that RA-ABC and RA-ABCDR outperform the widely\nused Tullock Optimal Prize Function, with RA-ABCDR achieving up to 54.6\\%\nhigher user retention and reducing auction cost by 22.2\\%, thereby ensuring\nmore efficient and sustainable system performance. Extensive simulations\nconfirm that dynamic user recruitment significantly enhances performance across\nstability, fairness, and cost-efficiency metrics.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aRA-ABC\u7684\u65b0\u65b9\u6cd5\uff0c\u5141\u8bb8\u7528\u6237\u5728\u6536\u96c6\u6570\u636e\u524d\u51fa\u4ef7\uff0c\u5e76\u901a\u8fc7\u52a8\u6001\u7528\u6237\u62db\u52df\u6269\u5c55RA-ABCDR\uff0c\u663e\u8457\u63d0\u5347\u7cfb\u7edf\u6027\u80fd\u548c\u7528\u6237\u4fdd\u7559\u7387\u3002", "motivation": "\u79fb\u52a8\u7fa4\u667a\u611f\u77e5\u7cfb\u7edf\u4e2d\uff0c\u7528\u6237\u901a\u8fc7\u9006\u5411\u7ade\u62cd\u6846\u67b6\u63d0\u4ea4\u6570\u636e\uff0c\u4f46\u4f20\u7edf\u65b9\u6cd5\u53ef\u80fd\u5bfc\u81f4\u8d44\u6e90\u6d6a\u8d39\u3002\u7814\u7a76\u65e8\u5728\u4f18\u5316\u7cfb\u7edf\u6548\u7528\uff0c\u51cf\u5c11\u8d44\u6e90\u6d88\u8017\u3002", "method": "\u63d0\u51faRA-ABC\u65b9\u6cd5\uff0c\u8ba9\u7528\u6237\u5728\u6536\u96c6\u6570\u636e\u524d\u51fa\u4ef7\uff0c\u51cf\u5c11\u4efb\u52a1\u6267\u884c\u6d6a\u8d39\uff1b\u8fdb\u4e00\u6b65\u6269\u5c55\u4e3aRA-ABCDR\uff0c\u652f\u6301\u52a8\u6001\u7528\u6237\u52a0\u5165\u7ade\u62cd\u3002", "result": "RA-ABC\u548cRA-ABCDR\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\uff0cRA-ABCDR\u7528\u6237\u4fdd\u7559\u7387\u63d0\u9ad854.6%\uff0c\u62cd\u5356\u6210\u672c\u964d\u4f4e22.2%\uff0c\u7cfb\u7edf\u6548\u7387\u548c\u53ef\u6301\u7eed\u6027\u663e\u8457\u63d0\u5347\u3002", "conclusion": "\u52a8\u6001\u7528\u6237\u62db\u52df\u663e\u8457\u4f18\u5316\u7cfb\u7edf\u6027\u80fd\uff0cRA-ABCDR\u5728\u7a33\u5b9a\u6027\u3001\u516c\u5e73\u6027\u548c\u6210\u672c\u6548\u76ca\u65b9\u9762\u8868\u73b0\u4f18\u5f02\uff0c\u4e3aMCS\u7cfb\u7edf\u63d0\u4f9b\u4e86\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.07136", "categories": ["cs.GR"], "pdf": "https://arxiv.org/pdf/2507.07136", "abs": "https://arxiv.org/abs/2507.07136", "authors": ["Wanhua Li", "Yujie Zhao", "Minghan Qin", "Yang Liu", "Yuanhao Cai", "Chuang Gan", "Hanspeter Pfister"], "title": "LangSplatV2: High-dimensional 3D Language Gaussian Splatting with 450+ FPS", "comment": "Project Page: https://langsplat-v2.github.io", "summary": "In this paper, we introduce LangSplatV2, which achieves high-dimensional\nfeature splatting at 476.2 FPS and 3D open-vocabulary text querying at 384.6\nFPS for high-resolution images, providing a 42 $\\times$ speedup and a 47\n$\\times$ boost over LangSplat respectively, along with improved query accuracy.\nLangSplat employs Gaussian Splatting to embed 2D CLIP language features into\n3D, significantly enhancing speed and learning a precise 3D language field with\nSAM semantics. Such advancements in 3D language fields are crucial for\napplications that require language interaction within complex scenes. However,\nLangSplat does not yet achieve real-time inference performance (8.2 FPS), even\nwith advanced A100 GPUs, severely limiting its broader application. In this\npaper, we first conduct a detailed time analysis of LangSplat, identifying the\nheavyweight decoder as the primary speed bottleneck. Our solution, LangSplatV2\nassumes that each Gaussian acts as a sparse code within a global dictionary,\nleading to the learning of a 3D sparse coefficient field that entirely\neliminates the need for a heavyweight decoder. By leveraging this sparsity, we\nfurther propose an efficient sparse coefficient splatting method with CUDA\noptimization, rendering high-dimensional feature maps at high quality while\nincurring only the time cost of splatting an ultra-low-dimensional feature. Our\nexperimental results demonstrate that LangSplatV2 not only achieves better or\ncompetitive query accuracy but is also significantly faster. Codes and demos\nare available at our project page: https://langsplat-v2.github.io.", "AI": {"tldr": "LangSplatV2\u901a\u8fc7\u9ad8\u7ef4\u7279\u5f81\u6e85\u5c04\u548c3D\u5f00\u653e\u8bcd\u6c47\u6587\u672c\u67e5\u8be2\uff0c\u663e\u8457\u63d0\u5347\u4e86\u901f\u5ea6\u548c\u67e5\u8be2\u7cbe\u5ea6\uff0c\u514b\u670d\u4e86LangSplat\u5728\u5b9e\u65f6\u63a8\u7406\u6027\u80fd\u4e0a\u7684\u4e0d\u8db3\u3002", "motivation": "LangSplat\u57283D\u8bed\u8a00\u573a\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5b9e\u65f6\u63a8\u7406\u6027\u80fd\u4e0d\u8db3\uff088.2 FPS\uff09\uff0c\u9650\u5236\u4e86\u5176\u5e7f\u6cdb\u5e94\u7528\uff0c\u56e0\u6b64\u5f00\u53d1\u4e86LangSplatV2\u4ee5\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "LangSplatV2\u5c06\u6bcf\u4e2a\u9ad8\u65af\u89c6\u4e3a\u5168\u5c40\u5b57\u5178\u4e2d\u7684\u7a00\u758f\u4ee3\u7801\uff0c\u5b66\u4e603D\u7a00\u758f\u7cfb\u6570\u573a\uff0c\u6d88\u9664\u4e86\u91cd\u578b\u89e3\u7801\u5668\u7684\u9700\u6c42\uff0c\u5e76\u901a\u8fc7CUDA\u4f18\u5316\u5b9e\u73b0\u4e86\u9ad8\u6548\u7a00\u758f\u7cfb\u6570\u6e85\u5c04\u3002", "result": "LangSplatV2\u5728476.2 FPS\u548c384.6 FPS\u4e0b\u8fd0\u884c\uff0c\u5206\u522b\u6bd4LangSplat\u5feb42\u500d\u548c47\u500d\uff0c\u540c\u65f6\u63d0\u9ad8\u4e86\u67e5\u8be2\u7cbe\u5ea6\u3002", "conclusion": "LangSplatV2\u4e0d\u4ec5\u663e\u8457\u63d0\u5347\u4e86\u901f\u5ea6\u548c\u7cbe\u5ea6\uff0c\u8fd8\u901a\u8fc7\u9ad8\u6548\u7684\u65b9\u6cd5\u89e3\u51b3\u4e86\u5b9e\u65f6\u63a8\u7406\u7684\u74f6\u9888\uff0c\u4e3a3D\u8bed\u8a00\u573a\u7684\u5e94\u7528\u63d0\u4f9b\u4e86\u5f3a\u5927\u652f\u6301\u3002"}}
{"id": "2507.07711", "categories": ["cs.GT"], "pdf": "https://arxiv.org/pdf/2507.07711", "abs": "https://arxiv.org/abs/2507.07711", "authors": ["Zhen Zhang", "Weian Li", "Yuhan Wang", "Qi Qi", "Kun Huang"], "title": "Hybrid Advertising in the Sponsored Search", "comment": null, "summary": "Online advertisements are a primary revenue source for e-commerce platforms.\nTraditional advertising models are store-centric, selecting winning stores\nthrough auction mechanisms. Recently, a new approach known as joint advertising\nhas emerged, which presents sponsored bundles combining one store and one brand\nin ad slots. Unlike traditional models, joint advertising allows platforms to\ncollect payments from both brands and stores. However, each of these two\nadvertising models appeals to distinct user groups, leading to low\nclick-through rates when users encounter an undesirable advertising model. To\naddress this limitation and enhance generality, we propose a novel advertising\nmodel called ''Hybrid Advertising''. In this model, each ad slot can be\nallocated to either an independent store or a bundle. To find the optimal\nauction mechanisms in hybrid advertising, while ensuring nearly dominant\nstrategy incentive compatibility and individual rationality, we introduce the\nHybrid Regret Network (HRegNet), a neural network architecture designed for\nthis purpose. Extensive experiments on both synthetic and real-world data\ndemonstrate that the mechanisms generated by HRegNet significantly improve\nplatform revenue compared to established baseline methods.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a\"\u6df7\u5408\u5e7f\u544a\"\u7684\u65b0\u6a21\u578b\uff0c\u901a\u8fc7\u7ed3\u5408\u72ec\u7acb\u5546\u5e97\u548c\u6346\u7ed1\u5e7f\u544a\u6765\u63d0\u9ad8\u5e7f\u544a\u7684\u901a\u7528\u6027\u548c\u70b9\u51fb\u7387\uff0c\u5e76\u901a\u8fc7\u6df7\u5408\u9057\u61be\u7f51\u7edc\u4f18\u5316\u62cd\u5356\u673a\u5236\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5e73\u53f0\u6536\u5165\u3002", "motivation": "\u4f20\u7edf\u5e7f\u544a\u6a21\u578b\u548c\u8054\u5408\u5e7f\u544a\u6a21\u578b\u5206\u522b\u5438\u5f15\u4e0d\u540c\u7684\u7528\u6237\u7fa4\u4f53\uff0c\u5bfc\u81f4\u70b9\u51fb\u7387\u4f4e\uff0c\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e00\u9650\u5236\u5e76\u63d0\u9ad8\u901a\u7528\u6027\uff0c\u63d0\u51fa\u4e86\u6df7\u5408\u5e7f\u544a\u6a21\u578b\u3002", "method": "\u63d0\u51fa\u6df7\u5408\u5e7f\u544a\u6a21\u578b\uff0c\u6bcf\u4e2a\u5e7f\u544a\u4f4d\u53ef\u4ee5\u5206\u914d\u7ed9\u72ec\u7acb\u5546\u5e97\u6216\u6346\u7ed1\u5e7f\u544a\uff0c\u5e76\u8bbe\u8ba1\u6df7\u5408\u9057\u61be\u7f51\u7edc\uff08HRegNet\uff09\u6765\u4f18\u5316\u62cd\u5356\u673a\u5236\u3002", "result": "\u5728\u5408\u6210\u548c\u771f\u5b9e\u6570\u636e\u4e0a\u7684\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0cHRegNet\u751f\u6210\u7684\u673a\u5236\u663e\u8457\u63d0\u9ad8\u4e86\u5e73\u53f0\u6536\u5165\uff0c\u4f18\u4e8e\u73b0\u6709\u57fa\u51c6\u65b9\u6cd5\u3002", "conclusion": "\u6df7\u5408\u5e7f\u544a\u6a21\u578b\u53ca\u5176\u4f18\u5316\u673a\u5236HRegNet\u5728\u63d0\u5347\u5e7f\u544a\u6548\u679c\u548c\u5e73\u53f0\u6536\u5165\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u4e3a\u5e7f\u544a\u6a21\u578b\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2507.07387", "categories": ["cs.GR", "cs.HC"], "pdf": "https://arxiv.org/pdf/2507.07387", "abs": "https://arxiv.org/abs/2507.07387", "authors": ["Chengan He", "Jorge Alejandro Amador Herrera", "Zhixin Shu", "Xin Sun", "Yao Feng", "S\u00f6ren Pirk", "Dominik L. Michels", "Meng Zhang", "Tuanfeng Y. Wang", "Julie Dorsey", "Holly Rushmeier", "Yi Zhou"], "title": "Digital Salon: An AI and Physics-Driven Tool for 3D Hair Grooming and Simulation", "comment": null, "summary": "We introduce Digital Salon, a comprehensive hair authoring system that\nsupports real-time 3D hair generation, simulation, and rendering. Unlike\nexisting methods that focus on isolated parts of 3D hair modeling and involve a\nheavy computation process or network training, Digital Salon offers a holistic\nand interactive system that lowers the technical barriers of 3D hair modeling\nthrough natural language-based interaction. The system guides users through\nfour key stages: text-guided hair retrieval, real-time hair simulation,\ninteractive hair refinement, and hair-conditioned image generation. This\ncohesive workflow makes advanced hair design accessible to users of varying\nskill levels and dramatically streamlines the creative process in digital media\nwith an intuitive, versatile, and efficient solution for hair modeling. User\nstudies show that our system can outperform traditional hair modeling workflows\nfor rapid prototyping. Furthermore, we provide insights into the benefits of\nour system with future potential of deploying our system in real salon\nenvironments. More details can be found on our project page:\nhttps://digital-salon.github.io/.", "AI": {"tldr": "Digital Salon\u662f\u4e00\u4e2a\u5168\u9762\u76843D\u5934\u53d1\u521b\u4f5c\u7cfb\u7edf\uff0c\u652f\u6301\u5b9e\u65f6\u751f\u6210\u3001\u6a21\u62df\u548c\u6e32\u67d3\uff0c\u901a\u8fc7\u81ea\u7136\u8bed\u8a00\u4ea4\u4e92\u964d\u4f4e\u6280\u672f\u95e8\u69db\u3002", "motivation": "\u73b0\u6709\u76843D\u5934\u53d1\u5efa\u6a21\u65b9\u6cd5\u901a\u5e38\u4e13\u6ce8\u4e8e\u5b64\u7acb\u90e8\u5206\uff0c\u6d89\u53ca\u7e41\u91cd\u7684\u8ba1\u7b97\u6216\u7f51\u7edc\u8bad\u7ec3\uff0cDigital Salon\u65e8\u5728\u63d0\u4f9b\u4e00\u79cd\u6574\u4f53\u4e14\u4ea4\u4e92\u5f0f\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u964d\u4f4e\u6280\u672f\u96be\u5ea6\u3002", "method": "\u7cfb\u7edf\u901a\u8fc7\u56db\u4e2a\u5173\u952e\u9636\u6bb5\u5b9e\u73b0\uff1a\u6587\u672c\u5f15\u5bfc\u7684\u5934\u53d1\u68c0\u7d22\u3001\u5b9e\u65f6\u5934\u53d1\u6a21\u62df\u3001\u4ea4\u4e92\u5f0f\u5934\u53d1\u7ec6\u5316\u53ca\u5934\u53d1\u6761\u4ef6\u7684\u56fe\u50cf\u751f\u6210\u3002", "result": "\u7528\u6237\u7814\u7a76\u8868\u660e\uff0c\u8be5\u7cfb\u7edf\u5728\u5feb\u901f\u539f\u578b\u8bbe\u8ba1\u65b9\u9762\u4f18\u4e8e\u4f20\u7edf\u7684\u5de5\u4f5c\u6d41\u7a0b\uff0c\u5e76\u5c55\u793a\u4e86\u5728\u771f\u5b9e\u6c99\u9f99\u73af\u5883\u4e2d\u90e8\u7f72\u7684\u6f5c\u529b\u3002", "conclusion": "Digital Salon\u4e3a\u5404\u79cd\u6280\u80fd\u6c34\u5e73\u7684\u7528\u6237\u63d0\u4f9b\u4e86\u76f4\u89c2\u3001\u591a\u529f\u80fd\u76843D\u5934\u53d1\u5efa\u6a21\u89e3\u51b3\u65b9\u6848\uff0c\u6781\u5927\u5730\u7b80\u5316\u4e86\u521b\u610f\u6d41\u7a0b\u3002"}}
{"id": "2507.07915", "categories": ["cs.GT"], "pdf": "https://arxiv.org/pdf/2507.07915", "abs": "https://arxiv.org/abs/2507.07915", "authors": ["George Christodoulou", "Vasilis Christoforidis", "Alkmini Sgouritsa", "Ioannis Vlachos"], "title": "Improving the Price of Anarchy via Predictions in Parallel-Link Networks", "comment": null, "summary": "We study non-atomic congestion games on parallel-link networks with affine\ncost functions. We investigate the power of machine-learned predictions in the\ndesign of coordination mechanisms aimed at minimizing the impact of\nselfishness. Our main results demonstrate that enhancing coordination\nmechanisms with a simple advice on the input rate can optimize the social cost\nwhenever the advice is accurate (consistency), while only incurring minimal\nlosses even when the predictions are arbitrarily inaccurate (bounded\nrobustness). Moreover, we provide a full characterization of the consistent\nmechanisms that holds for all monotone cost functions, and show that our\nsuggested mechanism is optimal with respect to the robustness. We further\nexplore the notion of smoothness within this context: we extend our mechanism\nto achieve error-tolerance, i.e. we provide an approximation guarantee that\ndegrades smoothly as a function of the prediction error, up to a predetermined\nthreshold, while achieving a bounded robustness.", "AI": {"tldr": "\u7814\u7a76\u4e86\u5e76\u884c\u94fe\u8def\u7f51\u7edc\u4e0a\u7684\u975e\u539f\u5b50\u62e5\u585e\u6e38\u620f\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u673a\u5668\u5b66\u4e60\u9884\u6d4b\u7684\u534f\u8c03\u673a\u5236\uff0c\u4ee5\u6700\u5c0f\u5316\u81ea\u79c1\u884c\u4e3a\u7684\u5f71\u54cd\u3002\u673a\u5236\u5728\u9884\u6d4b\u51c6\u786e\u65f6\u4f18\u5316\u793e\u4f1a\u6210\u672c\uff0c\u9884\u6d4b\u4e0d\u51c6\u786e\u65f6\u635f\u5931\u6700\u5c0f\u5316\u3002", "motivation": "\u7814\u7a76\u52a8\u673a\u5728\u4e8e\u63a2\u7d22\u5982\u4f55\u5229\u7528\u673a\u5668\u5b66\u4e60\u9884\u6d4b\u6765\u8bbe\u8ba1\u534f\u8c03\u673a\u5236\uff0c\u4ee5\u51cf\u5c11\u81ea\u79c1\u884c\u4e3a\u5bf9\u5e76\u884c\u94fe\u8def\u7f51\u7edc\u62e5\u585e\u6e38\u620f\u7684\u5f71\u54cd\u3002", "method": "\u901a\u8fc7\u589e\u5f3a\u534f\u8c03\u673a\u5236\uff0c\u5f15\u5165\u7b80\u5355\u7684\u8f93\u5165\u901f\u7387\u5efa\u8bae\uff0c\u5206\u6790\u5176\u5bf9\u5355\u8c03\u6210\u672c\u51fd\u6570\u7684\u7edf\u4e00\u5f71\u54cd\uff0c\u5e76\u63d0\u51fa\u4e00\u79cd\u5177\u6709\u8bef\u5dee\u5bb9\u5fcd\u6027\u7684\u673a\u5236\u3002", "result": "\u7ed3\u679c\u663e\u793a\uff0c\u673a\u5236\u5728\u9884\u6d4b\u51c6\u786e\u65f6\u80fd\u4f18\u5316\u793e\u4f1a\u6210\u672c\uff0c\u9884\u6d4b\u4e0d\u51c6\u786e\u65f6\u635f\u5931\u6709\u9650\u3002\u6269\u5c55\u673a\u5236\u53ef\u5728\u9884\u6d4b\u8bef\u5dee\u8303\u56f4\u5185\u5e73\u6ed1\u964d\u4f4e\u6027\u80fd\uff0c\u540c\u65f6\u4fdd\u6301\u7a33\u5065\u6027\u3002", "conclusion": "\u901a\u8fc7\u7814\u7a76\uff0c\u8bc1\u660e\u4e86\u5efa\u8bae\u673a\u5236\u5728\u4e00\u81f4\u6027\u548c\u7a33\u5065\u6027\u65b9\u9762\u7684\u4f18\u52bf\uff0c\u5e76\u4e3a\u5355\u8c03\u6210\u672c\u51fd\u6570\u63d0\u4f9b\u4e86\u5b8c\u6574\u7684\u7279\u5f81\u63cf\u8ff0\uff0c\u8868\u660e\u5176\u5728\u9884\u6d4b\u8bef\u5dee\u8303\u56f4\u5185\u7684\u6700\u4f18\u8868\u73b0\u3002"}}
{"id": "2507.07440", "categories": ["cs.GR"], "pdf": "https://arxiv.org/pdf/2507.07440", "abs": "https://arxiv.org/abs/2507.07440", "authors": ["Yue Li", "Gene Wei-Chin Lin", "Egor Larionov", "Aljaz Bozic", "Doug Roble", "Ladislav Kavan", "Stelian Coros", "Bernhard Thomaszewski", "Tuur Stuyck", "Hsiao-yu Chen"], "title": "Self-supervised Learning of Latent Space Dynamics", "comment": null, "summary": "Modeling the dynamic behavior of deformable objects is crucial for creating\nrealistic digital worlds. While conventional simulations produce high-quality\nmotions, their computational costs are often prohibitive. Subspace simulation\ntechniques address this challenge by restricting deformations to a\nlower-dimensional space, improving performance while maintaining visually\ncompelling results. However, even subspace methods struggle to meet the\nstringent performance demands of portable devices such as virtual reality\nheadsets and mobile platforms. To overcome this limitation, we introduce a\nnovel subspace simulation framework powered by a neural latent-space\nintegrator. Our approach leverages self-supervised learning to enhance\ninference stability and generalization. By operating entirely within latent\nspace, our method eliminates the need for full-space computations, resulting in\na highly efficient method well-suited for deployment on portable devices. We\ndemonstrate the effectiveness of our approach on challenging examples involving\nrods, shells, and solids, showcasing its versatility and potential for\nwidespread adoption.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u795e\u7ecf\u6f5c\u5728\u7a7a\u95f4\u79ef\u5206\u5668\u7684\u65b0\u578b\u5b50\u7a7a\u95f4\u6a21\u62df\u6846\u67b6\uff0c\u9002\u7528\u4e8e\u4fbf\u643a\u8bbe\u5907\uff0c\u901a\u8fc7\u81ea\u76d1\u7763\u5b66\u4e60\u63d0\u9ad8\u63a8\u7406\u7a33\u5b9a\u6027\u548c\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u4f20\u7edf\u6a21\u62df\u65b9\u6cd5\u8ba1\u7b97\u6210\u672c\u9ad8\uff0c\u5b50\u7a7a\u95f4\u65b9\u6cd5\u867d\u80fd\u63d0\u9ad8\u6027\u80fd\uff0c\u4f46\u4ecd\u96be\u4ee5\u6ee1\u8db3\u4fbf\u643a\u8bbe\u5907\u7684\u4e25\u683c\u6027\u80fd\u9700\u6c42\uff0c\u56e0\u6b64\u9700\u8981\u66f4\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u5f15\u5165\u795e\u7ecf\u6f5c\u5728\u7a7a\u95f4\u79ef\u5206\u5668\u7684\u5b50\u7a7a\u95f4\u6a21\u62df\u6846\u67b6\uff0c\u5b8c\u5168\u5728\u6f5c\u5728\u7a7a\u95f4\u5185\u64cd\u4f5c\uff0c\u907f\u514d\u4e86\u5168\u7a7a\u95f4\u8ba1\u7b97\uff0c\u5e76\u901a\u8fc7\u81ea\u76d1\u7763\u5b66\u4e60\u4f18\u5316\u63a8\u7406\u8fc7\u7a0b\u3002", "result": "\u5728\u6d89\u53ca\u6746\u3001\u58f3\u548c\u56fa\u4f53\u7b49\u590d\u6742\u793a\u4f8b\u4e2d\uff0c\u8be5\u65b9\u6cd5\u8868\u73b0\u51fa\u9ad8\u6548\u6027\u548c\u591a\u529f\u80fd\u6027\uff0c\u9002\u5408\u4fbf\u643a\u8bbe\u5907\u90e8\u7f72\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u663e\u8457\u63d0\u9ad8\u4e86\u6a21\u62df\u6548\u7387\uff0c\u4e3a\u4fbf\u643a\u8bbe\u5907\u4e0a\u7684\u52a8\u6001\u884c\u4e3a\u5efa\u6a21\u63d0\u4f9b\u4e86\u53ef\u884c\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5177\u6709\u5e7f\u6cdb\u7684\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2507.07465", "categories": ["cs.GR", "cs.CV"], "pdf": "https://arxiv.org/pdf/2507.07465", "abs": "https://arxiv.org/abs/2507.07465", "authors": ["Wei Yao", "Shuzhao Xie", "Letian Li", "Weixiang Zhang", "Zhixin Lai", "Shiqi Dai", "Ke Zhang", "Zhi Wang"], "title": "SD-GS: Structured Deformable 3D Gaussians for Efficient Dynamic Scene Reconstruction", "comment": null, "summary": "Current 4D Gaussian frameworks for dynamic scene reconstruction deliver\nimpressive visual fidelity and rendering speed, however, the inherent trade-off\nbetween storage costs and the ability to characterize complex physical motions\nsignificantly limits the practical application of these methods. To tackle\nthese problems, we propose SD-GS, a compact and efficient dynamic Gaussian\nsplatting framework for complex dynamic scene reconstruction, featuring two key\ncontributions. First, we introduce a deformable anchor grid, a hierarchical and\nmemory-efficient scene representation where each anchor point derives multiple\n3D Gaussians in its local spatiotemporal region and serves as the geometric\nbackbone of the 3D scene. Second, to enhance modeling capability for complex\nmotions, we present a deformation-aware densification strategy that adaptively\ngrows anchors in under-reconstructed high-dynamic regions while reducing\nredundancy in static areas, achieving superior visual quality with fewer\nanchors. Experimental results demonstrate that, compared to state-of-the-art\nmethods, SD-GS achieves an average of 60\\% reduction in model size and an\naverage of 100\\% improvement in FPS, significantly enhancing computational\nefficiency while maintaining or even surpassing visual quality.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7d27\u51d1\u9ad8\u6548\u7684\u52a8\u6001\u9ad8\u65af\u6846\u67b6SD-GS\uff0c\u901a\u8fc7\u53ef\u53d8\u5f62\u951a\u70b9\u7f51\u683c\u548c\u53d8\u5f62\u611f\u77e5\u7684\u5bc6\u96c6\u5316\u7b56\u7565\uff0c\u663e\u8457\u964d\u4f4e\u5b58\u50a8\u6210\u672c\u5e76\u63d0\u5347\u52a8\u6001\u573a\u666f\u91cd\u5efa\u7684\u89c6\u89c9\u8d28\u91cf\u548c\u8ba1\u7b97\u6548\u7387\u3002", "motivation": "\u73b0\u6709\u76844D\u9ad8\u65af\u6846\u67b6\u5728\u52a8\u6001\u573a\u666f\u91cd\u5efa\u4e2d\u5b58\u5728\u5b58\u50a8\u6210\u672c\u4e0e\u590d\u6742\u8fd0\u52a8\u8868\u5f81\u80fd\u529b\u4e4b\u95f4\u7684\u56fa\u6709\u6743\u8861\uff0c\u9650\u5236\u4e86\u5176\u5b9e\u9645\u5e94\u7528\u3002", "method": "SD-GS\u6846\u67b6\u5f15\u5165\u53ef\u53d8\u5f62\u951a\u70b9\u7f51\u683c\u4f5c\u4e3a\u5206\u5c42\u9ad8\u6548\u573a\u666f\u8868\u793a\uff0c\u5e76\u901a\u8fc7\u53d8\u5f62\u611f\u77e5\u5bc6\u96c6\u5316\u7b56\u7565\u52a8\u6001\u8c03\u6574\u951a\u70b9\u5206\u5e03\uff0c\u4f18\u5316\u590d\u6742\u8fd0\u52a8\u7684\u5efa\u6a21\u80fd\u529b\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cSD-GS\u76f8\u6bd4\u73b0\u6709\u65b9\u6cd5\u5e73\u5747\u51cf\u5c1160%\u6a21\u578b\u5927\u5c0f\uff0c\u63d0\u5347100%\u5e27\u7387\uff0c\u540c\u65f6\u4fdd\u6301\u6216\u63d0\u5347\u89c6\u89c9\u8d28\u91cf\u3002", "conclusion": "SD-GS\u901a\u8fc7\u521b\u65b0\u8bbe\u8ba1\u663e\u8457\u63d0\u5347\u4e86\u52a8\u6001\u573a\u666f\u91cd\u5efa\u7684\u6548\u7387\u548c\u6548\u679c\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u7684\u5c40\u9650\u3002"}}
{"id": "2507.07623", "categories": ["cs.GR", "cs.CV"], "pdf": "https://arxiv.org/pdf/2507.07623", "abs": "https://arxiv.org/abs/2507.07623", "authors": ["Hannah Dr\u00f6ge", "Janelle Pfeifer", "Saskia Rabich", "Markus Plack", "Reinhard Klein", "Matthias B. Hullin"], "title": "Capture Stage Environments: A Guide to Better Matting", "comment": null, "summary": "Capture stages are high-end sources of state-of-the-art recordings for\ndownstream applications in movies, games, and other media. One crucial step in\nalmost all pipelines is the matting of images to isolate the captured\nperformances from the background. While common matting algorithms deliver\nremarkable performance in other applications like teleconferencing and mobile\nentertainment, we found that they struggle significantly with the peculiarities\nof capture stage content. The goal of our work is to share insights into those\nchallenges as a curated list of those characteristics along with a constructive\ndiscussion for proactive intervention and present a guideline to practitioners\nfor an improved workflow to mitigate unresolved challenges. To this end, we\nalso demonstrate an efficient pipeline to adapt state-of-the-art approaches to\nsuch custom setups without the need of extensive annotations, both offline and\nreal-time. For an objective evaluation, we propose a validation methodology\nbased on a leading diffusion model that highlights the benefits of our\napproach.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63a2\u8ba8\u4e86\u5728\u9ad8\u7aef\u5f55\u5236\u573a\u666f\u4e2d\uff0c\u5e38\u89c1\u7684\u56fe\u50cf\u62a0\u56fe\u7b97\u6cd5\u5728\u5e94\u5bf9\u7279\u5b9a\u6311\u6218\u65f6\u7684\u4e0d\u8db3\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u6539\u8fdb\u7684\u5de5\u4f5c\u6d41\u7a0b\u548c\u9a8c\u8bc1\u65b9\u6cd5\u3002", "motivation": "\u7531\u4e8e\u73b0\u6709\u56fe\u50cf\u62a0\u56fe\u7b97\u6cd5\u5728\u9ad8\u5f55\u5236\u573a\u666f\u4e2d\u7684\u8868\u73b0\u4e0d\u4f73\uff0c\u8bba\u6587\u65e8\u5728\u5206\u4eab\u8fd9\u4e9b\u6311\u6218\u5e76\u63d0\u4f9b\u4e00\u4e2a\u6539\u8fdb\u7684\u5de5\u4f5c\u6d41\u7a0b\u6307\u5357\u3002", "method": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u9ad8\u6548\u7684\u7ba1\u9053\uff0c\u53ef\u4ee5\u65e0\u9700\u5927\u91cf\u6807\u6ce8\u5373\u53ef\u9002\u5e94\u5b9a\u5236\u5316\u8bbe\u7f6e\uff0c\u5e76\u63d0\u4f9b\u4e86\u4e00\u4e2a\u57fa\u4e8e\u6269\u6563\u6a21\u578b\u7684\u9a8c\u8bc1\u65b9\u6cd5\u3002", "result": "\u5c55\u793a\u4e86\u4e00\u79cd\u65e0\u9700\u5927\u91cf\u6807\u6ce8\u5373\u53ef\u9002\u5e94\u590d\u6742\u5f55\u5236\u573a\u666f\u7684\u65b9\u6cd5\uff0c\u5e76\u901a\u8fc7\u9a8c\u8bc1\u65b9\u6cd5\u8bc1\u660e\u4e86\u5176\u4f18\u52bf\u3002", "conclusion": "\u8bba\u6587\u4e3a\u5b9e\u8df5\u8005\u63d0\u4f9b\u4e86\u4e00\u4e2a\u6539\u8fdb\u7684\u56fe\u50cf\u62a0\u56fe\u5de5\u4f5c\u6d41\u7a0b\uff0c\u5e76\u9a8c\u8bc1\u4e86\u5176\u5728\u590d\u6742\u5f55\u5236\u573a\u666f\u4e2d\u7684\u6709\u6548\u6027\u3002"}}
{"id": "2507.07733", "categories": ["cs.GR", "cs.CV"], "pdf": "https://arxiv.org/pdf/2507.07733", "abs": "https://arxiv.org/abs/2507.07733", "authors": ["Yongyang Zhou", "Fang-Lue Zhang", "Zichen Wang", "Lei Zhang"], "title": "RTR-GS: 3D Gaussian Splatting for Inverse Rendering with Radiance Transfer and Reflection", "comment": "16 pages", "summary": "3D Gaussian Splatting (3DGS) has demonstrated impressive capabilities in\nnovel view synthesis. However, rendering reflective objects remains a\nsignificant challenge, particularly in inverse rendering and relighting. We\nintroduce RTR-GS, a novel inverse rendering framework capable of robustly\nrendering objects with arbitrary reflectance properties, decomposing BRDF and\nlighting, and delivering credible relighting results. Given a collection of\nmulti-view images, our method effectively recovers geometric structure through\na hybrid rendering model that combines forward rendering for radiance transfer\nwith deferred rendering for reflections. This approach successfully separates\nhigh-frequency and low-frequency appearances, mitigating floating artifacts\ncaused by spherical harmonic overfitting when handling high-frequency details.\nWe further refine BRDF and lighting decomposition using an additional\nphysically-based deferred rendering branch. Experimental results show that our\nmethod enhances novel view synthesis, normal estimation, decomposition, and\nrelighting while maintaining efficient training inference process.", "AI": {"tldr": "RTR-GS \u662f\u4e00\u4e2a\u65b0\u9896\u7684\u53cd\u5411\u6e32\u67d3\u6846\u67b6\uff0c\u901a\u8fc7\u7ed3\u5408\u6b63\u5411\u548c\u5ef6\u8fdf\u6e32\u67d3\u6765\u5206\u89e3BRDF\u548c\u5149\u7167\uff0c\u6709\u6548\u6e32\u67d3\u5177\u6709\u4efb\u610f\u53cd\u5c04\u5c5e\u6027\u7684\u7269\u4f53\uff0c\u63d0\u4f9b\u53ef\u4fe1\u7684\u91cd\u5149\u7167\u7ed3\u679c\u3002", "motivation": "\u5c3d\u7ba13D\u9ad8\u65af\u6e85\u5c04\uff083DGS\uff09\u5728\u65b0\u89c6\u89d2\u5408\u6210\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u6e32\u67d3\u53cd\u5c04\u7269\u4f53\u4ecd\u7136\u662f\u4e00\u4e2a\u91cd\u5927\u6311\u6218\uff0c\u5c24\u5176\u662f\u5728\u53cd\u5411\u6e32\u67d3\u548c\u91cd\u5149\u7167\u65b9\u9762\u3002", "method": "RTR-GS\u91c7\u7528\u6df7\u5408\u6e32\u67d3\u6a21\u578b\uff0c\u7ed3\u5408\u6b63\u5411\u6e32\u67d3\u548c\u5ef6\u8fdf\u6e32\u67d3\uff0c\u6709\u6548\u6062\u590d\u51e0\u4f55\u7ed3\u6784\u5e76\u5206\u89e3BRDF\u548c\u5149\u7167\uff0c\u540c\u65f6\u901a\u8fc7\u989d\u5916\u7684\u57fa\u4e8e\u7269\u7406\u7684\u5ef6\u8fdf\u6e32\u67d3\u5206\u652f\u8fdb\u4e00\u6b65\u4f18\u5316\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u65b0\u89c6\u89d2\u5408\u6210\u3001\u6cd5\u7ebf\u4f30\u8ba1\u3001BRDF\u548c\u5149\u7167\u5206\u89e3\u4ee5\u53ca\u91cd\u5149\u7167\u65b9\u9762\u8868\u73b0\u4f18\u5f02\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u9ad8\u6548\u7684\u8bad\u7ec3\u548c\u63a8\u7406\u8fc7\u7a0b\u3002", "conclusion": "RTR-GS\u6210\u529f\u89e3\u51b3\u4e86\u53cd\u5c04\u7269\u4f53\u7684\u6e32\u67d3\u95ee\u9898\uff0c\u63d0\u4f9b\u4e86\u9ad8\u8d28\u91cf\u7684\u5206\u89e3\u548c\u91cd\u5149\u7167\u7ed3\u679c\uff0c\u4e3a\u76f8\u5173\u9886\u57df\u7684\u7814\u7a76\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.07890", "categories": ["cs.GR"], "pdf": "https://arxiv.org/pdf/2507.07890", "abs": "https://arxiv.org/abs/2507.07890", "authors": ["Radi Muhammad Reza", "Benjamin A Watson"], "title": "Hi-d maps: An interactive visualization technique for multi-dimensional categorical data", "comment": null, "summary": "In this paper, we present Hi-D maps, a novel method for the visualization of\nmulti-dimensional categorical data. Our work addresses the scarcity of\ntechniques for visualizing a large number of data-dimensions in an effective\nand space-efficient manner. We have mapped the full data-space onto a 2D\nregular polygonal region. The polygon is cut hierarchically with lines parallel\nto a user-controlled, ordered sequence of sides, each representing a dimension.\nWe have used multiple visual cues such as orientation, thickness, color,\ncountable glyphs, and text to depict cross-dimensional information. We have\nadded interactivity and hierarchical browsing to facilitate flexible\nexploration of the display: small areas can be scrutinized for details. Thus,\nour method is also easily extendable to visualize hierarchical information. Our\nglyph animations add an engaging aesthetic during interaction. Like many\nvisualizations, Hi-D maps become less effective when a large number of\ndimensions stresses perceptual limits, but Hi-D maps may add clarity before\nthose limits are reached.", "AI": {"tldr": "Hi-D maps\u662f\u4e00\u79cd\u65b0\u9896\u7684\u591a\u7ef4\u5206\u7c7b\u6570\u636e\u53ef\u89c6\u5316\u65b9\u6cd5\uff0c\u901a\u8fc72D\u591a\u8fb9\u5f62\u533a\u57df\u548c\u591a\u79cd\u89c6\u89c9\u7ebf\u7d22\u5b9e\u73b0\u9ad8\u6548\u7684\u7a7a\u95f4\u5229\u7528\u548c\u4fe1\u606f\u5c55\u793a\u3002", "motivation": "\u5f53\u524d\u7f3a\u4e4f\u80fd\u591f\u6709\u6548\u4e14\u7a7a\u95f4\u9ad8\u6548\u5730\u53ef\u89c6\u5316\u5927\u91cf\u6570\u636e\u7ef4\u5ea6\u7684\u6280\u672f\uff0cHi-D maps\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u5c06\u5b8c\u6574\u6570\u636e\u7a7a\u95f4\u6620\u5c04\u52302D\u591a\u8fb9\u5f62\u533a\u57df\uff0c\u901a\u8fc7\u5e73\u884c\u4e8e\u7528\u6237\u63a7\u5236\u987a\u5e8f\u7684\u8fb9\u7ebf\u8fdb\u884c\u5206\u5c42\u5207\u5272\uff0c\u5e76\u5229\u7528\u591a\u79cd\u89c6\u89c9\u7ebf\u7d22\uff08\u5982\u65b9\u5411\u3001\u539a\u5ea6\u3001\u989c\u8272\u3001\u53ef\u8ba1\u6570\u6807\u8bb0\u548c\u6587\u672c\uff09\u5c55\u793a\u8de8\u7ef4\u5ea6\u4fe1\u606f\u3002", "result": "Hi-D maps\u63d0\u4f9b\u4e86\u4ea4\u4e92\u6027\u548c\u5206\u5c42\u6d4f\u89c8\u529f\u80fd\uff0c\u4fbf\u4e8e\u7075\u6d3b\u63a2\u7d22\u6570\u636e\u7ec6\u8282\uff0c\u540c\u65f6\u6807\u8bb0\u52a8\u753b\u589e\u52a0\u4e86\u4ea4\u4e92\u65f6\u7684\u89c6\u89c9\u5438\u5f15\u529b\u3002\u5c3d\u7ba1\u5728\u7ef4\u5ea6\u8fc7\u591a\u65f6\u4f1a\u964d\u4f4e\u6548\u679c\uff0c\u4f46\u5728\u8fbe\u5230\u611f\u77e5\u6781\u9650\u524d\u53ef\u80fd\u589e\u52a0\u6e05\u6670\u5ea6\u3002", "conclusion": "Hi-D maps\u4e3a\u591a\u7ef4\u5206\u7c7b\u6570\u636e\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u7684\u53ef\u89c6\u5316\u65b9\u6cd5\uff0c\u5177\u6709\u4ea4\u4e92\u6027\u548c\u5206\u5c42\u6d4f\u89c8\u7684\u6269\u5c55\u80fd\u529b\uff0c\u5c24\u5176\u5728\u7ef4\u5ea6\u6570\u91cf\u9002\u4e2d\u65f6\u6548\u679c\u663e\u8457\u3002"}}
