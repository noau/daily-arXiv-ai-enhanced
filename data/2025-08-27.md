<div id=toc></div>

# Table of Contents

- [cs.GR](#cs.GR) [Total: 5]
- [cs.PL](#cs.PL) [Total: 1]
- [cs.GT](#cs.GT) [Total: 3]


<div id='cs.GR'></div>

# cs.GR [[Back]](#toc)

### [1] [Controllable Single-shot Animation Blending with Temporal Conditioning](https://arxiv.org/abs/2508.18525)
*Eleni Tselepi,Spyridon Thermos,Gerasimos Potamianos*

Main category: cs.GR

TL;DR: 提出了一种单次运动生成模型，能够在无需额外数据或重新训练的情况下，实现多个运动的无缝混合，并通过时间条件化生成过程提供可控框架。


<details>
  <summary>Details</summary>
Motivation: 现有的单次运动生成方法缺乏明确的框架来控制多个运动的混合，这在动画制作中限制了灵活性和可控性。为了解决这一问题，本文提出了一种新的混合框架。

Method: 通过引入骨架感知的归一化机制，在生成过程中时间条件化，指导不同运动之间的过渡，从而实现对混合时间和方式的平滑控制。

Result: 模型在多种动画风格和不同骨架结构上进行了广泛评估，结果显示其能够以统一且高效的方式生成逼真、平滑且可控的运动混合结果。

Conclusion: 该方法为单次运动生成提供了可控的混合框架，显著提升了动画制作的灵活性和效率。

Abstract: Training a generative model on a single human skeletal motion sequence
without being bound to a specific kinematic tree has drawn significant
attention from the animation community. Unlike text-to-motion generation,
single-shot models allow animators to controllably generate variations of
existing motion patterns without requiring additional data or extensive
retraining. However, existing single-shot methods do not explicitly offer a
controllable framework for blending two or more motions within a single
generative pass. In this paper, we present the first single-shot motion
blending framework that enables seamless blending by temporally conditioning
the generation process. Our method introduces a skeleton-aware normalization
mechanism to guide the transition between motions, allowing smooth, data-driven
control over when and how motions blend. We perform extensive quantitative and
qualitative evaluations across various animation styles and different kinematic
skeletons, demonstrating that our approach produces plausible, smooth, and
controllable motion blends in a unified and efficient manner.

</details>


### [2] [Real-time 3D Visualization of Radiance Fields on Light Field Displays](https://arxiv.org/abs/2508.18540)
*Jonghyun Kim,Cheng Sun,Michael Stengel,Matthew Chan,Andrew Russell,Jaehyun Jung,Wil Braithwaite,Shalini De Mello,David Luebke*

Main category: cs.GR

TL;DR: 提出了一种统一高效的方法，用于在光场显示器上实时渲染辐射场，支持多种辐射场表示，并在不损失图像质量的情况下实现显著加速。


<details>
  <summary>Details</summary>
Motivation: 辐射场虽然在3D场景可视化方面具有革命性意义，但在与光场显示器集成时面临巨大的计算挑战。需要一种高效的方法来解决这一问题。

Method: 采用基于单通道平面扫描策略和共享非方向性组件缓存的统一架构，支持多种辐射场表示（如NeRFs、3D高斯泼溅和稀疏体素），避免跨视图的冗余计算。

Result: 在Looking Glass显示器上实现了200+ FPS的实时交互应用，512p分辨率下支持45个视图。标准测试中，相比独立渲染每个视图，实现了高达22倍的加速，同时保持图像质量。

Conclusion: 该框架为光场显示器上的辐射场渲染提供了一个高效且通用的解决方案，实现了实时交互和高质量的3D可视化效果。

Abstract: Radiance fields have revolutionized photo-realistic 3D scene visualization by
enabling high-fidelity reconstruction of complex environments, making them an
ideal match for light field displays. However, integrating these technologies
presents significant computational challenges, as light field displays require
multiple high-resolution renderings from slightly shifted viewpoints, while
radiance fields rely on computationally intensive volume rendering. In this
paper, we propose a unified and efficient framework for real-time radiance
field rendering on light field displays. Our method supports a wide range of
radiance field representations, including NeRFs, 3D Gaussian Splatting, and
Sparse Voxels, within a shared architecture based on a single-pass plane
sweeping strategy and caching of shared, non-directional components. The
framework generalizes across different scene formats without retraining, and
avoids redundant computation across views. We further demonstrate a real-time
interactive application on a Looking Glass display, achieving 200+ FPS at 512p
across 45 views, enabling seamless, immersive 3D interaction. On standard
benchmarks, our method achieves up to 22x speedup compared to independently
rendering each view, while preserving image quality.

</details>


### [3] [SemLayoutDiff: Semantic Layout Generation with Diffusion Model for Indoor Scene Synthesis](https://arxiv.org/abs/2508.18597)
*Xiaohao Sun,Divyam Goel,Angle X. Chang*

Main category: cs.GR

TL;DR: SemLayoutDiff是一个统一模型，用于合成多种3D室内场景，结合了语义图和物体属性的布局表示，利用扩散模型生成语义图并通过跨注意力网络预测家具布局，优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 为了解决现有方法无法考虑建筑约束的问题，提出了一种能够基于房间掩码显式生成场景的模型。

Method: 模型采用类别扩散模型生成语义图，并通过跨注意力网络预测家具布局，同时考虑门窗等建筑元素。

Result: 在3D-FRONT数据集上的实验表明，模型生成的空间连贯、真实且多样化的场景优于之前的方法。

Conclusion: SemLayoutDiff通过显式结合建筑约束，能够高效生成实用且无障碍的3D室内场景，表现出卓越的性能。

Abstract: We present SemLayoutDiff, a unified model for synthesizing diverse 3D indoor
scenes across multiple room types. The model introduces a scene layout
representation combining a top-down semantic map and attributes for each
object. Unlike prior approaches, which cannot condition on architectural
constraints, SemLayoutDiff employs a categorical diffusion model capable of
conditioning scene synthesis explicitly on room masks. It first generates a
coherent semantic map, followed by a cross-attention-based network to predict
furniture placements that respect the synthesized layout. Our method also
accounts for architectural elements such as doors and windows, ensuring that
generated furniture arrangements remain practical and unobstructed. Experiments
on the 3D-FRONT dataset show that SemLayoutDiff produces spatially coherent,
realistic, and varied scenes, outperforming previous methods.

</details>


### [4] [PanoHair: Detailed Hair Strand Synthesis on Volumetric Heads](https://arxiv.org/abs/2508.18944)
*Shashikant Verma,Shanmuganathan Raman*

Main category: cs.GR

TL;DR: PanoHair是一种新模型，通过知识蒸馏和生成式方法快速生成高保真头发几何结构，显著提高了效率和多样性。


<details>
  <summary>Details</summary>
Motivation: 当前的头发合成方法需要复杂的多视图数据采集，耗时且效率低下，PanoHair旨在解决这一问题。

Method: PanoHair利用预训练的生成式教师模型进行知识蒸馏，预测头发区域的语义分割掩码和3D方向，并通过潜在空间操作生成多样化的发型。

Result: PanoHair能够在5秒内生成干净的头发区域网格以及语义和方向图，实验显示其效率显著优于现有方法。

Conclusion: PanoHair提供了一种高效的头发合成方法，减少了复杂数据采集的需求，为生成逼真数字人类头发提供了新途径。

Abstract: Achieving realistic hair strand synthesis is essential for creating lifelike
digital humans, but producing high-fidelity hair strand geometry remains a
significant challenge. Existing methods require a complex setup for data
acquisition, involving multi-view images captured in constrained studio
environments. Additionally, these methods have longer hair volume estimation
and strand synthesis times, which hinder efficiency. We introduce PanoHair, a
model that estimates head geometry as signed distance fields using knowledge
distillation from a pre-trained generative teacher model for head synthesis.
Our approach enables the prediction of semantic segmentation masks and 3D
orientations specifically for the hair region of the estimated geometry. Our
method is generative and can generate diverse hairstyles with latent space
manipulations. For real images, our approach involves an inversion process to
infer latent codes and produces visually appealing hair strands, offering a
streamlined alternative to complex multi-view data acquisition setups. Given
the latent code, PanoHair generates a clean manifold mesh for the hair region
in under 5 seconds, along with semantic and orientation maps, marking a
significant improvement over existing methods, as demonstrated in our
experiments.

</details>


### [5] [A Bag of Tricks for Efficient Implicit Neural Point Clouds](https://arxiv.org/abs/2508.19140)
*Florian Hahlbohm,Linus Franke,Leon Overkämping,Paula Wespe,Susana Castillo,Martin Eisemann,Marcus Magnor*

Main category: cs.GR

TL;DR: 本文提出了一系列优化方法，显著提升了隐式神经点云（INPC）的训练和推理性能，同时保持了视觉保真度。


<details>
  <summary>Details</summary>
Motivation: 隐式神经点云（INPC）结合了神经场的表达能力和点云渲染的效率，在新视角合成中达到了最先进的图像质量。然而，由于渲染过程中需要查询神经网络，INPC的实际可用性受到较慢渲染速度的限制。

Method: 作者提出了一系列优化措施，包括改进的光栅化器实现、更有效的采样技术、预训练卷积神经网络用于填充空洞，以及在推理过程中将点建模为小型高斯分布以提升质量。

Result: 优化后的INPC管线实现了高达25%的训练加速、2倍的渲染速度提升和20%的显存使用减少，同时图像质量略有提升。

Conclusion: 本文的方法不仅显著提升了INPC的性能，还保持了其视觉保真度，同时展示了优化技术的广泛适用性。

Abstract: Implicit Neural Point Cloud (INPC) is a recent hybrid representation that
combines the expressiveness of neural fields with the efficiency of point-based
rendering, achieving state-of-the-art image quality in novel view synthesis.
However, as with other high-quality approaches that query neural networks
during rendering, the practical usability of INPC is limited by comparatively
slow rendering. In this work, we present a collection of optimizations that
significantly improve both the training and inference performance of INPC
without sacrificing visual fidelity. The most significant modifications are an
improved rasterizer implementation, more effective sampling techniques, and the
incorporation of pre-training for the convolutional neural network used for
hole-filling. Furthermore, we demonstrate that points can be modeled as small
Gaussians during inference to further improve quality in extrapolated, e.g.,
close-up views of the scene. We design our implementations to be broadly
applicable beyond INPC and systematically evaluate each modification in a
series of experiments. Our optimized INPC pipeline achieves up to 25% faster
training, 2x faster rendering, and 20% reduced VRAM usage paired with slight
image quality improvements.

</details>


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [6] [A Case Study on the Effectiveness of LLMs in Verification with Proof Assistants](https://arxiv.org/abs/2508.18587)
*Barış Bayazıt,Yao Li,Xujie Si*

Main category: cs.PL

TL;DR: 该论文研究了大型语言模型（LLMs）在验证中使用证明助手的有效性，通过对Rocq项目的案例分析，发现外部依赖和上下文对证明生成有帮助，LLMs在小证明中表现优秀，但在不同项目中有差异。


<details>
  <summary>Details</summary>
Motivation: 尽管大型语言模型（LLMs）在自动化证明中有潜力，但其在验证任务中的实际效果尚不明确，论文旨在通过案例研究评估其有效性。

Method: 论文基于两个成熟的Rocq项目（hs-to-coq工具和Verdi）进行案例研究，通过定量和定性分析评估LLMs生成证明的效果。

Result: 研究发现：(1)外部依赖和同一源文件中的上下文有助于证明生成；(2)LLMs擅长小证明，但也能够生成大型证明；(3)不同验证项目中LLMs表现不同；(4)LLMs能生成简洁聪明的证明，但在新定义中应用经典技术时也可能出现错误。

Conclusion: LLMs在验证任务中具有一定潜力，尤其是在小证明生成和利用上下文时表现优异，但不同项目和复杂情境中的表现仍需进一步研究。

Abstract: Large language models (LLMs) can potentially help with verification using
proof assistants by automating proofs. However, it is unclear how effective
LLMs are in this task. In this paper, we perform a case study based on two
mature Rocq projects: the hs-to-coq tool and Verdi. We evaluate the
effectiveness of LLMs in generating proofs by both quantitative and qualitative
analysis. Our study finds that: (1) external dependencies and context in the
same source file can significantly help proof generation; (2) LLMs perform
great on small proofs but can also generate large proofs; (3) LLMs perform
differently on different verification projects; and (4) LLMs can generate
concise and smart proofs, apply classical techniques to new definitions, but
can also make odd mistakes.

</details>


<div id='cs.GT'></div>

# cs.GT [[Back]](#toc)

### [7] [Facilitating Matches on Allocation Platforms](https://arxiv.org/abs/2508.18325)
*Yohai Trabelsi,Abhijin Adiga,Yonatan Aumann,Sarit Kraus,S. S. Ravi*

Main category: cs.GT

TL;DR: 论文研究了在资源分配平台中，通过鼓励代理人放松限制以提高整体效用的问题，并提出了优化算法和实证验证。


<details>
  <summary>Details</summary>
Motivation: 研究如何在资源分配平台中通过优化代理人限制的放松，以提高整体效用，同时确保不损害那些原本更好的代理人利益。

Method: 论文提供了问题的形式化定义和参与保障的层级，开发了多项式时间算法，处理了一对一和多对一分配场景。

Result: 通过三个真实世界数据集的广泛实验，展示了放松限制的益处以及不同参与保障的影响。

Conclusion: 研究证实了优化放松限制可以有效提升社会效益，并通过实验验证了不同参与保障的实用性。

Abstract: We consider a setting where goods are allocated to agents by way of an
allocation platform (e.g., a matching platform). An ``allocation facilitator''
aims to increase the overall utility/social-good of the allocation by
encouraging (some of the) agents to relax (some of) their restrictions. At the
same time, the advice must not hurt agents who would otherwise be better off.
Additionally, the facilitator may be constrained by a ``bound'' (a.k.a.
`budget'), limiting the number and/or type of restrictions it may seek to
relax. We consider the facilitator's optimization problem of choosing an
optimal set of restrictions to request to relax under the aforementioned
constraints. Our contributions are three-fold: (i) We provide a formal
definition of the problem, including the participation guarantees to which the
facilitator should adhere. We define a hierarchy of participation guarantees
and also consider several social-good functions. (ii) We provide polynomial
algorithms for solving various versions of the associated optimization
problems, including one-to-one and many-to-one allocation settings. (iii) We
demonstrate the benefits of such facilitation and relaxation, and the
implications of the different participation guarantees, using extensive
experimentation on three real-world datasets.

</details>


### [8] [Partitioned Combinatorial Optimization Games](https://arxiv.org/abs/2508.18449)
*Jiehua Chen,Christian Hatschka,Sofia Simola*

Main category: cs.GT

TL;DR: 提出了dPartitionedCombinatorial Optimization Games（PCOGs）这种合作博弈模型，并研究了核心的稳定性和算法复杂性。


<details>
  <summary>Details</summary>
Motivation: 研究合作博弈中组合结构（如图）的优化问题，核心稳定性是其中的关键问题。

Method: 提出了PCOGs模型，定义了每个联盟的价值，并分析了核心稳定性验证和存在性的算法复杂性。

Result: 对四种经典的图优化任务（最小顶点覆盖、最小支配集、最小生成树和最大匹配）的核心稳定性问题进行了复杂性分析。

Conclusion: PCOGs模型为研究合作博弈中的优化问题提供了新框架，核心稳定性的复杂性分析为其应用提供了理论基础。

Abstract: We propose a class of cooperative games, called d Partitioned Compbinatorial
Optimization Games (PCOGs). The input of PCOG consists of a set of agents and a
combinatorial structure (typically a graph) with a fixed optimization goal on
this structure (e.g., finding a minimum dominating set on a graph) such that
the structure is divided among the agents. The value of each coalition of
agents is derived from the optimal solution for the part of the structure
possessed by the coalition. We study two fundamental questions related to the
core: Core Stability Verification and Core Stability Existence. We analyze the
algorithmic complexity of both questions for four classic graph optimization
tasks: minimum vertex cover, minimum dominating set, minimum spanning tree, and
maximum matching.

</details>


### [9] [Bias-Adjusted LLM Agents for Human-Like Decision-Making via Behavioral Economics](https://arxiv.org/abs/2508.18600)
*Ayato Kitadai,Yusuke Fukasawa,Nariaki Nishino*

Main category: cs.GT

TL;DR: 通过基于角色的方法调整大型语言模型的内在偏见，使其更接近真实人类行为。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）在模拟人类决策时存在内在偏见，与真实人类行为存在差距，限制了其反映群体多样性的能力。

Method: 采用基于角色的方法，利用行为经济学的个体行为数据调整模型偏见，并应用于最后通牒游戏中。

Result: 在最后通牒游戏中，模拟行为与实证行为的一致性得到改善，特别是在回应者方面。

Conclusion: 角色条件化的大型语言模型在模拟大规模人类决策模式方面具有潜力，但特征表示仍需进一步优化。

Abstract: Large language models (LLMs) are increasingly used to simulate human
decision-making, but their intrinsic biases often diverge from real human
behavior--limiting their ability to reflect population-level diversity. We
address this challenge with a persona-based approach that leverages
individual-level behavioral data from behavioral economics to adjust model
biases. Applying this method to the ultimatum game--a standard but difficult
benchmark for LLMs--we observe improved alignment between simulated and
empirical behavior, particularly on the responder side. While further
refinement of trait representations is needed, our results demonstrate the
promise of persona-conditioned LLMs for simulating human-like decision patterns
at scale.

</details>
