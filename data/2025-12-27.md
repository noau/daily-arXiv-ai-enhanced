<div id=toc></div>

# Table of Contents

- [cs.GR](#cs.GR) [Total: 3]
- [cs.GT](#cs.GT) [Total: 3]


<div id='cs.GR'></div>

# cs.GR [[Back]](#toc)

### [1] [Efficient Computation of Integer-constrained Cones for Conformal Parameterizations](https://arxiv.org/abs/2512.20904)
*Wei Du,Qing Fang,Ligang Liu,Xiao-Ming Fu*

Main category: cs.GR

TL;DR: 本文提出了一种高效计算整数约束锥奇异点的方法，用于生成旋转无缝且低失真的共形参数化。通过交替优化顶点约束位置、整数约束角度和锥的数量，实现了高效收敛。


<details>
  <summary>Details</summary>
Motivation: 现有的共形参数化方法在处理高亏格表面时效率较低，本文旨在提出一种高效且低失真的参数化方法，以解决这一问题。

Method: 本文采用了交替优化离散变量（顶点约束位置、角度和锥的数量）的方法，并结合显式构造算法和新导数公式，大幅降低了优化问题的规模。

Result: 在大量测试数据上，该方法实现了旋转无缝且低失真的参数化，平均比现有方法快30倍，同时保持了相似的锥数量和失真水平。

Conclusion: 本文的方法在高效性和实用性上均有显著提升，为处理高亏格表面的共形参数化提供了可行的解决方案。

Abstract: We propose an efficient method to compute a small set of integer-constrained cone singularities, which induce a rotationally seamless conformal parameterization with low distortion. Since the problem only involves discrete variables, i.e., vertex-constrained positions, integer-constrained angles, and the number of cones, we alternately optimize these three types of variables to achieve tractable convergence. Central to high efficiency is an explicit construction algorithm that reduces the optimization problem scale to be slightly greater than the number of integer variables for determining the optimal angles with fixed positions and numbers, even for high-genus surfaces. In addition, we derive a new derivative formula that allows us to move the cones, effectively reducing distortion until convergence. Combined with other strategies, including repositioning and adding cones to decrease distortion, adaptively selecting a constrained number of integer variables for efficient optimization, and pairing cones to reduce the number, we quickly achieve a favorable tradeoff between the number of cones and the parameterization distortion. We demonstrate the effectiveness and practicability of our cones by using them to generate rotationally seamless and low-distortion parameterizations on a massive test data set. Our method demonstrates an order-of-magnitude speedup (30$\times$ faster on average) compared to state-of-the-art approaches while maintaining comparable cone numbers and parameterization distortion.

</details>


### [2] [AirGS: Real-Time 4D Gaussian Streaming for Free-Viewpoint Video Experiences](https://arxiv.org/abs/2512.20943)
*Zhe Wang,Jinghang Li,Yifei Zhu*

Main category: cs.GR

TL;DR: AirGS是一个优化的4D高斯泼溅（4DGS）框架，通过重新设计训练和传输管道，提升了自由视点视频（FVV）的质量和效率，实现了高质量、低延迟的体验。


<details>
  <summary>Details</summary>
Motivation: 现有的4DGS方法在长时间序列中存在质量下降问题，且带宽和存储开销较大，限制了其在实时和大规模部署中的应用。因此，需要一种更高效的方法来提升质量和减少开销。

Method: AirGS将高斯视频流转换为多通道2D格式，智能识别关键帧以提高重建质量，并结合时间相干性和膨胀损失来减少训练时间和表示大小。此外，通过整数线性规划建模传输问题，设计轻量级修剪级别选择算法来优化带宽使用。

Result: 实验表明，AirGS在场景变化时将PSNR的质量偏差降低超过20%，帧级PSNR始终保持在30以上，训练速度提高6倍，单帧传输大小减少近50%。

Conclusion: AirGS通过创新的方法显著提升了4DGS的质量和效率，为实时和大规模自由视点视频应用提供了可行的解决方案。

Abstract: Free-viewpoint video (FVV) enables immersive viewing experiences by allowing users to view scenes from arbitrary perspectives. As a prominent reconstruction technique for FVV generation, 4D Gaussian Splatting (4DGS) models dynamic scenes with time-varying 3D Gaussian ellipsoids and achieves high-quality rendering via fast rasterization. However, existing 4DGS approaches suffer from quality degradation over long sequences and impose substantial bandwidth and storage overhead, limiting their applicability in real-time and wide-scale deployments. Therefore, we present AirGS, a streaming-optimized 4DGS framework that rearchitects the training and delivery pipeline to enable high-quality, low-latency FVV experiences. AirGS converts Gaussian video streams into multi-channel 2D formats and intelligently identifies keyframes to enhance frame reconstruction quality. It further combines temporal coherence with inflation loss to reduce training time and representation size. To support communication-efficient transmission, AirGS models 4DGS delivery as an integer linear programming problem and design a lightweight pruning level selection algorithm to adaptively prune the Gaussian updates to be transmitted, balancing reconstruction quality and bandwidth consumption. Extensive experiments demonstrate that AirGS reduces quality deviation in PSNR by more than 20% when scene changes, maintains frame-level PSNR consistently above 30, accelerates training by 6 times, reduces per-frame transmission size by nearly 50% compared to the SOTA 4DGS approaches.

</details>


### [3] [TexAvatars : Hybrid Texel-3D Representations for Stable Rigging of Photorealistic Gaussian Head Avatars](https://arxiv.org/abs/2512.21099)
*Jaeseong Lee,Junyeong Ahn,Taewoong Kang,Jaegul Choo*

Main category: cs.GR

TL;DR: TexAvatars是一种混合虚拟头像表示方法，结合了解析绑定的几何基础与纹理空间的空间连续性，通过局部几何属性预测和网格感知的雅可比驱动3D变形，实现了高质量的逼真头部表情和姿态重现。


<details>
  <summary>Details</summary>
Motivation: 现有方法在极端重现场景中难以泛化，且通常过度依赖神经网络和启发式正则化，导致几何一致性和泛化能力受限。TexAvatars旨在结合解析绑定和纹理空间的优点，解决这些问题。

Method: TexAvatars采用混合设计，通过CNN在UV空间预测局部几何属性，并利用网格感知的雅可比驱动3D变形。该方法分离了语义建模和几何控制，提升了泛化能力和稳定性。

Result: TexAvatars在极端姿态和表情变化下表现出色，能够捕捉细粒度表达效果（如肌肉引起的皱纹和口腔几何），并在头部重现任务中展现出强大的泛化能力。

Conclusion: TexAvatars通过混合表示实现了高质量的头部虚拟头像重现，显著提升了泛化性、可解释性和稳定性，适用于复杂且非分布内的变形场景。

Abstract: Constructing drivable and photorealistic 3D head avatars has become a central task in AR/XR, enabling immersive and expressive user experiences. With the emergence of high-fidelity and efficient representations such as 3D Gaussians, recent works have pushed toward ultra-detailed head avatars. Existing approaches typically fall into two categories: rule-based analytic rigging or neural network-based deformation fields. While effective in constrained settings, both approaches often fail to generalize to unseen expressions and poses, particularly in extreme reenactment scenarios. Other methods constrain Gaussians to the global texel space of 3DMMs to reduce rendering complexity. However, these texel-based avatars tend to underutilize the underlying mesh structure. They apply minimal analytic deformation and rely heavily on neural regressors and heuristic regularization in UV space, which weakens geometric consistency and limits extrapolation to complex, out-of-distribution deformations. To address these limitations, we introduce TexAvatars, a hybrid avatar representation that combines the explicit geometric grounding of analytic rigging with the spatial continuity of texel space. Our approach predicts local geometric attributes in UV space via CNNs, but drives 3D deformation through mesh-aware Jacobians, enabling smooth and semantically meaningful transitions across triangle boundaries. This hybrid design separates semantic modeling from geometric control, resulting in improved generalization, interpretability, and stability. Furthermore, TexAvatars captures fine-grained expression effects, including muscle-induced wrinkles, glabellar lines, and realistic mouth cavity geometry, with high fidelity. Our method achieves state-of-the-art performance under extreme pose and expression variations, demonstrating strong generalization in challenging head reenactment settings.

</details>


<div id='cs.GT'></div>

# cs.GT [[Back]](#toc)

### [4] [Mechanism-Based Intelligence (MBI): Differentiable Incentives for Rational Coordination and Guaranteed Alignment in Multi-Agent Systems](https://arxiv.org/abs/2512.20688)
*Stefano Grassi*

Main category: cs.GT

TL;DR: 本文提出了一种名为“机制基础智能”（MBI）的范式，通过协调多个“大脑”而非单个大脑来解决多智能体系统中的信息分散和激励对齐问题。


<details>
  <summary>Details</summary>
Motivation: 多智能体系统在解决信息分散（哈耶克信息问题）和激励对齐（赫维茨激励问题）时表现出根本性脆弱，导致协调计算难以处理。

Method: 本文引入了可微分价格机制（DPM），计算动态的VCG等效激励信号，确保主导策略激励兼容性（DSIC）并收敛到全局最优，同时还提出了贝叶斯扩展以处理不对称信息（BIC）。

Result: 该框架在智能体数量上的计算复杂度线性增长（O(N)），比无模型强化学习快50倍，并能有效地将智能体自利与集体目标对齐。

Conclusion: 该方法基于经济原理，提供了一种可证明高效、可审计且通用的多智能体智能协调方法，具有可信赖性和扩展性。

Abstract: Autonomous multi-agent systems are fundamentally fragile: they struggle to solve the Hayekian Information problem (eliciting dispersed private knowledge) and the Hurwiczian Incentive problem (aligning local actions with global objectives), making coordination computationally intractable. I introduce Mechanism-Based Intelligence (MBI), a paradigm that reconceptualizes intelligence as emergent from the coordination of multiple "brains", rather than a single one. At its core, the Differentiable Price Mechanism (DPM) computes the exact loss gradient $$ \mathbf{G}_i = - \frac{\partial \mathcal{L}}{\partial \mathbf{x}_i} $$ as a dynamic, VCG-equivalent incentive signal, guaranteeing Dominant Strategy Incentive Compatibility (DSIC) and convergence to the global optimum. A Bayesian extension ensures incentive compatibility under asymmetric information (BIC). The framework scales linearly ($\mathcal{O}(N)$) with the number of agents, bypassing the combinatorial complexity of Dec-POMDPs and is empirically 50x faster than Model-Free Reinforcement Learning. By structurally aligning agent self-interest with collective objectives, it provides a provably efficient, auditable and generalizable approach to coordinated, trustworthy and scalable multi-agent intelligence grounded in economic principles.

</details>


### [5] [(Im)possibility of Incentive Design for Challenge-based Blockchain Protocols](https://arxiv.org/abs/2512.20864)
*Suhyeon Lee,Dieu-Huyen Nguyen,Donghwan Lee*

Main category: cs.GT

TL;DR: 该论文研究了区块链中挑战协议的经济激励机制，指出在单赢家设计中难以实现诚实验证者的激励与欺诈威慑的双重目标，但在多赢家设计中可以通过明确的简单条件实现。


<details>
  <summary>Details</summary>
Motivation: 区块链的链下计算挑战协议旨在减轻链上计算的负担，但现有研究未明确诚实验证者的激励机制与欺诈威慑的效果。论文旨在填补这一空白。

Method: 论文建立了一个包含勾结少数派、异质成本和三种排序模式的模型，并分析了在单赢家和多赢家设计中实现诚实验证者激励与欺诈威慑的可能性。

Result: 研究结果表明，单赢家设计难以或仅能在有限规模下实现双重目标，而多赢家设计则可以在明确的简单条件下同时满足这两个目标。

Conclusion: 论文结论是多赢家设计在区块链挑战协议的经济激励机制中更具优势，能够有效激励诚实验证者并威慑欺诈行为。

Abstract: Blockchains offer a decentralized and secure execution environment strong enough to host cryptocurrencies, but the state-replication model makes on-chain computation expensive. To avoid heavy on-chain workloads, systems like Truebit and optimistic rollups use challenge-based protocols, performing computations off-chain and invoking the chain only when challenged. This keeps normal-case costs low and, if at least one honest challenger exists, can catch fraud. What has been less clear is whether honest challengers are actually incentivized and a dishonest proposer is properly damaged under the worst case environment. We build a model with a colluding minority, heterogeneous costs, and three ordering modes. We then ask whether two goals can be met together: honest non-loss and fraud deterrence. Our results are clear: in single-winner designs, the incentive design is impossible or limited in scale. By contrast, in multi-winner designs, we obtain simple, explicit conditions under which both goals hold.

</details>


### [6] [Policy-Conditioned Policies for Multi-Agent Task Solving](https://arxiv.org/abs/2512.21024)
*Yue Lin,Shuhui Zhu,Wenhao Li,Ang Li,Dan Qiao,Pascal Poupart,Hongyuan Zha,Baoxiang Wang*

Main category: cs.GT

TL;DR: 本文提出了一种新范式，通过将策略表示为人类可解释的源代码，并利用大型语言模型（LLMs）作为近似解释器，解决了多任务代理中策略动态适应的挑战。


<details>
  <summary>Details</summary>
Motivation: 在多代理任务中，策略的动态适应是核心挑战。由于神经策略的不透明性和高维度性，直接基于对手策略进行调整在多任务强化学习中难以实现。

Method: 提出了一种将策略表示为人类可解释源代码的方法，并利用LLMs作为近似解释器，将其应用于程序均衡的游戏理论概念。使用LLMs直接在程序化策略空间中进行优化，提出了程序化迭代最佳响应（PIBR）算法。

Result: 实验表明，该方法成功解决了多个标准协调矩阵游戏和一个合作的基于等级的觅食环境问题。

Conclusion: 通过程序化表示和LLMs的应用，本研究为多任务代理中的策略动态适应提供了一种有效的新方法。

Abstract: In multi-agent tasks, the central challenge lies in the dynamic adaptation of strategies. However, directly conditioning on opponents' strategies is intractable in the prevalent deep reinforcement learning paradigm due to a fundamental ``representational bottleneck'': neural policies are opaque, high-dimensional parameter vectors that are incomprehensible to other agents. In this work, we propose a paradigm shift that bridges this gap by representing policies as human-interpretable source code and utilizing Large Language Models (LLMs) as approximate interpreters. This programmatic representation allows us to operationalize the game-theoretic concept of \textit{Program Equilibrium}. We reformulate the learning problem by utilizing LLMs to perform optimization directly in the space of programmatic policies. The LLM functions as a point-wise best-response operator that iteratively synthesizes and refines the ego agent's policy code to respond to the opponent's strategy. We formalize this process as \textit{Programmatic Iterated Best Response (PIBR)}, an algorithm where the policy code is optimized by textual gradients, using structured feedback derived from game utility and runtime unit tests. We demonstrate that this approach effectively solves several standard coordination matrix games and a cooperative Level-Based Foraging environment.

</details>
