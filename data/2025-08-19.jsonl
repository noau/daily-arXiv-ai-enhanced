{"id": "2508.11665", "categories": ["cs.PL", "cs.MA"], "pdf": "https://arxiv.org/pdf/2508.11665", "abs": "https://arxiv.org/abs/2508.11665", "authors": ["Xinkui Zhao", "Yifan Zhang", "Zhengyi Zhou", "Yueshen Xu"], "title": "StackPilot: Autonomous Function Agents for Scalable and Environment-Free Code Execution", "comment": null, "summary": "Recent advances in large language models (LLMs) have substantially enhanced\nautomated code generation across a wide range of programming languages.\nNonetheless, verifying the correctness and executability of LLM-generated code\nremains a significant challenge, as traditional methods rely on\nlanguage-specific compilers and environment-dependent runtimes. To overcome\nthese limitations, we introduce StackPilot, an LLM-native, multi-agent\nframework designed for language-agnostic code verification and execution, which\noperates independently of conventional toolchains. StackPilot offers three\nprincipal innovations: (1) a Function-as-Agents paradigm, in which each\nfunction is modeled as an autonomous agent capable of fine-grained reasoning\nand collaborative verification; (2) an LLM-as-Executor strategy, which enables\nscalable verification via stack-based scheduling; and (3) a novel snapshot\nmechanism that preserves complete execution contexts, facilitating\ndeterministic and lossless context switching during verification. Empirical\nevaluations demonstrate that StackPilot achieves framework reliability rates\nbetween 89% and 97%, substantially outperforming baseline approaches. These\nresults indicate that StackPilot can reliably verify and execute a\nsignificantly larger proportion of LLM-generated code across diverse\nprogramming tasks compared to existing methods."}
{"id": "2508.12054", "categories": ["cs.PL", "11A51", "D.3.1"], "pdf": "https://arxiv.org/pdf/2508.12054", "abs": "https://arxiv.org/abs/2508.12054", "authors": ["Guilherme de Oliveira Silva", "Fernando Magno Quintão Pereira"], "title": "Certified Compilation based on Gödel Numbers", "comment": "32 pages, 19 figures", "summary": "In his 1984 Turing Award lecture, Ken Thompson showed that a compiler could\nbe maliciously altered to insert backdoors into programs it compiles and\nperpetuate this behavior by modifying any compiler it subsequently builds.\nThompson's hack has been reproduced in real-world systems for demonstration\npurposes. Several countermeasures have been proposed to defend against\nThompson-style backdoors, including the well-known {\\it Diverse\nDouble-Compiling} (DDC) technique, as well as methods like translation\nvalidation and CompCert-style compilation. However, these approaches ultimately\ncircle back to the fundamental question: \"How can we trust the compiler used to\ncompile the tools we rely on?\" In this paper, we introduce a novel approach to\ngenerating certificates to guarantee that a binary image faithfully represents\nthe source code. These certificates ensure that the binary contains all and\nonly the statements from the source code, preserves their order, and maintains\nequivalent def-use dependencies. The certificate is represented as an integer\nderivable from both the source code and the binary using a concise set of\nderivation rules, each applied in constant time. To demonstrate the\npracticality of our method, we present Charon, a compiler designed to handle a\nsubset of C expressive enough to compile FaCT, the Flexible and Constant Time\ncryptographic programming language."}
{"id": "2508.12427", "categories": ["cs.PL", "D.3.3; F.3.2; F.3.3"], "pdf": "https://arxiv.org/pdf/2508.12427", "abs": "https://arxiv.org/abs/2508.12427", "authors": ["Paul Downen"], "title": "Controlling Copatterns: There and Back Again (Extended Version)", "comment": "To find the detailed step-by-step process, which serves as their\n  proof of correctness, see https://github.com/pdownen/derive-copat", "summary": "Copatterns give functional programs a flexible mechanism for responding to\ntheir context, and composition can greatly enhance their expressiveness.\nHowever, that same expressive power makes it harder to precisely specify the\nbehavior of programs. Using Danvy's functional and syntactic correspondence\nbetween different semantic artifacts, we derive a full suite of semantics for\ncopatterns, twice. First, a calculus of monolithic copatterns is taken on a\njourney from small-step operational semantics to abstract machine to\ncontinuation-passing style. Then within continuation-passing style, we refactor\nthe semantics to derive a more general calculus of compositional copatterns,\nand take the return journey back to derive the other semantic artifacts in\nreverse order."}
{"id": "2508.12475", "categories": ["cs.PL", "cs.FL"], "pdf": "https://arxiv.org/pdf/2508.12475", "abs": "https://arxiv.org/abs/2508.12475", "authors": ["Abhijit Paul"], "title": "Type-Driven Prompt Programming: From Typed Interfaces to a Calculus of Constraints", "comment": "Accepted as Extended Abstract in TyDe Workshop 2025,co-located with\n  ICFP", "summary": "Prompt programming treats large language model prompts as software components\nwith typed interfaces. Based on a literature survey of 15 recent works from\n2023 to 2025, we observe a consistent trend: type systems are central to\nemerging prompt programming frameworks. However, there are gaps in constraint\nexpressiveness and in supporting algorithms. To address these issues, we\nintroduce the notion of Lambda Prompt, a dependently typed calculus with\nprobabilistic refinements for syntactic and semantic constraints. While this is\nnot yet a full calculus, the formulation motivates a type-theoretic foundation\nfor prompt programming. Our catalog of 13 constraints highlights underexplored\nareas in constraint expressiveness (constraints 9 through 13). To address the\nalgorithmic gap, we propose a constraint-preserving optimization rule. Finally,\nwe outline research directions on developing a compiler for prompt programs."}
{"id": "2508.11695", "categories": ["cs.GR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.11695", "abs": "https://arxiv.org/abs/2508.11695", "authors": ["Yiyun Chen", "Weikai Yang"], "title": "RefAdGen: High-Fidelity Advertising Image Generation", "comment": null, "summary": "The rapid advancement of Artificial Intelligence Generated Content (AIGC)\ntechniques has unlocked opportunities in generating diverse and compelling\nadvertising images based on referenced product images and textual scene\ndescriptions. This capability substantially reduces human labor and production\ncosts in traditional marketing workflows. However, existing AIGC techniques\neither demand extensive fine-tuning for each referenced image to achieve high\nfidelity, or they struggle to maintain fidelity across diverse products, making\nthem impractical for e-commerce and marketing industries. To tackle this\nlimitation, we first construct AdProd-100K, a large-scale advertising image\ngeneration dataset. A key innovation in its construction is our dual data\naugmentation strategy, which fosters robust, 3D-aware representations crucial\nfor realistic and high-fidelity image synthesis. Leveraging this dataset, we\npropose RefAdGen, a generation framework that achieves high fidelity through a\ndecoupled design. The framework enforces precise spatial control by injecting a\nproduct mask at the U-Net input, and employs an efficient Attention Fusion\nModule (AFM) to integrate product features. This design effectively resolves\nthe fidelity-efficiency dilemma present in existing methods. Extensive\nexperiments demonstrate that RefAdGen achieves state-of-the-art performance,\nshowcasing robust generalization by maintaining high fidelity and remarkable\nvisual results for both unseen products and challenging real-world, in-the-wild\nimages. This offers a scalable and cost-effective alternative to traditional\nworkflows. Code and datasets are publicly available at\nhttps://github.com/Anonymous-Name-139/RefAdgen."}
{"id": "2508.11874", "categories": ["cs.GT", "cs.AI", "cs.DS", "cs.LO", "cs.PL"], "pdf": "https://arxiv.org/pdf/2508.11874", "abs": "https://arxiv.org/abs/2508.11874", "authors": ["Hanyu Li", "Dongchen Li", "Xiaotie Deng"], "title": "Discovering Expert-Level Nash Equilibrium Algorithms with Large Language Models", "comment": null, "summary": "Algorithm design and analysis is a cornerstone of computer science, but it\nconfronts a major challenge. Proving an algorithm's performance guarantee\nacross all inputs has traditionally required extensive and often error-prone\nhuman effort. While AI has shown great success in finding solutions to specific\nproblem instances, automating the discovery of general algorithms with such\nprovable guarantees has remained a significant barrier. This challenge stems\nfrom the difficulty of integrating the creative process of algorithm design\nwith the rigorous process of formal analysis. To address this gap, we propose\nLegoNE, a framework that tightly fuses these two processes for the fundamental\nand notoriously difficult problem of computing approximate Nash equilibria.\nLegoNE automatically translates any algorithm written by a simple Python-like\nlanguage into a constrained optimization problem. Solving this problem derives\nand proves the algorithm's approximation bound. Using LegoNE, a\nstate-of-the-art large language model rediscovered the state-of-the-art\nalgorithm for two-player games within hours, a feat that had taken human\nresearchers 15 years to achieve. For three-player games, the model discovered a\nnovel algorithm surpassing all existing human-designed ones. This work\ndemonstrates a new human-machine collaborative paradigm for theoretical\nscience: humans reason at a higher-abstract level, using symbols to compress\nthe search space, and AI explores within it, achieving what neither could\nalone."}
{"id": "2508.11722", "categories": ["cs.GR"], "pdf": "https://arxiv.org/pdf/2508.11722", "abs": "https://arxiv.org/abs/2508.11722", "authors": ["Chenfanfu Jiang"], "title": "Substepping the Material Point Method", "comment": "1 page", "summary": "Many Material Point Method implementations favor explicit time integration.\nHowever large time steps are often desirable for special reasons - for example,\nfor partitioned coupling with another large-step solver, or for imposing\nconstraints, projections, or multiphysics solves. We present a simple,\nplug-and-play algorithm that advances MPM with a large time step using\nsubsteps, effectively wrapping an explicit MPM integrator into a\npseudo-implicit one."}
{"id": "2508.12453", "categories": ["cs.GT", "cs.DS"], "pdf": "https://arxiv.org/pdf/2508.12453", "abs": "https://arxiv.org/abs/2508.12453", "authors": ["Martin Jupakkal Andersen", "Ioannis Caragiannis", "Anders Bo Ipsen", "Alexander Søltoft"], "title": "Computing Approximately Proportional Allocations of Indivisible Goods: Beyond Additive and Monotone Valuations", "comment": null, "summary": "Although approximate notions of envy-freeness-such as envy-freeness up to one\ngood (EF1)-have been extensively studied for indivisible goods, the seemingly\nsimpler fairness concept of proportionality up to one good (PROP1) has received\nfar less attention. For additive valuations, every EF1 allocation is PROP1, and\nwell-known algorithms such as Round-Robin and Envy-Cycle Elimination compute\nsuch allocations in polynomial time. PROP1 is also compatible with Pareto\nefficiency, as maximum Nash welfare allocations are EF1 and hence PROP1.\n  We ask whether these favorable properties extend to non-additive valuations.\nWe study a broad class of allocation instances with {\\em satiating goods},\nwhere agents have non-negative valuation functions that need not be monotone,\nallowing for negative marginal values. We present the following results:\n  - EF1 implies PROP1 for submodular valuations over satiating goods, ensuring\nexistence and efficient computation via Envy-Cycle Elimination for monotone\nsubmodular valuations;\n  - Round-robin computes a partial PROP1 allocation after the second-to-last\nround for satiating submodular goods and a complete PROP1 for monotone\nsubmodular valuations;\n  - PROP1 allocations for satiating subadditive goods can be computed in\npolynomial-time;\n  - Maximum Nash welfare allocations are PROP1 for monotone submodular goods,\nrevealing yet another facet of their ``unreasonable fairness.''"}
{"id": "2508.12179", "categories": ["cs.GR"], "pdf": "https://arxiv.org/pdf/2508.12179", "abs": "https://arxiv.org/abs/2508.12179", "authors": ["Yuta Noma", "Zhecheng Wang", "Chenxi Liu", "Karan Singh", "Alec Jacobson"], "title": "Mesh Processing Non-Meshes via Neural Displacement Fields", "comment": "14 pages", "summary": "Mesh processing pipelines are mature, but adapting them to newer non-mesh\nsurface representations -- which enable fast rendering with compact file size\n-- requires costly meshing or transmitting bulky meshes, negating their core\nbenefits for streaming applications.\n  We present a compact neural field that enables common geometry processing\ntasks across diverse surface representations. Given an input surface, our\nmethod learns a neural map from its coarse mesh approximation to the surface.\nThe full representation totals only a few hundred kilobytes, making it ideal\nfor lightweight transmission. Our method enables fast extraction of manifold\nand Delaunay meshes for intrinsic shape analysis, and compresses scalar fields\nfor efficient delivery of costly precomputed results. Experiments and\napplications show that our fast, compact, and accurate approach opens up new\npossibilities for interactive geometry processing."}
{"id": "2508.12549", "categories": ["cs.GT", "cs.DS", "cs.MA"], "pdf": "https://arxiv.org/pdf/2508.12549", "abs": "https://arxiv.org/abs/2508.12549", "authors": ["Atasi Panda", "Harsh Sharma", "Anand Louis", "Prajakta Nimbhorkar"], "title": "Group Fair Matchings using Convex Cost Functions", "comment": null, "summary": "We consider the problem of assigning items to platforms where each item has a\nutility associated with each of the platforms to which it can be assigned. Each\nplatform has a soft constraint over the total number of items it serves,\nmodeled via a convex cost function. Additionally, items are partitioned into\ngroups, and each platform also incurs group-specific convex cost over the\nnumber of items from each group that can be assigned to the platform. These\ncosts promote group fairness by penalizing imbalances, yielding a soft\nvariation of fairness notions introduced in prior work, such as Restricted\nDominance and Minority protection. Restricted Dominance enforces upper bounds\non group representation, while Minority protection enforces lower bounds. Our\napproach replaces such hard constraints with cost-based penalties, allowing\nmore flexible trade-offs. Our model also captures Nash Social Welfare kind of\nobjective.\n  The cost of an assignment is the sum of the values of all the cost functions\nacross all the groups and platforms. The objective is to find an assignment\nthat minimizes the cost while achieving a total utility that is at least a\nuser-specified threshold. The main challenge lies in balancing the overall\nplatform cost with group-specific costs, both governed by convex functions,\nwhile meeting the utility constraint. We present an efficient polynomial-time\napproximation algorithm, supported by theoretical guarantees and experimental\nevaluation. Our algorithm is based on techniques involving linear programming\nand network flows. We also provide an exact algorithm for a special case with\nuniform utilities and establish the hardness of the general problem when the\ngroups can intersect arbitrarily."}
{"id": "2508.12438", "categories": ["cs.GR", "cs.CV"], "pdf": "https://arxiv.org/pdf/2508.12438", "abs": "https://arxiv.org/abs/2508.12438", "authors": ["Yaron Aloni", "Rotem Shalev-Arkushin", "Yonatan Shafir", "Guy Tevet", "Ohad Fried", "Amit Haim Bermano"], "title": "Express4D: Expressive, Friendly, and Extensible 4D Facial Motion Generation Benchmark", "comment": null, "summary": "Dynamic facial expression generation from natural language is a crucial task\nin Computer Graphics, with applications in Animation, Virtual Avatars, and\nHuman-Computer Interaction. However, current generative models suffer from\ndatasets that are either speech-driven or limited to coarse emotion labels,\nlacking the nuanced, expressive descriptions needed for fine-grained control,\nand were captured using elaborate and expensive equipment. We hence present a\nnew dataset of facial motion sequences featuring nuanced performances and\nsemantic annotation. The data is easily collected using commodity equipment and\nLLM-generated natural language instructions, in the popular ARKit blendshape\nformat. This provides riggable motion, rich with expressive performances and\nlabels. We accordingly train two baseline models, and evaluate their\nperformance for future benchmarking. Using our Express4D dataset, the trained\nmodels can learn meaningful text-to-expression motion generation and capture\nthe many-to-many mapping of the two modalities. The dataset, code, and video\nexamples are available on our webpage: https://jaron1990.github.io/Express4D/"}
{"id": "2508.12691", "categories": ["cs.GR", "cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.12691", "abs": "https://arxiv.org/abs/2508.12691", "authors": ["Yuanxin Wei", "Lansong Diao", "Bujiao Chen", "Shenggan Cheng", "Zhengping Qian", "Wenyuan Yu", "Nong Xiao", "Wei Lin", "Jiangsu Du"], "title": "MixCache: Mixture-of-Cache for Video Diffusion Transformer Acceleration", "comment": "7 pages, 10 figures", "summary": "Leveraging the Transformer architecture and the diffusion process, video DiT\nmodels have emerged as a dominant approach for high-quality video generation.\nHowever, their multi-step iterative denoising process incurs high computational\ncost and inference latency. Caching, a widely adopted optimization method in\nDiT models, leverages the redundancy in the diffusion process to skip\ncomputations in different granularities (e.g., step, cfg, block). Nevertheless,\nexisting caching methods are limited to single-granularity strategies,\nstruggling to balance generation quality and inference speed in a flexible\nmanner. In this work, we propose MixCache, a training-free caching-based\nframework for efficient video DiT inference. It first distinguishes the\ninterference and boundary between different caching strategies, and then\nintroduces a context-aware cache triggering strategy to determine when caching\nshould be enabled, along with an adaptive hybrid cache decision strategy for\ndynamically selecting the optimal caching granularity. Extensive experiments on\ndiverse models demonstrate that, MixCache can significantly accelerate video\ngeneration (e.g., 1.94$\\times$ speedup on Wan 14B, 1.97$\\times$ speedup on\nHunyuanVideo) while delivering both superior generation quality and inference\nefficiency compared to baseline methods."}
