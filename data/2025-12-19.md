<div id=toc></div>

# Table of Contents

- [cs.GR](#cs.GR) [Total: 2]
- [cs.PL](#cs.PL) [Total: 4]
- [cs.GT](#cs.GT) [Total: 3]


<div id='cs.GR'></div>

# cs.GR [[Back]](#toc)

### [1] [A Fast Volumetric Capture and Reconstruction Pipeline for Dynamic Point Clouds and Gaussian Splats](https://arxiv.org/abs/2512.15719)
*Athanasios Charisoudis,Simone Croci,Lam Kit Yung,Pascal Frossard,Aljosa Smolic*

Main category: cs.GR

TL;DR: 介绍了一个高效快速的体积捕获与重建系统，支持RGB-D或RGB输入生成点云和高斯斑点形式的3D表示。


<details>
  <summary>Details</summary>
Motivation: 旨在提供一个易于部署的系统，支持在各种光照和背景下运行，并能处理灵活的摄像机配置。

Method: 改进了GPS-Gaussian回归器，以实现高质量的重建，同时减少计算开销。支持多种输出格式和实时预览。

Result: 系统能够在5-10 FPS下实现实时预览，并支持标准输出格式，适用于多种应用场景。

Conclusion: 该系统开源且易于部署，为3D捕获和重建提供了高效的解决方案，并促进了进一步的研究。

Abstract: We present a fast and efficient volumetric capture and reconstruction system that processes either RGB-D or RGB-only input to generate 3D representations in the form of point clouds and Gaussian splats. For Gaussian splat reconstructions, we took the GPS-Gaussian regressor and improved it, enabling high-quality reconstructions with minimal overhead. The system is designed for easy setup and deployment, supporting in-the-wild operation under uncontrolled illumination and arbitrary backgrounds, as well as flexible camera configurations, including sparse setups, arbitrary camera numbers and baselines. Captured data can be exported in standard formats such as PLY, MPEG V-PCC, and SPLAT, and visualized through a web-based viewer or Unity/Unreal plugins. A live on-location preview of both input and reconstruction is available at 5-10 FPS. We present qualitative findings focused on deployability and targeted ablations. The complete framework is open-source, facilitating reproducibility and further research.

</details>


### [2] [Enhancing Line Density Plots with Outlier Control and Bin-based Illumination](https://arxiv.org/abs/2512.16017)
*Yumeng Xue,Bin Chen,Patrick Paetzold,Yunhai Wang,Christophe Hurter,Oliver Deussen*

Main category: cs.GR

TL;DR: 提出了一种基于分箱的照明模型，用于增强线型数据集（如轨迹或时间序列）的可视化效果，同时保留原始色彩映射并突出异常值。


<details>
  <summary>Details</summary>
Motivation: 密度图在处理大量点时能有效减少重叠，但在处理线型数据集时会破坏路径连续性，掩盖平滑趋势和罕见异常值。

Method: 采用基于分箱的照明模型，通过分离结构和密度，并结合局部自适应照明，在亮度通道中突出所选模式（从主要趋势到异常路径）。

Result: 该方法不仅能揭示简单方法遗漏的细节，还能显著降低色彩失真（CIEDE2000），并支持多达10,000条线的交互式更新。

Conclusion: 该交互式方法允许分析师根据需要突出主要趋势或异常值，并在实际数据集中表现出优越的性能。

Abstract: Density plots effectively summarize large numbers of points, which would otherwise lead to severe overplotting in, for example, a scatter plot. However, when applied to line-based datasets, such as trajectories or time series, density plots alone are insufficient, as they disrupt path continuity, obscuring smooth trends and rare anomalies. We propose a bin-based illumination model that decouples structure from density to enhance flow and reveal sparse outliers while preserving the original colormap. We introduce a bin-based outlierness metric to rank trajectories. Guided by this ranking, we construct a structural normal map and apply locally-adaptive lighting in the luminance channel to highlight chosen patterns -- from dominant trends to atypical paths -- with acceptable color distortion. Our interactive method enables analysts to prioritize main trends, focus on outliers, or strike a balance between the two. We demonstrate our method on several real-world datasets, showing it reveals details missed by simpler alternatives, achieves significantly lower CIEDE2000 color distortion than standard shading, and supports interactive updates for up to 10,000 lines.

</details>


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [3] [LOOPRAG: Enhancing Loop Transformation Optimization with Retrieval-Augmented Large Language Models](https://arxiv.org/abs/2512.15766)
*Yijie Zhi,Yayu Cao,Jianhua Dai,Xiaoyang Han,Jingwen Pu,Qingran Wu,Sheng Cheng,Ming Cai*

Main category: cs.PL

TL;DR: 提出了LOOPRAG框架，利用检索增强生成技术指导LLM进行有效的循环优化，显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 现有的大型语言模型（LLM）在循环变换优化中表现不佳，导致性能提升的机会被浪费。

Method: LOOPRAG采用参数驱动的方法，结合循环感知算法和反馈迭代机制，生成多样且合法的优化代码示例。

Result: 在多种基准测试中，LOOPRAG平均性能提升超过基础编译器和LLM，最高达14.34倍。

Conclusion: LOOPRAG通过结合检索增强生成和循环优化技术，显著提升了LLM在代码优化中的性能和准确性。

Abstract: Loop transformations are semantics-preserving optimization techniques, widely used to maximize objectives such as parallelism. Despite decades of research, applying the optimal composition of loop transformations remains challenging due to inherent complexities, including cost modeling for optimization objectives. Recent studies have explored the potential of Large Language Models (LLMs) for code optimization. However, our key observation is that LLMs often struggle with effective loop transformation optimization, frequently leading to errors or suboptimal optimization, thereby missing opportunities for performance improvements. To bridge this gap, we propose LOOPRAG, a novel retrieval-augmented generation framework designed to guide LLMs in performing effective loop optimization on Static Control Part. We introduce a parameter-driven method to harness loop properties, which trigger various loop transformations, and generate diverse yet legal example codes serving as a demonstration source. To effectively obtain the most informative demonstrations, we propose a loop-aware algorithm based on loop features, which balances similarity and diversity for code retrieval. To enhance correct and efficient code generation, we introduce a feedback-based iterative mechanism that incorporates compilation, testing and performance results as feedback to guide LLMs. Each optimized code undergoes mutation, coverage and differential testing for equivalence checking. We evaluate LOOPRAG on PolyBench, TSVC and LORE benchmark suites, and compare it against compilers (GCC-Graphite, Clang-Polly, Perspective and ICX) and representative LLMs (DeepSeek and GPT-4). The results demonstrate average speedups over base compilers of up to 11.20$\times$, 14.34$\times$, and 9.29$\times$ for PolyBench, TSVC, and LORE, respectively, and speedups over base LLMs of up to 11.97$\times$, 5.61$\times$, and 11.59$\times$.

</details>


### [4] [Automated Formalization of Probabilistic Requirements from Structured Natural Language](https://arxiv.org/abs/2512.15788)
*Anastasia Mavridou,Marie Farrell,Gricel Vázquez,Tom Pressburger,Timothy E. Wang,Radu Calinescu,Michael Fisher*

Main category: cs.PL

TL;DR: 论文扩展了NASA的形式化需求获取工具FRET，支持通过结构化自然语言指定概率性需求，并将其自动转换为概率时序逻辑公式，从而减少错误并提高自主和自适应系统的形式化分析可行性。


<details>
  <summary>Details</summary>
Motivation: 在安全和任务关键系统中，不确定性难以明确捕获，且开发者直接使用复杂的形式化语言编写需求容易出错。

Method: 通过扩展FRET工具的结构化自然语言，支持概率性需求的明确和正确表达，并开发自动化方法将其转换为概率时序逻辑公式。

Result: 实现了从结构化自然语言到概率时序逻辑公式的自动转换，并通过验证框架和形式化证明确保公式的正确性和语义一致性。

Conclusion: 扩展后的FRET工具使开发者能够更实用、更少错误地指定和形式化分析自主和自适应系统的概率性需求。

Abstract: Integrating autonomous and adaptive behavior into software-intensive systems presents significant challenges for software development, as uncertainties in the environment or decision-making processes must be explicitly captured. These challenges are amplified in safety- and mission-critical systems, which must undergo rigorous scrutiny during design and development. Key among these challenges is the difficulty of specifying requirements that use probabilistic constructs to capture the uncertainty affecting these systems. To enable formal analysis, such requirements must be expressed in precise mathematical notations such as probabilistic logics. However, expecting developers to write requirements directly in complex formalisms is unrealistic and highly error-prone. We extend the structured natural language used by NASA's Formal Requirement Elicitation Tool (FRET) with support for the specification of unambiguous and correct probabilistic requirements, and develop an automated approach for translating these requirements into logical formulas. We propose and develop a formal, compositional, and automated approach for translating structured natural-language requirements into formulas in probabilistic temporal logic. To increase trust in our formalizations, we provide assurance that the generated formulas are well-formed and conform to the intended semantics through an automated validation framework and a formal proof. The extended FRET tool enables developers to specify probabilistic requirements in structured natural language, and to automatically translate them into probabilistic temporal logic, making the formal analysis of autonomous and adaptive systems more practical and less error-prone.

</details>


### [5] [A Neurosymbolic Approach to Loop Invariant Generation via Weakest Precondition Reasoning](https://arxiv.org/abs/2512.15816)
*Daragh King,Vasileios Koutavas,Laura Kovacs*

Main category: cs.PL

TL;DR: NeuroInv是一种结合神经推理与符号验证的循环不变式生成方法，显著提升了自动化程序验证的成功率。


<details>
  <summary>Details</summary>
Motivation: 自动化程序验证中循环不变式生成的瓶颈问题亟待解决，现有大型语言模型（LLM）方法缺乏结构化方法论和理论基础。

Method: NeuroInv包含两个模块：神经推理模块利用LLM和Hoare逻辑推导候选不变式，符号模块通过反例迭代修复不变式。

Result: 在150个Java程序的基准测试中，NeuroInv成功率达到99.5%，并在包含多个循环的更复杂场景中表现出色。

Conclusion: NeuroInv通过神经符号结合的方法，成功解决了循环不变式生成的挑战，并能扩展到复杂验证场景。

Abstract: Loop invariant generation remains a critical bottleneck in automated program verification. Recent work has begun to explore the use of Large Language Models (LLMs) in this area, yet these approaches tend to lack a reliable and structured methodology, with little reference to existing program verification theory. This paper presents NeuroInv, a neurosymbolic approach to loop invariant generation. NeuroInv comprises two key modules: (1) a neural reasoning module that leverages LLMs and Hoare logic to derive and refine candidate invariants via backward-chaining weakest precondition reasoning, and (2) a verification-guided symbolic module that iteratively repairs invariants using counterexamples from OpenJML. We evaluate NeuroInv on a comprehensive benchmark of 150 Java programs, encompassing single and multiple (sequential) loops, multiple arrays, random branching, and noisy code segments. NeuroInv achieves a $99.5\%$ success rate, substantially outperforming the other evaluated approaches. Additionally, we introduce a hard benchmark of $10$ larger multi-loop programs (with an average of $7$ loops each); NeuroInv's performance in this setting demonstrates that it can scale to more complex verification scenarios.

</details>


### [6] [Optimizing Agentic Language Model Inference via Speculative Tool Calls](https://arxiv.org/abs/2512.15834)
*Daniel Nichols,Prajwal Singhania,Charles Jekel,Abhinav Bhatele,Harshitha Menon*

Main category: cs.PL

TL;DR: 本文介绍了针对语言模型（LMs）在使用外部工具时引入的性能瓶颈的系统优化方法，通过推测工具调用和保持序列驻留来提升推理吞吐量。


<details>
  <summary>Details</summary>
Motivation: 现代语言模型（LMs）越来越依赖外部工具（如文件搜索、代码执行、API调用等）以提高能力，但这些工具同时也带来了推理过程中的性能瓶颈。

Method: 提出新的系统优化方法，包括推测工具调用和强制序列驻留在推理引擎中，以减少开销，并通过理论分析提出了性能最佳化的配置建议。

Result: 优化措施使得LM代理的推理吞吐量提升了每秒数百个token，同时提出了一种新的“工具缓存”API端点以便LM提供商轻松采用这些优化。

Conclusion: 本文的系统优化方法有效解决了LM使用工具时的性能瓶颈问题，为未来的优化和实现提供了理论和实践指导。

Abstract: Language models (LMs) are becoming increasingly dependent on external tools. LM-based agentic frameworks frequently interact with their environment via such tools to search files, run code, call APIs, etc. Further, modern reasoning-based LMs use tools such as web search and Python code execution to enhance their reasoning capabilities. While tools greatly improve the capabilities of LMs, they also introduce performance bottlenecks during the inference process. In this paper, we introduce novel systems optimizations to address such performance bottlenecks by speculating tool calls and forcing sequences to remain resident in the inference engine to minimize overheads. Our optimizations lead to throughput improvements of several hundred tokens per second when hosting inference for LM agents. We provide a theoretical analysis of our algorithms to provide insights into speculation configurations that will yield the best performance. Further, we recommend a new "tool cache" API endpoint to enable LM providers to easily adopt these optimizations.

</details>


<div id='cs.GT'></div>

# cs.GT [[Back]](#toc)

### [7] [Algorithmic Monetary Policies for Blockchain Participation Games](https://arxiv.org/abs/2512.16514)
*Diodato Ferraioli,Paolo Penna,Manvir Schneider,Carmine Ventre*

Main category: cs.GT

TL;DR: 区块链代币经济学中，短期性能激励与长期去中心化目标的对齐是核心挑战。本文提出了一种算法货币政策框架，通过重复参与游戏平衡这一矛盾。


<details>
  <summary>Details</summary>
Motivation: 区块链领域需要解决短期性能激励与长期去中心化目标之间的矛盾，以确保系统的稳定性和可持续性。

Method: 提出了一种框架，基于代理的类型（能力）和质押选择参与或放弃每轮任务；政策（概率性地）选择高效代理执行任务（最大化吞吐量），同时分配奖励以维持去中心化。分析了两种代理行为下的均衡：短视（短期效用最大化）和前瞻性（多轮规划）。

Result: 短视代理下，性能导向政策可能导致中心化风险，而前瞻性行为可实现稳定去中心化，但伴随代币价值的波动。虚拟质押（类型与质押的混合）作为一种替代方法，其初始分配对长期结果至关重要。

Conclusion: 政策设计需要间接管理去中心化，初始虚拟质押分布对系统的长期稳定性具有决定性影响。

Abstract: A central challenge in blockchain tokenomics is aligning short-term performance incentives with long-term decentralization goals. We propose a framework for algorithmic monetary policies that navigates this tradeoff in repeated participation games. Agents, characterized by type (capability) and stake, choose to participate or abstain at each round; the policy (probabilistically) selects high-type agents for task execution (maximizing throughput) while distributing rewards to sustain decentralization. We analyze equilibria under two agent behaviors: myopic (short-term utility maximization) and foresighted (multi-round planning). For myopic agents, performance-centric policies risk centralization, but foresight enables stable decentralization with some volatility to the token value. We further discuss virtual stake--a hybrid of type and stake--as an alternative approach. We show that the initial virtual stake distribution critically impacts long-term outcomes, suggesting that policies must indirectly manage decentralization.

</details>


### [8] [Online Resource Allocation via Static Bundle Pricing](https://arxiv.org/abs/2512.16570)
*Dimitris Fotakis,Charalampos Platanos,Thanos Tolias*

Main category: cs.GT

TL;DR: 该论文提出了一种统一的技术，用于解决具有互补性的在线资源分配问题，并在三个领域中取得了显著成果，同时提供了信息理论下的性能下界。


<details>
  <summary>Details</summary>
Motivation: 在线资源分配问题在买方对未来请求信息不完全的情况下，如何高效分配有限资源是一个挑战。现有的定价机制无法充分利用物品互补性，需要一种更具通用性的方法。

Method: 论文开发了一种统一的技术，适用于三个领域：具有最大组合大小的单心组合拍卖、一般单心组合拍卖以及基于图的路由模型。该方法使用静态匿名组合定价机制，性能随物品容量呈指数级提升。

Result: 在具有最小物品容量B的d-单心组合拍卖中，提出了O(d^(1/B))-竞争机制；在一般单心组合拍卖和图路由模型中，提出了O(m^(1/(B+1)))-竞争机制。同时，论文还通过信息理论证明了一些性能下界。

Conclusion: 论文提出的方法在多个领域中显著提升了资源分配效率，并通过信息理论揭示了性能下界，为在线资源分配问题提供了新视角。

Abstract: Online Resource Allocation addresses the problem of efficiently allocating limited resources to buyers with incomplete knowledge of future requests. In our setting, buyers arrive sequentially demanding a set of items, each with a value drawn from a known distribution. We study environments where buyers' valuations exhibit complementarities. In such settings, standard item-pricing mechanisms fail to leverage item multiplicities, while existing static bundle-pricing mechanisms rely on problem-specific arguments that do not generalize.
  We develop a unified technique for online resource allocation with complementarities for three domains: (i) single-minded combinatorial auctions with maximum bundle size $d$, (ii) general single-minded combinatorial auctions, and (iii) a graph-based routing model in which buyers request to route a unit of flow from a source node $s$ to a target node $t$ in a capacitated graph. Our approach yields static and anonymous bundle-pricing mechanisms whose performance improves exponentially with item capacities. For the $d$-single-minded setting with minimum item capacity $B$, we obtain an $O(d^{1/B})$-competitive mechanism, recovering the known $O(d)$ bound for unit capacities ($B=1$) and achieving exponentially better guarantees as capacities grow. For general single-minded combinatorial auctions and the graph-routing model, we obtain $O(m^{1/(B+1)})$-competitive mechanisms, where $m$ is the number of items.
  We complement these results with information-theoretic lower bounds. We show that no online algorithm can achieve a competitive ratio better than $Ω((m/\ln m)^{1/(B+2)})$ in the general single-minded setting and $Ω((d/\ln d)^{1/(B+1)})$ in the $d$-single-minded setting. In doing so, we reveal a deep connection to the extremal combinatorics problem of determining the maximum number of qualitatively independent partitions of a ground set.

</details>


### [9] [On the Edge of Core (Non-)Emptiness: An Automated Reasoning Approach to Approval-Based Multi-Winner Voting](https://arxiv.org/abs/2512.16895)
*Ratip Emin Berker,Emanuel Tewolde,Vincent Conitzer,Mingyu Guo,Marijn Heule,Lirong Xia*

Main category: cs.GT

TL;DR: 论文提出了...并验证了...


<details>
  <summary>Details</summary>
Motivation: 研究的动机是...

Method: 采用了...方法

Result: 实验结果显示...

Conclusion: 最终得出结论...

Abstract: Core stability is a natural and well-studied notion for group fairness in multi-winner voting, where the task is to select a committee from a pool of candidates. We study the setting where voters either approve or disapprove of each candidate; here, it remains a major open problem whether a core-stable committee always exists. In this work, we develop an approach based on mixed-integer linear programming for deciding whether and when core-stable committees are guaranteed to exist. In contrast to SAT-based approaches popular in computational social choice, our method can produce proofs for a specific number of candidates independent of the number of voters. In addition to these computational gains, our program lends itself to a novel duality-based reformulation of the core stability problem, from which we obtain new existence results in special cases. Further, we use our framework to reveal previously unknown relationships between core stability and other desirable properties, such as notions of priceability.

</details>
