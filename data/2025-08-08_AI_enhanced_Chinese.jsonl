{"id": "2508.04829", "categories": ["cs.PL"], "pdf": "https://arxiv.org/pdf/2508.04829", "abs": "https://arxiv.org/abs/2508.04829", "authors": ["Devora Chait-Roth", "Kedar S. Namjoshi", "Thomas Wies"], "title": "Consistent Updates for Scalable Microservices", "comment": null, "summary": "Online services are commonly implemented with a scalable microservice\narchitecture, where isomorphic worker processes service client requests,\nrecording persistent state in a backend data store. To maintain service, any\nmodifications to the service functionality must be made on the fly -- i.e., as\nthe service continues to process client requests -- but doing so is\nchallenging. The central difficulty is that of avoiding potential\ninconsistencies caused by ''mixed mode'' operation, where workers of current\nand new versions are concurrently active and interact via the data store. Some\nupdate methods avoid mixed mode altogether, but only at the cost of substantial\ninefficiency -- by doubling resources (memory and compute), or by halving\nthroughput. The alternative is a so-called ''rolling'' update, which is\nuncontrolled and runs the risk of serious service failures arising from\ninconsistent mixed-mode behavior.\n  In this paper, we present the first algorithms that guarantee consistency for\nmixed mode updates. The algorithms rely on semantic properties of service\nactions, such as commutativity. We show that semantic awareness is required, by\nproving that any semantically oblivious, mixed-mode update method cannot avoid\ninconsistencies. Ideally, it should appear to every client that a service\nupdate takes effect atomically; this ensures that a client is not exposed to\ninconsistent mixed-mode behavior. We introduce a framework that formalizes this\nintuition and develop foundational theory for reasoning about the consistency\nof mixed-mode updates, applying that theory to derive the new algorithms and\nestablish their correctness.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u9996\u4e2a\u4fdd\u8bc1\u6df7\u5408\u6a21\u5f0f\u66f4\u65b0\u7684\u7b97\u6cd5\uff0c\u5229\u7528\u670d\u52a1\u64cd\u4f5c\u7684\u8bed\u4e49\u5c5e\u6027\uff08\u5982\u53ef\u4ea4\u6362\u6027\uff09\u786e\u4fdd\u4e00\u81f4\u6027\uff0c\u5e76\u8bc1\u660e\u4e86\u8bed\u4e49\u611f\u77e5\u7684\u5fc5\u8981\u6027\u3002", "motivation": "\u5728\u7ebf\u670d\u52a1\u901a\u5e38\u91c7\u7528\u53ef\u6269\u5c55\u7684\u5fae\u670d\u52a1\u67b6\u6784\uff0c\u4f46\u670d\u52a1\u529f\u80fd\u7684\u52a8\u6001\u66f4\u65b0\u53ef\u80fd\u5bfc\u81f4\u4e0d\u4e00\u81f4\u6027\u95ee\u9898\u3002\u73b0\u6709\u65b9\u6cd5\u8981\u4e48\u727a\u7272\u6548\u7387\uff0c\u8981\u4e48\u5b58\u5728\u98ce\u9669\uff0c\u56e0\u6b64\u9700\u8981\u65b0\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u670d\u52a1\u64cd\u4f5c\u8bed\u4e49\u5c5e\u6027\uff08\u5982\u53ef\u4ea4\u6362\u6027\uff09\u7684\u7b97\u6cd5\u6846\u67b6\uff0c\u5e76\u5f00\u53d1\u4e86\u76f8\u5173\u7406\u8bba\u4ee5\u63a8\u5bfc\u65b0\u7b97\u6cd5\u5e76\u9a8c\u8bc1\u5176\u6b63\u786e\u6027\u3002", "result": "\u901a\u8fc7\u7406\u8bba\u8bc1\u660e\uff0c\u8bba\u6587\u5c55\u793a\u4e86\u8bed\u4e49\u611f\u77e5\u662f\u907f\u514d\u6df7\u5408\u6a21\u5f0f\u4e0d\u4e00\u81f4\u6027\u7684\u5fc5\u8981\u6761\u4ef6\uff0c\u5e76\u63d0\u51fa\u7684\u7b97\u6cd5\u80fd\u591f\u786e\u4fdd\u5ba2\u6237\u7aef\u7684\u64cd\u4f5c\u539f\u5b50\u6027\u3002", "conclusion": "\u8bba\u6587\u7684\u7406\u8bba\u548c\u7b97\u6cd5\u4e3a\u6df7\u5408\u6a21\u5f0f\u66f4\u65b0\u63d0\u4f9b\u4e86\u53ef\u9760\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u786e\u4fdd\u4e86\u670d\u52a1\u52a8\u6001\u66f4\u65b0\u65f6\u7684\u6570\u636e\u4e00\u81f4\u6027\u3002"}}
{"id": "2508.04825", "categories": ["cs.GR", "cs.AI", "cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.04825", "abs": "https://arxiv.org/abs/2508.04825", "authors": ["Seungyong Lee", "Jeong-gi Kwak"], "title": "Voost: A Unified and Scalable Diffusion Transformer for Bidirectional Virtual Try-On and Try-Off", "comment": "Project page: https://nxnai.github.io/Voost/", "summary": "Virtual try-on aims to synthesize a realistic image of a person wearing a\ntarget garment, but accurately modeling garment-body correspondence remains a\npersistent challenge, especially under pose and appearance variation. In this\npaper, we propose Voost - a unified and scalable framework that jointly learns\nvirtual try-on and try-off with a single diffusion transformer. By modeling\nboth tasks jointly, Voost enables each garment-person pair to supervise both\ndirections and supports flexible conditioning over generation direction and\ngarment category, enhancing garment-body relational reasoning without\ntask-specific networks, auxiliary losses, or additional labels. In addition, we\nintroduce two inference-time techniques: attention temperature scaling for\nrobustness to resolution or mask variation, and self-corrective sampling that\nleverages bidirectional consistency between tasks. Extensive experiments\ndemonstrate that Voost achieves state-of-the-art results on both try-on and\ntry-off benchmarks, consistently outperforming strong baselines in alignment\naccuracy, visual fidelity, and generalization.", "AI": {"tldr": "Voost\u662f\u4e00\u4e2a\u7edf\u4e00\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u8054\u5408\u5b66\u4e60\u865a\u62df\u8bd5\u7a7f\u548c\u8bd5\u8131\u4efb\u52a1\uff0c\u5229\u7528\u6269\u6563\u53d8\u6362\u5668\u589e\u5f3a\u8863\u7269\u4e0e\u4eba\u4f53\u5173\u7cfb\u7684\u63a8\u7406\u80fd\u529b\uff0c\u65e0\u9700\u4efb\u52a1\u7279\u5b9a\u7f51\u7edc\u6216\u989d\u5916\u6807\u7b7e\u3002", "motivation": "\u865a\u62df\u8bd5\u7a7f\u6280\u672f\u5728\u59ff\u52bf\u548c\u5916\u89c2\u53d8\u5316\u4e0b\u51c6\u786e\u5efa\u6a21\u8863\u7269\u4e0e\u4eba\u4f53\u5bf9\u5e94\u5173\u7cfb\u4ecd\u5177\u6709\u6311\u6218\u6027\uff0c\u56e0\u6b64\u63d0\u51fa\u4e86Voost\u6846\u67b6\u6765\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "Voost\u91c7\u7528\u6269\u6563\u53d8\u6362\u5668\u8054\u5408\u5b66\u4e60\u865a\u62df\u8bd5\u7a7f\u548c\u8bd5\u8131\u4efb\u52a1\uff0c\u652f\u6301\u751f\u6210\u65b9\u5411\u548c\u8863\u7269\u7c7b\u522b\u7684\u7075\u6d3b\u6761\u4ef6\uff0c\u5e76\u5f15\u5165\u6ce8\u610f\u529b\u6e29\u5ea6\u7f29\u653e\u548c\u81ea\u6211\u6821\u6b63\u91c7\u6837\u6280\u672f\u3002", "result": "Voost\u5728\u8bd5\u7a7f\u548c\u8bd5\u8131\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u5230\u4e86\u6700\u5148\u8fdb\u7684\u6c34\u5e73\uff0c\u5728\u5bf9\u9f50\u7cbe\u5ea6\u3001\u89c6\u89c9\u903c\u771f\u5ea6\u548c\u6cdb\u5316\u80fd\u529b\u4e0a\u5747\u4f18\u4e8e\u57fa\u7ebf\u6a21\u578b\u3002", "conclusion": "Voost\u901a\u8fc7\u8054\u5408\u5efa\u6a21\u548c\u63a8\u7406\u6280\u672f\uff0c\u663e\u8457\u63d0\u5347\u4e86\u865a\u62df\u8bd5\u7a7f\u548c\u8bd5\u8131\u4efb\u52a1\u7684\u6027\u80fd\uff0c\u4e3a\u8863\u7269\u4e0e\u4eba\u4f53\u5173\u7cfb\u63a8\u7406\u63d0\u4f9b\u4e86\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2508.04779", "categories": ["cs.GT", "cs.MA"], "pdf": "https://arxiv.org/pdf/2508.04779", "abs": "https://arxiv.org/abs/2508.04779", "authors": ["Themistoklis Melissourgos", "Nicos Protopapas"], "title": "Online EFX Allocations with Predictions", "comment": null, "summary": "We study an online fair division problem where a fixed number of goods arrive\nsequentially and must be allocated to a given set of agents. Once a good\narrives, its true value for each agent is revealed, and it has to be\nimmediately and irrevocably allocated to some agent. The ultimate goal is to\nensure envy-freeness up to any good (EFX) after all goods have been allocated.\nUnfortunately, as we show, approximate EFX allocations are unattainable in\ngeneral, even under restrictive assumptions on the valuation functions.\n  To address this, we follow a recent and fruitful trend of augmenting\nalgorithms with predictions. Specifically, we assume access to a prediction\nvector estimating the agents' true valuations -- e.g., generated by a machine\nlearning model trained on past data. Predictions may be unreliable, and we\nmeasure their error using the total variation distance from the true\nvaluations, that is, the percentage of predicted value-mass that disagrees with\nthe true values.\n  Focusing on the natural class of additive valuations, we prove impossibility\nresults even on approximate EFX allocations for algorithms that either ignore\npredictions or rely solely on them. We then turn to algorithms that use both\nthe predictions and the true values and show strong lower bounds on the\nprediction accuracy that is required by any algorithm to compute an approximate\nEFX. These negative results persist even under identical valuations, contrary\nto the offline setting where exact EFX allocations always exist without the\nnecessity of predictions. We then present an algorithm for two agents with\nidentical valuations that uses effectively the predictions and the true values.\nThe algorithm approximates EFX, with its guarantees improving as the accuracy\nof the predictions increases.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7814\u7a76\u4e86\u4e00\u4e2a\u5728\u7ebf\u516c\u5e73\u5206\u914d\u95ee\u9898\uff0c\u901a\u8fc7\u5f15\u5165\u9884\u6d4b\u6a21\u578b\u6765\u4f18\u5316\u5206\u914d\u7b56\u7565\uff0c\u6700\u7ec8\u76ee\u6807\u662f\u5b9e\u73b0\u8fd1\u4f3cEFX\uff08\u65e0\u5ac9\u5992\u5230\u4efb\u610f\u7269\u54c1\uff09\u5206\u914d\u3002", "motivation": "\u7814\u7a76\u5728\u7ebf\u516c\u5e73\u5206\u914d\u95ee\u9898\uff0c\u63a2\u7d22\u5982\u4f55\u5728\u7269\u54c1\u52a8\u6001\u5230\u8fbe\u4e14\u9700\u5373\u65f6\u5206\u914d\u7684\u60c5\u51b5\u4e0b\uff0c\u786e\u4fdd\u5206\u914d\u7ed3\u679c\u6ee1\u8db3EFX\u3002\u7531\u4e8e\u4e00\u822c\u6761\u4ef6\u4e0b\u8fd1\u4f3cEFX\u96be\u4ee5\u5b9e\u73b0\uff0c\u56e0\u6b64\u5f15\u5165\u9884\u6d4b\u673a\u5236\u4ee5\u4f18\u5316\u5206\u914d\u6548\u679c\u3002", "method": "\u91c7\u7528\u9884\u6d4b\u5411\u91cf\u4f30\u8ba1\u4ee3\u7406\u7684\u771f\u5b9e\u4f30\u503c\uff0c\u5e76\u7ed3\u5408\u771f\u5b9e\u503c\u8fdb\u884c\u5206\u914d\u51b3\u7b56\u3002\u7279\u522b\u9488\u5bf9\u52a0\u6cd5\u4f30\u503c\u51fd\u6570\u7c7b\uff0c\u7814\u7a76\u4e86\u4e0d\u540c\u7b97\u6cd5\uff08\u4ec5\u4f9d\u8d56\u9884\u6d4b\u3001\u5ffd\u7565\u9884\u6d4b\u6216\u4e24\u8005\u7ed3\u5408\uff09\u7684\u53ef\u884c\u6027\u3002", "result": "\u8bc1\u660e\u4e86\u5728\u5ffd\u7565\u9884\u6d4b\u6216\u4ec5\u4f9d\u8d56\u9884\u6d4b\u7684\u60c5\u51b5\u4e0b\uff0c\u8fd1\u4f3cEFX\u5206\u914d\u4e0d\u53ef\u884c\u3002\u63d0\u51fa\u4e86\u4e00\u79cd\u9488\u5bf9\u4e24\u4e2a\u4ee3\u7406\u4e14\u5177\u6709\u76f8\u540c\u4f30\u503c\u7684\u7b97\u6cd5\uff0c\u5176\u8fd1\u4f3cEFX\u7684\u6548\u679c\u968f\u9884\u6d4b\u51c6\u786e\u6027\u7684\u63d0\u9ad8\u800c\u63d0\u5347\u3002", "conclusion": "\u7814\u7a76\u8868\u660e\uff0c\u5373\u4f7f\u5728\u76f8\u540c\u4f30\u503c\u7684\u7b80\u5355\u60c5\u51b5\u4e0b\uff0c\u9884\u6d4b\u51c6\u786e\u6027\u5bf9\u5b9e\u73b0\u8fd1\u4f3cEFX\u81f3\u5173\u91cd\u8981\u3002\u63d0\u51fa\u7684\u7b97\u6cd5\u4e3a\u5b9e\u9645\u5e94\u7528\u4e2d\u7684\u516c\u5e73\u5206\u914d\u63d0\u4f9b\u4e86\u53ef\u884c\u65b9\u6848\u3002"}}
{"id": "2508.04965", "categories": ["cs.GR", "cs.CV", "cs.MM"], "pdf": "https://arxiv.org/pdf/2508.04965", "abs": "https://arxiv.org/abs/2508.04965", "authors": ["Zijian Wang", "Beizhen Zhao", "Hao Wang"], "title": "Perceive-Sample-Compress: Towards Real-Time 3D Gaussian Splatting", "comment": null, "summary": "Recent advances in 3D Gaussian Splatting (3DGS) have demonstrated remarkable\ncapabilities in real-time and photorealistic novel view synthesis. However,\ntraditional 3DGS representations often struggle with large-scale scene\nmanagement and efficient storage, particularly when dealing with complex\nenvironments or limited computational resources. To address these limitations,\nwe introduce a novel perceive-sample-compress framework for 3D Gaussian\nSplatting. Specifically, we propose a scene perception compensation algorithm\nthat intelligently refines Gaussian parameters at each level. This algorithm\nintelligently prioritizes visual importance for higher fidelity rendering in\ncritical areas, while optimizing resource usage and improving overall visible\nquality. Furthermore, we propose a pyramid sampling representation to manage\nGaussian primitives across hierarchical levels. Finally, to facilitate\nefficient storage of proposed hierarchical pyramid representations, we develop\na Generalized Gaussian Mixed model compression algorithm to achieve significant\ncompression ratios without sacrificing visual fidelity. The extensive\nexperiments demonstrate that our method significantly improves memory\nefficiency and high visual quality while maintaining real-time rendering speed.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u65b0\u9896\u7684perceive-sample-compress\u6846\u67b6\uff0c\u901a\u8fc7\u573a\u666f\u611f\u77e5\u8865\u507f\u7b97\u6cd5\u548c\u91d1\u5b57\u5854\u91c7\u6837\u8868\u793a\uff0c\u4f18\u53163D\u9ad8\u65af\u6cfc\u6e85\u6280\u672f\u7684\u5927\u89c4\u6a21\u573a\u666f\u7ba1\u7406\u548c\u5b58\u50a8\u6548\u7387\uff0c\u540c\u65f6\u4fdd\u6301\u5b9e\u65f6\u6e32\u67d3\u548c\u9ad8\u89c6\u89c9\u8d28\u91cf\u3002", "motivation": "\u4f20\u7edf3D\u9ad8\u65af\u6cfc\u6e85\u6280\u672f\u5728\u5904\u7406\u5927\u89c4\u6a21\u573a\u666f\u548c\u6709\u9650\u8ba1\u7b97\u8d44\u6e90\u65f6\u5b58\u5728\u6548\u7387\u548c\u5b58\u50a8\u7684\u5c40\u9650\u6027\uff0c\u9700\u8981\u4e00\u79cd\u66f4\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u91c7\u7528\u573a\u666f\u611f\u77e5\u8865\u507f\u7b97\u6cd5\u4f18\u5316\u9ad8\u65af\u53c2\u6570\uff0c\u63d0\u51fa\u91d1\u5b57\u5854\u91c7\u6837\u8868\u793a\u7ba1\u7406\u9ad8\u65af\u57fa\u5143\uff0c\u5e76\u5f00\u53d1\u4e86\u5e7f\u4e49\u9ad8\u65af\u6df7\u5408\u6a21\u578b\u538b\u7f29\u7b97\u6cd5\u4ee5\u63d0\u9ad8\u5b58\u50a8\u6548\u7387\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86\u5185\u5b58\u6548\u7387\u548c\u89c6\u89c9\u8d28\u91cf\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u5b9e\u65f6\u6e32\u67d3\u901f\u5ea6\u3002", "conclusion": "\u8be5\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e863D\u9ad8\u65af\u6cfc\u6e85\u6280\u672f\u5728\u5927\u89c4\u6a21\u573a\u666f\u7ba1\u7406\u548c\u5b58\u50a8\u6548\u7387\u4e0a\u7684\u95ee\u9898\uff0c\u4e3a\u590d\u6742\u73af\u5883\u4e0b\u7684\u5b9e\u65f6\u6e32\u67d3\u63d0\u4f9b\u4e86\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2508.05109", "categories": ["cs.GT", "math.OC"], "pdf": "https://arxiv.org/pdf/2508.05109", "abs": "https://arxiv.org/abs/2508.05109", "authors": ["Mandar Datar", "Mattia Merluzzi"], "title": "Toward Energy and Location-Aware Resource Allocation in Next Generation Networks", "comment": null, "summary": "Wireless networks are evolving from radio resource providers to complex\nsystems that also involve computing, with the latter being distributed across\nedge and cloud facilities. Also, their optimization is shifting more and more\nfrom a performance to a value-oriented paradigm. The two aspects shall be\nbalanced continuously, to maximize the utilities of Services Providers (SPs),\nusers quality of experience and fairness, while meeting global constraints in\nterms of energy consumption and carbon footprint among others, with all these\nheterogeneous resources contributing. In this paper, we tackle the problem of\ncommunication and compute resource allocation under energy constraints, with\nmultiple SPs competing to get their preferred resource bundle by spending a a\nfictitious currency budget. By modeling the network as a Fisher market, we\npropose a low complexity solution able to achieve high utilities and guarantee\nenergy constraints, while also promoting fairness among SPs, as compared to a\nsocial optimal solution. The market equilibrium is proved mathematically, and\nnumerical results show the multi-dimensional trade-off between utility and\nenergy at different locations, with communication and computation-intensive\nservices.", "AI": {"tldr": "\u8bba\u6587\u89e3\u51b3\u4e86\u65e0\u7ebf\u7f51\u7edc\u4e2d\u901a\u4fe1\u548c\u8ba1\u7b97\u8d44\u6e90\u5728\u80fd\u6e90\u7ea6\u675f\u4e0b\u7684\u5206\u914d\u95ee\u9898\uff0c\u5229\u7528Fisher\u5e02\u573a\u6a21\u578b\u63d0\u51fa\u4e86\u4e00\u79cd\u4f4e\u590d\u6742\u5ea6\u89e3\u51b3\u65b9\u6848\uff0c\u4ee5\u5e73\u8861\u670d\u52a1\u63d0\u4f9b\u5546\uff08SPs\uff09\u7684\u6548\u7528\u3001\u516c\u5e73\u6027\u548c\u80fd\u6e90\u9650\u5236\u3002", "motivation": "\u65e0\u7ebf\u7f51\u7edc\u6b63\u4ece\u5355\u7eaf\u7684\u65e0\u7ebf\u7535\u8d44\u6e90\u63d0\u4f9b\u8005\u8f6c\u53d8\u4e3a\u6d89\u53ca\u8ba1\u7b97\u7684\u590d\u6742\u7cfb\u7edf\uff0c\u4f18\u5316\u76ee\u6807\u4e5f\u4ece\u6027\u80fd\u5bfc\u5411\u8f6c\u5411\u4ef7\u503c\u5bfc\u5411\u3002\u9700\u8981\u5e73\u8861\u591a\u79cd\u8d44\u6e90\uff0c\u4ee5\u6ee1\u8db3\u670d\u52a1\u63d0\u4f9b\u5546\u548c\u7528\u6237\u7684\u6548\u7528\u3001\u516c\u5e73\u6027\u53ca\u5168\u7403\u80fd\u6e90\u548c\u78b3\u8db3\u8ff9\u7b49\u7ea6\u675f\u6761\u4ef6\u3002", "method": "\u901a\u8fc7\u5c06\u7f51\u7edc\u5efa\u6a21\u4e3aFisher\u5e02\u573a\uff0c\u63d0\u51fa\u4e86\u4e00\u4e2a\u4f4e\u590d\u6742\u5ea6\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5141\u8bb8\u591a\u4e2a\u670d\u52a1\u63d0\u4f9b\u5546\u901a\u8fc7\u82b1\u8d39\u865a\u6784\u8d27\u5e01\u9884\u7b97\u6765\u83b7\u53d6\u5176\u504f\u597d\u7684\u8d44\u6e90\u7ec4\u5408\u3002", "result": "\u6570\u5b66\u8bc1\u660e\u4e86\u5e02\u573a\u5747\u8861\uff0c\u6570\u503c\u7ed3\u679c\u663e\u793a\u5728\u4e0d\u540c\u4f4d\u7f6e\u548c\u901a\u4fe1\u4e0e\u8ba1\u7b97\u5bc6\u96c6\u578b\u670d\u52a1\u4e0b\uff0c\u6548\u7528\u4e0e\u80fd\u6e90\u4e4b\u95f4\u7684\u591a\u7ef4\u6743\u8861\u3002", "conclusion": "\u6240\u63d0\u65b9\u6848\u5728\u4fdd\u8bc1\u80fd\u6e90\u7ea6\u675f\u7684\u540c\u65f6\uff0c\u80fd\u591f\u5b9e\u73b0\u9ad8\u6548\u7528\u5e76\u4fc3\u8fdb\u670d\u52a1\u63d0\u4f9b\u5546\u4e4b\u95f4\u7684\u516c\u5e73\u6027\uff0c\u4f18\u4e8e\u793e\u4f1a\u6700\u4f18\u89e3\u3002"}}
{"id": "2508.04966", "categories": ["cs.GR", "cs.CV", "cs.MM"], "pdf": "https://arxiv.org/pdf/2508.04966", "abs": "https://arxiv.org/abs/2508.04966", "authors": ["Yifan Zhou", "Beizhen Zhao", "Pengcheng Wu", "Hao Wang"], "title": "Laplacian Analysis Meets Dynamics Modelling: Gaussian Splatting for 4D Reconstruction", "comment": null, "summary": "While 3D Gaussian Splatting (3DGS) excels in static scene modeling, its\nextension to dynamic scenes introduces significant challenges. Existing dynamic\n3DGS methods suffer from either over-smoothing due to low-rank decomposition or\nfeature collision from high-dimensional grid sampling. This is because of the\ninherent spectral conflicts between preserving motion details and maintaining\ndeformation consistency at different frequency. To address these challenges, we\npropose a novel dynamic 3DGS framework with hybrid explicit-implicit functions.\nOur approach contains three key innovations: a spectral-aware Laplacian\nencoding architecture which merges Hash encoding and Laplacian-based module for\nflexible frequency motion control, an enhanced Gaussian dynamics attribute that\ncompensates for photometric distortions caused by geometric deformation, and an\nadaptive Gaussian split strategy guided by KDTree-based primitive control to\nefficiently query and optimize dynamic areas. Through extensive experiments,\nour method demonstrates state-of-the-art performance in reconstructing complex\ndynamic scenes, achieving better reconstruction fidelity.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u578b\u7684\u52a8\u60013D\u9ad8\u65af\u6cfc\u6e85\u6846\u67b6\uff0c\u901a\u8fc7\u6df7\u5408\u663e\u5f0f-\u9690\u5f0f\u51fd\u6570\u89e3\u51b3\u4e86\u73b0\u6709\u52a8\u60013D\u9ad8\u65af\u6cfc\u6e85\u65b9\u6cd5\u5728\u8fd0\u52a8\u7ec6\u8282\u4fdd\u7559\u548c\u53d8\u5f62\u4e00\u81f4\u6027\u65b9\u9762\u7684\u6311\u6218\u3002", "motivation": "\u73b0\u6709\u7684\u52a8\u60013D\u9ad8\u65af\u6cfc\u6e85\u65b9\u6cd5\u5728\u5efa\u6a21\u52a8\u6001\u573a\u666f\u65f6\u5b58\u5728\u8fc7\u5ea6\u5e73\u6ed1\u6216\u7279\u5f81\u78b0\u649e\u7684\u95ee\u9898\uff0c\u4e3b\u8981\u662f\u7531\u4e8e\u5728\u4e0d\u540c\u9891\u7387\u4e0b\u4fdd\u6301\u8fd0\u52a8\u7ec6\u8282\u548c\u53d8\u5f62\u4e00\u81f4\u6027\u7684\u9891\u8c31\u51b2\u7a81\u3002", "method": "\u63d0\u51fa\u4e86\u4e09\u79cd\u5173\u952e\u521b\u65b0\uff1a\u9891\u8c31\u611f\u77e5\u7684\u62c9\u666e\u62c9\u65af\u7f16\u7801\u67b6\u6784\u3001\u589e\u5f3a\u7684\u9ad8\u65af\u52a8\u6001\u5c5e\u6027\u4ee5\u53ca\u57fa\u4e8eKDTree\u7684\u81ea\u9002\u5e94\u9ad8\u65af\u5206\u5272\u7b56\u7565\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u590d\u6742\u52a8\u6001\u573a\u666f\u91cd\u5efa\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u8fbe\u5230\u4e86\u66f4\u9ad8\u7684\u91cd\u5efa\u4fdd\u771f\u5ea6\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u52a8\u6001\u573a\u666f\u5efa\u6a21\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u91cd\u5efa\u8d28\u91cf\u548c\u8fd0\u52a8\u7ec6\u8282\u7684\u4fdd\u7559\u80fd\u529b\u3002"}}
{"id": "2508.05582", "categories": ["cs.GT"], "pdf": "https://arxiv.org/pdf/2508.05582", "abs": "https://arxiv.org/abs/2508.05582", "authors": ["Sourish Sarkar", "Aritrabha Majumdar", "Moutushi Chatterjee"], "title": "A New Three-Players Auction Bridge with Dynamic Opponents and Team Members", "comment": "10 pages, 2 figures", "summary": "This article presents a new three-player version of the bridge playing card\ngame for the purpose of ending fixed partnerships so that the play can be more\ndynamic and flexible. By dynamically redefining team makeup in real time, this\ngame design increases unpredictability and forces players to repeatedly update\nstrategy. A novel scoring system is introduced to reduce biases present in\nconventional rule-based games by favoring fairness via reward systems that\nenforce tactical decision making and risk assessment. Being subject to regular\nbridge rules, this version tests players to collaborate without fixed\nfriendships, requiring fluid adjustment and adaptive bidding behavior in real\ntime. Strategic issues involve aggressive and defensive bidding, adaptable\nplaying styles, and loss-seeking strategies specific to the three-player\nstructure. The article discusses probabilistic issues of bidding, trump and\nno-trump declarative effects, and algorithmic methods to trick-taking.\nSimulation outcomes illustrate the efficiency of diverse strategies. The game's\narchitecture is ideal for competitions and possibly influential in broadening\nentry pools for tournament card games.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86\u4e00\u79cd\u65b0\u7684\u4e09\u4eba\u7248\u6865\u724c\u6e38\u620f\uff0c\u65e8\u5728\u6d88\u9664\u56fa\u5b9a\u642d\u6863\uff0c\u4f7f\u6e38\u620f\u66f4\u52a8\u6001\u7075\u6d3b\u3002\u901a\u8fc7\u5b9e\u65f6\u91cd\u65b0\u5b9a\u4e49\u56e2\u961f\u7ec4\u6210\uff0c\u589e\u52a0\u4e86\u6e38\u620f\u7684\u4e0d\u53ef\u9884\u6d4b\u6027\uff0c\u8feb\u4f7f\u73a9\u5bb6\u4e0d\u65ad\u66f4\u65b0\u7b56\u7565\u3002", "motivation": "\u4f20\u7edf\u7684\u6865\u724c\u6e38\u620f\u4e2d\uff0c\u56fa\u5b9a\u642d\u6863\u53ef\u80fd\u4f1a\u5bfc\u81f4\u6e38\u620f\u7b56\u7565\u7684\u50f5\u5316\u548c\u4e0d\u516c\u5e73\u3002\u672c\u6587\u65e8\u5728\u901a\u8fc7\u8bbe\u8ba1\u4e00\u79cd\u65b0\u7684\u4e09\u4eba\u7248\u6865\u724c\u6e38\u620f\uff0c\u589e\u52a0\u52a8\u6001\u6027\u548c\u516c\u5e73\u6027\u3002", "method": "\u91c7\u7528\u5b9e\u65f6\u91cd\u65b0\u5b9a\u4e49\u56e2\u961f\u7ec4\u6210\u7684\u65b9\u6cd5\uff0c\u5f15\u5165\u65b0\u7684\u8bc4\u5206\u7cfb\u7edf\u4ee5\u51cf\u5c11\u504f\u89c1\uff0c\u5e76\u901a\u8fc7\u5956\u52b1\u7cfb\u7edf\u4fc3\u8fdb\u6218\u672f\u51b3\u7b56\u548c\u98ce\u9669\u8bc4\u4f30\u3002\u6e38\u620f\u4fdd\u7559\u4e86\u5e38\u89c4\u6865\u724c\u89c4\u5219\uff0c\u4f46\u8981\u6c42\u73a9\u5bb6\u5728\u6ca1\u6709\u56fa\u5b9a\u642d\u6863\u7684\u60c5\u51b5\u4e0b\u534f\u4f5c\u3002", "result": "\u6a21\u62df\u7ed3\u679c\u663e\u793a\uff0c\u8fd9\u79cd\u65b0\u6e38\u620f\u8bbe\u8ba1\u80fd\u591f\u6709\u6548\u4fc3\u8fdb\u591a\u6837\u5316\u7b56\u7565\u7684\u5e94\u7528\uff0c\u5e76\u5c55\u793a\u4e86\u52a8\u6001\u56e2\u961f\u7ec4\u6210\u5bf9\u73a9\u5bb6\u6218\u672f\u8c03\u6574\u7684\u79ef\u6781\u5f71\u54cd\u3002", "conclusion": "\u8fd9\u79cd\u4e09\u4eba\u7248\u6865\u724c\u6e38\u620f\u8bbe\u8ba1\u4e0d\u4ec5\u63d0\u9ad8\u4e86\u6e38\u620f\u7684\u52a8\u6001\u6027\u548c\u516c\u5e73\u6027\uff0c\u8fd8\u4e3a\u9526\u6807\u8d5b\u5361\u724c\u6e38\u620f\u7684\u63a8\u5e7f\u63d0\u4f9b\u4e86\u65b0\u7684\u53ef\u80fd\u6027\u3002"}}
{"id": "2508.05064", "categories": ["cs.GR", "cs.CL", "cs.CV"], "pdf": "https://arxiv.org/pdf/2508.05064", "abs": "https://arxiv.org/abs/2508.05064", "authors": ["Mahmoud Chick Zaouali", "Todd Charter", "Yehor Karpichev", "Brandon Haworth", "Homayoun Najjjaran"], "title": "A Study of the Framework and Real-World Applications of Language Embedding for 3D Scene Understanding", "comment": null, "summary": "Gaussian Splatting has rapidly emerged as a transformative technique for\nreal-time 3D scene representation, offering a highly efficient and expressive\nalternative to Neural Radiance Fields (NeRF). Its ability to render complex\nscenes with high fidelity has enabled progress across domains such as scene\nreconstruction, robotics, and interactive content creation. More recently, the\nintegration of Large Language Models (LLMs) and language embeddings into\nGaussian Splatting pipelines has opened new possibilities for text-conditioned\ngeneration, editing, and semantic scene understanding. Despite these advances,\na comprehensive overview of this emerging intersection has been lacking. This\nsurvey presents a structured review of current research efforts that combine\nlanguage guidance with 3D Gaussian Splatting, detailing theoretical\nfoundations, integration strategies, and real-world use cases. We highlight key\nlimitations such as computational bottlenecks, generalizability, and the\nscarcity of semantically annotated 3D Gaussian data and outline open challenges\nand future directions for advancing language-guided 3D scene understanding\nusing Gaussian Splatting.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7efc\u8ff0\u4e86\u7ed3\u5408\u8bed\u8a00\u6307\u5bfc\u4e0e3D\u9ad8\u65af\u6cfc\u6e85\u6280\u672f\u7684\u7814\u7a76\u73b0\u72b6\uff0c\u63a2\u8ba8\u4e86\u5176\u5728\u6587\u672c\u6761\u4ef6\u751f\u6210\u3001\u7f16\u8f91\u548c\u8bed\u4e49\u573a\u666f\u7406\u89e3\u4e2d\u7684\u5e94\u7528\uff0c\u5e76\u6307\u51fa\u4e86\u5f53\u524d\u7684\u5c40\u9650\u6027\u4e0e\u672a\u6765\u65b9\u5411\u3002", "motivation": "\u9ad8\u65af\u6cfc\u6e85\u6280\u672f\u4f5c\u4e3a\u4e00\u79cd\u9ad8\u6548\u76843D\u573a\u666f\u8868\u793a\u65b9\u6cd5\uff0c\u7ed3\u5408\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u548c\u8bed\u8a00\u5d4c\u5165\u540e\uff0c\u4e3a\u6587\u672c\u9a71\u52a8\u7684\u573a\u666f\u751f\u6210\u548c\u7406\u89e3\u63d0\u4f9b\u4e86\u65b0\u53ef\u80fd\u3002\u4f46\u76ee\u524d\u7f3a\u4e4f\u5bf9\u8fd9\u4e00\u4ea4\u53c9\u9886\u57df\u7684\u5168\u9762\u7efc\u8ff0\u3002", "method": "\u8bba\u6587\u901a\u8fc7\u5bf9\u5f53\u524d\u7814\u7a76\u8fdb\u884c\u7ed3\u6784\u5316\u56de\u987e\uff0c\u8be6\u7ec6\u603b\u7ed3\u4e86\u7ed3\u5408\u8bed\u8a00\u4e0e3D\u9ad8\u65af\u6cfc\u6e85\u6280\u672f\u7684\u7406\u8bba\u57fa\u7840\u3001\u6574\u5408\u7b56\u7565\u548c\u5b9e\u9645\u5e94\u7528\u6848\u4f8b\u3002", "result": "\u7efc\u8ff0\u63ed\u793a\u4e86\u8ba1\u7b97\u74f6\u9888\u3001\u6cdb\u5316\u80fd\u529b\u4ee5\u53ca\u8bed\u4e49\u6807\u6ce8\u6570\u636e\u7a00\u7f3a\u7b49\u5173\u952e\u9650\u5236\uff0c\u5e76\u5c55\u793a\u4e86\u8bed\u8a00\u5f15\u5bfc\u76843D\u573a\u666f\u7406\u89e3\u7684\u6f5c\u529b\u3002", "conclusion": "\u8bba\u6587\u63d0\u51fa\u4e86\u672a\u6765\u7814\u7a76\u7684\u65b9\u5411\uff0c\u65e8\u5728\u901a\u8fc7\u9ad8\u65af\u6cfc\u6e85\u6280\u672f\u8fdb\u4e00\u6b65\u63a8\u52a8\u8bed\u8a00\u5f15\u5bfc\u76843D\u573a\u666f\u7406\u89e3\u7684\u53d1\u5c55\u3002"}}
{"id": "2508.05115", "categories": ["cs.GR", "cs.CV", "cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2508.05115", "abs": "https://arxiv.org/abs/2508.05115", "authors": ["Fangyu Du", "Taiqing Li", "Ziwei Zhang", "Qian Qiao", "Tan Yu", "Dingcheng Zhen", "Xu Jia", "Yang Yang", "Shunshun Yin", "Siyuan Liu"], "title": "RAP: Real-time Audio-driven Portrait Animation with Video Diffusion Transformer", "comment": "11 pages, 9 figures", "summary": "Audio-driven portrait animation aims to synthesize realistic and natural\ntalking head videos from an input audio signal and a single reference image.\nWhile existing methods achieve high-quality results by leveraging\nhigh-dimensional intermediate representations and explicitly modeling motion\ndynamics, their computational complexity renders them unsuitable for real-time\ndeployment. Real-time inference imposes stringent latency and memory\nconstraints, often necessitating the use of highly compressed latent\nrepresentations. However, operating in such compact spaces hinders the\npreservation of fine-grained spatiotemporal details, thereby complicating\naudio-visual synchronization RAP (Real-time Audio-driven Portrait animation), a\nunified framework for generating high-quality talking portraits under real-time\nconstraints. Specifically, RAP introduces a hybrid attention mechanism for\nfine-grained audio control, and a static-dynamic training-inference paradigm\nthat avoids explicit motion supervision. Through these techniques, RAP achieves\nprecise audio-driven control, mitigates long-term temporal drift, and maintains\nhigh visual fidelity. Extensive experiments demonstrate that RAP achieves\nstate-of-the-art performance while operating under real-time constraints.", "AI": {"tldr": "RAP\u662f\u4e00\u4e2a\u5b9e\u65f6\u97f3\u9891\u9a71\u52a8\u7684\u8096\u50cf\u52a8\u753b\u6846\u67b6\uff0c\u901a\u8fc7\u6df7\u5408\u6ce8\u610f\u529b\u673a\u5236\u548c\u9759\u6001-\u52a8\u6001\u8bad\u7ec3\u63a8\u7406\u8303\u5f0f\uff0c\u5b9e\u73b0\u4e86\u9ad8\u8d28\u91cf\u7684\u8bf4\u8bdd\u5934\u90e8\u89c6\u9891\u751f\u6210\uff0c\u540c\u65f6\u6ee1\u8db3\u5b9e\u65f6\u6027\u80fd\u9700\u6c42\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u867d\u7136\u5728\u97f3\u9891\u9a71\u52a8\u7684\u8096\u50cf\u52a8\u753b\u4e2d\u5b9e\u73b0\u4e86\u9ad8\u8d28\u91cf\u7ed3\u679c\uff0c\u4f46\u8ba1\u7b97\u590d\u6742\u6027\u4f7f\u5176\u65e0\u6cd5\u6ee1\u8db3\u5b9e\u65f6\u90e8\u7f72\u7684\u9700\u6c42\u3002RAP\u65e8\u5728\u901a\u8fc7\u538b\u7f29\u7684\u6f5c\u5728\u8868\u793a\u548c\u4f18\u5316\u7684\u8bbe\u8ba1\uff0c\u5b9e\u73b0\u9ad8\u8d28\u91cf\u7684\u5b9e\u65f6\u52a8\u753b\u751f\u6210\u3002", "method": "RAP\u5f15\u5165\u4e86\u6df7\u5408\u6ce8\u610f\u529b\u673a\u5236\u4ee5\u5b9e\u73b0\u7cbe\u7ec6\u7684\u97f3\u9891\u63a7\u5236\uff0c\u5e76\u91c7\u7528\u9759\u6001-\u52a8\u6001\u8bad\u7ec3\u63a8\u7406\u8303\u5f0f\uff0c\u907f\u514d\u663e\u5f0f\u8fd0\u52a8\u76d1\u7763\uff0c\u4ece\u800c\u5728\u538b\u7f29\u7a7a\u95f4\u4e2d\u4fdd\u6301\u9ad8\u4fdd\u771f\u5ea6\u3002", "result": "\u7ecf\u8fc7\u5927\u91cf\u5b9e\u9a8c\u9a8c\u8bc1\uff0cRAP\u5728\u5b9e\u65f6\u7ea6\u675f\u4e0b\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u9ad8\u8d28\u91cf\u7684\u97f3\u9891-\u89c6\u89c9\u540c\u6b65\u548c\u89c6\u89c9\u4fdd\u771f\u5ea6\u3002", "conclusion": "RAP\u901a\u8fc7\u521b\u65b0\u7684\u6df7\u5408\u6ce8\u610f\u529b\u673a\u5236\u548c\u8bad\u7ec3\u8303\u5f0f\uff0c\u6210\u529f\u5730\u89e3\u51b3\u4e86\u5b9e\u65f6\u97f3\u9891\u9a71\u52a8\u8096\u50cf\u52a8\u753b\u7684\u6311\u6218\uff0c\u4e3a\u9ad8\u8d28\u91cf\u5b9e\u65f6\u52a8\u753b\u751f\u6210\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2508.05187", "categories": ["cs.GR", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2508.05187", "abs": "https://arxiv.org/abs/2508.05187", "authors": ["Mohamed Abdul Gafoor", "Marius Preda", "Titus Zaharia"], "title": "Refining Gaussian Splatting: A Volumetric Densification Approach", "comment": null, "summary": "Achieving high-quality novel view synthesis in 3D Gaussian Splatting (3DGS)\noften depends on effective point primitive management. The underlying Adaptive\nDensity Control (ADC) process addresses this issue by automating densification\nand pruning. Yet, the vanilla 3DGS densification strategy shows key\nshortcomings. To address this issue, in this paper we introduce a novel density\ncontrol method, which exploits the volumes of inertia associated to each\nGaussian function to guide the refinement process. Furthermore, we study the\neffect of both traditional Structure from Motion (SfM) and Deep Image Matching\n(DIM) methods for point cloud initialization. Extensive experimental\nevaluations on the Mip-NeRF 360 dataset demonstrate that our approach surpasses\n3DGS in reconstruction quality, delivering encouraging performance across\ndiverse scenes.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u5bc6\u5ea6\u63a7\u5236\u65b9\u6cd5\uff0c\u901a\u8fc7\u5229\u7528\u9ad8\u65af\u51fd\u6570\u7684\u60ef\u6027\u4f53\u79ef\u6765\u5f15\u5bfc3D\u9ad8\u65af\u6cfc\u6e85\uff083DGS\uff09\u4e2d\u7684\u7ec6\u5316\u8fc7\u7a0b\uff0c\u663e\u8457\u63d0\u5347\u4e86\u65b0\u89c6\u56fe\u5408\u6210\u7684\u8d28\u91cf\u3002", "motivation": "\u4f20\u7edf\u76843DGS\u5bc6\u5ea6\u63a7\u5236\u7b56\u7565\u5b58\u5728\u5c40\u9650\u6027\uff0c\u65e0\u6cd5\u6709\u6548\u7ba1\u7406\u70b9\u57fa\u5143\u3002\u672c\u6587\u65e8\u5728\u901a\u8fc7\u4f18\u5316\u5bc6\u5ea6\u63a7\u5236\u65b9\u6cd5\uff0c\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u9ad8\u65af\u51fd\u6570\u60ef\u6027\u4f53\u79ef\u7684\u5bc6\u5ea6\u63a7\u5236\u65b9\u6cd5\uff0c\u540c\u65f6\u7814\u7a76\u4e86\u4f20\u7edf\u8fd0\u52a8\u7ed3\u6784\u6062\u590d\uff08SfM\uff09\u548c\u6df1\u5ea6\u56fe\u50cf\u5339\u914d\uff08DIM\uff09\u5728\u70b9\u4e91\u521d\u59cb\u5316\u4e2d\u7684\u5e94\u7528\u3002", "result": "\u5728Mip-NeRF 360\u6570\u636e\u96c6\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u91cd\u5efa\u8d28\u91cf\u4e0a\u8d85\u8d8a\u4e863DGS\uff0c\u5e76\u5728\u591a\u6837\u573a\u666f\u4e2d\u8868\u73b0\u51fa\u8272\u3002", "conclusion": "\u901a\u8fc7\u5f15\u5165\u60ef\u6027\u4f53\u79ef\u5f15\u5bfc\u7684\u5bc6\u5ea6\u63a7\u5236\u65b9\u6cd5\uff0c\u672c\u6587\u663e\u8457\u63d0\u5347\u4e863DGS\u7684\u6027\u80fd\uff0c\u4e3a\u65b0\u89c6\u56fe\u5408\u6210\u63d0\u4f9b\u4e86\u9ad8\u8d28\u91cf\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2508.05524", "categories": ["cs.GR", "cs.CG", "cs.HC"], "pdf": "https://arxiv.org/pdf/2508.05524", "abs": "https://arxiv.org/abs/2508.05524", "authors": ["Sefat Rahman", "Tushar M. Athawale", "Paul Rosen"], "title": "GASP: A Gradient-Aware Shortest Path Algorithm for Boundary-Confined Visualization of 2-Manifold Reeb Graphs", "comment": null, "summary": "Reeb graphs are an important tool for abstracting and representing the\ntopological structure of a function defined on a manifold. We have identified\nthree properties for faithfully representing Reeb graphs in a visualization.\nNamely, they should be constrained to the boundary, compact, and aligned with\nthe function gradient. Existing algorithms for drawing Reeb graphs are agnostic\nto or violate these properties. In this paper, we introduce an algorithm to\ngenerate Reeb graph visualizations, called \\textit{GASP}, that is cognizant of\nthese properties, thereby producing visualizations that are more representative\nof the underlying data. To demonstrate the improvements, the resulting Reeb\ngraphs are evaluated both qualitatively and quantitatively against the\ngeometric barycenter algorithm, using its implementation available in the\nTopology ToolKit (TTK), a widely adopted tool for calculating and visualizing\nReeb graphs.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aGASP\u7684\u7b97\u6cd5\uff0c\u7528\u4e8e\u751f\u6210\u7b26\u5408\u8fb9\u754c\u3001\u7d27\u51d1\u4e14\u4e0e\u51fd\u6570\u68af\u5ea6\u5bf9\u9f50\u7684Reeb\u56fe\u53ef\u89c6\u5316\uff0c\u6539\u8fdb\u4e86\u73b0\u6709\u7b97\u6cd5\uff0c\u5e76\u901a\u8fc7\u5b9a\u6027\u548c\u5b9a\u91cf\u8bc4\u4f30\u9a8c\u8bc1\u5176\u4f18\u8d8a\u6027\u3002", "motivation": "\u73b0\u6709\u7684Reeb\u56fe\u7ed8\u5236\u7b97\u6cd5\u5ffd\u7565\u4e86\u6216\u8fdd\u53cd\u4e86\u8fb9\u754c\u7ea6\u675f\u3001\u7d27\u51d1\u6027\u548c\u4e0e\u51fd\u6570\u68af\u5ea6\u5bf9\u9f50\u8fd9\u4e09\u4e2a\u5173\u952e\u5c5e\u6027\uff0c\u5bfc\u81f4\u53ef\u89c6\u5316\u7ed3\u679c\u4e0d\u80fd\u5fe0\u5b9e\u53cd\u6620\u6570\u636e\u7684\u62d3\u6251\u7ed3\u6784\u3002", "method": "\u672c\u6587\u63d0\u51fa\u4e86GASP\u7b97\u6cd5\uff0c\u8be5\u7b97\u6cd5\u5728\u8bbe\u8ba1\u65f6\u8003\u8651\u4e86\u8fb9\u754c\u7ea6\u675f\u3001\u7d27\u51d1\u6027\u548c\u4e0e\u51fd\u6570\u68af\u5ea6\u7684\u5bf9\u9f50\uff0c\u4ece\u800c\u751f\u6210\u66f4\u5fe0\u5b9e\u4e8e\u539f\u59cb\u6570\u636e\u7684Reeb\u56fe\u53ef\u89c6\u5316\u3002", "result": "\u901a\u8fc7\u4e0eTopology ToolKit (TTK)\u4e2d\u7684\u51e0\u4f55\u91cd\u5fc3\u7b97\u6cd5\u8fdb\u884c\u5bf9\u6bd4\uff0c\u5b9a\u6027\u548c\u5b9a\u91cf\u8bc4\u4f30\u8868\u660e\uff0cGASP\u7b97\u6cd5\u751f\u6210\u7684Reeb\u56fe\u5728\u8868\u8fbe\u6570\u636e\u62d3\u6251\u7ed3\u6784\u65b9\u9762\u8868\u73b0\u66f4\u4f18\u3002", "conclusion": "GASP\u7b97\u6cd5\u901a\u8fc7\u4f18\u5316Reeb\u56fe\u7684\u53ef\u89c6\u5316\u5c5e\u6027\uff0c\u663e\u8457\u63d0\u5347\u4e86\u53ef\u89c6\u5316\u7ed3\u679c\u7684\u4ee3\u8868\u6027\u548c\u51c6\u786e\u6027\uff0c\u4e3a\u540e\u7eed\u7814\u7a76\u63d0\u4f9b\u4e86\u65b0\u7684\u5de5\u5177\u548c\u65b9\u6cd5\u3002"}}
{"id": "2508.05531", "categories": ["cs.GR", "cs.CV"], "pdf": "https://arxiv.org/pdf/2508.05531", "abs": "https://arxiv.org/abs/2508.05531", "authors": ["Davide Garavaso", "Federico Masi", "Pietro Musoni", "Umberto Castellani"], "title": "Point cloud segmentation for 3D Clothed Human Layering", "comment": null, "summary": "3D Cloth modeling and simulation is essential for avatars creation in several\nfields, such as fashion, entertainment, and animation. Achieving high-quality\nresults is challenging due to the large variability of clothed body especially\nin the generation of realistic wrinkles. 3D scan acquisitions provide more\naccuracy in the representation of real-world objects but lack semantic\ninformation that can be inferred with a reliable semantic reconstruction\npipeline. To this aim, shape segmentation plays a crucial role in identifying\nthe semantic shape parts. However, current 3D shape segmentation methods are\ndesigned for scene understanding and interpretation and only few work is\ndevoted to modeling. In the context of clothed body modeling the segmentation\nis a preliminary step for fully semantic shape parts reconstruction namely the\nunderlying body and the involved garments. These parts represent several layers\nwith strong overlap in contrast with standard segmentation methods that provide\ndisjoint sets. In this work we propose a new 3D point cloud segmentation\nparadigm where each 3D point can be simultaneously associated to different\nlayers. In this fashion we can estimate the underlying body parts and the\nunseen clothed regions, i.e., the part of a cloth occluded by the clothed-layer\nabove. We name this segmentation paradigm clothed human layering. We create a\nnew synthetic dataset that simulates very realistic 3D scans with the ground\ntruth of the involved clothing layers. We propose and evaluate different neural\nnetwork settings to deal with 3D clothing layering. We considered both coarse\nand fine grained per-layer garment identification. Our experiments demonstrates\nthe benefit in introducing proper strategies for the segmentation on the\ngarment domain on both the synthetic and real-world scan datasets.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u76843D\u70b9\u4e91\u5206\u5272\u8303\u5f0f\uff0c\u7528\u4e8e\u540c\u65f6\u5173\u8054\u4e0d\u540c\u5c42\u6b21\u7684\u670d\u88c5\u5c42\uff0c\u4ee5\u5b9e\u73b0\u8bed\u4e49\u91cd\u5efa\u548c\u9ad8\u8d28\u91cf\u7684\u4e09\u7ef4\u670d\u88c5\u5efa\u6a21\u3002", "motivation": "\u4e09\u7ef4\u670d\u88c5\u5efa\u6a21\u548c\u6a21\u62df\u5728\u65f6\u5c1a\u3001\u5a31\u4e50\u548c\u52a8\u753b\u7b49\u9886\u57df\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u7531\u4e8e\u670d\u88c5\u4e0e\u8eab\u4f53\u7684\u9ad8\u53d8\u5f02\u6027\uff0c\u751f\u6210\u903c\u771f\u7684\u76b1\u7eb9\u7b49\u7ec6\u8282\u5177\u6709\u6311\u6218\u6027\u3002\u73b0\u6709\u76843D\u626b\u63cf\u6570\u636e\u7f3a\u4e4f\u8bed\u4e49\u4fe1\u606f\uff0c\u5f53\u524d\u76843D\u5f62\u72b6\u5206\u5272\u65b9\u6cd5\u4e3b\u8981\u7528\u4e8e\u573a\u666f\u7406\u89e3\uff0c\u800c\u975e\u5efa\u6a21\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u76843D\u70b9\u4e91\u5206\u5272\u8303\u5f0f\uff0c\u79f0\u4e3a\u201cclothed human layering\u201d\uff0c\u5141\u8bb8\u6bcf\u4e2a3D\u70b9\u540c\u65f6\u5173\u8054\u5230\u4e0d\u540c\u5c42\u6b21\u3002\u521b\u5efa\u4e86\u4e00\u4e2a\u5408\u6210\u6570\u636e\u96c6\uff0c\u5e76\u8bc4\u4f30\u4e86\u4e0d\u540c\u795e\u7ecf\u7f51\u7edc\u8bbe\u7f6e\u4ee5\u5b9e\u73b0\u7c97\u7c92\u5ea6\u548c\u7ec6\u7c92\u5ea6\u7684\u670d\u88c5\u5c42\u8bc6\u522b\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u5408\u6210\u548c\u771f\u5b9e\u626b\u63cf\u6570\u636e\u96c6\u4e0a\u7684\u670d\u88c5\u9886\u57df\u5206\u5272\u6548\u679c\u663e\u8457\u3002", "conclusion": "\u63d0\u51fa\u7684\u5206\u5272\u8303\u5f0f\u4e3a\u4e09\u7ef4\u670d\u88c5\u5efa\u6a21\u63d0\u4f9b\u4e86\u66f4\u51c6\u786e\u7684\u8bed\u4e49\u91cd\u5efa\uff0c\u5c24\u5176\u5728\u5904\u7406\u91cd\u53e0\u670d\u88c5\u5c42\u65f6\u8868\u73b0\u4f18\u8d8a\u3002"}}
{"id": "2508.05626", "categories": ["cs.GR", "cs.CV", "I.4"], "pdf": "https://arxiv.org/pdf/2508.05626", "abs": "https://arxiv.org/abs/2508.05626", "authors": ["Chris Careaga", "Ya\u011f\u0131z Aksoy"], "title": "Physically Controllable Relighting of Photographs", "comment": "Proc. SIGGRAPH 2025, 10 pages, 9 figures", "summary": "We present a self-supervised approach to in-the-wild image relighting that\nenables fully controllable, physically based illumination editing. We achieve\nthis by combining the physical accuracy of traditional rendering with the\nphotorealistic appearance made possible by neural rendering. Our pipeline works\nby inferring a colored mesh representation of a given scene using monocular\nestimates of geometry and intrinsic components. This representation allows\nusers to define their desired illumination configuration in 3D. The scene under\nthe new lighting can then be rendered using a path-tracing engine. We send this\napproximate rendering of the scene through a feed-forward neural renderer to\npredict the final photorealistic relighting result. We develop a differentiable\nrendering process to reconstruct in-the-wild scene illumination, enabling\nself-supervised training of our neural renderer on raw image collections. Our\nmethod represents a significant step in bringing the explicit physical control\nover lights available in typical 3D computer graphics tools, such as Blender,\nto in-the-wild relighting.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u81ea\u76d1\u7763\u7684\u56fe\u50cf\u91cd\u5149\u7167\u65b9\u6cd5\uff0c\u7ed3\u5408\u4f20\u7edf\u6e32\u67d3\u7684\u7269\u7406\u7cbe\u786e\u6027\u548c\u795e\u7ecf\u6e32\u67d3\u7684\u903c\u771f\u5916\u89c2\uff0c\u5b9e\u73b0\u5b8c\u5168\u53ef\u63a7\u76843D\u5149\u7167\u7f16\u8f91\u3002", "motivation": "\u4f20\u7edf3D\u8ba1\u7b97\u673a\u56fe\u5f62\u5de5\u5177\uff08\u5982Blender\uff09\u5bf9\u5149\u7167\u7684\u7269\u7406\u63a7\u5236\u5728\u771f\u5b9e\u573a\u666f\u4e2d\u96be\u4ee5\u5b9e\u73b0\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u7ed3\u5408\u7269\u7406\u7cbe\u786e\u6027\u548c\u795e\u7ecf\u6e32\u67d3\u7684\u65b9\u6cd5\uff0c\u4ee5\u5b9e\u73b0\u5bf9\u91ce\u5916\u56fe\u50cf\u7684\u9ad8\u8d28\u91cf\u91cd\u5149\u7167\u3002", "method": "\u901a\u8fc7\u5355\u76ee\u51e0\u4f55\u548c\u5185\u5728\u7ec4\u4ef6\u7684\u4f30\u8ba1\u63a8\u65ad\u573a\u666f\u7684\u5f69\u8272\u7f51\u683c\u8868\u793a\uff0c\u7ed3\u5408\u8def\u5f84\u8ffd\u8e2a\u5f15\u64ce\u548c\u795e\u7ecf\u6e32\u67d3\u5668\uff0c\u5e76\u4f7f\u7528\u81ea\u76d1\u7763\u5b66\u4e60\u5728\u539f\u59cb\u56fe\u50cf\u96c6\u4e0a\u8bad\u7ec3\u795e\u7ecf\u6e32\u67d3\u5668\u3002", "result": "\u8be5\u65b9\u6cd5\u80fd\u591f\u63d0\u4f9b\u7269\u7406\u7cbe\u786e\u4e14\u903c\u771f\u7684\u5149\u7167\u6548\u679c\uff0c\u9002\u7528\u4e8e\u91ce\u5916\u573a\u666f\u7684\u91cd\u5149\u7167\u4efb\u52a1\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5c06\u4f20\u7edf3D\u56fe\u5f62\u5de5\u5177\u7684\u5149\u7167\u63a7\u5236\u80fd\u529b\u6269\u5c55\u5230\u91ce\u5916\u56fe\u50cf\u91cd\u5149\u7167\u9886\u57df\uff0c\u4e3a\u76f8\u5173\u5e94\u7528\u63d0\u4f9b\u4e86\u65b0\u7684\u53ef\u80fd\u6027\u3002"}}
