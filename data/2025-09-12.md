<div id=toc></div>

# Table of Contents

- [cs.GR](#cs.GR) [Total: 2]
- [cs.PL](#cs.PL) [Total: 2]
- [cs.GT](#cs.GT) [Total: 4]


<div id='cs.GR'></div>

# cs.GR [[Back]](#toc)

### [1] [Morphology-Preserving Remeshing Approach to Particulate Microstructures via Harmonic Decomposition](https://arxiv.org/abs/2509.08855)
*Mahmoud Shaqfa*

Main category: cs.GR

TL;DR: 提出了一种基于层次扩散的方法，用于重新参数化曲面以生成均匀的三角网格，显著提高了表面三角化的质量。


<details>
  <summary>Details</summary>
Motivation: 传统谐波分解方法在均匀采样时未考虑基函数雅可比矩阵的局部变化，导致重建或生成后的离散化不均匀，影响数值模拟的精度和时间步长。

Method: 采用非线性扩散方法对分析域的曲线坐标进行重新采样，类似于热问题，以扩大表面上的小三角形为代价缩小大三角形。

Result: 测试结果表明，各向同性和各向异性扩散方案在球体和半球体谐波方法中显著改善了表面三角化的质量。

Conclusion: 所提出的方法在保持表面形态、面积和体积的同时，显著提高了网格质量，适用于大型2D和3D微结构的数值模拟。

Abstract: Harmonic decomposition of surfaces, such as spherical and spheroidal
harmonics, is used to analyze morphology, reconstruct, and generate surface
inclusions of particulate microstructures. However, obtaining high-quality
meshes of engineering microstructures using these approaches remains an open
question. In harmonic approaches, we usually reconstruct surfaces by evaluating
the harmonic bases on equidistantly sampled simplicial complexes of the base
domains (e.g., triangular spheroids and disks). However, this traditional
sampling does not account for local changes in the Jacobian of the basis
functions, resulting in nonuniform discretization after reconstruction or
generation. As it impacts the accuracy and time step, high-quality
discretization of microstructures is crucial for efficient numerical
simulations (e.g., finite element and discrete element methods). To circumvent
this issue, we propose an efficient hierarchical diffusion-based approach for
resampling the surface-i.e., performing a reparameterization-to yield an
equalized mesh triangulation. Analogous to heat problems, we use nonlinear
diffusion to resample the curvilinear coordinates of the analysis domain,
thereby enlarging small triangles at the expense of large triangles on
surfaces. We tested isotropic and anisotropic diffusion schemes on the recent
spheroidal and hemispheroidal harmonics methods. The results show a substantial
improvement in the quality metrics for surface triangulation. Unlike
traditional surface reconstruction and meshing techniques, this approach
preserves surface morphology, along with the areas and volumes of surfaces. We
discuss the results and the associated computational costs for large 2D and 3D
microstructures, such as digital twins of concrete and stone masonry, and their
future applications.

</details>


### [2] [CameraVDP: Perceptual Display Assessment with Uncertainty Estimation via Camera and Visual Difference Prediction](https://arxiv.org/abs/2509.08947)
*Yancheng Cai,Robert Wanat,Rafal Mantiuk*

Main category: cs.GR

TL;DR: 提出了CameraVDP框架，结合相机重建流程和视觉差异预测器，用于电子显示器的感知评估，解决了传统测量方法无法捕捉高频和像素级失真的问题。


<details>
  <summary>Details</summary>
Motivation: 传统显示器测量方法无法有效捕捉空间变化的显示伪影和高频失真，而相机测量又引入光学和光度失真。因此需要一种结合高精度相机测量和视觉差异预测的方法。

Method: CameraVDP框架结合HDR图像堆栈、MTF反转、渐晕校正、几何校正、单应变换和颜色校正，并与视觉差异预测器（VDP）结合，模拟人类视觉系统对不同刺激的感知。

Result: 在缺陷像素检测、色彩边纹感知和显示器非均匀性评估三个应用中验证了CameraVDP的有效性，并提出了不确定性分析框架。

Conclusion: CameraVDP框架成功解决了传统测量方法的局限性，为显示器感知评估提供了一种高精度且感知上可靠的方法。

Abstract: Accurate measurement of images produced by electronic displays is critical
for the evaluation of both traditional and computational displays. Traditional
display measurement methods based on sparse radiometric sampling and fitting a
model are inadequate for capturing spatially varying display artifacts, as they
fail to capture high-frequency and pixel-level distortions. While cameras offer
sufficient spatial resolution, they introduce optical, sampling, and
photometric distortions. Furthermore, the physical measurement must be combined
with a model of a visual system to assess whether the distortions are going to
be visible. To enable perceptual assessment of displays, we propose a
combination of a camera-based reconstruction pipeline with a visual difference
predictor, which account for both the inaccuracy of camera measurements and
visual difference prediction. The reconstruction pipeline combines HDR image
stacking, MTF inversion, vignetting correction, geometric undistortion,
homography transformation, and color correction, enabling cameras to function
as precise display measurement instruments. By incorporating a Visual
Difference Predictor (VDP), our system models the visibility of various stimuli
under different viewing conditions for the human visual system. We validate the
proposed CameraVDP framework through three applications: defective pixel
detection, color fringing awareness, and display non-uniformity evaluation. Our
uncertainty analysis framework enables the estimation of the theoretical upper
bound for defect pixel detection performance and provides confidence intervals
for VDP quality scores.

</details>


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [3] [Towards Verified Compilation of Floating-point Optimization in Scientific Computing Programs](https://arxiv.org/abs/2509.09019)
*Mohit Tekriwal,John Sarracino*

Main category: cs.PL

TL;DR: 本文探讨了科学计算程序中浮点优化的正确性验证，特别是LLVM IR级别的快速数学优化，并利用Verified LLVM框架在Rocq定理证明器中验证了Fused-Multiply-Add（FMA）优化的正确性。


<details>
  <summary>Details</summary>
Motivation: 科学计算程序通过激进的编译器优化实现高性能和资源高效利用，但需要确保这些优化的正确性。本文重点关注浮点优化，尤其是快速数学优化。

Method: 利用Rocq定理证明器中的Verified LLVM框架，验证了基本块中实现算术表达式$a * b + c$的FMA优化的正确性。

Result: 初步验证了FMA优化的正确性，并提出了扩展方法，以支持更多程序功能和浮点优化。

Conclusion: 本研究为验证浮点优化的正确性提供了初步成果，并展示了进一步扩展的潜力。

Abstract: Scientific computing programs often undergo aggressive compiler optimization
to achieve high performance and efficient resource utilization. While
performance is critical, we also need to ensure that these optimizations are
correct. In this paper, we focus on a specific class of optimizations,
floating-point optimizations, notably due to fast math, at the LLVM IR level.
We present a preliminary work, which leverages the Verified LLVM framework in
the Rocq theorem prover, to prove the correctness of Fused-Multiply-Add (FMA)
optimization for a basic block implementing the arithmetic expression $a * b +
c$ . We then propose ways to extend this preliminary results by adding more
program features and fast math floating-point optimizations.

</details>


### [4] [Dependent-Type-Preserving Memory Allocation](https://arxiv.org/abs/2509.09059)
*Paulette Koronkevich,William J. Bowman*

Main category: cs.PL

TL;DR: 本文探讨依赖类型编程语言在编译过程中规范可能被外部程序破坏的问题，并提出一种类型保留编译方法来解决。


<details>
  <summary>Details</summary>
Motivation: 依赖类型语言如Coq、Agda等允许程序员详细规范程序并通过类型检查验证，但这些规范在编译时可能被外部程序（如C程序）破坏，即使使用已验证的编译器也可能引发安全问题。

Method: 提出一种类型保留编译方法，开发支持依赖内存分配的中间语言，并通过类型检查防止与未初始化内存程序链接。

Result: 正在开发一种支持依赖内存分配的中间语言及类型保留编译过程，旨在确保链接时不违反原始程序规范。

Conclusion: 通过类型保留编译和中间语言设计，可以有效防止外部程序破坏依赖类型程序的规范，提升程序安全性。

Abstract: Dependently typed programming languages such as Coq, Agda, Idris, and F*,
allow programmers to write detailed specifications of their programs and prove
their programs meet these specifications. However, these specifications can be
violated during compilation since they are erased after type checking. External
programs linked with the compiled program can violate the specifications of the
original program and change the behavior of the compiled program -- even when
compiled with a verified compiler. For example, since Coq does not allow
explicitly allocating memory, a programmer might link their Coq program with a
C program that can allocate memory. Even if the Coq program is compiled with a
verified compiler, the external C program can still violate the memory-safe
specification of the Coq program by providing an uninitialized pointer to
memory. This error could be ruled out by type checking in a language expressive
enough to indicate whether memory is initialized versus uninitialized. Linking
with a program with an uninitialized pointer could be considered ill-typed, and
our linking process could prevent linking with ill-typed programs. To
facilitate type checking during linking, we can use type-preserving
compilation, which preserves the types through the compilation process. In this
ongoing work, we develop a typed intermediate language that supports dependent
memory allocation, as well as a dependent-type-preserving compiler pass for
memory allocation.

</details>


<div id='cs.GT'></div>

# cs.GT [[Back]](#toc)

### [5] [Toward a Multi-Echelon Cyber Warfare Theory: A Meta-Game-Theoretic Paradigm for Defense and Dominance](https://arxiv.org/abs/2509.08976)
*Ya-Ting Yang,Quanyan Zhu*

Main category: cs.GT

TL;DR: 本文探讨了网络战争中游戏理论的应用，强调其在整合攻防技术、资源分配和风险缓解中的重要性。


<details>
  <summary>Details</summary>
Motivation: 网络战争已成为现代冲突的核心领域，需要综合防御和进攻技术以形成连贯策略。以往研究侧重孤立战术，但整体理解对资源部署和风险缓解至关重要。

Method: 采用游戏理论作为统一框架，结合现代AI技术，模拟攻防交互，进行均衡分析、风险评估和战略规划。

Result: 游戏理论模型能够设计和优化多层次网络战争策略，例如通过RedCyber案例展示其有效性。

Conclusion: 游戏理论为网络战争提供了量化工具和战略指导，未来研究方向包括韧性、跨级别规划和AI的进一步发展。

Abstract: Cyber warfare has become a central element of modern conflict, especially
within multi-domain operations. As both a distinct and critical domain, cyber
warfare requires integrating defensive and offensive technologies into coherent
strategies. While prior research has emphasized isolated tactics or fragmented
technologies, a holistic understanding is essential for effective resource
deployment and risk mitigation. Game theory offers a unifying framework for
this purpose. It not only models attacker-defender interactions but also
provides quantitative tools for equilibrium analysis, risk assessment, and
strategic reasoning. Integrated with modern AI techniques, game-theoretic
models enable the design and optimization of strategies across multiple levels
of cyber warfare, from policy and strategy to operations, tactics, and
technical implementations. These models capture the paradoxical logic of
conflict, where more resources do not always translate into greater advantage,
and where nonlinear dynamics govern outcomes. To illustrate the approach, this
chapter examines RedCyber, a synthetic cyber conflict, demonstrating how
game-theoretic methods capture the interdependencies of cyber operations. The
chapter concludes with directions for future research on resilience,
cros-echelon planning, and the evolving role of AI in cyber warfare.

</details>


### [6] [Persuasion Gains and Losses from Peer Communication](https://arxiv.org/abs/2509.09099)
*Toygar T. Kerman,Anastas P. Tenev,Konstantin Zabarnyi*

Main category: cs.GT

TL;DR: 研究贝叶斯说服模型，探讨发送者如何通过部分信息揭示说服网络中的接收者群体，揭示网络扩展并不总是单调增加发送者的效用。


<details>
  <summary>Details</summary>
Motivation: 探讨在通信网络中，发送者如何利用部分信息说服接收者群体，并研究网络扩展对发送者效用的影响。

Method: 基于贝叶斯说服模型，分析发送者在不同网络结构下的策略和效用变化，重点关注网络扩展的影响。

Result: 研究表明，网络扩展可以严格增加发送者的效用，但并非所有扩展都有效；某些网络结构（如信息层次清晰的网络）能实现效用上限。

Conclusion: 研究发现更多通信不一定带来更好的集体结果，需谨慎评估网络扩展对信息传播的影响。

Abstract: We study a Bayesian persuasion setting in which a sender wants to persuade a
critical mass of receivers by revealing partial information about the state to
them. The homogeneous binary-action receivers are located on a communication
network, and each observes the private messages sent to them and their
immediate neighbors. We examine how the sender's expected utility varies with
increased communication among receivers. We show that for general families of
networks, extending the network can strictly benefit the sender. Thus, the
sender's gain from persuasion is not monotonic in network density. Moreover,
many network extensions can achieve the upper bound on the sender's expected
utility among all networks, which corresponds to the payoff in an empty
network. This is the case in networks reflecting a clear informational
hierarchy (e.g., in global corporations), as well as in decentralized networks
in which information originates from multiple sources (e.g., influencers in
social media). Finally, we show that a slight modification to the structure of
some of these networks precludes the possibility of such beneficial extensions.
Overall, our results caution against presuming that more communication
necessarily leads to better collective outcomes.

</details>


### [7] [Mechanism Design with Outliers and Predictions](https://arxiv.org/abs/2509.09561)
*Argyrios Deligkas,Eduard Eiben,Sophie Klumper,Guido Schäfer,Artem Tsikiridis*

Main category: cs.GT

TL;DR: 该论文首次研究了带有异常值的机制设计问题，设计师可以排除z个代理以减少社会成本。研究发现，尽管直觉上认为排除异常值可以提高效率，但结果可能相反，并且在不同目标下策略证明机制的近似性差异显著。


<details>
  <summary>Details</summary>
Motivation: 研究动机在于探索如何在社会成本计算中排除异常代理（如极端偏好的代理）的影响，以优化机制设计，特别是在设施选址等场景中。

Method: 方法是研究在线上设施选址问题中，设计师可以排除z个最远代理时的社会成本最小化策略。分析了确定性和随机性策略证明机制在功利主义和平均主义目标下的性能。

Result: 结果表明，当z≥n/2时，策略证明机制无法实现有界近似。对于平均主义成本，选择第(z+1)阶统计量是策略证明且2-近似的，且这是最优的。功利主义成本下，机制对异常值的利用效率较低，但预测信息可改善性能。

Conclusion: 结论指出，排除异常值并非总能提升效率，且不同目标下策略证明机制的性能差异显著。预测信息的引入可以优化机制的稳健性和一致性。

Abstract: We initiate the study of mechanism design with outliers, where the designer
can discard $z$ agents from the social cost objective. This setting is
particularly relevant when some agents exhibit extreme or atypical preferences.
As a natural case study, we consider facility location on the line: $n$
strategic agents report their preferred locations, and a mechanism places a
facility to minimize a social cost function. In our setting, the $z$ agents
farthest from the chosen facility are excluded from the social cost. While it
may seem intuitive that discarding outliers improves efficiency, our results
reveal that the opposite can hold.
  We derive tight bounds for deterministic strategyproof mechanisms under the
two most-studied objectives: utilitarian and egalitarian social cost. Our
results offer a comprehensive view of the impact of outliers. We first show
that when $z \ge n/2$, no strategyproof mechanism can achieve a bounded
approximation for either objective. For egalitarian cost, selecting the $(z +
1)$-th order statistic is strategyproof and 2-approximate. In fact, we show
that this is best possible by providing a matching lower bound. Notably, this
lower bound of 2 persists even when the mechanism has access to a prediction of
the optimal location, in stark contrast to the setting without outliers. For
utilitarian cost, we show that strategyproof mechanisms cannot effectively
exploit outliers, leading to the counterintuitive outcome that approximation
guarantees worsen as the number of outliers increases. However, in this case,
access to a prediction allows us to design a strategyproof mechanism achieving
the best possible trade-off between consistency and robustness. Finally, we
also establish lower bounds for randomized mechanisms that are truthful in
expectation.

</details>


### [8] [Maximizing social welfare among EF1 allocations at the presence of two types of agents](https://arxiv.org/abs/2509.09641)
*Jiaxuan Ma,Yong Chen,Guangting Chen,Mingyang Gong,Guohui Lin,An Zhang*

Main category: cs.GT

TL;DR: 研究了在只有两种效用函数的情况下，如何公平分配不可分割物品以最大化社会福利，提出了改进的近似算法。


<details>
  <summary>Details</summary>
Motivation: 研究在有限效用函数条件下，提出更高效的公平分配算法，以改进现有的近似比率。

Method: 采用近似算法，针对两种归一化和非归一化的效用函数，分别设计了不同的分配策略。

Result: 对于两种归一化效用函数，提出了2-近似算法；对于三种代理和归一化效用函数，提出了5/3-近似算法；非归一化情况下，提出了2-近似算法。

Conclusion: 在有限效用函数条件下，提出的算法显著改进现有结果，验证了问题的APX复杂性。

Abstract: We study the fair allocation of indivisible items to $n$ agents to maximize
the utilitarian social welfare, where the fairness criterion is envy-free up to
one item and there are only two different utility functions shared by the
agents. We present a $2$-approximation algorithm when the two utility functions
are normalized, improving the previous best ratio of $16 \sqrt{n}$ shown for
general normalized utility functions; thus this constant ratio approximation
algorithm confirms the APX-completeness in this special case previously shown
APX-hard. When there are only three agents, i.e., $n = 3$, the previous best
ratio is $3$ shown for general utility functions, and we present an improved
and tight $\frac 53$-approximation algorithm when the two utility functions are
normalized, and a best possible and tight $2$-approximation algorithm when the
two utility functions are unnormalized.

</details>
