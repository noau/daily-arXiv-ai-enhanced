{"id": "2509.25387", "categories": ["cs.GR", "cs.HC"], "pdf": "https://arxiv.org/pdf/2509.25387", "abs": "https://arxiv.org/abs/2509.25387", "authors": ["S. Sandra Bae", "Takanori Fujiwara", "Danielle Albers Szafir", "Ellen Yi-Luen Do", "Michael L. Rivera"], "title": "Computational Design and Single-Wire Sensing of 3D Printed Objects with Integrated Capacitive Touchpoints", "comment": "19 pages, 14 figures, to be published in Proceedings of ACM SCF 2025", "summary": "Producing interactive 3D printed objects currently requires laborious 3D\ndesign and post-instrumentation with off-the-shelf electronics. Multi-material\n3D printing using conductive PLA presents opportunities to mitigate these\nchallenges. We present a computational design pipeline that embeds multiple\ncapacitive touchpoints into any 3D model that has a closed mesh without\nself-intersection. With our pipeline, users define touchpoints on the 3D\nobject's surface to indicate interactive regions. Our pipeline then\nautomatically generates a conductive path to connect the touch regions. This\npath is optimized to output unique resistor-capacitor delays when each region\nis touched, resulting in all regions being able to be sensed through a\ndouble-wire or single-wire connection. We illustrate our approach's utility\nwith five computational and sensing performance evaluations (achieving 93.35%\nmean accuracy for single-wire) and six application examples. Our sensing\ntechnique supports existing uses (e.g., prototyping) and highlights the growing\npromise to produce interactive devices entirely with 3D printing.\n  Project website: https://github.com/d-rep-lab/3dp-singlewire-sensing"}
{"id": "2509.25392", "categories": ["cs.GR"], "pdf": "https://arxiv.org/pdf/2509.25392", "abs": "https://arxiv.org/abs/2509.25392", "authors": ["Yutian Tao", "Maurizio Chiaramonte", "Pablo Fernandez"], "title": "Interpolated Adaptive Linear Reduced Order Modeling for Deformation Dynamics", "comment": null, "summary": "Linear reduced-order modeling (ROM) is widely used for efficient simulation\nof deformation dynamics, but its accuracy is often limited by the fixed\nlinearization of the reduced mapping. We propose a new adaptive strategy for\nlinear ROM that allows the reduced mapping to vary dynamically in response to\nthe evolving deformation state, significantly improving accuracy over\ntraditional linear approaches. To further handle large deformations, we\nintroduce a historical displacement basis combined with Grassmann\ninterpolation, enabling the system to recover robustly even in challenging\nscenarios. We evaluate our method through quantitative online-error analysis\nand qualitative comparisons with principal component analysis (PCA)-based\nlinear ROM simulations, demonstrating substantial accuracy gains while\npreserving comparable computational costs."}
{"id": "2509.25600", "categories": ["cs.GR", "cs.RO"], "pdf": "https://arxiv.org/pdf/2509.25600", "abs": "https://arxiv.org/abs/2509.25600", "authors": ["Wontaek Kim", "Tianyu Li", "Sehoon Ha"], "title": "MoReFlow: Motion Retargeting Learning through Unsupervised Flow Matching", "comment": null, "summary": "Motion retargeting holds a premise of offering a larger set of motion data\nfor characters and robots with different morphologies. Many prior works have\napproached this problem via either handcrafted constraints or paired motion\ndatasets, limiting their applicability to humanoid characters or narrow\nbehaviors such as locomotion. Moreover, they often assume a fixed notion of\nretargeting, overlooking domain-specific objectives like style preservation in\nanimation or task-space alignment in robotics. In this work, we propose\nMoReFlow, Motion Retargeting via Flow Matching, an unsupervised framework that\nlearns correspondences between characters' motion embedding spaces. Our method\nconsists of two stages. First, we train tokenized motion embeddings for each\ncharacter using a VQ-VAE, yielding compact latent representations. Then, we\nemploy flow matching with conditional coupling to align the latent spaces\nacross characters, which simultaneously learns conditioned and unconditioned\nmatching to achieve robust but flexible retargeting. Once trained, MoReFlow\nenables flexible and reversible retargeting without requiring paired data.\nExperiments demonstrate that MoReFlow produces high-quality motions across\ndiverse characters and tasks, offering improved controllability,\ngeneralization, and motion realism compared to the baselines."}
{"id": "2509.25857", "categories": ["cs.GR", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2509.25857", "abs": "https://arxiv.org/abs/2509.25857", "authors": ["Xinding Zhu", "Xinye Yang", "Shuyang Zheng", "Zhexin Zhang", "Fei Gao", "Jing Huang", "Jiazhou Chen"], "title": "Vector sketch animation generation with differentialable motion trajectories", "comment": "14 pages, 12 figures", "summary": "Sketching is a direct and inexpensive means of visual expression. Though\nimage-based sketching has been well studied, video-based sketch animation\ngeneration is still very challenging due to the temporal coherence requirement.\nIn this paper, we propose a novel end-to-end automatic generation approach for\nvector sketch animation. To solve the flickering issue, we introduce a\nDifferentiable Motion Trajectory (DMT) representation that describes the\nframe-wise movement of stroke control points using differentiable\npolynomial-based trajectories. DMT enables global semantic gradient propagation\nacross multiple frames, significantly improving the semantic consistency and\ntemporal coherence, and producing high-framerate output. DMT employs a\nBernstein basis to balance the sensitivity of polynomial parameters, thus\nachieving more stable optimization. Instead of implicit fields, we introduce\nsparse track points for explicit spatial modeling, which improves efficiency\nand supports long-duration video processing. Evaluations on DAVIS and LVOS\ndatasets demonstrate the superiority of our approach over SOTA methods.\nCross-domain validation on 3D models and text-to-video data confirms the\nrobustness and compatibility of our approach."}
{"id": "2509.25565", "categories": ["cs.GT"], "pdf": "https://arxiv.org/pdf/2509.25565", "abs": "https://arxiv.org/abs/2509.25565", "authors": ["Paul Duetting", "Safwan Hossain", "Tao Lin", "Renato Paes Leme", "Sai Srivatsa Ravindranath", "Haifeng Xu", "Song Zuo"], "title": "Information Design With Large Language Models", "comment": null, "summary": "Information design is typically studied through the lens of Bayesian\nsignaling, where signals shape beliefs based on their correlation with the true\nstate of the world. However, Behavioral Economics and Psychology emphasize that\nhuman decision-making is more complex and can depend on how information is\nframed. This paper formalizes a language-based notion of framing and bridges\nthis to the popular Bayesian-persuasion model. We model framing as a possibly\nnon-Bayesian, linguistic way to influence a receiver's belief, while a\nsignaling (or recommendation) scheme can further refine this belief in the\nclassic Bayesian way. A key challenge in systematically optimizing in this\nframework is the vast space of possible framings and the difficulty of\npredicting their effects on receivers. Based on growing evidence that Large\nLanguage Models (LLMs) can effectively serve as proxies for human behavior, we\nformulate a theoretical model based on access to a framing-to-belief oracle.\nThis model then enables us to precisely characterize when solely optimizing\nframing or jointly optimizing framing and signaling is tractable. We\nsubstantiate our theoretical analysis with an empirical algorithm that\nleverages LLMs to (1) approximate the framing-to-belief oracle, and (2)\noptimize over language space using a hill-climbing method. We apply this to two\nmarketing-inspired case studies and validate the effectiveness through\nanalytical and human evaluation."}
{"id": "2509.26055", "categories": ["cs.GR", "cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.26055", "abs": "https://arxiv.org/abs/2509.26055", "authors": ["Zhenyu Shu", "Junlong Yu", "Kai Chao", "Shiqing Xin", "Ligang Liu"], "title": "GaussEdit: Adaptive 3D Scene Editing with Text and Image Prompts", "comment": null, "summary": "This paper presents GaussEdit, a framework for adaptive 3D scene editing\nguided by text and image prompts. GaussEdit leverages 3D Gaussian Splatting as\nits backbone for scene representation, enabling convenient Region of Interest\nselection and efficient editing through a three-stage process. The first stage\ninvolves initializing the 3D Gaussians to ensure high-quality edits. The second\nstage employs an Adaptive Global-Local Optimization strategy to balance global\nscene coherence and detailed local edits and a category-guided regularization\ntechnique to alleviate the Janus problem. The final stage enhances the texture\nof the edited objects using a sophisticated image-to-image synthesis technique,\nensuring that the results are visually realistic and align closely with the\ngiven prompts. Our experimental results demonstrate that GaussEdit surpasses\nexisting methods in editing accuracy, visual fidelity, and processing speed. By\nsuccessfully embedding user-specified concepts into 3D scenes, GaussEdit is a\npowerful tool for detailed and user-driven 3D scene editing, offering\nsignificant improvements over traditional methods."}
{"id": "2509.25618", "categories": ["cs.GT", "cs.AI", "cs.MA"], "pdf": "https://arxiv.org/pdf/2509.25618", "abs": "https://arxiv.org/abs/2509.25618", "authors": ["Sam Ganzfried"], "title": "Quadratic Programming Approach for Nash Equilibrium Computation in Multiplayer Imperfect-Information Games", "comment": null, "summary": "There has been significant recent progress in algorithms for approximation of\nNash equilibrium in large two-player zero-sum imperfect-information games and\nexact computation of Nash equilibrium in multiplayer strategic-form games.\nWhile counterfactual regret minimization and fictitious play are scalable to\nlarge games and have convergence guarantees in two-player zero-sum games, they\ndo not guarantee convergence to Nash equilibrium in multiplayer games. We\npresent an approach for exact computation of Nash equilibrium in multiplayer\nimperfect-information games that solves a quadratically-constrained program\nbased on a nonlinear complementarity problem formulation from the sequence-form\ngame representation. This approach capitalizes on recent advances for solving\nnonconvex quadratic programs. Our algorithm is able to quickly solve\nthree-player Kuhn poker after removal of dominated actions. Of the available\nalgorithms in the Gambit software suite, only the logit quantal response\napproach is successfully able to solve the game; however, the approach takes\nlonger than our algorithm and also involves a degree of approximation. Our\nformulation also leads to a new approach for computing Nash equilibrium in\nmultiplayer strategic-form games which we demonstrate to outperform a previous\nquadratically-constrained program formulation."}
{"id": "2509.26213", "categories": ["cs.GR"], "pdf": "https://arxiv.org/pdf/2509.26213", "abs": "https://arxiv.org/abs/2509.26213", "authors": ["Dominik Drees", "Benjamin Risse"], "title": "Palace: A Library for Interactive GPU-Accelerated Large Tensor Processing and Visualization", "comment": null, "summary": "Tensor datasets (two-, three-, or higher-dimensional) are fundamental to many\nscientific fields utilizing imaging or simulation technologies. Advances in\nthese methods have led to ever-increasing data sizes and, consequently,\ninterest and development of out-of-core processing and visualization\ntechniques, although mostly as specialized solutions. Here we present Palace,\nan open-source, cross-platform, general-purpose library for interactive and\naccelerated out-of-core tensor processing and visualization. Through a\nhigh-performance asynchronous concurrent architecture and a simple\ncompute-graph interface, Palace enables the interactive development of\nout-of-core pipelines on workstation hardware. We demonstrate on benchmarks\nthat Palace outperforms or matches state-of-the-art systems for volume\nrendering and hierarchical random-walker segmentation and demonstrate\napplicability in use cases involving tensors from 2D images up to 4D time\nseries datasets."}
{"id": "2509.25921", "categories": ["cs.GT"], "pdf": "https://arxiv.org/pdf/2509.25921", "abs": "https://arxiv.org/abs/2509.25921", "authors": ["Seref Taha Kiremitci", "Ahmed Said Donmez", "Muhammed O. Sayin"], "title": "Achieving Pareto Optimality in Games via Single-bit Feedback", "comment": null, "summary": "Efficient coordination in multi-agent systems often incurs high communication\noverhead or slow convergence rates, making scalable welfare optimization\ndifficult. We propose Single-Bit Coordination Dynamics for Pareto-Efficient\nOutcomes (SBC-PE), a decentralized learning algorithm requiring only a\nsingle-bit satisfaction signal per agent each round. Despite this extreme\nefficiency, SBC-PE guarantees convergence to the exact optimal solution in\narbitrary finite games. We establish explicit regret bounds, showing expected\nregret grows only logarithmically with the horizon, i.e., O(log T). Compared\nwith prior payoff-based or bandit-style rules, SBC-PE uniquely combines minimal\nsignaling, general applicability, and finite-time guarantees. These results\nshow scalable welfare optimization is achievable under minimal communication\nconstraints."}
{"id": "2509.26233", "categories": ["cs.GR", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2509.26233", "abs": "https://arxiv.org/abs/2509.26233", "authors": ["Balamurugan Thambiraja", "Malte Prinzler", "Sadegh Aliakbarian", "Darren Cosker", "Justus Thies"], "title": "3DiFACE: Synthesizing and Editing Holistic 3D Facial Animation", "comment": null, "summary": "Creating personalized 3D animations with precise control and realistic head\nmotions remains challenging for current speech-driven 3D facial animation\nmethods. Editing these animations is especially complex and time consuming,\nrequires precise control and typically handled by highly skilled animators.\nMost existing works focus on controlling style or emotion of the synthesized\nanimation and cannot edit/regenerate parts of an input animation. They also\noverlook the fact that multiple plausible lip and head movements can match the\nsame audio input. To address these challenges, we present 3DiFACE, a novel\nmethod for holistic speech-driven 3D facial animation. Our approach produces\ndiverse plausible lip and head motions for a single audio input and allows for\nediting via keyframing and interpolation. Specifically, we propose a\nfully-convolutional diffusion model that can leverage the viseme-level\ndiversity in our training corpus. Additionally, we employ a speaking-style\npersonalization and a novel sparsely-guided motion diffusion to enable precise\ncontrol and editing. Through quantitative and qualitative evaluations, we\ndemonstrate that our method is capable of generating and editing diverse\nholistic 3D facial animations given a single audio input, with control between\nhigh fidelity and diversity. Code and models are available here:\nhttps://balamuruganthambiraja.github.io/3DiFACE"}
