<div id=toc></div>

# Table of Contents

- [cs.GR](#cs.GR) [Total: 5]
- [cs.PL](#cs.PL) [Total: 7]
- [cs.GT](#cs.GT) [Total: 3]


<div id='cs.GR'></div>

# cs.GR [[Back]](#toc)

### [1] [PaMO: Parallel Mesh Optimization for Intersection-Free Low-Poly Modeling on the GPU](https://arxiv.org/abs/2509.05595)
*Seonghun Oh,Xiaodi Yuan,Xinyue Wei,Ruoxi Shi,Fanbo Xiang,Minghua Liu,Hao Su*

Main category: cs.GR

TL;DR: 提出了一种基于GPU的新型网格优化方法，包括并行重新网格化、稳健的并行简化和安全投影算法，显著提高了大规模网格处理的效率和几何特征保留。


<details>
  <summary>Details</summary>
Motivation: 现有网格简化方法存在自相交、重新网格化导致表面偏移和尖锐特征丢失、以及处理大规模网格耗时的问题。为了解决这些问题，研究者提出了新的GPU优化方法。

Method: 方法包括三部分：(1) 并行重新网格化算法，生成无自相交、流形且紧密的网格；(2) 稳健的并行简化算法，确保简化过程中无自相交；(3) 基于优化的安全投影算法，恢复原始尖锐特征并消除表面偏移。

Result: 该方法在RTX4090上仅用3秒即可将2百万面的网格简化到2万面，并在Thingi10K数据集上表现出卓越的几何特征保存和速度优势。

Conclusion: 该GPU优化方法有效解决了现有网格简化技术的局限性，显著提升了处理效率和几何特征保留能力，适用于大规模网格处理需求。

Abstract: Reducing the triangle count in complex 3D models is a basic geometry
preprocessing step in graphics pipelines such as efficient rendering and
interactive editing. However, most existing mesh simplification methods exhibit
a few issues. Firstly, they often lead to self-intersections during decimation,
a major issue for applications such as 3D printing and soft-body simulation.
Second, to perform simplification on a mesh in the wild, one would first need
to perform re-meshing, which often suffers from surface shifts and losses of
sharp features. Finally, existing re-meshing and simplification methods can
take minutes when processing large-scale meshes, limiting their applications in
practice. To address the challenges, we introduce a novel GPU-based mesh
optimization approach containing three key components: (1) a parallel
re-meshing algorithm to turn meshes in the wild into watertight, manifold, and
intersection-free ones, and reduce the prevalence of poorly shaped triangles;
(2) a robust parallel simplification algorithm with intersection-free
guarantees; (3) an optimization-based safe projection algorithm to realign the
simplified mesh with the input, eliminating the surface shift introduced by
re-meshing and recovering the original sharp features. The algorithm
demonstrates remarkable efficiency, simplifying a 2-million-face mesh to 20k
triangles in 3 seconds on RTX4090. We evaluated the approach on the Thingi10K
dataset and showcased its exceptional performance in geometry preservation and
speed.

</details>


### [2] [Programming tension in 3D printed networks inspired by spiderwebs](https://arxiv.org/abs/2509.05855)
*Thijs Masmeijer,Caleb Swain,Jeff Hill,Ed Habtour*

Main category: cs.GR

TL;DR: 该研究提出了一种直接3D打印具有预设张力梯度的结构网络的算法，类似于蜘蛛网的形成方式，解决了传统制造中的张力梯度不准确问题。


<details>
  <summary>Details</summary>
Motivation: 研究目标是解决传统制造方法中因扁平化网络而导致的张力梯度不准确性，从而实现对复杂结构网络的精确制造。

Method: 算法包括：（i）定义目标网络并用力密度方法设定张力梯度；（ii）通过数值优化顶点位置将网络转换为未拉伸版本；（iii）分解网络为可打印路径；（iv）可选步骤包括扁平化2D或3D网络；（v）自动解决扁平化过程中产生的交叉问题。

Result: 实验验证显示，该方法对粘弹性细丝的2D单元实现了平均应变误差小于1.0%的精确张力梯度。适用于最小长度5.8毫米和最大应力7.3兆帕的网络。

Conclusion: 该方法成功应用于制造复杂网络结构，如平面蜘蛛网、曲面网格和张拉整体系统，为新型应用（如医疗支撑结构）提供了可能性。

Abstract: Each element in tensioned structural networks -- such as tensegrity,
architectural fabrics, or medical braces/meshes -- requires a specific tension
level to achieve and maintain the desired shape, stability, and compliance.
These structures are challenging to manufacture, 3D print, or assemble because
flattening the network during fabrication introduces multiplicative
inaccuracies in the network's final tension gradients. This study overcomes
this challenge by offering a fabrication algorithm for direct 3D printing of
such networks with programmed tension gradients, an approach analogous to the
spinning of spiderwebs. The algorithm: (i) defines the desired network and
prescribes its tension gradients using the force density method; (ii) converts
the network into an unstretched counterpart by numerically optimizing vertex
locations toward target element lengths and converting straight elements into
arcs to resolve any remaining error; and (iii) decomposes the network into
printable toolpaths; Optional additional steps are: (iv) flattening curved 2D
networks or 3D networks to ensure 3D printing compatibility; and (v)
automatically resolving any unwanted crossings introduced by the flattening
process. The proposed method is experimentally validated using 2D unit cells of
viscoelastic filaments, where accurate tension gradients are achieved with an
average element strain error of less than 1.0\%. The method remains effective
for networks with element minimum length and maximum stress of 5.8 mm and 7.3
MPa, respectively. The method is used to demonstrate the fabrication of three
complex cases: a flat spiderweb, a curved mesh, and a tensegrity system. The
programmable tension gradient algorithm can be utilized to produce compact,
integrated cable networks, enabling novel applications such as moment-exerting
structures in medical braces and splints.

</details>


### [3] [From Rigging to Waving: 3D-Guided Diffusion for Natural Animation of Hand-Drawn Characters](https://arxiv.org/abs/2509.06573)
*Jie Zhou,Linzi Qu,Miu-Ling Lam,Hongbo Fu*

Main category: cs.GR

TL;DR: 提出了一种结合骨骼动画和视频扩散的混合动画系统，通过几何引导和纹理增强改进手绘角色动画，特别是在复杂非刚性元素（如飘动的头发和裙子）的处理上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 手绘角色动画在几何一致性和动态表达之间存在挑战，传统骨骼动画在复杂非刚性元素上表现不佳，而视频扩散模型在风格化绘画中容易产生几何失真。本文旨在结合两者的优点。

Method: 系统首先生成基于骨骼动画的几何引导图像，再通过视频扩散模型进行纹理和次级动态增强，采用域适应扩散模型和次级动态注入策略（SDI），并结合头发分层建模（HLM）技术处理长头发角色的动画。

Result: 实验表明，该系统在定量和定性评估中均优于现有方法，特别是在复杂非刚性元素的动画表现和自然变形方面。

Conclusion: 提出的混合系统有效地结合了骨骼动画和视频扩散的优点，显著提升了手绘角色动画的表现力和真实性。

Abstract: Hand-drawn character animation is a vibrant field in computer graphics,
presenting challenges in achieving geometric consistency while conveying
expressive motion. Traditional skeletal animation methods maintain geometric
consistency but struggle with complex non-rigid elements like flowing hair and
skirts, leading to unnatural deformation. Conversely, video diffusion models
synthesize realistic dynamics but often create geometric distortions in
stylized drawings due to domain gaps. This work proposes a hybrid animation
system that combines skeletal animation and video diffusion. Initially, coarse
images are generated from characters retargeted with skeletal animations for
geometric guidance. These images are then enhanced in texture and secondary
dynamics using video diffusion priors, framing this enhancement as an
inpainting task. A domain-adapted diffusion model refines user-masked regions
needing improvement, especially for secondary dynamics. To enhance motion
realism further, we introduce a Secondary Dynamics Injection (SDI) strategy in
the denoising process, incorporating features from a pre-trained diffusion
model enriched with human motion priors. Additionally, to tackle unnatural
deformations from low-poly single-mesh character modeling, we present a Hair
Layering Modeling (HLM) technique that uses segmentation maps to separate hair
from the body, allowing for more natural animation of long-haired characters.
Extensive experiments show that our system outperforms state-of-the-art methods
in both quantitative and qualitative evaluations.

</details>


### [4] [From Skin to Skeleton: Towards Biomechanically Accurate 3D Digital Humans](https://arxiv.org/abs/2509.06607)
*Marilyn Keller,Keenon Werling,Soyong Shin,Scott Delp,Sergi Pujades,C. Karen Liu,Michael J. Black*

Main category: cs.GR

TL;DR: SKEL是一个基于SMPL的改进模型，通过引入生物力学准确的骨骼结构，解决了现有人体模型与真实骨骼系统不匹配的问题。


<details>
  <summary>Details</summary>
Motivation: 现有的人体模型（如SMPL）的简化运动结构不匹配真实的人类骨骼系统，限制了在生物力学中的应用。需要一种既准确又易于操作的参数化3D人体模型。

Method: 通过优化AMASS序列中的SMPL网格内的骨骼数据，训练从SMPL网格顶点到优化关节位置和骨骼旋转的回归器，并重新参数化SMPL网格。

Result: SKEL模型相比SMPL具有更准确的关节位置和更少的自由度，能够更好地拟合身体表面。

Conclusion: SKEL为“野外生物力学”提供了新工具，并为视觉和图形研究提供了更真实的人体关节模型。模型、代码和数据已公开。

Abstract: Great progress has been made in estimating 3D human pose and shape from
images and video by training neural networks to directly regress the parameters
of parametric human models like SMPL. However, existing body models have
simplified kinematic structures that do not correspond to the true joint
locations and articulations in the human skeletal system, limiting their
potential use in biomechanics. On the other hand, methods for estimating
biomechanically accurate skeletal motion typically rely on complex motion
capture systems and expensive optimization methods. What is needed is a
parametric 3D human model with a biomechanically accurate skeletal structure
that can be easily posed. To that end, we develop SKEL, which re-rigs the SMPL
body model with a biomechanics skeleton. To enable this, we need training data
of skeletons inside SMPL meshes in diverse poses.
  We build such a dataset by optimizing biomechanically accurate skeletons
inside SMPL meshes from AMASS sequences. We then learn a regressor from SMPL
mesh vertices to the optimized joint locations and bone rotations. Finally, we
re-parametrize the SMPL mesh with the new kinematic parameters. The resulting
SKEL model is animatable like SMPL but with fewer, and
biomechanically-realistic, degrees of freedom. We show that SKEL has more
biomechanically accurate joint locations than SMPL, and the bones fit inside
the body surface better than previous methods. By fitting SKEL to SMPL meshes
we are able to "upgrade" existing human pose and shape datasets to include
biomechanical parameters. SKEL provides a new tool to enable biomechanics in
the wild, while also providing vision and graphics researchers with a better
constrained and more realistic model of human articulation. The model, code,
and data are available for research at https://skel.is.tue.mpg.de..

</details>


### [5] [Scaling Transformer-Based Novel View Synthesis Models with Token Disentanglement and Synthetic Data](https://arxiv.org/abs/2509.06950)
*Nithin Gopalakrishnan Nair,Srinivas Kaza,Xuan Luo,Vishal M. Patel,Stephen Lombardi,Jungyeon Park*

Main category: cs.GR

TL;DR: 该论文提出了一种基于变压器架构的令牌解缠方法，通过引入合成数据训练，提升了新颖视角合成的泛化能力和重建质量，同时在多个基准测试中达到了最优性能。


<details>
  <summary>Details</summary>
Motivation: 现有的基于变压器的通用新颖视角合成模型受限于公开场景数据集的多样性不足，导致对真实场景的泛化能力有限。为了解决这一问题，论文提出利用扩散模型生成的合成数据来提升模型的泛化能力。

Method: 论文提出了一种变压器架构中的令牌解缠过程，通过增强特征分离来提高学习效率。同时利用扩散模型生成的合成数据进行训练，以弥补公开数据集的局限性。

Result: 该方法在数据集内和跨数据集评估中均优于现有模型，实现了多个基准测试的最优性能，并显著降低了计算成本。

Conclusion: 通过结合合成数据训练和令牌解缠技术，该论文提出的方法显著提升了新颖视角合成的泛化能力和重建质量，为未来的研究提供了可扩展的解决方案。

Abstract: Large transformer-based models have made significant progress in
generalizable novel view synthesis (NVS) from sparse input views, generating
novel viewpoints without the need for test-time optimization. However, these
models are constrained by the limited diversity of publicly available scene
datasets, making most real-world (in-the-wild) scenes out-of-distribution. To
overcome this, we incorporate synthetic training data generated from diffusion
models, which improves generalization across unseen domains. While synthetic
data offers scalability, we identify artifacts introduced during data
generation as a key bottleneck affecting reconstruction quality. To address
this, we propose a token disentanglement process within the transformer
architecture, enhancing feature separation and ensuring more effective
learning. This refinement not only improves reconstruction quality over
standard transformers but also enables scalable training with synthetic data.
As a result, our method outperforms existing models on both in-dataset and
cross-dataset evaluations, achieving state-of-the-art results across multiple
benchmarks while significantly reducing computational costs. Project page:
https://scaling3dnvs.github.io/

</details>


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [6] [Comparing Methods for the Cross-Level Verification of SystemC Peripherals with Symbolic Execution](https://arxiv.org/abs/2509.05504)
*Karl Aaron Rudkowski,Sallar Ahmadi-Pour,Rolf Drechsler*

Main category: cs.PL

TL;DR: 论文提出了CrosSym和SEFOS两种方法，用于外围设备的符号执行验证，分别通过修改SystemC内核和符号执行引擎实现跨级别验证。


<details>
  <summary>Details</summary>
Motivation: 现代硬件设计中，虚拟原型（VPs）是关键工具，但其验证子系统时存在局限，现有方法要么需要修改SystemC内核，要么忽略跨级别场景的需求。

Method: 论文提出CrosSym和SEFOS两种方法，CrosSym修改SystemC内核，SEFOS修改符号执行引擎。

Result: 通过对不同抽象级别的外围设备进行广泛评估，SEFOS保持了未修改的SystemC内核和外围设备，而CrosSym在运行时和内存使用上稍优。

Conclusion: 与仅限于事务级别建模（TLM）的现有方法相比，CrosSym和SEFOS实现了符号执行的跨级别验证，同时保持可比较的运行时性能。

Abstract: Virtual Prototypes (VPs) are important tools in modern hardware development.
At high abstractions, they are often implemented in SystemC and offer early
analysis of increasingly complex designs. These complex designs often combine
one or more processors, interconnects, and peripherals to perform tasks in
hardware or interact with the environment. Verifying these subsystems is a
well-suited task for VPs, as they allow reasoning across different abstraction
levels. While modern verification techniques like symbolic execution can be
seamlessly integrated into VP-based workflows, they require modifications in
the SystemC kernel. Hence, existing approaches therefore modify and replace the
SystemC kernel, or ignore the opportunity of cross-level scenarios completely,
and would not allow focusing on special challenges of particular subsystems
like peripherals. We propose CrosSym and SEFOS, two opposing approaches for a
versatile symbolic execution of peripherals. CrosSym modifies the SystemC
kernel, while SEFOS instead modifies a modern symbolic execution engine. Our
extensive evaluation applies our tools to various peripherals on different
levels of abstractions. Both tools extensive sets of features are demonstrated
for (1) different verification scenarios, and (2) identifying 300+ mutants. In
comparison with each other, SEFOS convinces with the unmodified SystemC kernel
and peripheral, while CrosSym offers slightly better runtime and memory usage.
In comparison to the state-of-the-art, that is limited to Transaction Level
Modelling (TLM), our tools offered comparable runtime, while enabling
cross-level verification with symbolic execution.

</details>


### [7] [Fixed Parameter Tractable Linearizability Monitoring for Stack, Queue and Anagram Agnostic Data Types](https://arxiv.org/abs/2509.05586)
*Lee Zheng Han,Umang Mathur*

Main category: cs.PL

TL;DR: 本文提出了几种固定参数易处理的算法，用于在有限并发条件下监控栈、队列和异序无关数据类型（AADTs）的线性一致性验证。


<details>
  <summary>Details</summary>
Motivation: 并发数据结构的线性一致性验证即使在简单类型下也是NP难问题，因此需要高效的方法来解决这一问题。

Method: 利用前沿图和分区状态来限制搜索空间。对于AADTs，线性化的等价性使得监控可以在对数线性时间内完成；对于栈，引入了基于语法的方法并降为矩阵乘法；对于队列，采用了支持高效动态规划的分割序列转移系统。

Result: 研究表明，在有限并发条件下，这些方法能够统一处理顺序敏感和异序无关的数据类型，并提供时间复杂度保证。

Conclusion: 本文提出的方法为并发数据结构的线性一致性验证提供了高效的统一框架，并在理论上证明了其可行性。

Abstract: Verifying linearizability of concurrent data structures is NP-hard, even for
simple types. We present fixed-parameter tractable algorithms for monitoring
stacks, queues, and anagram-agnostic data types (AADTs), parameterized by the
maximum concurrency. Our approach leverages frontier graphs and partition
states to bound the search space. For AADTs, equivalence of linearizations
enables monitoring in log-linear time. For stacks, we introduce a grammar-based
method with a sub-cubic reduction to matrix multiplication, and for queues, a
split-sequence transition system supporting efficient dynamic programming.
These results unify tractability guarantees for both order-sensitive and
anagram-agnostic data types under bounded concurrency.

</details>


### [8] [Pacing Types: Safe Monitoring of Asynchronous Streams](https://arxiv.org/abs/2509.06724)
*Florian Kohn,Arthur Correnson,Jan Baumeister,Bernd Finkbeiner*

Main category: cs.PL

TL;DR: 本文提出了一种名为“pacing types”的新型类型系统，用于确保异步数据流的监视器在运行时行为良好。


<details>
  <summary>Details</summary>
Motivation: 由于监视器是安全关键组件，必须确保其在运行时没有潜在错误。异步数据流的不同步性是设计可靠监视器的主要挑战之一。

Method: 通过在RTLola中实现“pacing types”类型系统，并结合新的逻辑关系进行形式化验证，确保监视器的正确性。

Result: 文中形式化了RTLola核心片段的“pacing types”类型系统的本质，并提供了其正确性的声明确认。

Conclusion: “pacing types”类型系统有效解决了异步数据流监视器设计中的运行时错误问题，提升了系统的可靠性。

Abstract: Stream-based monitoring is a real-time safety assurance mechanism for complex
cyber-physical systems such as unmanned aerial vehicles. In this context, a
monitor aggregates streams of input data from sensors and other sources to give
real-time statistics and assessments of the system's health. Since monitors are
safety-critical components, it is crucial to ensure that they are free of
potential runtime errors. One of the central challenges in designing reliable
stream-based monitors is to deal with the asynchronous nature of data streams:
in concrete applications, the different sensors being monitored produce values
at different speeds, and it is the monitor's responsibility to correctly react
to the asynchronous arrival of different streams of values. To ease this
process, modern frameworks for stream-based monitoring such as RTLola feature
an expressive specification language that allows to finely specify data
synchronization policies. While this feature dramatically simplifies the design
of monitors, it can also lead to subtle runtime errors. To mitigate this issue,
this paper presents pacing types, a novel type system implemented in RTLola to
ensure that monitors for asynchronous streams are well-behaved at runtime. We
formalize the essence of pacing types for a core fragment of RTLola, and
present a soundness proof of the pacing type system using a new logical
relation.

</details>


### [9] [Termination Analysis of Linear-Constraint Programs](https://arxiv.org/abs/2509.06752)
*Amir M. Ben-Amram,Samir Genaim,Joël Ouaknine,James Worrell*

Main category: cs.PL

TL;DR: 这篇综述探讨了针对具有数值变量和线性约束转换的程序的终止分析技术，重点关注可解性、排序函数和离散良基转换不变量等方法，并讨论了非终止证据的验证。


<details>
  <summary>Details</summary>
Motivation: 由于程序终止分析中存在不可判定性问题，该综述旨在系统性地探索减轻这一固有难度的技术方法。

Method: 综述总结了基础的可解性结果、排序函数的使用、离散良基转换不变量，以及非终止证据的验证方法。

Result: 综述展示了不同方法在表达能力和计算复杂度之间的权衡，并分析了其算法和复杂性特征。

Conclusion: 该综述未涉及对现实世界编程语言的终止分析，也未考虑包含非线性算术、概率选择或项重写系统的抽象模型。

Abstract: This Survey provides an overview of techniques in termination analysis for
programs with numerical variables and transitions defined by linear
constraints. This subarea of program analysis is challenging due to the
existence of undecidable problems, and this Survey systematically explores
approaches that mitigate this inherent difficulty. These include foundational
decidability results, the use of ranking functions, and disjunctive
well-founded transition invariants. The Survey also discusses non-termination
witnesses, used to prove that a program will not halt. We examine the
algorithmic and complexity aspects of these methods, showing how different
approaches offer a trade-off between expressive power and computational
complexity. The Survey does not discuss how termination analysis is performed
on real-world programming languages, nor does it consider more expressive
abstract models that include non-linear arithmetic, probabilistic choice, or
term rewriting systems.

</details>


### [10] [Dato: A Task-Based Programming Model for Dataflow Accelerators](https://arxiv.org/abs/2509.06794)
*Shihan Fang,Hongzheng Chen,Niansong Zhang,Jiajie Li,Han Meng,Adrian Liu,Zhiru Zhang*

Main category: cs.PL

TL;DR: Dato是一种针对数据流加速器的任务编程模型，通过将数据通信和分片作为一等类型构造，显著提升性能并降低开发负担。


<details>
  <summary>Details</summary>
Motivation: 现代数据流加速器在处理深度学习工作负载时存在内存系统瓶颈，现有编程模型难以有效利用其能力，Dato旨在解决这一问题。

Method: Dato采用Python嵌入式任务编程模型，开发者通过显式流类型和布局类型编写任务图，并由编译器生成物理映射。

Result: 在AMD Ryzen AI NPU和Alveo FPGA上，Dato实现了高性能，GEMM硬件利用率达84%，注意力核速度提升2.81倍，FPGA性能接近理论峰值98%。

Conclusion: Dato证明了其在数据流加速器中的高效性和易用性，为优化代码开发提供了可行的解决方案。

Abstract: Recent deep learning workloads increasingly push computational demand beyond
what current memory systems can sustain, with many kernels stalling on data
movement rather than computation. While modern dataflow accelerators
incorporate on-chip streaming to mitigate off-chip bandwidth limitations,
existing programming models struggle to harness these capabilities effectively.
Low-level interfaces provide fine-grained control but impose significant
development overhead, whereas high-level tile-based languages abstract away
communication details, restricting optimization and forcing compilers to
reconstruct the intended dataflow. We present Dato, a Python-embedded,
task-based programming model for dataflow accelerators that elevates data
communication and sharding to first-class type constructs. Developers write
programs as a graph of tasks connected via explicit stream types, with sharded
inputs specified using layout types. These tasks are first mapped virtually
onto the accelerator's spatial fabric, and the compiler then generates a
physical mapping that respects hardware constraints. Experimental results on
both AMD Ryzen AI NPU and Alveo FPGA devices demonstrate that Dato achieves
high performance while significantly reducing the burden of writing optimized
code. On the NPU, Dato attains up to 84% hardware utilization for GEMM and
delivers a 2.81x speedup on attention kernels compared to a state-of-the-art
commercial framework. On the FPGA, Dato surpasses leading frameworks in
performance when generating custom systolic arrays, achieving 98% of the
theoretical peak performance.

</details>


### [11] [MIO: Multiverse Debugging in the Face of Input/Output -- Extended Version with Additional Appendices](https://arxiv.org/abs/2509.06845)
*Tom Lauwaerts,Maarten Steevens,Christophe Scholliers*

Main category: cs.PL

TL;DR: 该论文提出了一种新的多宇宙调试方法MIO，支持广泛的输入/输出操作，并确保调试器仅探索常规执行中可达到的程序状态。


<details>
  <summary>Details</summary>
Motivation: 现有的多宇宙调试器在调试涉及输入/输出操作的程序时，可能会探索不可达的程序状态，增加调试难度甚至误导程序员。

Method: 论文提出了一种新的多宇宙调试方法，通过语义定义和正确性证明，确保调试器仅探索常规执行中可达到的状态。原型MIO基于WARDuino WebAssembly虚拟机实现。

Result: MIO原型展示了方法的可行性和效率，并通过乐高Mindstorms电机和颜色传感器的示例验证了其在STM32微控制器上的多宇宙调试能力。

Conclusion: 该方法成功解决了现有调试器在输入/输出操作中探索不可达状态的问题，为微控制器上的非确定性程序调试提供了有效工具。

Abstract: Debugging non-deterministic programs on microcontrollers is notoriously
challenging, especially when bugs manifest in unpredictable, input-dependent
execution paths. A recent approach, called multiverse debugging, makes it
easier to debug non-deterministic programs by allowing programmers to explore
all potential execution paths. Current multiverse debuggers enable both forward
and backward traversal of program paths, and some facilitate jumping to any
previously visited states, potentially branching into alternative execution
paths within the state space.
  Unfortunately, debugging programs that involve input/output operations using
existing multiverse debuggers can reveal inaccessible program states, i.e.
states which are not encountered during regular execution. This can
significantly hinder the debugging process, as the programmer may spend
substantial time exploring and examining inaccessible program states, or worse,
may mistakenly assume a bug is present in the code, when in fact, the issue is
caused by the debugger.
  This paper presents a novel approach to multiverse debugging, which can
accommodate a broad spectrum of input/output operations. We provide the
semantics of our approach and prove the correctness of our debugger, ensuring
that despite having support for a wide range of input/output operations the
debugger will only explore those program states which can be reached during
regular execution.
  We have developed a prototype, called MIO, leveraging the WARDuino
WebAssembly virtual machine to demonstrate the feasibility and efficiency of
our techniques. As a demonstration of the approach we highlight a color dial
built with a Lego Mindstorms motor, and color sensor, providing a tangible
example of how our approach enables multiverse debugging for programs running
on an STM32 microcontroller.

</details>


### [12] [Mechanized Metatheory of Forward Reasoning for End-to-End Linearizability Proofs](https://arxiv.org/abs/2509.06872)
*Zachary Kent,Ugur Y. Yavuz,Siddhartha Jayanti,Stephanie Balzer,Guy Blelloch*

Main category: cs.PL

TL;DR: 本文总结了近十年来证明并发数据结构线性化的技术，重点介绍了Jayanti等人的“正向推理”方法，并提出了在Rocq中形式化和机械化该方法以减小信任计算基础的思路。


<details>
  <summary>Details</summary>
Motivation: 已有技术虽能证明并发数据结构的线性化，但缺乏对这些方法可靠性的机械化验证，导致无法生成完全验证的端到端证明。

Method: 作者在Rocq中形式化了Jayanti等人的正向推理技术，并机械化了其可靠性和完整性的证明。

Result: 通过案例研究，作者成功生成了一个简单并发寄存器的验证端到端线性化证明。

Conclusion: 通过在Rocq中形式化和机械化验证正向推理技术，作者减小了信任计算基础，并为生成完全验证的线性化证明奠定了基础。

Abstract: In the past decade, many techniques have been developed to prove
linearizability, the gold standard of correctness for concurrent data
structures. Intuitively, linearizability requires that every operation on a
concurrent data structure appears to take place instantaneously, even when
interleaved with other operations. Most recently, Jayanti et al. presented the
first sound and complete "forward reasoning" technique for proving
linearizability that relates the behavior of a concurrent data structure to a
reference atomic data structure as time moves forward. This technique can be
used to produce machine-checked proofs of linearizability in TLA+. However,
while Jayanti et al.'s approach is shown to be sound and complete, a
mechanization of this important metatheoretic result is still outstanding. As a
result, it is not possible to produce verified end-to-end proofs of
linearizability. To reduce the size of this trusted computing base, we
formalize this forward reasoning technique and mechanize proofs of its
soundness and completeness in Rocq. As a case study, we use the approach to
produce a verified end-to-end proof of linearizability for a simple concurrent
register.

</details>


<div id='cs.GT'></div>

# cs.GT [[Back]](#toc)

### [13] [Bi-Level Game-Theoretic Planning of Cyber Deception for Cognitive Arbitrage](https://arxiv.org/abs/2509.05498)
*Ya-Ting Yang,Quanyan Zhu*

Main category: cs.GT

TL;DR: 该论文研究了如何利用高级持续性威胁（APT）攻击者的认知漏洞，提出了认知感知防御策略，通过战略级和操作级的欺骗机制，增强防御者的战术优势。


<details>
  <summary>Details</summary>
Motivation: 人类的认知能力差异和认知偏差影响了决策过程，攻击者可利用这些漏洞获得战略优势。论文旨在探索防御者如何反过来利用攻击者的认知漏洞，设计有效的防御机制。

Method: 论文提出了一个双层网络战博弈模型，结合战略级的欺骗机制设计和操作级的战术执行，使用博弈论进行定量建模和分析。

Result: 模拟结果显示，防御者通过适时部署欺骗技术，能将攻击者的负收益转为正收益，并在执行阶段实现至少40%的总收益提升，长期保持战略优势。

Conclusion: 研究表明，防御者可通过认知仲裁策略放大初始优势，持续对抗攻击者的威胁，实现保护关键资产的长期目标。

Abstract: Cognitive vulnerabilities shape human decision-making and arise primarily
from two sources: (1) cognitive capabilities, which include disparities in
knowledge, education, expertise, or access to information, and (2) cognitive
biases, such as rational inattention, confirmation bias, and base rate neglect,
which influence how individuals perceive and process information. Exploiting
these vulnerabilities allows an entity with superior cognitive awareness to
gain a strategic advantage, a concept referred to as cognitive arbitrage. This
paper investigates how to exploit the cognitive vulnerabilities of Advanced
Persistent Threat (APT) attackers and proposes cognition-aware defenses that
leverage windows of superiority to counteract attacks. Specifically, the
proposed bi-level cyber warfare game focuses on "strategic-level" design for
defensive deception mechanisms, which then facilitates "operational-level"
actions and tactical-level execution of Tactics, Techniques, and Procedures
(TTPs). Game-theoretic reasoning and analysis play a significant role in the
cross-echelon quantitative modeling and design of cognitive arbitrage
strategies. Our numerical results demonstrate that although the defender's
initial advantage diminishes over time, strategically timed and deployed
deception techniques can turn a negative value for the attacker into a positive
one during the planning phase, and achieve at least a 40% improvement in total
rewards during execution. This demonstrates that the defender can amplify even
small initial advantages, sustain a strategic edge over the attacker, and
secure long-term objectives, such as protecting critical assets throughout the
attacker's lifecycle.

</details>


### [14] [Knapsack Contracts and the Importance of Return-on-Investment](https://arxiv.org/abs/2509.05956)
*Zohar Barak,Asnat Berlin,Ilan Reuven Cohen,Alon Eden,Omri Porat,Inbal Talgam-Cohen*

Main category: cs.GT

TL;DR: 论文提出了Knapsack Contracts问题，结合了随机背包问题和契约设计的随机性，通过投资回报率（ROI）的逆（IOR）参数，揭示了其复杂性和近似性，并开发了一种不依赖适应性的O(α)-近似算法。


<details>
  <summary>Details</summary>
Motivation: 研究随机优化和契约设计中共同存在的随机性问题，特别是在主体激励代理人完成具有随机处理时间的任务时，如何量化代理人的努力对任务结果的影响。

Method: 将Knapsack Contracts视为具有成本和多选择的随机背包问题，引入IOR参数，开发了一种不依赖适应性的O(α)-近似算法，并证明了其下界。

Result: IOR参数精确刻画了随机背包问题的近似保证在其战略对应问题中的扩展程度，算法在IOR为α时实现了O(α)-近似。

Conclusion: IOR是理解Knapsack Contracts复杂性和近似性的关键参数，对其进行界定是实现非平凡近似保证的必要且充分条件。

Abstract: We formulate the Knapsack Contracts problem -- a strategic version of the
classic Stochastic Knapsack problem, which builds upon the inherent randomness
shared by stochastic optimization and contract design. In this problem, the
principal incentivizes agents to perform jobs with stochastic processing times,
the realization of which depends on the agents' efforts.
  Algorithmically, we show that Knapsack Contracts can be viewed as Stochastic
Knapsack with costs and multi-choice, features that introduce significant new
challenges. We identify a crucial and economically meaningful parameter -- the
Return on Investment (ROI) value. We show that the Inverse of ROI (or IOR for
short) precisely characterizes the extent to which the approximation guarantees
for Stochastic Knapsack extend to its strategic counterpart.
  For IOR of $\alpha$, we develop an algorithm that finds an
$O(\alpha)$-approximation policy that does not rely on adaptivity. We establish
matching $\Omega(\alpha)$ lower bounds, both on the adaptivity gap, and on what
can be achieved without full distributional knowledge of the processing times.
Taken together, our results show that IOR is fundamental to understanding the
complexity and approximability of Knapsack Contracts, and bounding it is both
necessary and sufficient for achieving non-trivial approximation guarantees.
Our results highlight the computational challenges arising when stochasticity
in optimization problems is controlled by strategic effort.

</details>


### [15] [The Keychain Problem: On Minimizing the Opportunity Cost of Uncertainty](https://arxiv.org/abs/2509.06187)
*Ramiro N. Deo-Campo Vuong,Robert Kleinberg,Aditya Prasad,Eric Xiao,Haifeng Xu*

Main category: cs.GT

TL;DR: 论文介绍了“钥匙链问题”，一种探索动作集以最大化预期收益的序列决策问题，并针对不同测试顺序假设提出了精确算法和近似算法。


<details>
  <summary>Details</summary>
Motivation: 研究目的是解决在每一阶段仅有部分动作可用时，如何通过序列决策最大化预期收益的问题。

Method: 论文提出了三种测试顺序假设（固定顺序、随机顺序和自主选择顺序），并分别给出了精确算法和基于组合拍卖与策略设计新联系的近似算法。

Result: 在概率情景设置中，论文展示了近似算法的有效性，并将其应用于在线二分图匹配及其扩展问题中。

Conclusion: 研究表明，通过组合拍卖与策略设计的联系，可以高效解决序列决策问题，并将方法推广到其他领域。

Abstract: In this paper, we introduce a family of sequential decision-making problems,
collectively called the Keychain Problem, that involve exploring a set of
actions to maximize expected payoff when only a subset of actions are available
in each stage. In an instance of the Keychain Problem, a locksmith faces a
sequence of choices, each of which involves selecting one key from a specified
subset (a keychain) to attempt to open a lock. Given a Bayesian prior on the
effectiveness of keys, the locksmith's goal is to maximize the expected number
of rounds in which the lock is opened -- or equivalently, minimize the
opportunity cost which is the expected number of rounds in which the chain has
a correct key but our selected key is incorrect. We investigate Keychain
Problems under three assumptions on the order in which keychains are tested by
the locksmith: a fixed, known order; a random order sampled from a known
distribution on a set of ``scenarios''; or an order selected by the locksmith
themself. We present an exact algorithm for the simplest of these settings, and
we present approximation algorithms and hardness results for the others. In the
Probabilistic Scenarios setting, our approximation algorithm is based on a
novel connection between combinatorial auctions and policy design for
sequential decision-making problems. To illustrate the generality of this
technique, we apply the same ideas to obtain Philosopher Inequalities for
Online Bipartite Matching and some of its extensions.

</details>
