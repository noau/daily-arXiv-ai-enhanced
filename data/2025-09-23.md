<div id=toc></div>

# Table of Contents

- [cs.GR](#cs.GR) [Total: 11]
- [cs.PL](#cs.PL) [Total: 3]
- [cs.GT](#cs.GT) [Total: 3]


<div id='cs.GR'></div>

# cs.GR [[Back]](#toc)

### [1] [Neural Atlas Graphs for Dynamic Scene Decomposition and Editing](https://arxiv.org/abs/2509.16336)
*Jan Philipp Schneider,Pratik Singh Bisht,Ilya Chugunov,Andreas Kolb,Michael Moeller,Felix Heide*

Main category: cs.GR

TL;DR: 本文提出了Neural Atlas Graphs（NAGs），一种混合高分辨率场景表示方法，结合了神经地图的2D编辑能力和场景图的3D空间关系，显著提升了动态场景的编辑性和复杂性支持。


<details>
  <summary>Details</summary>
Motivation: 现有方法在动态场景的高分辨率表示中面临编辑性与场景复杂性之间的权衡问题，本文旨在通过结合神经地图和场景图的优势，提出一种更优的解决方案。

Method: NAGs通过将每个图节点表示为视角依赖的神经地图，实现了2D外观编辑和3D场景元素的排序与定位的兼容。

Result: 在Waymo Open Dataset上，NAGs的PSNR提升了5 dB，并在DAVIS视频数据集上的表现优于现有方法7 dB以上。

Conclusion: NAGs不仅在高分辨率动态场景表示中表现出色，还支持复杂的场景编辑，适用于自动驾驶和创意编辑等多个领域。

Abstract: Learning editable high-resolution scene representations for dynamic scenes is
an open problem with applications across the domains from autonomous driving to
creative editing - the most successful approaches today make a trade-off
between editability and supporting scene complexity: neural atlases represent
dynamic scenes as two deforming image layers, foreground and background, which
are editable in 2D, but break down when multiple objects occlude and interact.
In contrast, scene graph models make use of annotated data such as masks and
bounding boxes from autonomous-driving datasets to capture complex 3D spatial
relationships, but their implicit volumetric node representations are
challenging to edit view-consistently. We propose Neural Atlas Graphs (NAGs), a
hybrid high-resolution scene representation, where every graph node is a
view-dependent neural atlas, facilitating both 2D appearance editing and 3D
ordering and positioning of scene elements. Fit at test-time, NAGs achieve
state-of-the-art quantitative results on the Waymo Open Dataset - by 5 dB PSNR
increase compared to existing methods - and make environmental editing possible
in high resolution and visual quality - creating counterfactual driving
scenarios with new backgrounds and edited vehicle appearance. We find that the
method also generalizes beyond driving scenes and compares favorably - by more
than 7 dB in PSNR - to recent matting and video editing baselines on the DAVIS
video dataset with a diverse set of human and animal-centric scenes.

</details>


### [2] [Brain Connectivity Network Structure Learning For Brain Disorder Diagnosis](https://arxiv.org/abs/2509.16735)
*Dongdong Chen,Linlin Yao,Mengjun Liu,Zhenrong Shen,Yuqi Hu,Zhiyun Song,Shengyu Lu,Qian Wang,Dinggang Shen,Lichi Zhang*

Main category: cs.GR

TL;DR: 论文提出了一种自监督框架，用于学习和优化脑功能连接网络的结构和表示，解决了传统方法中因手动设定阈值而引入的冗余或遗漏问题，并通过大规模无标记数据预训练提升了泛化能力。


<details>
  <summary>Details</summary>
Motivation: 传统的脑功能连接网络构建方法依赖预定义的手动设定阈值，可能导致冗余或遗漏重要连接，且标记数据不足限制了泛化能力。

Method: 采用自监督框架，结合互补脑网络结构学习器和多状态图编码器，通过联合迭代学习策略优化网络结构和表示。

Result: 实验表明，该方法在跨数据集脑疾病诊断中表现优于现有技术，验证了其有效性和泛化性。

Conclusion: 该自监督框架通过无监督学习和预训练，显著提升了脑功能连接网络的构建质量和诊断性能，适用于多种脑疾病任务。

Abstract: Recent studies in neuroscience highlight the significant potential of brain
connectivity networks, which are commonly constructed from functional magnetic
resonance imaging (fMRI) data for brain disorder diagnosis. Traditional brain
connectivity networks are typically obtained using predefined methods that
incorporate manually-set thresholds to estimate inter-regional relationships.
However, such approaches often introduce redundant connections or overlook
essential interactions, compromising the value of the constructed networks.
Besides, the insufficiency of labeled data further increases the difficulty of
learning generalized representations of intrinsic brain characteristics. To
mitigate those issues, we propose a self-supervised framework to learn an
optimal structure and representation for brain connectivity networks, focusing
on individualized generation and optimization in an unsupervised manner. We
firstly employ two existing whole-brain connectomes to adaptively construct
their complementary brain network structure learner, and then introduce a
multi-state graph-based encoder with a joint iterative learning strategy to
simultaneously optimize both the generated network structure and its
representation. By leveraging self-supervised pretraining on large-scale
unlabeled brain connectivity data, our framework enables the brain connectivity
network learner to generalize e ffectively to unseen disorders, while requiring
only minimal finetuning of the encoder for adaptation to new diagnostic tasks.
Extensive experiments on cross-dataset brain disorder diagnosis demonstrate
that our method consistently outperforms state-of-the-art approaches,
validating its effectiveness and generalizability. The code is publicly
available at https://github.com/neochen1/BCNSL.

</details>


### [3] [PhysHDR: When Lighting Meets Materials and Scene Geometry in HDR Reconstruction](https://arxiv.org/abs/2509.16869)
*Hrishav Bakul Barua,Kalin Stefanov,Ganesh Krishnasamy,KokSheik Wong,Abhinav Dhall*

Main category: cs.GR

TL;DR: 该论文提出了PhysHDR，一种基于潜在扩散的生成模型，用于从低动态范围（LDR）图像重建高动态范围（HDR）图像，通过建模光照、深度和材料特性提高了重建质量。


<details>
  <summary>Details</summary>
Motivation: 现有的LDR到HDR图像转换方法缺乏对光照、阴影和材料属性的显式建模，限制了HDR图像的重建质量。本文旨在通过建模材料特性（如镜面反射和漫反射）来解决这一问题。

Method: 论文提出PhysHDR，一种基于潜在扩散的生成模型，其去噪过程通过光照和深度信息进行条件化，并通过新颖的损失函数引入场景中表面的材料属性。

Result: 实验结果表明，PhysHDR在HDR图像重建质量上优于多种最新的先进方法。

Conclusion: PhysHDR通过显式建模光照、深度和材料特性，显著提升了HDR图像的重建效果，展示了其在计算视觉任务中的潜力。

Abstract: Low Dynamic Range (LDR) to High Dynamic Range (HDR) image translation is a
fundamental task in many computational vision problems. Numerous data-driven
methods have been proposed to address this problem; however, they lack explicit
modeling of illumination, lighting, and scene geometry in images. This limits
the quality of the reconstructed HDR images. Since lighting and shadows
interact differently with different materials, (e.g., specular surfaces such as
glass and metal, and lambertian or diffuse surfaces such as wood and stone),
modeling material-specific properties (e.g., specular and diffuse reflectance)
has the potential to improve the quality of HDR image reconstruction. This
paper presents PhysHDR, a simple yet powerful latent diffusion-based generative
model for HDR image reconstruction. The denoising process is conditioned on
lighting and depth information and guided by a novel loss to incorporate
material properties of surfaces in the scene. The experimental results
establish the efficacy of PhysHDR in comparison to a number of recent
state-of-the-art methods.

</details>


### [4] [SemanticGarment: Semantic-Controlled Generation and Editing of 3D Gaussian Garments](https://arxiv.org/abs/2509.16960)
*Ruiyan Wang,Zhengxue Cheng,Zonghao Lin,Jun Ling,Yuzhou Liu,Yanru An,Rong Xie,Li Song*

Main category: cs.GR

TL;DR: 提出了一种基于3D高斯的方法SemanticGarment，实现了从文本或图像提示生成高保真3D服装，并支持基于语义的交互式编辑。


<details>
  <summary>Details</summary>
Motivation: 传统方法在满足日益增长的需求时因技术复杂性和高资源成本而受限，而学习型方法虽然提高了速度和多样性，但仍面临多视角一致性和对详细拓扑结构的依赖等问题。

Method: 利用结构人体先验知识，通过引入3D语义服装模型来初始化几何结构，支持快速、多样化的修改，并开发了自遮挡优化策略以减少单图像重建的伪影。

Result: 大量实验证明，该方法在3D服装生成和编辑方面表现优越。

Conclusion: SemanticGarment方法无需重新生成或依赖现有网格模板，实现了高保真3D服装生成和灵活的交互式编辑。

Abstract: 3D digital garment generation and editing play a pivotal role in fashion
design, virtual try-on, and gaming. Traditional methods struggle to meet the
growing demand due to technical complexity and high resource costs.
Learning-based approaches offer faster, more diverse garment synthesis based on
specific requirements and reduce human efforts and time costs. However, they
still face challenges such as inconsistent multi-view geometry or textures and
heavy reliance on detailed garment topology and manual rigging. We propose
SemanticGarment, a 3D Gaussian-based method that realizes high-fidelity 3D
garment generation from text or image prompts and supports semantic-based
interactive editing for flexible user customization. To ensure multi-view
consistency and garment fitting, we propose to leverage structural human priors
for the generative model by introducing a 3D semantic clothing model, which
initializes the geometry structure and lays the groundwork for view-consistent
garment generation and editing. Without the need to regenerate or rely on
existing mesh templates, our approach allows for rapid and diverse
modifications to existing Gaussians, either globally or within a local region.
To address the artifacts caused by self-occlusion for garment reconstruction
based on single image, we develop a self-occlusion optimization strategy to
mitigate holes and artifacts that arise when directly animating self-occluded
garments. Extensive experiments are conducted to demonstrate our superior
performance in 3D garment generation and editing.

</details>


### [5] [Beat on Gaze: Learning Stylized Generation of Gaze and Head Dynamics](https://arxiv.org/abs/2509.17168)
*Chengwei Shi,Chong Cao,Xin Tong,Xukun Shen*

Main category: cs.GR

TL;DR: 该论文提出了一种名为StyGazeTalk的方法，通过音频驱动生成同步的注视和头部运动风格，解决了现有方法在表达3D面部动画时忽略各组件协调性的问题。


<details>
  <summary>Details</summary>
Motivation: 现有方法在处理面部动画时，往往孤立地处理各组件，忽视了注视、头部运动和语音之间的复杂协调性，且缺乏高质量的注视标注数据集。

Method: 论文提出StyGazeTalk方法，采用多层LSTM结构和风格编码器，从注视-头部序列中提取说话者特定的运动特征，生成多样化的动画风格。

Result: 实验结果表明，该方法能够生成逼真、时间连贯且具有风格意识的头部-注视运动，显著提升了音频驱动面部动画的性能。

Conclusion: StyGazeTalk方法及其引入的多模态数据集为头部和注视控制模型的研究提供了重要资源，推动了音频驱动面部动画技术的进步。

Abstract: Head and gaze dynamics are crucial in expressive 3D facial animation for
conveying emotion and intention. However, existing methods frequently address
facial components in isolation, overlooking the intricate coordination between
gaze, head motion, and speech. The scarcity of high-quality gaze-annotated
datasets hinders the development of data-driven models capable of capturing
realistic, personalized gaze control. To address these challenges, we propose
StyGazeTalk, an audio-driven method that generates synchronized gaze and head
motion styles. We extract speaker-specific motion traits from gaze-head
sequences with a multi-layer LSTM structure incorporating a style encoder,
enabling the generation of diverse animation styles. We also introduce a
high-precision multimodal dataset comprising eye-tracked gaze, audio, head
pose, and 3D facial parameters, providing a valuable resource for training and
evaluating head and gaze control models. Experimental results demonstrate that
our method generates realistic, temporally coherent, and style-aware head-gaze
motions, significantly advancing the state-of-the-art in audio-driven facial
animation.

</details>


### [6] [High Resolution UDF Meshing via Iterative Networks](https://arxiv.org/abs/2509.17212)
*Federico Stella,Nicolas Talabot,Hieu Le,Pascal Fua*

Main category: cs.GR

TL;DR: 本文提出了一种迭代神经网络方法，通过多次传递和邻居信息推理，显著提高了开放表面无符号距离场（UDF）的三角网格生成精度和完整性。


<details>
  <summary>Details</summary>
Motivation: 无符号距离场（UDF）是开放表面的自然隐式表示，但在高分辨率下因其噪声水平较高，难以捕捉细节，且现有方法通常在单个体素内工作，导致表面缺失和孔洞。

Method: 通过多次迭代神经网络，整合新检测的表面、距离值和梯度信息，逐步改进每个体素内的表面恢复，并通过空间传播邻居信息纠正错误。

Result: 实验表明，该方法在多样化的3D模型上显著提高了网格的准确性和完整性，尤其在复杂几何结构和高分辨率情况下优于现有方法。

Conclusion: 提出的迭代方法有效地解决了UDF三角网格生成的挑战，适用于高分辨率场景，弥补了传统方法的不足。

Abstract: Unsigned Distance Fields (UDFs) are a natural implicit representation for
open surfaces but, unlike Signed Distance Fields (SDFs), are challenging to
triangulate into explicit meshes. This is especially true at high resolutions
where neural UDFs exhibit higher noise levels, which makes it hard to capture
fine details. Most current techniques perform within single voxels without
reference to their neighborhood, resulting in missing surface and holes where
the UDF is ambiguous or noisy. We show that this can be remedied by performing
several passes and by reasoning on previously extracted surface elements to
incorporate neighborhood information. Our key contribution is an iterative
neural network that does this and progressively improves surface recovery
within each voxel by spatially propagating information from increasingly
distant neighbors. Unlike single-pass methods, our approach integrates newly
detected surfaces, distance values, and gradients across multiple iterations,
effectively correcting errors and stabilizing extraction in challenging
regions. Experiments on diverse 3D models demonstrate that our method produces
significantly more accurate and complete meshes than existing approaches,
particularly for complex geometries, enabling UDF surface extraction at higher
resolutions where traditional methods fail.

</details>


### [7] ["I don't like my avatar": Investigating Human Digital Doubles](https://arxiv.org/abs/2509.17748)
*Siyi Liu,Kazi Injamamul Haque,Zerrin Yumak*

Main category: cs.GR

TL;DR: 研究表明，高度真实的数字替身更容易被识别和感知为真实，但熟悉的面孔（尤其是高真实性替身）会降低这些感受。


<details>
  <summary>Details</summary>
Motivation: 研究数字替身的风格（真实vs卡通）和熟悉度（自己、熟人、陌生人）如何影响自我/他人识别、真实感、亲和力及社交存在感。

Method: 通过离线实验，使用MetaHumans和ReadyPlayerMe分别创建真实和卡通风格的替身，并利用动作捕捉生成面部动画，通过问卷调查收集数据。

Result: 高度真实的替身提升了识别度、真实感和社交存在感，但熟悉面孔（尤其是高真实性替身）降低了这些感受。参与者普遍不喜欢自己的真实替身，但对熟人或陌生人的替身更宽容。

Conclusion: 数字替身的真实性和熟悉度对用户体验有复杂影响，真实风格虽增强识别和存在感，但熟悉面孔可能带来负面情绪。

Abstract: Creating human digital doubles is becoming easier and much more accessible to
everyone using consumer grade devices. In this work, we investigate how avatar
style (realistic vs cartoon) and avatar familiarity (self, acquaintance,
unknown person) affect self/other-identification, perceived realism, affinity
and social presence with a controlled offline experiment. We created two styles
of avatars (realistic-looking MetaHumans and cartoon-looking ReadyPlayerMe
avatars) and facial animations stimuli for them using performance capture.
Questionnaire responses demonstrate that higher appearance realism leads to a
higher level of identification, perceived realism and social presence. However,
avatars with familiar faces, especially those with high appearance realism,
lead to a lower level of identification, perceived realism, and affinity.
Although participants identified their digital doubles as their own, they
consistently did not like their avatars, especially of realistic appearance.
But they were less critical and more forgiving about their acquaintance's or an
unknown person's digital double.

</details>


### [8] [Effect of Appearance and Animation Realism on the Perception of Emotionally Expressive Virtual Humans](https://arxiv.org/abs/2509.17803)
*Nabila Amadou,Kazi Injamamul Haque,Zerrin Yumak*

Main category: cs.GR

TL;DR: 研究了3D虚拟人类的感知效果，发现更高的外观和动画真实感可以提高社交存在感和吸引力。


<details>
  <summary>Details</summary>
Motivation: 虚拟人类技术在多个领域有潜在应用，但目前缺乏对其外观和动画真实感的情感表达感知研究。

Method: 设计了用户实验，分析不同情感条件下外观和动画真实感对感知的影响。

Result: 高外观和动画真实感提升了社交存在感和吸引力；动画真实感显著影响感知真实感和情感强度。

Conclusion: 研究揭示了高真实感虚拟人类在情感表达中的感知效果，为未来发展方向提供了指导。

Abstract: 3D Virtual Human technology is growing with several potential applications in
health, education, business and telecommunications. Investigating the
perception of these virtual humans can help guide to develop better and more
effective applications. Recent developments show that the appearance of the
virtual humans reached to a very realistic level. However, there is not yet
adequate analysis on the perception of appearance and animation realism for
emotionally expressive virtual humans. In this paper, we designed a user
experiment and analyzed the effect of a realistic virtual human's appearance
realism and animation realism in varying emotion conditions. We found that
higher appearance realism and higher animation realism leads to higher social
presence and higher attractiveness ratings. We also found significant effects
of animation realism on perceived realism and emotion intensity levels. Our
study sheds light into how appearance and animation realism effects the
perception of highly realistic virtual humans in emotionally expressive
scenarios and points out to future directions.

</details>


### [9] [A Comparative Study of Different Edit Distance-Based Methods for Feature Tracking using Merge Trees on Time-Varying Scalar Fields](https://arxiv.org/abs/2509.17974)
*Son Le Thanh,Tino Weinkauf*

Main category: cs.GR

TL;DR: 该论文比较了四种基于合并树编辑距离的特征跟踪方法，展示了它们在分析和真实数据集上的不同表现，并探讨了结果差异的影响因素。


<details>
  <summary>Details</summary>
Motivation: 时间变化标量场中的特征跟踪是科学计算中的基础任务，合并树作为拓扑描述符在此任务中具有重要作用。论文旨在比较不同编辑距离方法的有效性。

Method: 论文采用了四种基于合并树编辑距离的方法，通过分析和真实数据集比较它们的表现，并分析了结果差异的原因。

Result: 实验结果显示，即使是同一类别的方法，其追踪特征的结果也存在显著差异。

Conclusion: 研究表明，不同合并树编辑距离方法在特征追踪中表现出显著差异，这些差异受多种因素影响，为未来研究提供了重要参考。

Abstract: Feature tracking in time-varying scalar fields is a fundamental task in
scientific computing. Topological descriptors, which summarize important
features of data, have proved to be viable tools to facilitate this task. The
merge tree is a topological descriptor that captures the connectivity behaviors
of the sub- or superlevel sets of a scalar field. Edit distances between merge
trees play a vital role in effective temporal data tracking. Existing methods
to compute them fall into two main classes, namely whether they are dependent
or independent of the branch decomposition. These two classes represent the
most prominent approaches for producing tracking results. In this paper, we
compare four different merge tree edit distance-based methods for feature
tracking. We demonstrate that these methods yield distinct results with both
analytical and real-world data sets. Furthermore, we investigate how these
results vary and identify the factors that influence them. Our experiments
reveal significant differences in tracked features over time, even among those
produced by techniques within the same category.

</details>


### [10] [Towards Seeing Bones at Radio Frequency](https://arxiv.org/abs/2509.17979)
*Yiwen Song,Hongyang Li,Kuang Yuan,Ran Bi,Swarun Kumar*

Main category: cs.GR

TL;DR: 本文提出了一种名为MCT的射频成像系统，能够在毫米分辨率下成像骨骼，显著优于以往的射频穿透成像技术。


<details>
  <summary>Details</summary>
Motivation: 无线传感领域长期以来希望通过射频实现类似X射线的视觉能力，但现有技术难以成像骨骼。本文旨在突破这一限制，实现更高分辨率的射频骨骼成像。

Method: 通过结合基于穿透的合成孔径算法和学习型衍射校正流水线，克服了射频在穿透组织时的长波长、衰减和复杂衍射问题。

Result: 在肉类模型上的详细评估显示，分辨率从亚分米提升到亚厘米，显著优于现有技术。

Conclusion: MCT系统为射频穿透成像技术带来了显著的分辨率提升，为未来的无线传感应用开辟了新的可能性。

Abstract: Wireless sensing literature has long aspired to achieve X-ray-like vision at
radio frequencies. Yet, state-of-the-art wireless sensing literature has yet to
generate the archetypal X-ray image: one of the bones beneath flesh. In this
paper, we explore MCT, a penetration-based RF-imaging system for imaging bones
at mm-resolution, one that significantly exceeds prior penetration-based RF
imaging literature. Indeed the long wavelength, significant attenuation and
complex diffraction that occur as RF propagates through flesh, have long
limited imaging resolution (to several centimeters at best). We address these
concerns through a novel penetration-based synthetic aperture algorithm,
coupled with a learning-based pipeline to correct for diffraction-induced
artifacts. A detailed evaluation of meat models demonstrates a resolution
improvement from sub-decimeter to sub-centimeter over prior art in RF
penetrative imaging.

</details>


### [11] [VideoFrom3D: 3D Scene Video Generation via Complementary Image and Video Diffusion Models](https://arxiv.org/abs/2509.17985)
*Geonung Kim,Janghyeok Han,Sunghyun Cho*

Main category: cs.GR

TL;DR: VideoFrom3D是一个新颖框架，通过粗几何、相机轨迹和参考图像生成高质量3D场景视频，简化设计流程并提升生成效果。


<details>
  <summary>Details</summary>
Motivation: 现有视频扩散模型在生成复杂场景的高保真视频时表现不佳，难以同时满足视觉质量、运动和时序一致性的要求。

Method: 结合图像和视频扩散模型的互补优势，框架包括稀疏锚点视图生成(SAG)和几何引导生成中间帧(GGI)模块。

Result: 综合实验表明，该方法在多样化和挑战性场景下生成高质量、风格一致的视频，优于基线和扩展基线。

Conclusion: VideoFrom3D框架无需成对的3D场景模型和自然图像数据集，便能高效生成高质量3D场景视频，具有实际应用价值。

Abstract: In this paper, we propose VideoFrom3D, a novel framework for synthesizing
high-quality 3D scene videos from coarse geometry, a camera trajectory, and a
reference image. Our approach streamlines the 3D graphic design workflow,
enabling flexible design exploration and rapid production of deliverables. A
straightforward approach to synthesizing a video from coarse geometry might
condition a video diffusion model on geometric structure. However, existing
video diffusion models struggle to generate high-fidelity results for complex
scenes due to the difficulty of jointly modeling visual quality, motion, and
temporal consistency. To address this, we propose a generative framework that
leverages the complementary strengths of image and video diffusion models.
Specifically, our framework consists of a Sparse Anchor-view Generation (SAG)
and a Geometry-guided Generative Inbetweening (GGI) module. The SAG module
generates high-quality, cross-view consistent anchor views using an image
diffusion model, aided by Sparse Appearance-guided Sampling. Building on these
anchor views, GGI module faithfully interpolates intermediate frames using a
video diffusion model, enhanced by flow-based camera control and structural
guidance. Notably, both modules operate without any paired dataset of 3D scene
models and natural images, which is extremely difficult to obtain.
Comprehensive experiments show that our method produces high-quality,
style-consistent scene videos under diverse and challenging scenarios,
outperforming simple and extended baselines.

</details>


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [12] [VerilogMonkey: Exploring Parallel Scaling for Automated Verilog Code Generation with LLMs](https://arxiv.org/abs/2509.16246)
*Juxin Niu,Yuxin Du,Dan Niu,Xi Wang,Zhe Jiang,Nan Guan*

Main category: cs.PL

TL;DR: VerilogMonkey研究表明，在自动生成Verilog的任务中，通过并行扩展采样数百个输出，无需额外增强方法，即可超越以往基于LLM的Verilog生成成果，同时在时间和成本上具有性价比。


<details>
  <summary>Details</summary>
Motivation: 探索并行扩展在自动生成Verilog任务中的效果，这一任务目前研究较少。通过实证研究，验证并行扩展对提升LLM性能的潜力。

Method: 采用并行扩展方法，采样大量输出，并在多个基准测试和主流LLMs上进行验证。分析输出随机性对并行扩展效果的影响。

Result: 研究发现，扩展到数百个样本时，无需额外增强（如后训练或代理方法），即可在时间和成本上高效地超越以往LLM-based Verilog生成的结果。

Conclusion: 并行扩展是一种高效且成本低廉的方法，能够在自动生成Verilog任务中显著提升LLM性能，输出随机性对其效果有重要影响。

Abstract: We present VerilogMonkey, an empirical study of parallel scaling for the
under-explored task of automated Verilog generation. Parallel scaling improves
LLM performance by sampling many outputs in parallel. Across multiple
benchmarks and mainstream LLMs, we find that scaling to hundreds of samples is
cost-effective in both time and money and, even without any additional
enhancements such as post-training or agentic methods, surpasses prior results
on LLM-based Verilog generation. We further dissect why parallel scaling
delivers these gains and show how output randomness in LLMs affects its
effectiveness.

</details>


### [13] [GraphMend: Code Transformations for Fixing Graph Breaks in PyTorch 2](https://arxiv.org/abs/2509.16248)
*Savini Kashmira,Jayanaka Dantanarayana,Thamirawaran Sathiyalogeswaran,Yichao Yuan,Nishil Talati,Krisztian Flautner,Lingjia Tang,Jason Mars*

Main category: cs.PL

TL;DR: GraphMend是一个高级编译器，通过分析和转换源代码来解决PyTorch 2程序中的FX图中断问题，显著提升性能和易用性。


<details>
  <summary>Details</summary>
Motivation: PyTorch 2的TorchDynamo和TorchInductor虽然实现了即时图编译，但动态控制流和不支持的Python构造仍然会导致模型碎片化，频繁回退到eager模式并降低优化机会。GraphMend旨在解决这一问题。

Method: GraphMend基于Jac编译框架，引入两种代码转换技术，消除动态控制流和Python I/O函数导致的图中断，无需开发者手动重构代码。

Result: 在八个Hugging Face模型上的评估显示，GraphMend消除了所有可修复的图中断，在NVIDIA RTX 3090和A40 GPU上实现了高达75%的延迟降低和8%的吞吐量提升。

Conclusion: 研究表明，高级代码转换是PyTorch动态JIT编译管道的有效补充，显著提升了性能和易用性。

Abstract: This paper presents GraphMend, a high-level compiler that eliminates FX graph
breaks in PyTorch 2 programs. Although PyTorch 2 introduced TorchDynamo and
TorchInductor to enable just-in-time graph compilation, unresolved dynamic
control flow and unsupported Python constructs often fragment models into
multiple FX graphs. These fragments force frequent fallbacks to eager mode,
incur costly CPU-to-GPU synchronizations, and reduce optimization
opportunities. GraphMend addresses this limitation by analyzing and
transforming source code before execution. Built on the Jac compilation
framework, GraphMend introduces two code transformations that remove graph
breaks due to dynamic control flow and Python I/O functions. This design allows
PyTorch's compilation pipeline to capture larger, uninterrupted FX graphs
without requiring manual refactoring by developers. Evaluation across eight
Hugging Face models shows that GraphMend removes all fixable graph breaks due
to dynamic control flow and Python I/O functions, driving the break count to 0
in 6 models and reducing it from 5 to 2 in another model. On NVIDIA RTX 3090
and A40 GPUs, GraphMend achieves up to 75% latency reductions and up to 8%
higher end-to-end throughput. These results demonstrate that high-level code
transformation is an effective complement to PyTorch's dynamic JIT compilation
pipeline, substantially improving both usability and performance.

</details>


### [14] [Efficient Linearizability Monitoring](https://arxiv.org/abs/2509.17795)
*Parosh Aziz Abdulla,Samuel Grahn,Bengt Jonsson,Shankaranarayanan Krishna,Om Swostik Mishra*

Main category: cs.PL

TL;DR: 本文重新审视了监控并发栈、队列、集合和多集的线性可化性问题，提出了复杂度分别为O(n²)、O(n log n)和O(n)的监控算法，并通过实验验证了新工具LiMo的高效性和可扩展性。


<details>
  <summary>Details</summary>
Motivation: 当前关于监控线性可化性的算法存在时间复杂度较高（立方时间）以及正确性问题，包括缺乏证明、证明错误或算法本身错误。本文旨在提供更高效的算法，并通过详细证明确保其正确性。

Method: 针对栈、队列、（多）集，分别提出了复杂度为O(n²)、O(n log n)和O(n)的新监控算法。这些算法基于数据独立性假设，并通过LiMo工具实现。

Result: 实验评估表明，LiMo在效率和可扩展性上优于现有工具Violin。同时，新算法的复杂度显著降低，且通过详细证明确保了正确性。

Conclusion: 本文提出的新算法显著提升了监控线性可化性的效率，并通过实验和理论证明验证了其正确性和优越性。

Abstract: This paper revisits the fundamental problem of monitoring the linearizability
of concurrent stacks, queues, sets, and multisets. Given a history of a library
implementing one of these abstract data types, the monitoring problem is to
answer whether the given history is linearizable. For stacks, queues, and
(multi)sets, we present monitoring algorithms with complexities
$\mathcal{O}(n^2)$, $\mathcal{O}(n\; log\, n)$, and $\mathcal{O}{(n)}$,
respectively, where $n$ is the number of operations in the input history. For
stacks and queues, our results hold under the standard assumption of {\it
data-independence}, i.e., the behavior of the library is not sensitive to the
actual values stored in the data structure. Past works to solve the same
problems have cubic time complexity and (more seriously) have correctness
issues: they either (i) lack correctness proofs or (ii) the suggested
correctness proofs are erroneous (we present counter-examples), or (iii) have
incorrect algorithms. Our improved complexity results rely on substantially
different algorithms for which we provide detailed proofs of correctness. We
have implemented our stack and queue algorithms in LiMo (Linearizability
Monitor). We evaluate LiMo and compare it with the state-of-the-art tool Violin
-- whose correctness proofs we have found errors in -- which checks for
linearizability violations. Our experimental evaluation confirms that LiMo
outperforms Violin regarding both efficiency and scalability.

</details>


<div id='cs.GT'></div>

# cs.GT [[Back]](#toc)

### [15] [On the Existence and Complexity of Core-Stable Data Exchanges](https://arxiv.org/abs/2509.16450)
*Jiaxin Song,Pooja Kulkarni,Parnian Shahkar,Bhaskar Ray Chaudhury*

Main category: cs.GT

TL;DR: 研究数据交换中的核心稳定性问题，证明在凹收益函数和凸成本函数条件下存在核心稳定的数据交换，并探讨其计算复杂性。


<details>
  <summary>Details</summary>
Motivation: 数据驱动技术的快速发展以及各种数据共享模式的出现，凸显了对高效稳定数据交换协议的需求。

Method: 采用合作博弈理论中的核心稳定性概念，分析数据交换中的收益和成本函数，证明核心稳定条件的存在性及其计算复杂性。

Result: 在凹收益函数和凸成本函数情况下，核心稳定的数据交换存在；突破这些条件则可能导致稳定性缺失。计算核心稳定的交换是PPAD难的。

Conclusion: 数据交换可建模为平衡多人博弈，Scarf定理提供的pivoting算法在实践中表现良好。

Abstract: The rapid growth of data-driven technologies and the emergence of various
data-sharing paradigms have underscored the need for efficient and stable data
exchange protocols. In any such exchange, agents must carefully balance the
benefit of acquiring valuable data against the cost of sharing their own.
Ensuring stability in these exchanges is essential to prevent agents -- or
groups of agents -- from departing and conducting local (and potentially more
favorable) exchanges among themselves. To address this, we study a model where
agents participate in a data exchange. Each agent has an associated payoff for
the data acquired from other agents and a cost incurred during sharing its own
data. The net utility of an agent is payoff minus the cost. We adapt the
classical notion of core-stability from cooperative game theory to data
exchange. A data exchange is core-stable if no subset of agents has any
incentive to deviate to a different exchange. We show that a core-stable data
exchange is guaranteed to exist when agents have concave payoff functions and
convex cost functions -- a setting typical in domains like PAC learning and
random discovery models. We show that relaxing either of the foregoing
conditions may result in the nonexistence of core-stable data exchanges. Then,
we prove that finding a core-stable exchange is PPAD-hard, even when the
potential blocking coalitions are restricted to constant size. To the best of
our knowledge, this provides the first known PPAD-hardness result for core-like
guarantees in data economics. Finally, we show that data exchange can be
modelled as a balanced $n$-person game. This immediately gives a pivoting
algorithm via Scarf's theorem \cite{Scarf1967core}. We show that the pivoting
algorithm works well in practice through our empirical results.

</details>


### [16] [Discrepancy And Fair Division For Non-Additive Valuations](https://arxiv.org/abs/2509.16802)
*Max Dupre la Tour,Kaito Fujii*

Main category: cs.GT

TL;DR: 将组合差异扩展至非加性函数，提出一个关于非加性k色差异的上界，并应用于公平分配问题，改善了两个应用场景的公平性界限。


<details>
  <summary>Details</summary>
Motivation: 研究组合差异在非加性函数中的扩展，特别是为了解决公平分配问题中的设计挑战。

Method: 扩展组合差异为非加性函数，推导出非加性k色差异的上界，并将其应用于共识分割和嫉妒自由分配问题。

Result: 当k为素数的幂时，非加性k色差异上界为O(√(n log(nk)))，并在公平分配问题中取得了改进结果。

Conclusion: 该研究不仅扩展了组合差异的理论基础，还为公平分配问题提供了新的理论保证和改进方法。

Abstract: We extend the notion of combinatorial discrepancy to \emph{non-additive}
functions. Our main result is an upper bound of $O(\sqrt{n \log(nk)})$ on the
non-additive $k$-color discrepancy when $k$ is a prime power. We demonstrate
two applications of this result to problems in fair division. First, we
establish a bound for a consensus halving problem, where fairness is measured
by the minimum number of items that must be transferred between the two parts
to eliminate envy. Second, we improve the upper bound on the total subsidy
required to achieve an envy-free allocation when the number of agents is a
prime power, obtaining an $O(n \sqrt{n \log n})$ bound. This constitutes the
first known subquadratic guarantee in this setting.

</details>


### [17] [Tight Bounds On the Distortion of Randomized and Deterministic Distributed Voting](https://arxiv.org/abs/2509.17134)
*Mohammad Ali Abam,Davoud Kareshki,Marzieh Nilipour,Mohammad Hossein Paydar,Masoud Seddighin*

Main category: cs.GT

TL;DR: 研究了分布式投票中的度量失真问题，改进了确定性和随机机制的失真界限，提供了近乎完整的失真特征描述。


<details>
  <summary>Details</summary>
Motivation: 研究分布式投票中的度量失真问题，旨在模拟如美国总统选举等系统中本地决策如何影响全局结果。

Method: 采用不同成本目标（$\avgavg$、$\avgmax$、$\maxavg$、$\maxmax$）分析确定性和随机机制的失真界限。

Result: 在确定性机制中，改进了多个失真界限；在随机机制中，提出了两种设置下的紧界或接近紧界的界限。

Conclusion: 研究提供了分布式投票中度量失真的近乎完整特征描述，改进了现有的理论界限。

Abstract: We study metric distortion in distributed voting, where $n$ voters are
partitioned into $k$ groups, each selecting a local representative, and a final
winner is chosen from these representatives (or from the entire set of
candidates). This setting models systems like U.S. presidential elections,
where state-level decisions determine the national outcome. We focus on four
cost objectives from \citep{anshelevich2022distortion}: $\avgavg$, $\avgmax$,
$\maxavg$, and $\maxmax$. We present improved distortion bounds for both
deterministic and randomized mechanisms, offering a near-complete
characterization of distortion in this model.
  For deterministic mechanisms, we reduce the upper bound for $\avgmax$ from
$11$ to $7$, establish a tight lower bound of $5$ for $\maxavg$ (improving on
$2+\sqrt{5}$), and tighten the upper bound for $\maxmax$ from $5$ to $3$.
  For randomized mechanisms, we consider two settings: (i) only the second
stage is randomized, and (ii) both stages may be randomized. In case (i), we
prove tight bounds: $5\!-\!2/k$ for $\avgavg$, $3$ for $\avgmax$ and $\maxmax$,
and $5$ for $\maxavg$. In case (ii), we show tight bounds of $3$ for $\maxavg$
and $\maxmax$, and nearly tight bounds for $\avgavg$ and $\avgmax$ within
$[3\!-\!2/n,\ 3\!-\!2/(kn^*)]$ and $[3\!-\!2/n,\ 3]$, respectively, where $n^*$
denotes the largest group size.

</details>
