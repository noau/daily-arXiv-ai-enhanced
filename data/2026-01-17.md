<div id=toc></div>

# Table of Contents

- [cs.GR](#cs.GR) [Total: 1]
- [cs.PL](#cs.PL) [Total: 3]
- [cs.GT](#cs.GT) [Total: 2]


<div id='cs.GR'></div>

# cs.GR [[Back]](#toc)

### [1] [Extrinsic Vector Field Processing](https://arxiv.org/abs/2601.10621)
*Hongyi Liu,Oded Stein,Amir Vaxman,Mirela Ben-Chen,Misha Kazhdan*

Main category: cs.GR

TL;DR: 提出了一种新的三角形网格切线矢量场离散化方法，通过Phong映射定义连续切线矢量场的外在基，并利用Rodrigues旋转将顶点处的切线矢量传输到三角形内部。


<details>
  <summary>Details</summary>
Motivation: 为了改进三角形网格上矢量场处理的离散化方法，提出一种连续的、弱可微的切线矢量场定义方式，以支持协变导数和标准算子的计算。

Method: 使用Phong映射和Rodrigues旋转技术定义连续切线矢量场的外在基，并通过协变导数的分解定义Hodge Laplacian能量、Connection Laplacian能量和Killing能量等标准算子。

Result: 该方法能够支持协变导数的点式评估，并定义Lie括号，从而实现矢量场的有效处理和分析。

Conclusion: 提出的离散化方法为三角形网格上的矢量场处理提供了一种有效的工具，支持多种标准算子的计算和应用。

Abstract: We propose a novel discretization of tangent vector fields for triangle meshes. Starting with a Phong map continuously assigning normals to all points on the mesh, we define an extrinsic bases for continuous tangent vector fields by using the Rodrigues rotation to transport tangent vectors assigned to vertices to tangent vectors in the interiors of the triangles. As our vector fields are continuous and weakly differentiable, we can use them to define a covariant derivative field that is evaluatable almost-everywhere on the mesh. Decomposing the covariant derivative in terms of diagonal multiple of the identity, anti-symmetric, and trace-less symmetric components, we can define the standard operators used for vector field processing including the Hodge Laplacian energy, Connection Laplacian energy, and Killing energy. Additionally, the ability to perform point-wise evaluation of the covariant derivative also makes it possible for us to define the Lie bracket.

</details>


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [2] [From Dynamic to Lexical: A Comparative Exploration of Scoping Rules in SAS and R](https://arxiv.org/abs/2601.09808)
*Chen Ling,Yachen Wang*

Main category: cs.PL

TL;DR: 本文探讨了SAS和R中的变量作用域规则，重点比较了SAS的动态作用域和R的静态作用域，并通过示例展示了它们对代码行为的影响。


<details>
  <summary>Details</summary>
Motivation: 研究SAS和R中不同的变量作用域规则，以提高代码效率和组织的理解与实践。

Method: 通过分析SAS的动态作用域（使用符号表）和R的静态作用域（使用环境），并结合示例展示其差异。

Result: 展示了两种作用域规则对代码行为的影响，并提供了调试和优化的实用方法。

Conclusion: 本文为程序员提供了优化变量管理的工具和方法，提升了在SAS和R中编程的精度和可靠性。

Abstract: Variable scoping dictates how and where variables are accessible within programming languages, playing a crucial role in code efficiency and organization. This paper examines the distinct scoping rules in SAS and R, focusing on SAS's dynamic scoping and R's lexical scoping. In SAS, dynamic scoping utilizes symbol tables, resolving variables at runtime by dynamically searching through active macro layers. R, in contrast, employs lexical scoping, using environments to resolve variables based on the structure in which functions are defined. Illustrative examples highlight the differences between these scoping strategies, showcasing their impact on code behavior. Additionally, the paper outlines methods for inspecting variables in SAS's symbol tables and R's environments, offering practical insights for debugging and optimization. Strategies for controlling variable scope in both languages are discussed, enhancing code precision and reliability. This exploration equips programmers with critical understanding to optimize variable management, improving their programming practices in SAS and R.

</details>


### [3] [Lazy Evaluation: A Comparative Analysis of SAS MACROs and R Functions](https://arxiv.org/abs/2601.09839)
*Chen Ling,Yachen Wang*

Main category: cs.PL

TL;DR: 本文比较了SAS MACROs和R函数中的惰性求值机制，探讨了其对编程效率的影响。


<details>
  <summary>Details</summary>
Motivation: 研究SAS和R中惰性求值的差异，以帮助程序员优化代码，特别是在制药行业从SAS转向R的趋势下。

Method: 分析了R的Promise数据结构和SAS的符号表机制，并比较了它们的惰性求值策略（call-by-need与call-by-name）。

Result: 展示了R和SAS惰性求值策略的不同如何影响编程效率，并通过示例进行了说明。

Conclusion: 理解这些惰性求值技术有助于程序员在SAS和R中优化代码，提高效率和性能。

Abstract: Lazy evaluation is a powerful technique that can optimize code execution by deferring evaluations until their results are required, thus enhancing efficiency. In most modern programming languages, like R, lazy evaluation is commonly applied to function arguments. However, the application of lazy evaluation in SAS has not been extensively explored. This paper focuses on the mechanisms of lazy evaluation in SAS MACROs and R functions, offering a comparative analysis of the underlying principles that drive these processes.
  R's lazy evaluation is driven by a data structure called Promise, which postpones evaluation and does not occupy memory until the value is needed, utilizing a call-by-need strategy. SAS, on the other hand, achieves lazy evaluation through its symbol tables, employing memory to store parameters, and operates on a call-by-name basis. These discrepancies in lazy evaluation strategies can notably impact the results of R functions and SAS MACROs. By examining these distinct approaches, the paper illuminates the impact of lazy evaluation on programming efficiency, supported by illustrative examples. As the shift from SAS to R becomes increasingly prevalent in the pharmaceutical industry, understanding these techniques enables programmers to optimize their code for greater efficacy. This exploration serves as a guide to enhance programming capabilities and performance in both languages.

</details>


### [4] [Outrunning Big KATs: Efficient Decision Procedures for Variants of GKAT](https://arxiv.org/abs/2601.09986)
*Cheng Zhang,Qiancheng Fu,Hang Ji,Ines Santacruz Del Valle,Alexandra Silva,Marco Gaboardi*

Main category: cs.PL

TL;DR: 本文提出了几种高效的GKAT自动机的迹等价决策程序，利用SAT求解器的即时符号化技术，并在Rust中实现，展示了相对于现有KAT和CF-GKAT实现的性能显著提升。


<details>
  <summary>Details</summary>
Motivation: 研究动机是为了提高GKAT自动机迹等价验证的效率，并通过实际应用验证算法的实用性，特别是在控制流变换验证领域。

Method: 方法包括开发基于SAT求解器的符号化技术，设计CF-GKAT的符号导数，并在Rust中实现算法。

Result: 实验结果表明，相对于现有的KAT和CF-GKAT实现，性能有了数量级的提升，并发现了Ghidra反编译器中的一个错误，证明了系统的实用性。

Conclusion: 结论是该方法不仅在理论上可行，还能在实际应用中显著提升性能并发现潜在问题，突出了其实际价值。

Abstract: This paper presents several efficient decision procedures for trace equivalence of GKAT automata, which make use of on-the-fly symbolic techniques via SAT solvers. To demonstrate applicability of our algorithms, we designed symbolic derivatives for CF-GKAT, a practical system based on GKAT designed to validate control-flow transformations. We implemented the algorithms in Rust and evaluated them on both randomly generated benchmarks and real-world control-flow transformations. Indeed, we observed order-of-magnitude performance improvements against existing implementations for both KAT and CF-GKAT. Notably, our experiments also revealed a bug in Ghidra, an industry-standard decompiler, highlighting the practical viability of these systems.

</details>


<div id='cs.GT'></div>

# cs.GT [[Back]](#toc)

### [5] [A Control Theoretic Approach to Decentralized AI Economy Stabilization via Dynamic Buyback-and-Burn Mechanisms](https://arxiv.org/abs/2601.09961)
*Zehua Cheng,Wei Dai,Zhipeng Wang,Rui Sun,Nick Wen,Jiahao Sun*

Main category: cs.GT

TL;DR: 本文提出了一种动态控制回购机制（DCBM），通过PID控制器和严格偿付约束管理代币经济，显著降低了波动性和运营商流失率，为去中心化智能网络的可持续发展提供了解决方案。


<details>
  <summary>Details</summary>
Motivation: 当前代币经济模型依赖静态或阈值回购启发式方法，无法应对复杂系统动态，且在市场低迷时会加剧不稳定性，因此需要一种更有效的调控机制。

Method: 提出动态控制回购机制（DCBM），采用PID控制理论和严格偿付约束框架，对代币经济进行动态系统化管理，并通过基于代理的模拟验证其有效性。

Result: 实验表明，DCBM在高度波动环境中显著降低了代币价格波动（约66%）和运营商流失率（从19.5%降至8.1%），优于静态模型。

Conclusion: 将代币经济规则从静态转换为连续且结构受限的控制循环，是实现安全、可持续去中心化智能网络的必要条件。

Abstract: The democratization of artificial intelligence through decentralized networks represents a paradigm shift in computational provisioning, yet the long-term viability of these ecosystems is critically endangered by the extreme volatility of their native economic layers. Current tokenomic models, which predominantly rely on static or threshold-based buyback heuristics, are ill-equipped to handle complex system dynamics and often function pro-cyclically, exacerbating instability during market downturns. To bridge this gap, we propose the Dynamic-Control Buyback Mechanism (DCBM), a formalized control-theoretic framework that utilizes a Proportional-Integral-Derivative (PID) controller with strict solvency constraints to regulate the token economy as a dynamical system. Extensive agent-based simulations utilizing Jump-Diffusion processes demonstrate that DCBM fundamentally outperforms static baselines, reducing token price volatility by approximately 66% and lowering operator churn from 19.5% to 8.1% in high-volatility regimes. These findings establish that converting tokenomics from static rules into continuous, structurally constrained control loops is a necessary condition for secure and sustainable decentralized intelligence networks.

</details>


### [6] [Inverse Learning in $2\times2$ Games: From Synthetic Interactions to Traffic Simulation](https://arxiv.org/abs/2601.10367)
*Daniela Aguirre Salazar,Firas Moatemri,Tatiana Tatarenko*

Main category: cs.GT

TL;DR: 该论文研究了两种不同的逆向博弈理论学习方法，分别关注静态均衡一致性和动态行为现实性，并在合成游戏和交通场景中评估其性能。


<details>
  <summary>Details</summary>
Motivation: 理解多智能体系统（如交通和机器人）中智能体如何从有限的行为数据中协调或竞争，是建模战略互动的核心问题。

Method: 提出了两种方法：(i) 专为2×2游戏设计的闭式相关均衡最大似然估计器（CE-ML），(ii) 通过随机响应过程捕捉长期适应动态的Logit最佳响应最大似然估计器（LBR-ML）。

Result: 在合成游戏和SUMO模拟的交通场景中评估了这两种方法，发现了模型在可解释性、计算可行性和行为表达性之间的明显权衡。

Conclusion: 研究表明，CE-ML和LBR-ML在静态均衡和动态行为现实性之间提供了一个互补的视角，揭示了不同模型的适用性和局限性。

Abstract: Understanding how agents coordinate or compete from limited behavioral data is central to modeling strategic interactions in traffic, robotics, and other multi-agent systems. In this work, we investigate the following complementary formulations of inverse game-theoretic learning: (i) a Closed-form Correlated Equilibrium Maximum-Likelihood estimator (CE-ML) specialized for $2\times2$ games; and (ii) a Logit Best Response Maximum-Likelihood estimator (LBR-ML) that captures long-run adaptation dynamics via stochastic response processes. Together, these approaches span the spectrum between static equilibrium consistency and dynamic behavioral realism. We evaluate them on synthetic "chicken-dare" games and traffic-interaction scenarios simulated in SUMO, comparing parameter recovery and distributional fit. Results reveal clear trade-offs between interpretability, computational tractability, and behavioral expressiveness across models.

</details>
