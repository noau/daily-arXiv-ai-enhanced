{"id": "2509.08855", "categories": ["cs.GR", "cs.CG"], "pdf": "https://arxiv.org/pdf/2509.08855", "abs": "https://arxiv.org/abs/2509.08855", "authors": ["Mahmoud Shaqfa"], "title": "Morphology-Preserving Remeshing Approach to Particulate Microstructures via Harmonic Decomposition", "comment": null, "summary": "Harmonic decomposition of surfaces, such as spherical and spheroidal\nharmonics, is used to analyze morphology, reconstruct, and generate surface\ninclusions of particulate microstructures. However, obtaining high-quality\nmeshes of engineering microstructures using these approaches remains an open\nquestion. In harmonic approaches, we usually reconstruct surfaces by evaluating\nthe harmonic bases on equidistantly sampled simplicial complexes of the base\ndomains (e.g., triangular spheroids and disks). However, this traditional\nsampling does not account for local changes in the Jacobian of the basis\nfunctions, resulting in nonuniform discretization after reconstruction or\ngeneration. As it impacts the accuracy and time step, high-quality\ndiscretization of microstructures is crucial for efficient numerical\nsimulations (e.g., finite element and discrete element methods). To circumvent\nthis issue, we propose an efficient hierarchical diffusion-based approach for\nresampling the surface-i.e., performing a reparameterization-to yield an\nequalized mesh triangulation. Analogous to heat problems, we use nonlinear\ndiffusion to resample the curvilinear coordinates of the analysis domain,\nthereby enlarging small triangles at the expense of large triangles on\nsurfaces. We tested isotropic and anisotropic diffusion schemes on the recent\nspheroidal and hemispheroidal harmonics methods. The results show a substantial\nimprovement in the quality metrics for surface triangulation. Unlike\ntraditional surface reconstruction and meshing techniques, this approach\npreserves surface morphology, along with the areas and volumes of surfaces. We\ndiscuss the results and the associated computational costs for large 2D and 3D\nmicrostructures, such as digital twins of concrete and stone masonry, and their\nfuture applications."}
{"id": "2509.08947", "categories": ["cs.GR", "cs.CV"], "pdf": "https://arxiv.org/pdf/2509.08947", "abs": "https://arxiv.org/abs/2509.08947", "authors": ["Yancheng Cai", "Robert Wanat", "Rafal Mantiuk"], "title": "CameraVDP: Perceptual Display Assessment with Uncertainty Estimation via Camera and Visual Difference Prediction", "comment": "Accepted by SIGGRAPH Asia 2025", "summary": "Accurate measurement of images produced by electronic displays is critical\nfor the evaluation of both traditional and computational displays. Traditional\ndisplay measurement methods based on sparse radiometric sampling and fitting a\nmodel are inadequate for capturing spatially varying display artifacts, as they\nfail to capture high-frequency and pixel-level distortions. While cameras offer\nsufficient spatial resolution, they introduce optical, sampling, and\nphotometric distortions. Furthermore, the physical measurement must be combined\nwith a model of a visual system to assess whether the distortions are going to\nbe visible. To enable perceptual assessment of displays, we propose a\ncombination of a camera-based reconstruction pipeline with a visual difference\npredictor, which account for both the inaccuracy of camera measurements and\nvisual difference prediction. The reconstruction pipeline combines HDR image\nstacking, MTF inversion, vignetting correction, geometric undistortion,\nhomography transformation, and color correction, enabling cameras to function\nas precise display measurement instruments. By incorporating a Visual\nDifference Predictor (VDP), our system models the visibility of various stimuli\nunder different viewing conditions for the human visual system. We validate the\nproposed CameraVDP framework through three applications: defective pixel\ndetection, color fringing awareness, and display non-uniformity evaluation. Our\nuncertainty analysis framework enables the estimation of the theoretical upper\nbound for defect pixel detection performance and provides confidence intervals\nfor VDP quality scores."}
{"id": "2509.09019", "categories": ["cs.PL"], "pdf": "https://arxiv.org/pdf/2509.09019", "abs": "https://arxiv.org/abs/2509.09019", "authors": ["Mohit Tekriwal", "John Sarracino"], "title": "Towards Verified Compilation of Floating-point Optimization in Scientific Computing Programs", "comment": null, "summary": "Scientific computing programs often undergo aggressive compiler optimization\nto achieve high performance and efficient resource utilization. While\nperformance is critical, we also need to ensure that these optimizations are\ncorrect. In this paper, we focus on a specific class of optimizations,\nfloating-point optimizations, notably due to fast math, at the LLVM IR level.\nWe present a preliminary work, which leverages the Verified LLVM framework in\nthe Rocq theorem prover, to prove the correctness of Fused-Multiply-Add (FMA)\noptimization for a basic block implementing the arithmetic expression $a * b +\nc$ . We then propose ways to extend this preliminary results by adding more\nprogram features and fast math floating-point optimizations."}
{"id": "2509.08976", "categories": ["cs.GT", "cs.ET", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2509.08976", "abs": "https://arxiv.org/abs/2509.08976", "authors": ["Ya-Ting Yang", "Quanyan Zhu"], "title": "Toward a Multi-Echelon Cyber Warfare Theory: A Meta-Game-Theoretic Paradigm for Defense and Dominance", "comment": null, "summary": "Cyber warfare has become a central element of modern conflict, especially\nwithin multi-domain operations. As both a distinct and critical domain, cyber\nwarfare requires integrating defensive and offensive technologies into coherent\nstrategies. While prior research has emphasized isolated tactics or fragmented\ntechnologies, a holistic understanding is essential for effective resource\ndeployment and risk mitigation. Game theory offers a unifying framework for\nthis purpose. It not only models attacker-defender interactions but also\nprovides quantitative tools for equilibrium analysis, risk assessment, and\nstrategic reasoning. Integrated with modern AI techniques, game-theoretic\nmodels enable the design and optimization of strategies across multiple levels\nof cyber warfare, from policy and strategy to operations, tactics, and\ntechnical implementations. These models capture the paradoxical logic of\nconflict, where more resources do not always translate into greater advantage,\nand where nonlinear dynamics govern outcomes. To illustrate the approach, this\nchapter examines RedCyber, a synthetic cyber conflict, demonstrating how\ngame-theoretic methods capture the interdependencies of cyber operations. The\nchapter concludes with directions for future research on resilience,\ncros-echelon planning, and the evolving role of AI in cyber warfare."}
{"id": "2509.09059", "categories": ["cs.PL"], "pdf": "https://arxiv.org/pdf/2509.09059", "abs": "https://arxiv.org/abs/2509.09059", "authors": ["Paulette Koronkevich", "William J. Bowman"], "title": "Dependent-Type-Preserving Memory Allocation", "comment": "Submitted and received second place at the Student Research\n  Competition at Principles of Programming Languages 2022", "summary": "Dependently typed programming languages such as Coq, Agda, Idris, and F*,\nallow programmers to write detailed specifications of their programs and prove\ntheir programs meet these specifications. However, these specifications can be\nviolated during compilation since they are erased after type checking. External\nprograms linked with the compiled program can violate the specifications of the\noriginal program and change the behavior of the compiled program -- even when\ncompiled with a verified compiler. For example, since Coq does not allow\nexplicitly allocating memory, a programmer might link their Coq program with a\nC program that can allocate memory. Even if the Coq program is compiled with a\nverified compiler, the external C program can still violate the memory-safe\nspecification of the Coq program by providing an uninitialized pointer to\nmemory. This error could be ruled out by type checking in a language expressive\nenough to indicate whether memory is initialized versus uninitialized. Linking\nwith a program with an uninitialized pointer could be considered ill-typed, and\nour linking process could prevent linking with ill-typed programs. To\nfacilitate type checking during linking, we can use type-preserving\ncompilation, which preserves the types through the compilation process. In this\nongoing work, we develop a typed intermediate language that supports dependent\nmemory allocation, as well as a dependent-type-preserving compiler pass for\nmemory allocation."}
{"id": "2509.09099", "categories": ["cs.GT", "econ.TH"], "pdf": "https://arxiv.org/pdf/2509.09099", "abs": "https://arxiv.org/abs/2509.09099", "authors": ["Toygar T. Kerman", "Anastas P. Tenev", "Konstantin Zabarnyi"], "title": "Persuasion Gains and Losses from Peer Communication", "comment": null, "summary": "We study a Bayesian persuasion setting in which a sender wants to persuade a\ncritical mass of receivers by revealing partial information about the state to\nthem. The homogeneous binary-action receivers are located on a communication\nnetwork, and each observes the private messages sent to them and their\nimmediate neighbors. We examine how the sender's expected utility varies with\nincreased communication among receivers. We show that for general families of\nnetworks, extending the network can strictly benefit the sender. Thus, the\nsender's gain from persuasion is not monotonic in network density. Moreover,\nmany network extensions can achieve the upper bound on the sender's expected\nutility among all networks, which corresponds to the payoff in an empty\nnetwork. This is the case in networks reflecting a clear informational\nhierarchy (e.g., in global corporations), as well as in decentralized networks\nin which information originates from multiple sources (e.g., influencers in\nsocial media). Finally, we show that a slight modification to the structure of\nsome of these networks precludes the possibility of such beneficial extensions.\nOverall, our results caution against presuming that more communication\nnecessarily leads to better collective outcomes."}
{"id": "2509.09561", "categories": ["cs.GT"], "pdf": "https://arxiv.org/pdf/2509.09561", "abs": "https://arxiv.org/abs/2509.09561", "authors": ["Argyrios Deligkas", "Eduard Eiben", "Sophie Klumper", "Guido Schäfer", "Artem Tsikiridis"], "title": "Mechanism Design with Outliers and Predictions", "comment": null, "summary": "We initiate the study of mechanism design with outliers, where the designer\ncan discard $z$ agents from the social cost objective. This setting is\nparticularly relevant when some agents exhibit extreme or atypical preferences.\nAs a natural case study, we consider facility location on the line: $n$\nstrategic agents report their preferred locations, and a mechanism places a\nfacility to minimize a social cost function. In our setting, the $z$ agents\nfarthest from the chosen facility are excluded from the social cost. While it\nmay seem intuitive that discarding outliers improves efficiency, our results\nreveal that the opposite can hold.\n  We derive tight bounds for deterministic strategyproof mechanisms under the\ntwo most-studied objectives: utilitarian and egalitarian social cost. Our\nresults offer a comprehensive view of the impact of outliers. We first show\nthat when $z \\ge n/2$, no strategyproof mechanism can achieve a bounded\napproximation for either objective. For egalitarian cost, selecting the $(z +\n1)$-th order statistic is strategyproof and 2-approximate. In fact, we show\nthat this is best possible by providing a matching lower bound. Notably, this\nlower bound of 2 persists even when the mechanism has access to a prediction of\nthe optimal location, in stark contrast to the setting without outliers. For\nutilitarian cost, we show that strategyproof mechanisms cannot effectively\nexploit outliers, leading to the counterintuitive outcome that approximation\nguarantees worsen as the number of outliers increases. However, in this case,\naccess to a prediction allows us to design a strategyproof mechanism achieving\nthe best possible trade-off between consistency and robustness. Finally, we\nalso establish lower bounds for randomized mechanisms that are truthful in\nexpectation."}
{"id": "2509.09641", "categories": ["cs.GT", "cs.DS"], "pdf": "https://arxiv.org/pdf/2509.09641", "abs": "https://arxiv.org/abs/2509.09641", "authors": ["Jiaxuan Ma", "Yong Chen", "Guangting Chen", "Mingyang Gong", "Guohui Lin", "An Zhang"], "title": "Maximizing social welfare among EF1 allocations at the presence of two types of agents", "comment": "A shorter version appears in ISAAC 2025; 20 pages in this full\n  version", "summary": "We study the fair allocation of indivisible items to $n$ agents to maximize\nthe utilitarian social welfare, where the fairness criterion is envy-free up to\none item and there are only two different utility functions shared by the\nagents. We present a $2$-approximation algorithm when the two utility functions\nare normalized, improving the previous best ratio of $16 \\sqrt{n}$ shown for\ngeneral normalized utility functions; thus this constant ratio approximation\nalgorithm confirms the APX-completeness in this special case previously shown\nAPX-hard. When there are only three agents, i.e., $n = 3$, the previous best\nratio is $3$ shown for general utility functions, and we present an improved\nand tight $\\frac 53$-approximation algorithm when the two utility functions are\nnormalized, and a best possible and tight $2$-approximation algorithm when the\ntwo utility functions are unnormalized."}
