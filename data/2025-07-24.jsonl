{"id": "2507.16869", "categories": ["cs.GR", "cs.CV"], "pdf": "https://arxiv.org/pdf/2507.16869", "abs": "https://arxiv.org/abs/2507.16869", "authors": ["Yue Ma", "Kunyu Feng", "Zhongyuan Hu", "Xinyu Wang", "Yucheng Wang", "Mingzhe Zheng", "Xuanhua He", "Chenyang Zhu", "Hongyu Liu", "Yingqing He", "Zeyu Wang", "Zhifeng Li", "Xiu Li", "Wei Liu", "Dan Xu", "Linfeng Zhang", "Qifeng Chen"], "title": "Controllable Video Generation: A Survey", "comment": "project page:\n  https://github.com/mayuelala/Awesome-Controllable-Video-Generation", "summary": "With the rapid development of AI-generated content (AIGC), video generation\nhas emerged as one of its most dynamic and impactful subfields. In particular,\nthe advancement of video generation foundation models has led to growing demand\nfor controllable video generation methods that can more accurately reflect user\nintent. Most existing foundation models are designed for text-to-video\ngeneration, where text prompts alone are often insufficient to express complex,\nmulti-modal, and fine-grained user requirements. This limitation makes it\nchallenging for users to generate videos with precise control using current\nmodels. To address this issue, recent research has explored the integration of\nadditional non-textual conditions, such as camera motion, depth maps, and human\npose, to extend pretrained video generation models and enable more controllable\nvideo synthesis. These approaches aim to enhance the flexibility and practical\napplicability of AIGC-driven video generation systems. In this survey, we\nprovide a systematic review of controllable video generation, covering both\ntheoretical foundations and recent advances in the field. We begin by\nintroducing the key concepts and commonly used open-source video generation\nmodels. We then focus on control mechanisms in video diffusion models,\nanalyzing how different types of conditions can be incorporated into the\ndenoising process to guide generation. Finally, we categorize existing methods\nbased on the types of control signals they leverage, including single-condition\ngeneration, multi-condition generation, and universal controllable generation.\nFor a complete list of the literature on controllable video generation\nreviewed, please visit our curated repository at\nhttps://github.com/mayuelala/Awesome-Controllable-Video-Generation."}
{"id": "2507.17029", "categories": ["cs.GR", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2507.17029", "abs": "https://arxiv.org/abs/2507.17029", "authors": ["Luchuan Song", "Yang Zhou", "Zhan Xu", "Yi Zhou", "Deepali Aneja", "Chenliang Xu"], "title": "StreamME: Simplify 3D Gaussian Avatar within Live Stream", "comment": "12 pages, 15 Figures", "summary": "We propose StreamME, a method focuses on fast 3D avatar reconstruction. The\nStreamME synchronously records and reconstructs a head avatar from live video\nstreams without any pre-cached data, enabling seamless integration of the\nreconstructed appearance into downstream applications. This exceptionally fast\ntraining strategy, which we refer to as on-the-fly training, is central to our\napproach. Our method is built upon 3D Gaussian Splatting (3DGS), eliminating\nthe reliance on MLPs in deformable 3DGS and relying solely on geometry, which\nsignificantly improves the adaptation speed to facial expression. To further\nensure high efficiency in on-the-fly training, we introduced a simplification\nstrategy based on primary points, which distributes the point clouds more\nsparsely across the facial surface, optimizing points number while maintaining\nrendering quality. Leveraging the on-the-fly training capabilities, our method\nprotects the facial privacy and reduces communication bandwidth in VR system or\nonline conference. Additionally, it can be directly applied to downstream\napplication such as animation, toonify, and relighting. Please refer to our\nproject page for more details: https://songluchuan.github.io/StreamME/."}
{"id": "2507.17174", "categories": ["cs.GR", "cs.HC", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.17174", "abs": "https://arxiv.org/abs/2507.17174", "authors": ["Myeongwon Jung", "Takanori Fujiwara", "Jaemin Jo"], "title": "GhostUMAP2: Measuring and Analyzing (r,d)-Stability of UMAP", "comment": null, "summary": "Despite the widespread use of Uniform Manifold Approximation and Projection\n(UMAP), the impact of its stochastic optimization process on the results\nremains underexplored. We observed that it often produces unstable results\nwhere the projections of data points are determined mostly by chance rather\nthan reflecting neighboring structures. To address this limitation, we\nintroduce (r,d)-stability to UMAP: a framework that analyzes the stochastic\npositioning of data points in the projection space. To assess how stochastic\nelements, specifically initial projection positions and negative sampling,\nimpact UMAP results, we introduce \"ghosts\", or duplicates of data points\nrepresenting potential positional variations due to stochasticity. We define a\ndata point's projection as (r,d)-stable if its ghosts perturbed within a circle\nof radius r in the initial projection remain confined within a circle of radius\nd for their final positions. To efficiently compute the ghost projections, we\ndevelop an adaptive dropping scheme that reduces a runtime up to 60% compared\nto an unoptimized baseline while maintaining approximately 90% of unstable\npoints. We also present a visualization tool that supports the interactive\nexploration of the (r,d)-stability of data points. Finally, we demonstrate the\neffectiveness of our framework by examining the stability of projections of\nreal-world datasets and present usage guidelines for the effective use of our\nframework."}
{"id": "2507.17184", "categories": ["cs.GR", "cs.NA", "math.NA"], "pdf": "https://arxiv.org/pdf/2507.17184", "abs": "https://arxiv.org/abs/2507.17184", "authors": ["Hui Zhao"], "title": "A Scientist Question: Research on the Impact of Super Structured Quadrilateral Meshes on Convergence and Accuracy of Finite Element Analysis", "comment": "in Chinese and English", "summary": "In the current practices of both industry and academia, the convergence and\naccuracy of finite element calculations are closely related to the methods and\nquality of mesh generation. For years, the research on high-quality mesh\ngeneration in the domestic academic field has mainly referred to the local\nquality of quadrilaterals and hexahedrons approximating that of squares and\ncubes. The main contribution of this paper is to propose a brand-new research\ndirection and content: it is necessary to explore and study the influence of\nthe overall global arrangement structure and pattern of super structured\nquadrilateral meshes on the convergence and calculation accuracy of finite\nelement calculations. Through the research in this new field, it can help solve\nthe non-rigorous state of serious reliance on \"experience\" in the mesh\ngeneration stage during simulation in the current industry and academia, and\nmake clear judgments on which global arrangements of mesh generation can ensure\nthe convergence of finite element calculations. In order to generate and design\nsuper-structured quadrilateral meshes with controllable overall arrangement\nstructures, a large number of modern two-dimensional and three-dimensional\ngeometric topology theories are required, such as moduli space, Teichm\\\"uller\nspace, harmonic foliations, dynamical systems, surface mappings, meromorphic\nquadratic differentials, surface mappings, etc."}
{"id": "2507.17233", "categories": ["cs.PL"], "pdf": "https://arxiv.org/pdf/2507.17233", "abs": "https://arxiv.org/abs/2507.17233", "authors": ["Marco Ciccalè", "Daniel Jurjo-Rivas", "Jose F. Morales", "Pedro López-García", "Manuel V. Hermenegildo"], "title": "Hiord: An Approach to the Specification and Verification of Higher-Order (C)LP Programs", "comment": "Accepted for publication in Theory and Practice of Logic Programming\n  (TPLP)", "summary": "Higher-order constructs enable more expressive and concise code by allowing\nprocedures to be parameterized by other procedures. Assertions allow expressing\npartial program specifications, which can be verified either at compile time\n(statically) or run time (dynamically). In higher-order programs, assertions\ncan also describe higher-order arguments. While in the context of (C)LP,\nrun-time verification of higher-order assertions has received some attention,\ncompile-time verification remains relatively unexplored. We propose a novel\napproach for statically verifying higher-order (C)LP programs with higher-order\nassertions. Although we use the Ciao assertion language for illustration, our\napproach is quite general and we believe is applicable to similar contexts.\nHigher-order arguments are described using predicate properties -- a special\nkind of property which exploits the (Ciao) assertion language. We refine the\nsyntax and semantics of these properties and introduce an abstract criterion to\ndetermine conformance to a predicate property at compile time, based on a\nsemantic order relation comparing the predicate property with the predicate\nassertions. We then show how to handle these properties using an abstract\ninterpretation-based static analyzer for programs with first-order assertions\nby reducing predicate properties to first-order properties. Finally, we report\non a prototype implementation and evaluate it through various examples within\nthe Ciao system."}
{"id": "2507.17183", "categories": ["cs.GT", "cs.AI", "cs.MA"], "pdf": "https://arxiv.org/pdf/2507.17183", "abs": "https://arxiv.org/abs/2507.17183", "authors": ["Die Hu", "Shuyue Hu", "Chunjiang Mu", "Shiqi Fan", "Chen Chu", "Jinzhuo Liu", "Zhen Wang"], "title": "Regret Minimization in Population Network Games: Vanishing Heterogeneity and Convergence to Equilibria", "comment": null, "summary": "Understanding and predicting the behavior of large-scale multi-agents in\ngames remains a fundamental challenge in multi-agent systems. This paper\nexamines the role of heterogeneity in equilibrium formation by analyzing how\nsmooth regret-matching drives a large number of heterogeneous agents with\ndiverse initial policies toward unified behavior. By modeling the system state\nas a probability distribution of regrets and analyzing its evolution through\nthe continuity equation, we uncover a key phenomenon in diverse multi-agent\nsettings: the variance of the regret distribution diminishes over time, leading\nto the disappearance of heterogeneity and the emergence of consensus among\nagents. This universal result enables us to prove convergence to quantal\nresponse equilibria in both competitive and cooperative multi-agent settings.\nOur work advances the theoretical understanding of multi-agent learning and\noffers a novel perspective on equilibrium selection in diverse game-theoretic\nscenarios."}
{"id": "2507.17265", "categories": ["cs.GR", "cs.HC"], "pdf": "https://arxiv.org/pdf/2507.17265", "abs": "https://arxiv.org/abs/2507.17265", "authors": ["Xin Chen", "Yunhai Wang", "Huaiwei Bao", "Kecheng Lu", "Jaemin Jo", "Chi-Wing Fu", "Jean-Daniel Fekete"], "title": "Visualization-Driven Illumination for Density Plots", "comment": null, "summary": "We present a novel visualization-driven illumination model for density plots,\na new technique to enhance density plots by effectively revealing the detailed\nstructures in high- and medium-density regions and outliers in low-density\nregions, while avoiding artifacts in the density field's colors. When\nvisualizing large and dense discrete point samples, scatterplots and dot\ndensity maps often suffer from overplotting, and density plots are commonly\nemployed to provide aggregated views while revealing underlying structures.\nYet, in such density plots, existing illumination models may produce color\ndistortion and hide details in low-density regions, making it challenging to\nlook up density values, compare them, and find outliers. The key novelty in\nthis work includes (i) a visualization-driven illumination model that\ninherently supports density-plot-specific analysis tasks and (ii) a new image\ncomposition technique to reduce the interference between the image shading and\nthe color-encoded density values. To demonstrate the effectiveness of our\ntechnique, we conducted a quantitative study, an empirical evaluation of our\ntechnique in a controlled study, and two case studies, exploring twelve\ndatasets with up to two million data point samples."}
{"id": "2507.17336", "categories": ["cs.GR"], "pdf": "https://arxiv.org/pdf/2507.17336", "abs": "https://arxiv.org/abs/2507.17336", "authors": ["Hyeongmin Lee", "Kyungjune Baek"], "title": "Temporal Smoothness-Aware Rate-Distortion Optimized 4D Gaussian Splatting", "comment": "21 pages, 10 figures", "summary": "Dynamic 4D Gaussian Splatting (4DGS) effectively extends the high-speed\nrendering capabilities of 3D Gaussian Splatting (3DGS) to represent volumetric\nvideos. However, the large number of Gaussians, substantial temporal\nredundancies, and especially the absence of an entropy-aware compression\nframework result in large storage requirements. Consequently, this poses\nsignificant challenges for practical deployment, efficient edge-device\nprocessing, and data transmission. In this paper, we introduce a novel\nend-to-end RD-optimized compression framework tailored for 4DGS, aiming to\nenable flexible, high-fidelity rendering across varied computational platforms.\nLeveraging Fully Explicit Dynamic Gaussian Splatting (Ex4DGS), one of the\nstate-of-the-art 4DGS methods, as our baseline, we start from the existing 3DGS\ncompression methods for compatibility while effectively addressing additional\nchallenges introduced by the temporal axis. In particular, instead of storing\nmotion trajectories independently per point, we employ a wavelet transform to\nreflect the real-world smoothness prior, significantly enhancing storage\nefficiency. This approach yields significantly improved compression ratios and\nprovides a user-controlled balance between compression efficiency and rendering\nquality. Extensive experiments demonstrate the effectiveness of our method,\nachieving up to 91x compression compared to the original Ex4DGS model while\nmaintaining high visual fidelity. These results highlight the applicability of\nour framework for real-time dynamic scene rendering in diverse scenarios, from\nresource-constrained edge devices to high-performance environments."}
{"id": "2507.17440", "categories": ["cs.GR"], "pdf": "https://arxiv.org/pdf/2507.17440", "abs": "https://arxiv.org/abs/2507.17440", "authors": ["Christoph Schied", "Alexander Keller"], "title": "Parametric Integration with Neural Integral Operators", "comment": null, "summary": "Real-time rendering imposes strict limitations on the sampling budget for\nlight transport simulation, often resulting in noisy images. However, denoisers\nhave demonstrated that it is possible to produce noise-free images through\nfiltering. We enhance image quality by removing noise before material shading,\nrather than filtering already shaded noisy images. This approach allows for\nmaterial-agnostic denoising (MAD) and leverages machine learning by\napproximating the light transport integral operator with a neural network,\neffectively performing parametric integration with neural operators. Our method\noperates in real-time, requires data from only a single frame, seamlessly\nintegrates with existing denoisers and temporal anti-aliasing techniques, and\nis efficient to train. Additionally, it is straightforward to incorporate with\nphysically based rendering algorithms."}
