{"id": "2511.00835", "categories": ["cs.GT"], "pdf": "https://arxiv.org/pdf/2511.00835", "abs": "https://arxiv.org/abs/2511.00835", "authors": ["Taikun Zhu", "Kai Jin", "Ruixi Luo", "Song Cao"], "title": "Optimal Allocations under Strongly Pigou-Dalton Criteria: Hidden Layer Structure & Efficient Combinatorial Approach", "comment": null, "summary": "We investigate optimal social welfare allocations of $m$ items to $n$ agents\nwith binary additive or submodular valuations. For binary additive valuations,\nwe prove that the set of optimal allocations coincides with the set of\nso-called \\emph{stable allocations}, as long as the employed criterion for\nevaluating social welfare is strongly Pigou-Dalton (SPD) and symmetric. Many\ncommon criteria are SPD and symmetric, such as Nash social welfare, leximax,\nleximin, Gini index, entropy, and envy sum. We also design efficient algorithms\nfor finding a stable allocation, including an $O(m^2n)$ time algorithm for the\ncase of indivisible items, and an $O(m^2n^5)$ time one for the case of\ndivisible items. The first is faster than the existing algorithms or has a\nsimpler analysis. The latter is the first combinatorial algorithm for that\nproblem. It utilizes a hidden layer partition of items and agents admitted by\nall stable allocations, and cleverly reduces the case of divisible items to the\ncase of indivisible items.\n  In addition, we show that the profiles of different optimal allocations have\na small Chebyshev distance, which is 0 for the case of divisible items under\nbinary additive valuations, and is at most 1 for the case of indivisible items\nunder binary submodular valuations."}
{"id": "2511.00847", "categories": ["cs.GT", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.00847", "abs": "https://arxiv.org/abs/2511.00847", "authors": ["Yuhan Cao", "Yu Wang", "Sitong Liu", "Miao Li", "Yixin Tao", "Tianxing He"], "title": "Pay for The Second-Best Service: A Game-Theoretic Approach Against Dishonest LLM Providers", "comment": "13 pages, 4 figures", "summary": "The widespread adoption of Large Language Models (LLMs) through Application\nProgramming Interfaces (APIs) induces a critical vulnerability: the potential\nfor dishonest manipulation by service providers. This manipulation can manifest\nin various forms, such as secretly substituting a proclaimed high-performance\nmodel with a low-cost alternative, or inflating responses with meaningless\ntokens to increase billing. This work tackles the issue through the lens of\nalgorithmic game theory and mechanism design. We are the first to propose a\nformal economic model for a realistic user-provider ecosystem, where a user can\niteratively delegate $T$ queries to multiple model providers, and providers can\nengage in a range of strategic behaviors. As our central contribution, we prove\nthat for a continuous strategy space and any $\\epsilon\\in(0,\\frac12)$, there\nexists an approximate incentive-compatible mechanism with an additive\napproximation ratio of $O(T^{1-\\epsilon}\\log T)$, and a guaranteed quasi-linear\nsecond-best user utility. We also prove an impossibility result, stating that\nno mechanism can guarantee an expected user utility that is asymptotically\nbetter than our mechanism. Furthermore, we demonstrate the effectiveness of our\nmechanism in simulation experiments with real-world API settings."}
{"id": "2511.00986", "categories": ["cs.GT"], "pdf": "https://arxiv.org/pdf/2511.00986", "abs": "https://arxiv.org/abs/2511.00986", "authors": ["Kamesh Munagala", "Qilin Ye", "Ian Zhang"], "title": "Deliberation via Matching", "comment": null, "summary": "We study deliberative social choice, where voters refine their preferences\nthrough small-group discussions before collective aggregation. We introduce a\nsimple and easily implementable deliberation-via-matching protocol: for each\npair of candidates, we form an arbitrary maximum matching among voters who\ndisagree on that pair, and each matched pair deliberates. The resulting\npreferences (individual and deliberative) are then appropriately weighted and\naggregated using the weighted uncovered set tournament rule.\n  We show that our protocol has a tight distortion bound of $3$ within the\nmetric distortion framework. This breaks the previous lower bound of $3.11$ for\ntournament rules without deliberation and matches the lower bound for\ndeterministic social choice rules without deliberation. Our result conceptually\nshows that tournament rules are just as powerful as general social choice\nrules, when the former are given the minimal added power of pairwise\ndeliberations. We prove our bounds via a novel bilinear relaxation of the\nnon-linear program capturing optimal distortion, whose vertices we can\nexplicitly enumerate, leading to an analytic proof. Loosely speaking, our key\ntechnical insight is that the distortion objective, as a function of metric\ndistances to any three alternatives, is both supermodular and convex. We\nbelieve this characterization provides a general analytical framework for\nstudying the distortion of other deliberative protocols, and may be of\nindependent interest."}
{"id": "2511.01157", "categories": ["cs.GT", "econ.TH"], "pdf": "https://arxiv.org/pdf/2511.01157", "abs": "https://arxiv.org/abs/2511.01157", "authors": ["Ce Li", "Qianfan Zhang", "Weiqiang Zheng"], "title": "From Best Responses to Learning: Investment Efficiency in Dynamic Environment", "comment": null, "summary": "We study the welfare of a mechanism in a dynamic environment where a learning\ninvestor can make a costly investment to change her value. In many real-world\nproblems, the common assumption that the investor always makes the best\nresponses, i.e., choosing her utility-maximizing investment option, is\nunrealistic due to incomplete information in a dynamically evolving\nenvironment. To address this, we consider an investor who uses a no-regret\nonline learning algorithm to adaptively select investments through repeated\ninteractions with the environment. We analyze how the welfare guarantees of\napproximation allocation algorithms extend from static to dynamic settings when\nthe investor learns rather than best-responds, by studying the approximation\nratio for optimal welfare as a measurement of an algorithm's performance\nagainst different benchmarks in the dynamic learning environment. First, we\nshow that the approximation ratio in the static environment remains unchanged\nin the dynamic environment against the best-in-hindsight benchmark. Second, we\nprovide tight characterizations of the approximation upper and lower bounds\nrelative to a stronger time-varying benchmark. Bridging mechanism design with\nonline learning theory, our work shows how robust welfare guarantees can be\nmaintained even when an agent cannot make best responses but learns their\ninvestment strategies in complex, uncertain environments."}
{"id": "2511.00403", "categories": ["cs.PL"], "pdf": "https://arxiv.org/pdf/2511.00403", "abs": "https://arxiv.org/abs/2511.00403", "authors": ["Wentao Peng", "Ruyi Ji", "Yingfei Xiong"], "title": "Equality Saturation Guided by Large Language Models", "comment": "presented at EGRAPHS 2025", "summary": "One critical issue with large language models (LLMs) is their inability to\nguarantee correctness. Although this problem can be addressed by applying LLMs\nto formal rewrite systems, current LLMs are still far from adequate to generate\nsound rewrite chains. To bridge this gap, this paper proposes LLM-guided\nequality saturation, dubbed LGuess, by incorporating e-graphs as an\nintermediate layer between LLMs and rewrite systems. LGuess queries LLMs only\nfor high-level rewrite checkpoints and uses e-graphs to supply low-level\nrewrite chains between these checkpoints. The key technical challenge in this\nprocedure lies in effectively extracting a suitable checkpoint from a saturated\ne-graph, which LGuess addresses by learning a probabilistic model from the LLM.\nThe model predicts probable checkpoints while remaining simple enough for\neffective extraction. We implement a prototype of LGuess and evaluate it on the\nproblem of factorizing multivariable polynomials. The results demonstrate a\nsignificant advantage of LGuess compared to both straightforward equality\nsaturation and the approach that queries the LLM directly for the rewrite\nchain."}
{"id": "2511.00702", "categories": ["cs.GR", "cs.CV"], "pdf": "https://arxiv.org/pdf/2511.00702", "abs": "https://arxiv.org/abs/2511.00702", "authors": ["Alberto Di Biase"], "title": "Applying Medical Imaging Tractography Techniques to Painterly Rendering of Images", "comment": "Exploratory investigation applying medical imaging tractography\n  techniques to painterly image rendering. Code available at\n  https://github.com/tito21/st-python", "summary": "Doctors and researchers routinely use diffusion tensor imaging (DTI) and\ntractography to visualize the fibrous structure of tissues in the human body.\nThis paper explores the connection of these techniques to the painterly\nrendering of images. Using a tractography algorithm the presented method can\nplace brush strokes that mimic the painting process of human artists,\nanalogously to how fibres are tracked in DTI. The analogue to the diffusion\ntensor for image orientation is the structural tensor, which can provide better\nlocal orientation information than the gradient alone. I demonstrate this\ntechnique in portraits and general images, and discuss the parallels between\nfibre tracking and brush stroke placement, and frame it in the language of\ntractography. This work presents an exploratory investigation into the\ncross-domain application of diffusion tensor imaging techniques to painterly\nrendering of images. All the code is available at\nhttps://github.com/tito21/st-python"}
{"id": "2511.01421", "categories": ["cs.GT", "cs.MA"], "pdf": "https://arxiv.org/pdf/2511.01421", "abs": "https://arxiv.org/abs/2511.01421", "authors": ["Yusuf Saltan", "Jyun-Jhe Wang", "Arda Kosay", "Chung-Wei Lin", "Muhammed O. Sayin"], "title": "Designing Non-monetary Intersection Control Mechanisms for Efficient Selfish Routing", "comment": null, "summary": "Urban traffic congestion stems from the misalignment between self-interested\nrouting decisions and socially optimal flows. Intersections, as critical\nbottlenecks, amplify these inefficiencies because existing control schemes\noften neglect drivers' strategic behavior. Autonomous intersections, enabled by\nvehicle-to-infrastructure communication, permit vehicle-level scheduling based\non individual requests. Leveraging this fine-grained control, we propose a\nnon-monetary mechanism that strategically adjusts request timestamps-delaying\nor advancing passage times-to incentivize socially efficient routing. We\npresent a hierarchical architecture separating local scheduling by roadside\nunits from network-wide timestamp adjustments by a central planner. We\nestablish an experimentally validated analytical model, prove the existence and\nessential uniqueness of equilibrium flows and formulate the planner's problem\nas an offline bilevel optimization program solvable with standard tools.\nExperiments on the Sioux Falls network show up to a 68% reduction in the\nefficiency gap between equilibrium and optimal flows, demonstrating scalability\nand effectiveness."}
{"id": "2511.00488", "categories": ["cs.PL", "cs.CL"], "pdf": "https://arxiv.org/pdf/2511.00488", "abs": "https://arxiv.org/abs/2511.00488", "authors": ["Jun Gao", "Yun Peng", "Xiaoxue Ren"], "title": "\\texttt{ReMind}: Understanding Deductive Code Reasoning in LLMs", "comment": null, "summary": "Large Language Models (LLMs) have achieved remarkable progress in\ncode-related tasks. Despite their advancement, empirical evidence reveals that\nthey still struggle with \\emph{deductive code reasoning}, the ability to reason\nabout the program execution process. While prior studies have recognized this\nlimitation, the underlying causes remain largely underexplored. In this paper,\nwe begin by presenting a comprehensive empirical study that reveals three key\nchallenges undermining deductive code reasoning: (1) an intrinsic gap between\ngeneration and reasoning abilities, (2) a consistent bias towards code sources,\nand (3) weak zero-shot generalization on complex benchmarks. In light of these\nchallenges, we propose \\texttt{ReMind}, a multi-agent framework composed of\n\\texttt{Mutator}, \\texttt{Executor}, and \\texttt{Inspector}. The\n\\texttt{Mutator} generates code variants to mitigate bias towards code sources,\nthe \\texttt{Executor} traces variable states step-by-step to expose\ninconsistency, and the \\texttt{Inspector} identifies problematic reasoning\nsteps and provides control-flow refinement to bridge the intrinsic reasoning\ngap. Through their coordinated collaboration, \\texttt{ReMind} systematically\nidentifies and refines reasoning flaws, achieving outstanding performance and\nenabling robust zero-shot generalization. Extensive experiments on two\nbenchmarks with five LLMs demonstrate the superior advantages of\n\\texttt{ReMind} compared to baseline approaches in deductive code reasoning."}
{"id": "2511.00898", "categories": ["cs.GR"], "pdf": "https://arxiv.org/pdf/2511.00898", "abs": "https://arxiv.org/abs/2511.00898", "authors": ["Heng Zhang", "Jing Liu", "Jiajun Wu", "Haochen You", "Lubin Gan", "Yuling Shi", "Xiaodong Gu", "Zijian Zhang", "Shuai Chen", "Wenjun Huang", "Jin Huang"], "title": "Empowering LLMs with Structural Role Inference for Zero-Shot Graph Learning", "comment": null, "summary": "Large Language Models have emerged as a promising approach for graph learning\ndue to their powerful reasoning capabilities. However, existing methods exhibit\nsystematic performance degradation on structurally important nodes such as\nbridges and hubs. We identify the root cause of these limitations. Current\napproaches encode graph topology into static features but lack reasoning\nscaffolds to transform topological patterns into role-based interpretations.\nThis limitation becomes critical in zero-shot scenarios where no training data\nestablishes structure-semantics mappings. To address this gap, we propose\nDuoGLM, a training-free dual-perspective framework for structure-aware graph\nreasoning. The local perspective constructs relation-aware templates capturing\nsemantic interactions between nodes and neighbors. The global perspective\nperforms topology-to-role inference to generate functional descriptions of\nstructural positions. These complementary perspectives provide explicit\nreasoning mechanisms enabling LLMs to distinguish topologically similar but\nsemantically different nodes. Extensive experiments across eight benchmark\ndatasets demonstrate substantial improvements. DuoGLM achieves 14.3\\% accuracy\ngain in zero-shot node classification and 7.6\\% AUC improvement in cross-domain\ntransfer compared to existing methods. The results validate the effectiveness\nof explicit role reasoning for graph understanding with LLMs."}
{"id": "2511.01852", "categories": ["cs.GT", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.01852", "abs": "https://arxiv.org/abs/2511.01852", "authors": ["Yang Cai", "Constantinos Daskalakis", "Haipeng Luo", "Chen-Yu Wei", "Weiqiang Zheng"], "title": "Proximal Regret and Proximal Correlated Equilibria: A New Tractable Solution Concept for Online Learning and Games", "comment": "This paper presents proximal regret and proximal correlated\n  equilibria results that do not appear in the NeurIPS version of\n  arXiv:2403.08171", "summary": "Learning and computation of equilibria are central problems in algorithmic\ngame theory. In this work, we introduce proximal regret, a new notion of regret\nbased on proximal operators that lies strictly between external and swap\nregret. When every player employs a no-proximal-regret algorithm in a general\nconvex game, the empirical distribution of play converges to proximal\ncorrelated equilibria (PCE), a refinement of coarse correlated equilibria. Our\nframework unifies several emerging notions in online learning and game theory\n-- such as gradient equilibrium and semicoarse correlated equilibrium -- and\nintroduces new ones. Our main result shows that the classic Online Gradient\nDescent (GD) algorithm achieves an optimal $O(\\sqrt{T})$ bound on proximal\nregret, revealing that GD, without modification, minimizes a stronger regret\nnotion than external regret. This provides a new explanation for the\nempirically superior performance of gradient descent in online learning and\ngames. We further extend our analysis to Mirror Descent in the Bregman setting\nand to Optimistic Gradient Descent, which yields faster convergence in smooth\nconvex games."}
{"id": "2511.00592", "categories": ["cs.PL", "cs.DC", "cs.LG", "cs.PF"], "pdf": "https://arxiv.org/pdf/2511.00592", "abs": "https://arxiv.org/abs/2511.00592", "authors": ["Massinissa Merouani", "Islem Kara Bernou", "Riyadh Baghdadi"], "title": "Agentic Auto-Scheduling: An Experimental Study of LLM-Guided Loop Optimization", "comment": "Accepted at the 34th International Conference on Parallel\n  Architectures and Compilation Techniques (PACT 2025). 12 pages, plus appendix", "summary": "Automatic code optimization remains a difficult challenge, particularly for\ncomplex loop nests on modern hardware. This paper investigates a novel approach\nto code optimization where Large Language Models (LLMs) guide the process\nthrough a closed-loop interaction with a compiler. We present ComPilot, an\nexperimental framework that leverages off-the-shelf LLMs, without any\ntask-specific fine-tuning, as interactive optimization agents. ComPilot\nestablishes a feedback loop where an LLM proposes transformations for a given\nloop nest to a compiler. The compiler attempts the transformations, reporting\nback legality status and measured speedup or slowdown. The LLM utilizes this\nconcrete feedback to iteratively refine its optimization strategy. Our\nextensive evaluation across the PolyBench benchmark suite demonstrates the\neffectiveness of this zero-shot approach. ComPilot achieves geometric mean\nspeedups of 2.66x (single run) and 3.54x (best-of-5 runs) over the original\ncode. Furthermore, ComPilot demonstrates competitive performance against the\nstate-of-the-art Pluto polyhedral optimizer, outperforming it in many cases.\nThis experimental study demonstrates that general-purpose LLMs can effectively\nguide the code optimization process when grounded by compiler feedback, opening\npromising research directions for agentic AI in code optimization."}
{"id": "2511.00911", "categories": ["cs.GR"], "pdf": "https://arxiv.org/pdf/2511.00911", "abs": "https://arxiv.org/abs/2511.00911", "authors": ["Heng Zheng", "Haochen You", "Zijun Liu", "Zijian Zhang", "Lubin Gan", "Hao Zhang", "Wenjun Huang", "Jin Huang"], "title": "G2rammar: Bilingual Grammar Modeling for Enhanced Text-attributed Graph Learning", "comment": null, "summary": "Text-attributed graphs require models to effectively integrate both\nstructural topology and semantic content. Recent approaches apply large\nlanguage models to graphs by linearizing structures into token sequences\nthrough random walks. These methods create concise graph vocabularies to\nreplace verbose natural language descriptions. However, they overlook a\ncritical component that makes language expressive: grammar. In natural\nlanguage, grammar assigns syntactic roles to words and defines their functions\nwithin sentences. Similarly, nodes in graphs play distinct structural roles as\nhubs, bridges, or peripheral members. Current graph language methods provide\ntokens without grammatical annotations to indicate these structural or semantic\nroles. This absence limits language models' ability to reason about graph\ntopology effectively. We propose \\textbf{G2rammar}, a bilingual grammar\nframework that explicitly encodes both structural and semantic grammar for\ntext-attributed graphs. Structural grammar characterizes topological roles\nthrough centrality and neighborhood patterns. Semantic grammar captures content\nrelationships through textual informativity. The framework implements two-stage\nlearning with structural grammar pre-training followed by semantic grammar\nfine-tuning. Extensive experiments on real-world datasets demonstrate that\nG2rammar consistently outperforms competitive baselines by providing language\nmodels with the grammatical context needed to understand graph structures."}
{"id": "2511.00740", "categories": ["cs.PL"], "pdf": "https://arxiv.org/pdf/2511.00740", "abs": "https://arxiv.org/abs/2511.00740", "authors": ["Igor Engel", "Ekaterina Verbitskaia"], "title": "Typed Embedding of miniKanren for Functional Conversion", "comment": null, "summary": "Relational programming enables program synthesis through a verifier-to-solver\napproach. An earlier paper introduced a functional conversion that mitigated\nsome of the inherent performance overhead. However, the conversion was\ninelegant: it was oblivious to types, demanded determinism annotations, and\nimplicit generator threading. In this paper, we address these issues by\nproviding a typed tagless-final embedding of miniKanren into Haskell. This\nimprovement significantly reduces boilerplate while preserving, and sometimes\nenhancing, earlier speedups."}
{"id": "2511.01259", "categories": ["cs.GR", "physics.flu-dyn"], "pdf": "https://arxiv.org/pdf/2511.01259", "abs": "https://arxiv.org/abs/2511.01259", "authors": ["Zhiqi Li", "Jinjin He", "Barnabás Börcsök", "Taiyuan Zhang", "Duowen Chen", "Tao Du", "Ming C. Lin", "Greg Turk", "Bo Zhu"], "title": "An Adjoint Method for Differentiable Fluid Simulation on Flow Maps", "comment": "15 pages, 16 figures", "summary": "This paper presents a novel adjoint solver for differentiable fluid\nsimulation based on bidirectional flow maps. Our key observation is that the\nforward fluid solver and its corresponding backward, adjoint solver share the\nsame flow map as the forward simulation. In the forward pass, this map\ntransports fluid impulse variables from the initial frame to the current frame\nto simulate vortical dynamics. In the backward pass, the same map propagates\nadjoint variables from the current frame back to the initial frame to compute\ngradients. This shared long-range map allows the accuracy of gradient\ncomputation to benefit directly from improvements in flow map construction.\nBuilding on this insight, we introduce a novel adjoint solver that solves the\nadjoint equations directly on the flow map, enabling long-range and accurate\ndifferentiation of incompressible flows without differentiating intermediate\nnumerical steps or storing intermediate variables, as required in conventional\nadjoint methods. To further improve efficiency, we propose a long-short\ntime-sparse flow map representation for evolving adjoint variables. Our\napproach has low memory usage, requiring only 6.53GB of data at a resolution of\n$192^3$ while preserving high accuracy in tracking vorticity, enabling new\ndifferentiable simulation tasks that require precise identification,\nprediction, and control of vortex dynamics."}
{"id": "2511.01736", "categories": ["cs.PL", "quant-ph"], "pdf": "https://arxiv.org/pdf/2511.01736", "abs": "https://arxiv.org/abs/2511.01736", "authors": ["Charles Yuan"], "title": "Cobble: Compiling Block Encodings for Quantum Computational Linear Algebra", "comment": "20 pages, 12 figures", "summary": "Quantum algorithms for computational linear algebra promise up to exponential\nspeedups for applications such as simulation and regression, making them prime\ncandidates for hardware realization. But these algorithms execute in a model\nthat cannot efficiently store matrices in memory like a classical algorithm\ndoes, instead requiring developers to implement complex expressions for matrix\narithmetic in terms of correct and efficient quantum circuits. Among the\nchallenges for the developer is navigating a cost model in which conventional\noptimizations for linear algebra, such as subexpression reuse, can be\ninapplicable or unprofitable.\n  In this work, we present Cobble, a language for programming with quantum\ncomputational linear algebra. Cobble enables developers to express and\nmanipulate the quantum representations of matrices, known as block encodings,\nusing high-level notation that automatically compiles to correct quantum\ncircuits. Cobble features analyses that estimate leading factors in time and\nspace usage of programs, as well as optimizations that reduce overhead and\ngenerate efficient circuits using leading techniques such as the quantum\nsingular value transformation. We evaluate Cobble on benchmark kernels for\nsimulation, regression, search, and other applications, showing 2.6x-25.4x\nspeedups not achieved by existing circuit optimizers on these benchmarks."}
