<div id=toc></div>

# Table of Contents

- [cs.GR](#cs.GR) [Total: 4]
- [cs.PL](#cs.PL) [Total: 4]
- [cs.GT](#cs.GT) [Total: 3]


<div id='cs.GR'></div>

# cs.GR [[Back]](#toc)

### [1] [PartMotionEdit: Fine-Grained Text-Driven 3D Human Motion Editing via Part-Level Modulation](https://arxiv.org/abs/2512.24200)
*Yujie Yang,Zhichao Zhang,Jiazhou Chen,Zichao Wu*

Main category: cs.GR

TL;DR: PartMotionEdit是一种新颖的细粒度3D人体运动编辑框架，通过部分级语义调制实现精确控制，优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有的文本驱动3D人体运动编辑方法难以精确控制局部动作，因此需要一种细粒度的运动编辑框架。

Method: 提出PartMotionEdit框架，包含Part-aware Motion Modulation (PMM)模块和Bidirectional Motion Interaction (BMI)模块，通过部分级语义调制实现精确编辑。

Result: 在知名基准测试上的定量和定性评估表明，PartMotionEdit优于现有方法。

Conclusion: PartMotionEdit通过部分级语义调制和双向运动交互模块，实现了更精确和可解释的运动编辑。

Abstract: Existing text-driven 3D human motion editing methods have demonstrated significant progress, but are still difficult to precisely control over detailed, part-specific motions due to their global modeling nature. In this paper, we propose PartMotionEdit, a novel fine-grained motion editing framework that operates via part-level semantic modulation. The core of PartMotionEdit is a Part-aware Motion Modulation (PMM) module, which builds upon a predefined five-part body decomposition. PMM dynamically predicts time-varying modulation weights for each body part, enabling precise and interpretable editing of local motions. To guide the training of PMM, we also introduce a part-level similarity curve supervision mechanism enhanced with dual-layer normalization. This mechanism assists PMM in learning semantically consistent and editable distributions across all body parts. Furthermore, we design a Bidirectional Motion Interaction (BMI) module. It leverages bidirectional cross-modal attention to achieve more accurate semantic alignment between textual instructions and motion semantics. Extensive quantitative and qualitative evaluations on a well-known benchmark demonstrate that PartMotionEdit outperforms the state-of-the-art methods.

</details>


### [2] [BATISNet: Instance Segmentation of Tooth Point Clouds with Boundary Awareness](https://arxiv.org/abs/2512.24201)
*Yating Cai,Yanghui Xu,Zehua Hu,Jiazhou Chen,Jing Huang*

Main category: cs.GR

TL;DR: BATISNet是一种边界感知实例网络，用于牙齿点云分割，解决了现有方法在复杂牙科案例中语义分割效果不佳的问题。


<details>
  <summary>Details</summary>
Motivation: 现有方法主要依赖语义分割，但在牙齿紧密排列、边界不清及复杂牙科案例（如牙齿缺失或错位）中效果不佳，因此需要一种更鲁棒且精确的分割方法。

Method: BATISNet结合特征提取主干和实例分割模块，不仅关注牙齿的语义特征，还学习单个牙齿的实例特征，并设计了边界感知损失函数以优化边界分割。

Result: 实验表明，BATISNet在牙齿完整性和边界分割上优于现有方法，为临床提供了更可靠的数据支持。

Conclusion: BATISNet通过关注实例特征和边界分割，显著提升了复杂牙科案例中的点云分割效果，具有重要的临床应用价值。

Abstract: Accurate segmentation of the tooth point cloud is of great significance for diagnosis clinical assisting and treatment planning. Existing methods mostly employ semantic segmentation, focusing on the semantic feature between different types of teeth. However, due to the tightly packed structure of teeth, unclear boundaries, and the diversity of complex cases such as missing teeth, malposed teeth, semantic segmentation often struggles to achieve satisfactory results when dealing with complex dental cases. To address these issues, this paper propose BATISNet, a boundary-aware instance network for tooth point cloud segmentation. This network model consists of a feature extraction backbone and an instance segmentation module. It not only focuses on extracting the semantic features of different types of teeth but also learns the instance features of individual teeth. It helps achieve more robust and accurate tooth instance segmentation in complex clinical scenarios such as missing teeth and malposed teeth. Additionally, to further enhance the completeness and accuracy of tooth boundary segmentation, a boundary-aware loss function is designed to specifically supervise the boundary segmentation between instances. It mitigates effectively tooth adhesion and boundary ambiguity issues. Extensive experimental results show that BATISNet outperforms existing methods in tooth integrity segmentation, providing more reliable and detailed data support for practical clinical applications.

</details>


### [3] [The Uncanny Valley in medical simulation-based training: a visual summary](https://arxiv.org/abs/2512.24240)
*Eleni Grigoriou,Manos Kamarianakis,George Papagiannakis*

Main category: cs.GR

TL;DR: 本文旨在通过文献综述和证据基础的视觉指南，探讨‘恐怖谷效应’（UV）对医学虚拟现实模拟培训的深远影响及其解决策略。


<details>
  <summary>Details</summary>
Motivation: 理解并解决‘恐怖谷效应’在医学虚拟现实模拟培训中的影响至关重要，因为真实感和沉浸感对于有效学习具有关键作用。

Method: 研究团队由计算机图形学、虚拟现实和医学教育领域的专家组成，采用跨学科视角，结合先进的计算机图形系统和VR角色模拟技术展开研究。

Result: 研究发现，‘恐怖谷效应’在医学虚拟现实训练中可能导致不适感，需要通过技术优化和教育创新来解决。

Conclusion: 本文强调了跨学科合作和技术创新的重要性，以克服‘恐怖谷效应’对医学虚拟现实培训的负面影响，从而提升学习效果。

Abstract: The purpose of this review article is to provide a bibliographical as well as evidence-based visual guide regarding the effect of ``Uncanny Valley'' (UV) and how it profoundly influences medical virtual reality simulation-based training. The phenomenon, where increasingly realistic virtual humans elicit discomfort due to subtle imperfections, is crucial to understand and address in the context of medical training, where realism and immersion are key to effective learning.
  Our research team, consisting of experts in computer graphics, virtual reality, and medical education, brings a diverse and multidisciplinary perspective to this subject. Our collective experience spans developing advanced computer graphics systems, VR character simulation, and innovative educational technologies. We have collaborated across institutions and industries to push the boundaries of VR applications in medical training.

</details>


### [4] [PhysTalk: Language-driven Real-time Physics in 3D Gaussian Scenes](https://arxiv.org/abs/2512.24986)
*Luca Collorone,Mert Kiray,Indro Spinelli,Fabio Galasso,Benjamin Busam*

Main category: cs.GR

TL;DR: PhysTalk是一个基于文本输入的开放词汇视觉效果生成框架，通过3D高斯泼溅技术和物理模拟器实时生成交互式4D动画，无需训练且计算轻量。


<details>
  <summary>Details</summary>
Motivation: 当前视觉效果生成流程缺乏物理真实性和高效的语言接口，且依赖耗时的离线优化。PhysTalk旨在通过结合3D高斯泼溅技术与物理模拟器，提供实时、物理真实的交互式动画生成。

Method: PhysTalk使用大型语言模型生成可执行代码，直接修改3D高斯泼溅参数，并通过轻量级代理和粒子动力学实现物理模拟，无需依赖耗时的网格提取。

Result: PhysTalk首次将3D高斯泼溅技术与物理模拟器直接结合，实现了开放词汇的交互式3D高斯动画生成，支持多材质物体的碰撞感知和物理操作。

Conclusion: PhysTalk的设计使其无需训练且计算轻量，推动了4D动画生成的普及，并将工作流程从“渲染等待”转变为交互式对话模式。

Abstract: Realistic visual simulations are omnipresent, yet their creation requires computing time, rendering, and expert animation knowledge. Open-vocabulary visual effects generation from text inputs emerges as a promising solution that can unlock immense creative potential. However, current pipelines lack both physical realism and effective language interfaces, requiring slow offline optimization. In contrast, PhysTalk takes a 3D Gaussian Splatting (3DGS) scene as input and translates arbitrary user prompts into real time, physics based, interactive 4D animations. A large language model (LLM) generates executable code that directly modifies 3DGS parameters through lightweight proxies and particle dynamics. Notably, PhysTalk is the first framework to couple 3DGS directly with a physics simulator without relying on time consuming mesh extraction. While remaining open vocabulary, this design enables interactive 3D Gaussian animation via collision aware, physics based manipulation of arbitrary, multi material objects. Finally, PhysTalk is train-free and computationally lightweight: this makes 4D animation broadly accessible and shifts these workflows from a "render and wait" paradigm toward an interactive dialogue with a modern, physics-informed pipeline.

</details>


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [5] [Enforcing Temporal Constraints for LLM Agents](https://arxiv.org/abs/2512.23738)
*Adharsh Kamath,Sishen Zhang,Calvin Xu,Shubham Ugare,Gagandeep Singh,Sasa Misailovic*

Main category: cs.PL

TL;DR: Agent-C是一个新颖的框架，通过运行时保证确保基于LLM的代理遵守时间安全策略，提升任务实用性并实现100%的安全性。


<details>
  <summary>Details</summary>
Motivation: 当前的安全护栏系统无法有效防止代理违反时间安全策略，特别是在需要推理动作序列而非单一步骤的场景中。

Method: Agent-C引入了一种领域特定语言用于表达时间属性，将规范转化为一阶逻辑，并利用SMT求解在生成过程中检测不合规动作。

Result: 在零售客服和机票预订系统的实验中，Agent-C实现了100%的安全性和更高的任务实用性，显著优于现有技术。

Conclusion: Agent-C为可靠代理推理设定了新的技术前沿，同时提升了实用性和安全性。

Abstract: LLM-based agents are deployed in safety-critical applications, yet current guardrail systems fail to prevent violations of temporal safety policies, requirements that govern the ordering and sequencing of agent actions. For instance, agents may access sensitive data before authenticating users or process refunds to unauthorized payment methods, violations that require reasoning about sequences of action rather than an individual action. Existing guardrails rely on imprecise natural language instructions or post-hoc monitoring, and provide no formal guarantees that agents will satisfy temporal constraints. We present Agent-C, a novel framework that provides run-time guarantees ensuring LLM agents adhere to formal temporal safety properties. Agent-C introduces a domain-specific language for expressing temporal properties (e.g., authenticate before accessing data), translates specifications to first-order logic, and uses SMT solving to detect non-compliant agent actions during token generation. When the LLM attempts to generate a non-compliant tool call, Agent-C leverages constrained generation techniques to ensure that every action generated by the LLM complies with the specification, and to generate a compliant alternative to a non-compliant agent action. We evaluate Agent-C across two real-world applications: retail customer service and airline ticket reservation system, and multiple language models (open and closed-source). Our results demonstrate that Agent-C achieves perfect safety (100% conformance, 0% harm), while improving task utility compared to state-of-the-art guardrails and unrestricted agents. On SoTA closed-source models, Agent-C improves conformance (77.4% to 100% for Claude Sonnet 4.5 and 83.7% to 100% for GPT-5), while simultaneously increasing utility (71.8% to 75.2% and 66.1% to 70.6%, respectively), representing a new SoTA frontier for reliable agentic reasoning.

</details>


### [6] [Towards representation agnostic probabilistic programming](https://arxiv.org/abs/2512.23740)
*Ole Fenske,Maximilian Popko,Sebastian Bader,Thomas Kirste*

Main category: cs.PL

TL;DR: 本文介绍了一种通用的因子抽象方法，提供了一个统一接口，支持在单一框架内混合使用不同表示的因子，从而解决了现有概率编程语言和工具中模型表示与特定推理算法紧密耦合的问题。


<details>
  <summary>Details</summary>
Motivation: 现有的概率编程语言和工具将模型表示与特定推理算法紧密绑定，限制了对新表示或混合离散-连续模型的实验探索。

Method: 提出了一个因子抽象概念，定义了五种基本操作作为底层表示无关的统一接口，允许用户在单一框架内自由混合使用不同表示的因子。

Result: 该方法实现了表示无关的概率编程，能够在复杂混合模型中进行实用推理，这是当前工具无法充分表达的。

Conclusion: 因子抽象为概率编程提供了一个通用且灵活的框架，支持多种表示方式的混合使用，解决了现有工具的局限性。

Abstract: Current probabilistic programming languages and tools tightly couple model representations with specific inference algorithms, preventing experimentation with novel representations or mixed discrete-continuous models. We introduce a factor abstraction with five fundamental operations that serve as a universal interface for manipulating factors regardless of their underlying representation. This enables representation-agnostic probabilistic programming where users can freely mix different representations (e.g. discrete tables, Gaussians distributions, sample-based approaches) within a single unified framework, allowing practical inference in complex hybrid models that current toolkits cannot adequately express.

</details>


### [7] [VGC: A High-Performance Zone-Based Garbage Collector Architecture for Python with Partitioning and Parallel Execution](https://arxiv.org/abs/2512.23768)
*Abdulla M*

Main category: cs.PL

TL;DR: 虚拟垃圾回收器（VGC）提出了一种新颖的内存管理框架，旨在优化从嵌入式设备到高性能并行架构的多样化系统的性能。


<details>
  <summary>Details</summary>
Motivation: 传统的垃圾回收器在多线程环境下性能不足，尤其是在资源受限的设备和高性能并行架构中。VGC通过创新的双层次架构解决这些问题。

Method: VGC采用双层次架构：Active VGC通过并发的标记-清除策略动态管理运行时对象，减少暂停时间；Passive VGC在编译时优化静态对象分配，减少内存碎片。

Result: 在多线程基准测试中，VGC将暂停时间减少30%，内存总量减少25%，并显著提高了现代并行应用的可扩展性。

Conclusion: VGC通过结合编译时和运行时优化，为内存密集型系统提供了一个稳健且适应性强的解决方案，适用于各种编程环境。

Abstract: The Virtual Garbage Collector (VGC) introduces a novel memory management framework designed to optimize performance across diverse systems, ranging from resource constrained embedded devices to high performance parallel architectures. Unlike conventional garbage collectors, VGC employs a dual layer architecture consisting of Active VGC and Passive VGC to enable efficient, low overhead memory management. Active VGC dynamically manages runtime objects using a concurrent mark and sweep strategy tailored for parallel workloads, reducing pause times by up to 30 percent compared to generational collectors in multithreaded benchmarks. Passive VGC operates at compile time and optimizes static object allocation through predictive memory mapping, minimizing fragmentation by aligning objects to cache boundaries. This separation of responsibilities ensures predictable memory access patterns, reduces total memory usage by up to 25 percent, and improves scalability for modern parallel applications. By integrating compile time and runtime optimizations, VGC provides a robust and adaptable solution for memory intensive systems across both low level and high level programming environments.

</details>


### [8] [State Space Estimation for DPOR-based Model Checkers](https://arxiv.org/abs/2512.23996)
*A. R. Balasubramanian,Mohammad Hossein Khoshechin Jorshari,Rupak Majumdar,Umang Mathur,Minjian Zhang*

Main category: cs.PL

TL;DR: 本文研究了并发程序的估计问题，提出了一个蒙特卡洛方法来高效估算Mazurkiewicz迹等价类的数量，解决了枚举型模型检测中的资源分配问题。


<details>
  <summary>Details</summary>
Motivation: 并发程序中Mazurkiewicz迹等价类的数量是模型检测中的重要指标，可以帮助评估模型检测的运行时间和搜索空间覆盖率。

Method: 提出了一种蒙特卡洛方法，通过将无状态最优DPOR算法转换为无偏估计器，并结合Knuth的经典估计器和随机枚举技术，以控制方差。

Result: 在JMC模型检测器上的实验表明，该方法在中等预算下能提供稳定的估计结果，误差通常在20%以内，并且适用于状态空间高达10^5--10^6类的情况。

Conclusion: 该算法首次提供了多项式时间的无偏估计器，解决了模型检测资源分配中的关键问题。

Abstract: We study the estimation problem for concurrent programs: given a bounded program $P$, estimate the number of Mazurkiewicz trace-equivalence classes induced by its interleavings. This quantity informs two practical questions for enumeration-based model checking: how long a model checking run is likely to take, and what fraction of the search space has been covered so far. We first show the counting problem is #P-hard even for restricted programs and, unless $P=NP$, inapproximable within any subexponential factor, ruling out efficient exact or randomized approximation algorithms. We give a Monte Carlo approach to obtain a poly-time unbiased estimator: we convert a stateless optimal DPOR algorithm into an unbiased estimator by viewing its exploration as a bounded-depth, bounded-width tree whose leaves are the maximal Mazurkiewicz traces. A classical estimator by Knuth, when run on this tree, yields an unbiased estimate. To control the variance, we apply stochastic enumeration by maintaining a small population of partial paths per depth whose evolution is coupled. We have implemented our estimator in the JMC model checker and evaluated it on shared-memory benchmarks. With modest budgets, our estimator yields stable estimates, typically within a 20% band, within a few hundred trials, even when the state space has $10^5$--$10^6$ classes. We also show how the same machinery estimates model-checking cost by weighting all explored graphs, not only complete traces. Our algorithms provide the first provable poly-time unbiased estimators for counting traces, a problem of considerable importance when allocating model checking resources.

</details>


<div id='cs.GT'></div>

# cs.GT [[Back]](#toc)

### [9] [Personalized Promotions in Practice: Dynamic Allocation and Reference Effects](https://arxiv.org/abs/2512.23781)
*Jackie Baek,Will Ma,Dmitry Mitrofanov*

Main category: cs.GT

TL;DR: 该论文提出了一种高效的每日个性化促销策略，针对超过2000万客户，通过全球分配约束优化促销选择（如10%、12%、15%、17%或20%折扣），在实际A/B测试中实现了4.5%的收入增长。


<details>
  <summary>Details</summary>
Motivation: 研究动机在于解决大规模在线零售环境中个性化促销的复杂性，同时满足全球分配约束，并通过学习和利用客户的跨时期行为模式来优化促销效果。

Method: 方法包括设计一个高效的策略，每天为每位客户选择最佳促销方案，并开发了一种新的组合模型，该模型考虑了客户的参考效应（即客户记住过去几天看到的最佳促销作为"参考值"）。

Result: 在A/B测试中，该方法成功实现了4.5%的收入增长。理论模型分析表明，最优策略是在提供较差促销值ℓ次后提供一次良好促销值。

Conclusion: 结论表明，通过结合实践和理论模型，可以有效地优化个性化促销策略，显著提升收入，并为未来研究提供了新的组合模型框架。

Abstract: Partnering with a large online retailer, we consider the problem of sending daily personalized promotions to a userbase of over 20 million customers. We propose an efficient policy for determining, every day, the promotion that each customer should receive (10%, 12%, 15%, 17%, or 20% off), while respecting global allocation constraints. This policy was successfully deployed to see a 4.5% revenue increase during an A/B test, by better targeting promotion-sensitive customers and also learning intertemporal patterns across customers.
  We also consider theoretically modeling the intertemporal state of the customer. The data suggests a simple new combinatorial model of pricing with reference effects, where the customer remembers the best promotion they saw over the past $\ell$ days as the "reference value", and is more likely to purchase if this value is poor. We tightly characterize the structure of optimal policies for maximizing long-run average revenue under this model -- they cycle between offering poor promotion values $\ell$ times and offering good values once.

</details>


### [10] [Multilevel Fair Allocation](https://arxiv.org/abs/2512.24105)
*Maxime Lucet,Nawal Benabbou,Aurélie Beynier,Nicolas Maudet*

Main category: cs.GT

TL;DR: 这篇论文引入了多层级资源公平分配的概念，资源分配的代理人之间呈现树状层次关系，并提出两种算法来解决多层级分配问题。


<details>
  <summary>Details</summary>
Motivation: 在多层级资源分配中，如何设计算法以保持公平性和效率是一个主要挑战，尤其是在代理人之间存在树状层次关系的情况下。

Method: 论文提出了两种算法：一种是在多项式时间内运行的通用顺序算法，具有理论上的效率和公平性保证；另一种是将General Yankee Swap扩展到多层级设置，保留了实践的公平性。

Result: 第一种算法在效率和公平性方面提供了理论保证；第二种算法在实际应用中表现出优秀的公平性。

Conclusion: 论文通过两种算法解决了多层级资源分配中的公平性和效率问题，并展示了它们在理论和实践中的有效性。

Abstract: We introduce the concept of multilevel fair allocation of resources with tree-structured hierarchical relations among agents. While at each level it is possible to consider the problem locally as an allocation of an agent to its children, the multilevel allocation can be seen as a trace capturing the fact that the process is iterated until the leaves of the tree. In principle, each intermediary node may have its own local allocation mechanism. The main challenge is then to design algorithms which can retain good fairness and efficiency properties. In this paper we propose two original algorithms under the assumption that leaves of the tree have matroid-rank utility functions and the utility of any internal node is the sum of the utilities of its children. The first one is a generic polynomial-time sequential algorithm that comes with theoretical guarantees in terms of efficiency and fairness. It operates in a top-down fashion -- as commonly observed in real-world applications -- and is compatible with various local algorithms. The second one extends the recently proposed General Yankee Swap to the multilevel setting. This extension comes with efficiency guarantees only, but we show that it preserves excellent fairness properties in practice.

</details>


### [11] [On the Difficulty of Measuring Divisiveness of Proposals under Ranked Preferences](https://arxiv.org/abs/2512.24467)
*Ulle Endriss*

Main category: cs.GT

TL;DR: 该论文探讨了如何在公共政策提案中识别最具争议的提案，并指出了满足一定规范性要求时面临的困难。


<details>
  <summary>Details</summary>
Motivation: 设计在线参与平台以支持数字民主倡议，需要识别最具争议的提案，以便引导讨论。

Method: 采用社会选择理论中的公理化方法，定义并选择最分歧的提案。

Result: 研究发现，满足某些看似温和的规范性要求时，选择最分歧提案存在根本性困难。

Conclusion: 论文揭示了在设计数字民主平台时，识别争议提案的复杂性和挑战。

Abstract: Given the stated preferences of several people over a number of proposals regarding public policy initiatives, some of those proposals might be judged to be more ``divisive'' than others. When designing online participatory platforms to support digital democracy initiatives enabling citizens to deliberate over such proposals, we might wish to equip those platforms with the functionality to retrieve the most divisive proposals currently under discussion. Such a service would be useful for analysing the progress of deliberation and steering discussion towards issues that still require further debate. Guided by this use case, we explore possibilities for providing a clear definition of what it means to select a set of most divisive proposals on the basis of people's stated preferences over proposals. Then, employing the axiomatic method familiar from social choice theory, we show that the task of selecting the most divisive proposals in a manner that satisfies certain seemingly mild normative requirements faces a number of fundamental difficulties.

</details>
