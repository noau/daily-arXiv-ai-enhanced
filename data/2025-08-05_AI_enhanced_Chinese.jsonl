{"id": "2508.01199", "categories": ["cs.PL"], "pdf": "https://arxiv.org/pdf/2508.01199", "abs": "https://arxiv.org/abs/2508.01199", "authors": ["Avinash Malik"], "title": "Efficient compilation and execution of synchronous programs via type-state programming", "comment": null, "summary": "Synchronous programs are used extensively in implementation of safety\ncritical embedded software. Imperative synchronous programming languages model\nmultiple Finite State Machines (FSMs) executing in lockstep at logical clock\nticks. The synchronous view of time along with the FSM based design enables\neasier formal verification. The synchronous composition of multiple FSMs,\nduring compilation, results in the well known state space explosion problem.\nHence, efficiently compiling imperative synchronous programs into small and\nfast executables is challenging. This paper introduces a novel linear time\ncompilation technique for automata based compilation of synchronous programs.\nGraph based rewrite rules for kernel programming constructs are introduced. A\nlinear time algorithm applies these rules to produce a FSM. The FSM is then\nencoded into a type-state program using template meta-programming in C++.\nExperimental results show that the compilation time and generated binary size\nis comparable, while the execution times are on average 31-60% faster than\ncurrent state-of-the-art compilers.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u7ebf\u6027\u65f6\u95f4\u7f16\u8bd1\u6280\u672f\uff0c\u7528\u4e8e\u540c\u6b65\u7a0b\u5e8f\u7684\u6709\u9650\u72b6\u6001\u673a\uff08FSM\uff09\u7f16\u8bd1\uff0c\u901a\u8fc7\u57fa\u4e8e\u56fe\u7684\u6539\u5199\u89c4\u5219\u548c\u6a21\u677f\u5143\u7f16\u7a0b\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u6267\u884c\u901f\u5ea6\u3002", "motivation": "\u540c\u6b65\u7a0b\u5e8f\u5728\u5b89\u5168\u5173\u952e\u5d4c\u5165\u5f0f\u8f6f\u4ef6\u4e2d\u5e7f\u6cdb\u5e94\u7528\uff0c\u4f46\u5176\u7f16\u8bd1\u8fc7\u7a0b\u4e2d\u7531\u4e8e\u72b6\u6001\u7a7a\u95f4\u7206\u70b8\u95ee\u9898\uff0c\u751f\u6210\u9ad8\u6548\u4e14\u7d27\u51d1\u7684\u53ef\u6267\u884c\u6587\u4ef6\u5177\u6709\u6311\u6218\u6027\u3002", "method": "\u5f15\u5165\u4e86\u57fa\u4e8e\u56fe\u7684\u6539\u5199\u89c4\u5219\u548c\u7ebf\u6027\u65f6\u95f4\u7b97\u6cd5\uff0c\u5c06\u540c\u6b65\u7a0b\u5e8f\u8f6c\u6362\u4e3a\u6709\u9650\u72b6\u6001\u673a\uff0c\u5e76\u901a\u8fc7C++\u6a21\u677f\u5143\u7f16\u7a0b\u5c06FSM\u7f16\u7801\u4e3a\u7c7b\u578b\u72b6\u6001\u7a0b\u5e8f\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u7f16\u8bd1\u65f6\u95f4\u548c\u751f\u6210\u4e8c\u8fdb\u5236\u5927\u5c0f\u4e0e\u73b0\u6709\u6280\u672f\u76f8\u5f53\uff0c\u4f46\u6267\u884c\u65f6\u95f4\u5e73\u5747\u5feb31-60%\u3002", "conclusion": "\u63d0\u51fa\u7684\u7ebf\u6027\u65f6\u95f4\u7f16\u8bd1\u6280\u672f\u6709\u6548\u89e3\u51b3\u4e86\u540c\u6b65\u7a0b\u5e8f\u7f16\u8bd1\u4e2d\u7684\u6311\u6218\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6267\u884c\u6548\u7387\u3002"}}
{"id": "2508.02305", "categories": ["cs.PL"], "pdf": "https://arxiv.org/pdf/2508.02305", "abs": "https://arxiv.org/abs/2508.02305", "authors": ["Rose Bohrer"], "title": "Proceedings 14th International Workshop on Trends in Functional Programming in Education", "comment": null, "summary": "The goal of TFPIE is to gather researchers, teachers and professionals that\nuse, or are interested in the use of, functional programming in education.\nTFPIE aims to be a venue where novel ideas, classroom-tested ideas and\nwork-in-progress on the use of functional programming in education are\ndiscussed. The one-day workshop will foster a spirit of open discussion by\nhaving a review process for publication after the workshop.", "AI": {"tldr": "TFPIE\u662f\u4e00\u4e2a\u65e8\u5728\u805a\u96c6\u4f7f\u7528\u6216\u5bf9\u6559\u80b2\u4e2d\u4f7f\u7528\u51fd\u6570\u5f0f\u7f16\u7a0b\u611f\u5174\u8da3\u7684\u7814\u7a76\u4eba\u5458\u3001\u6559\u5e08\u548c\u4e13\u4e1a\u4eba\u58eb\u7684\u5de5\u4f5c\u574a\uff0c\u76ee\u7684\u662f\u8ba8\u8bba\u76f8\u5173\u65b0\u9896\u6216\u5df2\u6d4b\u8bd5\u7684\u60f3\u6cd5\u3002", "motivation": "\u76ee\u7684\u662f\u4e3a\u4e86\u5728\u6559\u80b2\u9886\u57df\u4e2d\u63a8\u5e7f\u548c\u63a2\u8ba8\u51fd\u6570\u5f0f\u7f16\u7a0b\u7684\u5e94\u7528\uff0c\u63d0\u4f9b\u4e00\u4e2a\u5f00\u653e\u7684\u8ba8\u8bba\u5e73\u53f0\u3002", "method": "\u4e3e\u529e\u4e3a\u671f\u4e00\u5929\u7684\u5de5\u4f5c\u574a\uff0c\u91c7\u7528\u4f1a\u540e\u518d\u53d1\u8868\u7684\u5f62\u5f0f\uff0c\u4ee5\u4fc3\u8fdb\u5f00\u653e\u8ba8\u8bba\u7684\u6c1b\u56f4\u3002", "result": "N/A", "conclusion": "\u901a\u8fc7TFPIE\uff0c\u53ef\u4ee5\u4fc3\u8fdb\u6559\u80b2\u4e2d\u51fd\u6570\u5f0f\u7f16\u7a0b\u7684\u4ea4\u6d41\u4e0e\u53d1\u5c55\u3002"}}
{"id": "2508.00950", "categories": ["cs.GR"], "pdf": "https://arxiv.org/pdf/2508.00950", "abs": "https://arxiv.org/abs/2508.00950", "authors": ["Ying Zhang", "Niklas Groene", "Karsten Klein", "Giuseppe Liotta", "Falk Schreiber"], "title": "Investigating Crossing Perception in 3D Graph Visualisation", "comment": null, "summary": "Human perception of graph drawings is influenced by a variety of impact\nfactors for which quality measures are used as a proxy indicator. The\ninvestigation of those impact factors and their effects is important to\nevaluate and improve quality measures and drawing algorithms. The number of\nedge crossings in a 2D graph drawing has long been a main quality measure for\ndrawing evaluation. The use of stereoscopic 3D graph visualisations has gained\nattraction over the last years, and results from several studies indicate that\nthey can improve analysis efficiency for a range of analysis scenarios. While\nedge crossings can also occur in 3D, there are edge configurations in space\nthat are not crossings but might be perceived as such from a specific\nviewpoint. Such configurations create crossings when projected on the\ncorresponding 2D image plane and could impact readability similar to 2D\ncrossings. In 3D drawings, the additional depth aspect and the subsequent\nimpact factors of edge distance and relative edge direction in space might\nfurther influence the importance of those configurations for readability. We\ninvestigate the impact of such factors in an empirical study and report on\nfindings of difference between major factor categories.", "AI": {"tldr": "\u7814\u7a76\u4e863D\u56fe\u5f62\u7ed8\u56fe\u4e2d\u8fb9\u7f18\u4ea4\u53c9\u6784\u578b\u7684\u611f\u77e5\u5f71\u54cd\u53ca\u5176\u5bf9\u53ef\u8bfb\u6027\u7684\u91cd\u8981\u6027\u3002", "motivation": "\u63a2\u8ba83D\u56fe\u5f62\u7ed8\u56fe\u4e2d\u8fb9\u7f18\u4ea4\u53c9\u6784\u578b\u5bf9\u611f\u77e5\u548c\u53ef\u8bfb\u6027\u7684\u5f71\u54cd\uff0c\u4ee5\u6539\u8fdb\u8d28\u91cf\u8bc4\u4f30\u548c\u7ed8\u56fe\u7b97\u6cd5\u3002", "method": "\u901a\u8fc7\u5b9e\u8bc1\u7814\u7a76\u5206\u6790\u4e863D\u56fe\u5f62\u7ed8\u56fe\u4e2d\u8fb9\u7f18\u8ddd\u79bb\u548c\u76f8\u5bf9\u65b9\u5411\u7b49\u56e0\u7d20\u7684\u5f71\u54cd\u3002", "result": "\u7814\u7a76\u53d1\u73b03D\u7ed8\u56fe\u4e2d\u7684\u8fb9\u7f18\u4ea4\u53c9\u6784\u578b\u5bf9\u53ef\u8bfb\u6027\u6709\u663e\u8457\u5f71\u54cd\uff0c\u5e76\u4e0e2D\u7ed8\u56fe\u4e2d\u7684\u4ea4\u53c9\u6709\u6240\u4e0d\u540c\u3002", "conclusion": "3D\u56fe\u5f62\u7ed8\u56fe\u7684\u611f\u77e5\u548c\u8d28\u91cf\u8bc4\u4f30\u9700\u8981\u8003\u8651\u989d\u5916\u7684\u6df1\u5ea6\u56e0\u7d20\u53ca\u5176\u5bf9\u8fb9\u7f18\u914d\u7f6e\u7684\u5f71\u54cd\u3002"}}
{"id": "2508.01242", "categories": ["cs.GR", "cs.CV"], "pdf": "https://arxiv.org/pdf/2508.01242", "abs": "https://arxiv.org/abs/2508.01242", "authors": ["Shuangkang Fang", "I-Chao Shen", "Yufeng Wang", "Yi-Hsuan Tsai", "Yi Yang", "Shuchang Zhou", "Wenrui Ding", "Takeo Igarashi", "Ming-Hsuan Yang"], "title": "MeshLLM: Empowering Large Language Models to Progressively Understand and Generate 3D Mesh", "comment": "Accepted by ICCV", "summary": "We present MeshLLM, a novel framework that leverages large language models\n(LLMs) to understand and generate text-serialized 3D meshes. Our approach\naddresses key limitations in existing methods, including the limited dataset\nscale when catering to LLMs' token length and the loss of 3D structural\ninformation during mesh serialization. We introduce a Primitive-Mesh\ndecomposition strategy, which divides 3D meshes into structurally meaningful\nsubunits. This enables the creation of a large-scale dataset with 1500k+\nsamples, almost 50 times larger than previous methods, which aligns better with\nthe LLM scaling law principles. Furthermore, we propose inferring face\nconnectivity from vertices and local mesh assembly training strategies,\nsignificantly enhancing the LLMs' ability to capture mesh topology and spatial\nstructures. Experiments show that MeshLLM outperforms the state-of-the-art\nLLaMA-Mesh in both mesh generation quality and shape understanding,\nhighlighting its great potential in processing text-serialized 3D meshes.", "AI": {"tldr": "MeshLLM\u662f\u4e00\u4e2a\u5229\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7406\u89e3\u548c\u751f\u6210\u6587\u672c\u5e8f\u5217\u53163D\u7f51\u683c\u7684\u65b0\u6846\u67b6\uff0c\u901a\u8fc7Primitive-Mesh\u5206\u89e3\u7b56\u7565\u548c\u5927\u89c4\u6a21\u6570\u636e\u96c6\u8bad\u7ec3\uff0c\u663e\u8457\u63d0\u5347\u4e86\u7f51\u683c\u751f\u6210\u8d28\u91cf\u548c\u5f62\u72b6\u7406\u89e3\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u9002\u914dLLM\u7684\u4ee4\u724c\u957f\u5ea6\u65f6\u5b58\u5728\u6570\u636e\u96c6\u89c4\u6a21\u6709\u9650\u7684\u7f3a\u9677\uff0c\u4e14\u5728\u7f51\u683c\u5e8f\u5217\u5316\u8fc7\u7a0b\u4e2d\u4e22\u5931\u4e863D\u7ed3\u6784\u4fe1\u606f\u3002MeshLLM\u65e8\u5728\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\u3002", "method": "\u63d0\u51faPrimitive-Mesh\u5206\u89e3\u7b56\u7565\uff0c\u5c063D\u7f51\u683c\u5212\u5206\u4e3a\u7ed3\u6784\u4e0a\u6709\u610f\u4e49\u7684\u5b50\u5355\u5143\uff0c\u6784\u5efa\u4e86\u4e00\u4e2a\u5305\u542b1500k+\u6837\u672c\u7684\u5927\u89c4\u6a21\u6570\u636e\u96c6\u3002\u6b64\u5916\uff0c\u901a\u8fc7\u63a8\u65ad\u9762\u8fde\u63a5\u6027\u548c\u5c40\u90e8\u7f51\u683c\u7ec4\u88c5\u8bad\u7ec3\u7b56\u7565\u6765\u63d0\u5347LLM\u5bf9\u7f51\u683c\u62d3\u6251\u4e0e\u7a7a\u95f4\u7ed3\u6784\u7684\u6355\u6349\u80fd\u529b\u3002", "result": "MeshLLM\u5728\u7f51\u683c\u751f\u6210\u8d28\u91cf\u548c\u5f62\u72b6\u7406\u89e3\u4efb\u52a1\u4e0a\u8d85\u8d8a\u4e86\u5f53\u524d\u6700\u5148\u8fdb\u7684LLaMA-Mesh\u65b9\u6cd5\uff0c\u663e\u793a\u51fa\u5176\u5728\u5904\u7406\u6587\u672c\u5e8f\u5217\u53163D\u7f51\u683c\u65b9\u9762\u7684\u5de8\u5927\u6f5c\u529b\u3002", "conclusion": "MeshLLM\u901a\u8fc7\u5f15\u5165\u521b\u65b0\u7684\u5206\u89e3\u548c\u8bad\u7ec3\u7b56\u7565\uff0c\u4e3a3D\u7f51\u683c\u7684\u6587\u672c\u5e8f\u5217\u5316\u5904\u7406\u63d0\u4f9b\u4e86\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\uff0c\u5c55\u73b0\u51fa\u5e7f\u6cdb\u7684\u5e94\u7528\u524d\u666f\u3002"}}
{"id": "2508.01381", "categories": ["cs.GR", "cs.CV"], "pdf": "https://arxiv.org/pdf/2508.01381", "abs": "https://arxiv.org/abs/2508.01381", "authors": ["Onat Vuran", "Hsuan-I Ho"], "title": "ReMu: Reconstructing Multi-layer 3D Clothed Human from Image Layers", "comment": "BMVC 2025 paper, 17 pages, 10 figures", "summary": "The reconstruction of multi-layer 3D garments typically requires expensive\nmulti-view capture setups and specialized 3D editing efforts. To support the\ncreation of life-like clothed human avatars, we introduce ReMu for\nreconstructing multi-layer clothed humans in a new setup, Image Layers, which\ncaptures a subject wearing different layers of clothing with a single RGB\ncamera. To reconstruct physically plausible multi-layer 3D garments, a unified\n3D representation is necessary to model these garments in a layered manner.\nThus, we first reconstruct and align each garment layer in a shared coordinate\nsystem defined by the canonical body pose. Afterwards, we introduce a\ncollision-aware optimization process to address interpenetration and further\nrefine the garment boundaries leveraging implicit neural fields. It is worth\nnoting that our method is template-free and category-agnostic, which enables\nthe reconstruction of 3D garments in diverse clothing styles. Through our\nexperiments, we show that our method reconstructs nearly penetration-free 3D\nclothed humans and achieves competitive performance compared to\ncategory-specific methods. Project page: https://eth-ait.github.io/ReMu/", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aReMu\u7684\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7\u5355\u76eeRGB\u76f8\u673a\u91cd\u5efa\u591a\u5c423D\u670d\u9970\uff0c\u65e0\u9700\u6a21\u677f\u4e14\u9002\u7528\u4e8e\u591a\u79cd\u670d\u88c5\u98ce\u683c\u3002", "motivation": "\u73b0\u6709\u7684\u591a\u5c423D\u670d\u9970\u91cd\u5efa\u901a\u5e38\u9700\u8981\u6602\u8d35\u7684\u591a\u89c6\u89d2\u6355\u6349\u8bbe\u5907\u548c\u4e13\u4e1a\u76843D\u7f16\u8f91\u5de5\u4f5c\uff0c\u9650\u5236\u4e86\u903c\u771f\u670d\u88c5\u5316\u865a\u62df\u4eba\u7684\u521b\u5efa\u3002", "method": "ReMu\u4f7f\u7528\u7edf\u4e00\u76843D\u8868\u793a\u65b9\u6cd5\uff0c\u5728\u5171\u4eab\u5750\u6807\u7cfb\u4e2d\u91cd\u5efa\u548c\u5bf9\u9f50\u6bcf\u5c42\u670d\u9970\uff0c\u5e76\u901a\u8fc7\u78b0\u649e\u611f\u77e5\u4f18\u5316\u548c\u9690\u5f0f\u795e\u7ecf\u573a\u8fdb\u4e00\u6b65\u7ec6\u5316\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u80fd\u591f\u91cd\u5efa\u51e0\u4e4e\u65e0\u7a7f\u900f\u76843D\u670d\u9970\u5316\u4eba\u4f53\uff0c\u5e76\u5728\u6027\u80fd\u4e0a\u4e0e\u7279\u5b9a\u7c7b\u522b\u65b9\u6cd5\u76f8\u7ade\u4e89\u3002", "conclusion": "ReMu\u4e3a\u903c\u771f\u591a\u5c42\u670d\u9970\u5316\u865a\u62df\u4eba\u7684\u91cd\u5efa\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u4e14\u901a\u7528\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2508.01590", "categories": ["cs.GR", "cs.CV"], "pdf": "https://arxiv.org/pdf/2508.01590", "abs": "https://arxiv.org/abs/2508.01590", "authors": ["Hua Yu", "Jiao Liu", "Xu Gui", "Melvin Wong", "Yaqing Hou", "Yew-Soon Ong"], "title": "A Plug-and-Play Multi-Criteria Guidance for Diverse In-Betweening Human Motion Generation", "comment": null, "summary": "In-betweening human motion generation aims to synthesize intermediate motions\nthat transition between user-specified keyframes. In addition to maintaining\nsmooth transitions, a crucial requirement of this task is to generate diverse\nmotion sequences. It is still challenging to maintain diversity, particularly\nwhen it is necessary for the motions within a generated batch sampling to\ndiffer meaningfully from one another due to complex motion dynamics. In this\npaper, we propose a novel method, termed the Multi-Criteria Guidance with\nIn-Betweening Motion Model (MCG-IMM), for in-betweening human motion\ngeneration. A key strength of MCG-IMM lies in its plug-and-play nature: it\nenhances the diversity of motions generated by pretrained models without\nintroducing additional parameters This is achieved by providing a sampling\nprocess of pretrained generative models with multi-criteria guidance.\nSpecifically, MCG-IMM reformulates the sampling process of pretrained\ngenerative model as a multi-criteria optimization problem, and introduces an\noptimization process to explore motion sequences that satisfy multiple\ncriteria, e.g., diversity and smoothness. Moreover, our proposed plug-and-play\nmulti-criteria guidance is compatible with different families of generative\nmodels, including denoised diffusion probabilistic models, variational\nautoencoders, and generative adversarial networks. Experiments on four popular\nhuman motion datasets demonstrate that MCG-IMM consistently state-of-the-art\nmethods in in-betweening motion generation task.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aMCG-IMM\u7684\u65b0\u65b9\u6cd5\uff0c\u7528\u4e8e\u5728\u4eba\u7c7b\u52a8\u4f5c\u751f\u6210\u4efb\u52a1\u4e2d\u751f\u6210\u591a\u6837\u4e14\u5e73\u6ed1\u7684\u8fc7\u6e21\u52a8\u4f5c\uff0c\u65e0\u9700\u989d\u5916\u53c2\u6570\u3002", "motivation": "\u5728\u4eba\u7c7b\u52a8\u4f5c\u751f\u6210\u4efb\u52a1\u4e2d\uff0c\u4fdd\u6301\u591a\u6837\u6027\u548c\u5e73\u6ed1\u8fc7\u6e21\u662f\u4e00\u4e2a\u6311\u6218\uff0c\u5c24\u5176\u662f\u5728\u590d\u6742\u52a8\u4f5c\u52a8\u529b\u5b66\u4e0b\u9700\u8981\u751f\u6210\u6709\u610f\u4e49\u7684\u591a\u6837\u5316\u52a8\u4f5c\u5e8f\u5217\u3002", "method": "MCG-IMM\u901a\u8fc7\u5c06\u9884\u8bad\u7ec3\u751f\u6210\u6a21\u578b\u7684\u91c7\u6837\u8fc7\u7a0b\u91cd\u65b0\u8868\u8ff0\u4e3a\u591a\u51c6\u5219\u4f18\u5316\u95ee\u9898\uff0c\u5f15\u5165\u4f18\u5316\u8fc7\u7a0b\u4ee5\u63a2\u7d22\u6ee1\u8db3\u591a\u6837\u6027\u548c\u5e73\u6ed1\u6027\u7b49\u591a\u51c6\u5219\u7684\u52a8\u4f5c\u5e8f\u5217\u3002", "result": "\u5728\u56db\u4e2a\u6d41\u884c\u7684\u4eba\u7c7b\u52a8\u4f5c\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cMCG-IMM\u5728\u52a8\u4f5c\u751f\u6210\u4efb\u52a1\u4e2d\u4f18\u4e8e\u73b0\u6709\u6700\u4f18\u65b9\u6cd5\u3002", "conclusion": "MCG-IMM\u4f5c\u4e3a\u4e00\u79cd\u5373\u63d2\u5373\u7528\u7684\u65b9\u6cd5\uff0c\u80fd\u591f\u663e\u8457\u63d0\u5347\u9884\u8bad\u7ec3\u6a21\u578b\u751f\u6210\u52a8\u4f5c\u7684\u591a\u6837\u6027\uff0c\u517c\u5bb9\u591a\u79cd\u751f\u6210\u6a21\u578b\u5bb6\u65cf\uff0c\u6548\u679c\u663e\u8457\u3002"}}
{"id": "2508.02443", "categories": ["cs.GR", "cs.CV"], "pdf": "https://arxiv.org/pdf/2508.02443", "abs": "https://arxiv.org/abs/2508.02443", "authors": ["Thomas Gottwald", "Edgar Heinert", "Matthias Rottmann"], "title": "Uncertainty Estimation for Novel Views in Gaussian Splatting from Primitive-Based Representations of Error and Visibility", "comment": null, "summary": "In this work, we present a novel method for uncertainty estimation (UE) in\nGaussian Splatting. UE is crucial for using Gaussian Splatting in critical\napplications such as robotics and medicine. Previous methods typically estimate\nthe variance of Gaussian primitives and use the rendering process to obtain\npixel-wise uncertainties. Our method establishes primitive representations of\nerror and visibility of trainings views, which carries meaningful uncertainty\ninformation. This representation is obtained by projection of training error\nand visibility onto the primitives. Uncertainties of novel views are obtained\nby rendering the primitive representations of uncertainty for those novel\nviews, yielding uncertainty feature maps. To aggregate these uncertainty\nfeature maps of novel views, we perform a pixel-wise regression on holdout\ndata. In our experiments, we analyze the different components of our method,\ninvestigating various combinations of uncertainty feature maps and regression\nmodels. Furthermore, we considered the effect of separating splatting into\nforeground and background. Our UEs show high correlations to true errors,\noutperforming state-of-the-art methods, especially on foreground objects. The\ntrained regression models show generalization capabilities to new scenes,\nallowing uncertainty estimation without the need for holdout data.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u9ad8\u65af\u6563\u5e03\u4e0d\u786e\u5b9a\u6027\u4f30\u8ba1\u65b9\u6cd5\uff0c\u901a\u8fc7\u8bad\u7ec3\u8bef\u5dee\u548c\u53ef\u89c1\u6027\u7684\u539f\u59cb\u8868\u793a\u6765\u83b7\u53d6\u66f4\u6709\u610f\u4e49\u7684\u50cf\u7d20\u7ea7\u4e0d\u786e\u5b9a\u6027\u4fe1\u606f\uff0c\u5e76\u5728\u5b9e\u9a8c\u4e2d\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u9ad8\u65af\u6563\u5e03\u5728\u673a\u5668\u4eba\u548c\u533b\u5b66\u7b49\u5173\u952e\u5e94\u7528\u4e2d\u9700\u8981\u53ef\u9760\u7684\u4e0d\u786e\u5b9a\u6027\u4f30\u8ba1\uff0c\u73b0\u6709\u65b9\u6cd5\u901a\u5e38\u4ec5\u4f30\u8ba1\u9ad8\u65af\u539f\u59cb\u4ef6\u7684\u65b9\u5dee\uff0c\u672a\u80fd\u5145\u5206\u6355\u6349\u4e0d\u786e\u5b9a\u6027\u4fe1\u606f\u3002", "method": "\u901a\u8fc7\u5c06\u8bad\u7ec3\u8bef\u5dee\u548c\u53ef\u89c1\u6027\u6295\u5f71\u5230\u539f\u59cb\u4ef6\u4e0a\uff0c\u5efa\u7acb\u539f\u59cb\u4ef6\u7684\u8868\u793a\uff0c\u5e76\u901a\u8fc7\u6e32\u67d3\u65b0\u89c6\u56fe\u7684\u539f\u59cb\u4ef6\u8868\u793a\u6765\u83b7\u53d6\u4e0d\u786e\u5b9a\u6027\u7279\u5f81\u56fe\u3002\u6700\u540e\u901a\u8fc7\u4fdd\u7559\u6570\u636e\u7684\u50cf\u7d20\u7ea7\u56de\u5f52\u805a\u5408\u4e0d\u786e\u5b9a\u6027\u7279\u5f81\u56fe\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u771f\u5b9e\u8bef\u5dee\u4e0a\u8868\u73b0\u51fa\u9ad8\u76f8\u5173\u6027\uff0c\u5c24\u5176\u5728\u524d\u666f\u5bf9\u8c61\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002\u56de\u5f52\u6a21\u578b\u5bf9\u65b0\u573a\u666f\u5177\u6709\u6cdb\u5316\u80fd\u529b\uff0c\u65e0\u9700\u4fdd\u7559\u6570\u636e\u5373\u53ef\u4f30\u8ba1\u4e0d\u786e\u5b9a\u6027\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u9ad8\u65af\u6563\u5e03\u7684\u4e0d\u786e\u5b9a\u6027\u4f30\u8ba1\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u5c24\u5176\u5728\u5173\u952e\u5e94\u7528\u4e2d\u5177\u6709\u663e\u8457\u4f18\u52bf\u3002"}}
