<div id=toc></div>

# Table of Contents

- [cs.GR](#cs.GR) [Total: 4]
- [cs.PL](#cs.PL) [Total: 4]
- [cs.GT](#cs.GT) [Total: 3]


<div id='cs.GR'></div>

# cs.GR [[Back]](#toc)

### [1] [PartMotionEdit: Fine-Grained Text-Driven 3D Human Motion Editing via Part-Level Modulation](https://arxiv.org/abs/2512.24200)
*Yujie Yang,Zhichao Zhang,Jiazhou Chen,Zichao Wu*

Main category: cs.GR

TL;DR: 提出了一种名为PartMotionEdit的细粒度3D人体运动编辑框架，通过部分级语义调制实现精确控制，优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 当前基于文本驱动的3D人体运动编辑方法难以精确控制局部运动，因其全局建模特性导致细节控制不足。

Method: 提出PartMotionEdit框架，包含Part-aware Motion Modulation (PMM)模块和Bidirectional Motion Interaction (BMI)模块，通过部分级语义调制和双向跨模态注意力实现精确编辑。

Result: 在知名基准测试中，PartMotionEdit在定量和定性评估中均优于现有方法。

Conclusion: PartMotionEdit通过部分级语义调制和双向交互，显著提升了3D人体运动编辑的精确性和可解释性。

Abstract: Existing text-driven 3D human motion editing methods have demonstrated significant progress, but are still difficult to precisely control over detailed, part-specific motions due to their global modeling nature. In this paper, we propose PartMotionEdit, a novel fine-grained motion editing framework that operates via part-level semantic modulation. The core of PartMotionEdit is a Part-aware Motion Modulation (PMM) module, which builds upon a predefined five-part body decomposition. PMM dynamically predicts time-varying modulation weights for each body part, enabling precise and interpretable editing of local motions. To guide the training of PMM, we also introduce a part-level similarity curve supervision mechanism enhanced with dual-layer normalization. This mechanism assists PMM in learning semantically consistent and editable distributions across all body parts. Furthermore, we design a Bidirectional Motion Interaction (BMI) module. It leverages bidirectional cross-modal attention to achieve more accurate semantic alignment between textual instructions and motion semantics. Extensive quantitative and qualitative evaluations on a well-known benchmark demonstrate that PartMotionEdit outperforms the state-of-the-art methods.

</details>


### [2] [BATISNet: Instance Segmentation of Tooth Point Clouds with Boundary Awareness](https://arxiv.org/abs/2512.24201)
*Yating Cai,Yanghui Xu,Zehua Hu,Jiazhou Chen,Jing Huang*

Main category: cs.GR

TL;DR: BATISNet是一种边界感知实例网络，用于解决复杂牙齿点云分割问题，结合语义和实例特征，并通过边界感知损失函数提高分割完整性和准确性。


<details>
  <summary>Details</summary>
Motivation: 现有语义分割方法在处理牙齿紧密排列、边界不清晰及复杂病例（如缺牙、错位牙）时效果不佳，需改进以实现更精准的分割。

Method: 提出BATISNet网络，包括特征提取主干和实例分割模块，结合语义与实例特征，并使用边界感知损失函数监督边界分割。

Result: 实验表明，BATISNet在复杂临床场景中优于现有方法，提高了牙齿完整性分割的准确性和可靠性。

Conclusion: BATISNet成功解决了牙齿点云分割中的边界模糊和牙齿粘连问题，为临床提供了更可靠的数据支持。

Abstract: Accurate segmentation of the tooth point cloud is of great significance for diagnosis clinical assisting and treatment planning. Existing methods mostly employ semantic segmentation, focusing on the semantic feature between different types of teeth. However, due to the tightly packed structure of teeth, unclear boundaries, and the diversity of complex cases such as missing teeth, malposed teeth, semantic segmentation often struggles to achieve satisfactory results when dealing with complex dental cases. To address these issues, this paper propose BATISNet, a boundary-aware instance network for tooth point cloud segmentation. This network model consists of a feature extraction backbone and an instance segmentation module. It not only focuses on extracting the semantic features of different types of teeth but also learns the instance features of individual teeth. It helps achieve more robust and accurate tooth instance segmentation in complex clinical scenarios such as missing teeth and malposed teeth. Additionally, to further enhance the completeness and accuracy of tooth boundary segmentation, a boundary-aware loss function is designed to specifically supervise the boundary segmentation between instances. It mitigates effectively tooth adhesion and boundary ambiguity issues. Extensive experimental results show that BATISNet outperforms existing methods in tooth integrity segmentation, providing more reliable and detailed data support for practical clinical applications.

</details>


### [3] [The Uncanny Valley in medical simulation-based training: a visual summary](https://arxiv.org/abs/2512.24240)
*Eleni Grigoriou,Manos Kamarianakis,George Papagiannakis*

Main category: cs.GR

TL;DR: 该综述文章旨在提供关于“恐怖谷效应”（UV）及其对医学虚拟现实模拟训练影响的文献和证据指南。


<details>
  <summary>Details</summary>
Motivation: 研究团队由计算机图形学、虚拟现实和医学教育领域的专家组成，旨在从多学科角度探讨这一问题，以推动医学训练中VR应用的边界。

Method: 文章采用了文献综述和证据分析的方法，结合团队在计算机图形系统、VR角色模拟和创新教育技术方面的经验。

Result: 研究揭示了恐怖谷效应对医学虚拟现实训练的重要性，强调了真实感和沉浸感在有效学习中的关键作用。

Conclusion: 理解和解决恐怖谷效应对于提升医学虚拟现实训练的效果至关重要，团队的多学科合作为此提供了宝贵视角。

Abstract: The purpose of this review article is to provide a bibliographical as well as evidence-based visual guide regarding the effect of ``Uncanny Valley'' (UV) and how it profoundly influences medical virtual reality simulation-based training. The phenomenon, where increasingly realistic virtual humans elicit discomfort due to subtle imperfections, is crucial to understand and address in the context of medical training, where realism and immersion are key to effective learning.
  Our research team, consisting of experts in computer graphics, virtual reality, and medical education, brings a diverse and multidisciplinary perspective to this subject. Our collective experience spans developing advanced computer graphics systems, VR character simulation, and innovative educational technologies. We have collaborated across institutions and industries to push the boundaries of VR applications in medical training.

</details>


### [4] [PhysTalk: Language-driven Real-time Physics in 3D Gaussian Scenes](https://arxiv.org/abs/2512.24986)
*Luca Collorone,Mert Kiray,Indro Spinelli,Fabio Galasso,Benjamin Busam*

Main category: cs.GR

TL;DR: PhysTalk是一种基于3D高斯分布和物理模拟的实时4D动画生成框架，通过语言模型直接生成可执行代码，避免了传统方法的时间消耗和复杂性。


<details>
  <summary>Details</summary>
Motivation: 现有的视觉特效生成方法缺乏物理真实性和高效的语言界面，且需要耗时的离线优化。PhysTalk旨在通过结合3D高斯分布和物理模拟，实现开放词汇的实时交互式动画生成。

Method: PhysTalk以3D高斯分布场景为输入，利用大型语言模型生成可直接修改3D高斯分布参数的代码，通过轻量级代理和粒子动力学实现实时物理模拟。

Result: PhysTalk首次将3D高斯分布直接与物理模拟器耦合，无需依赖耗时的网格提取，可实现基于碰撞感知的交互式多材质物体动画。

Conclusion: PhysTalk无需训练且计算轻量，使得4D动画生成更加普及，将传统的"渲染并等待"模式转变为基于物理的实时交互对话。

Abstract: Realistic visual simulations are omnipresent, yet their creation requires computing time, rendering, and expert animation knowledge. Open-vocabulary visual effects generation from text inputs emerges as a promising solution that can unlock immense creative potential. However, current pipelines lack both physical realism and effective language interfaces, requiring slow offline optimization. In contrast, PhysTalk takes a 3D Gaussian Splatting (3DGS) scene as input and translates arbitrary user prompts into real time, physics based, interactive 4D animations. A large language model (LLM) generates executable code that directly modifies 3DGS parameters through lightweight proxies and particle dynamics. Notably, PhysTalk is the first framework to couple 3DGS directly with a physics simulator without relying on time consuming mesh extraction. While remaining open vocabulary, this design enables interactive 3D Gaussian animation via collision aware, physics based manipulation of arbitrary, multi material objects. Finally, PhysTalk is train-free and computationally lightweight: this makes 4D animation broadly accessible and shifts these workflows from a "render and wait" paradigm toward an interactive dialogue with a modern, physics-informed pipeline.

</details>


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [5] [Enforcing Temporal Constraints for LLM Agents](https://arxiv.org/abs/2512.23738)
*Adharsh Kamath,Sishen Zhang,Calvin Xu,Shubham Ugare,Gagandeep Singh,Sasa Misailovic*

Main category: cs.PL

TL;DR: Agent-C是一个为LLM代理提供运行时保证的框架，确保其遵守形式化的时间安全策略，相比现有方法在安全性和任务效用上均有显著提升。


<details>
  <summary>Details</summary>
Motivation: 当前的安全防护系统未能有效防止LLM代理违反时间安全策略（如用户未验证即访问敏感数据），且缺乏形式化保证。Agent-C旨在填补这一空白。

Method: Agent-C通过领域特定语言表达时间属性，将其转化为一阶逻辑，并使用SMT求解在生成标记时检测违规行为，同时利用约束生成技术确保动作合规。

Result: 在零售客服和机票预订系统中，Agent-C实现了100%的安全合规性，同时提高了任务效用，优于现有防护措施和无限制代理。

Conclusion: Agent-C为LLM代理提供了可靠的时间安全策略保障，是提升代理推理可靠性的前沿技术。

Abstract: LLM-based agents are deployed in safety-critical applications, yet current guardrail systems fail to prevent violations of temporal safety policies, requirements that govern the ordering and sequencing of agent actions. For instance, agents may access sensitive data before authenticating users or process refunds to unauthorized payment methods, violations that require reasoning about sequences of action rather than an individual action. Existing guardrails rely on imprecise natural language instructions or post-hoc monitoring, and provide no formal guarantees that agents will satisfy temporal constraints. We present Agent-C, a novel framework that provides run-time guarantees ensuring LLM agents adhere to formal temporal safety properties. Agent-C introduces a domain-specific language for expressing temporal properties (e.g., authenticate before accessing data), translates specifications to first-order logic, and uses SMT solving to detect non-compliant agent actions during token generation. When the LLM attempts to generate a non-compliant tool call, Agent-C leverages constrained generation techniques to ensure that every action generated by the LLM complies with the specification, and to generate a compliant alternative to a non-compliant agent action. We evaluate Agent-C across two real-world applications: retail customer service and airline ticket reservation system, and multiple language models (open and closed-source). Our results demonstrate that Agent-C achieves perfect safety (100% conformance, 0% harm), while improving task utility compared to state-of-the-art guardrails and unrestricted agents. On SoTA closed-source models, Agent-C improves conformance (77.4% to 100% for Claude Sonnet 4.5 and 83.7% to 100% for GPT-5), while simultaneously increasing utility (71.8% to 75.2% and 66.1% to 70.6%, respectively), representing a new SoTA frontier for reliable agentic reasoning.

</details>


### [6] [Towards representation agnostic probabilistic programming](https://arxiv.org/abs/2512.23740)
*Ole Fenske,Maximilian Popko,Sebastian Bader,Thomas Kirste*

Main category: cs.PL

TL;DR: 该论文提出了一种因子抽象方法，通过五个基本操作作为通用接口，实现了不依赖具体表示的通用概率编程框架，支持混合离散-连续模型的推理。


<details>
  <summary>Details</summary>
Motivation: 现有的概率编程语言和工具将模型表示与特定推理算法紧密耦合，限制了在新颖表示或混合离散-连续模型上的实验。

Method: 引入了一种因子抽象方法，提供了五个基本操作作为通用接口，支持对不同表示（如离散表、高斯分布、基于样本的方法）的统一操纵。

Result: 该方法实现了表示无关的概率编程，允许在单一框架内自由混合不同表示，从而支持复杂混合模型的实际推理，解决了现有工具无法充分表达的问题。

Conclusion: 因子抽象方法为概率编程提供了一种灵活且通用的解决方案，显著提升了复杂混合模型的推理能力。

Abstract: Current probabilistic programming languages and tools tightly couple model representations with specific inference algorithms, preventing experimentation with novel representations or mixed discrete-continuous models. We introduce a factor abstraction with five fundamental operations that serve as a universal interface for manipulating factors regardless of their underlying representation. This enables representation-agnostic probabilistic programming where users can freely mix different representations (e.g. discrete tables, Gaussians distributions, sample-based approaches) within a single unified framework, allowing practical inference in complex hybrid models that current toolkits cannot adequately express.

</details>


### [7] [VGC: A High-Performance Zone-Based Garbage Collector Architecture for Python with Partitioning and Parallel Execution](https://arxiv.org/abs/2512.23768)
*Abdulla M*

Main category: cs.PL

TL;DR: VGC是一种新型内存管理框架，通过双层架构（Active VGC和Passive VGC）优化性能，适用于从嵌入式设备到高性能并行架构的多样化系统。


<details>
  <summary>Details</summary>
Motivation: 传统垃圾收集器在多样化系统和并行架构中表现不佳，VGC旨在通过双层架构提供高效、低开销的内存管理解决方案。

Method: VGC采用双层架构：Active VGC使用并发的标记-清除策略动态管理运行时对象，Passive VGC在编译时通过预测性内存映射优化静态对象分配。

Result: VGC在多线程基准测试中暂停时间减少30%，内存占用减少25%，并提供可预测的内存访问模式，提升了现代并行应用的可扩展性。

Conclusion: VGC通过集成编译时和运行时优化，为内存密集型系统提供了一个健壮且适应性强的解决方案。

Abstract: The Virtual Garbage Collector (VGC) introduces a novel memory management framework designed to optimize performance across diverse systems, ranging from resource constrained embedded devices to high performance parallel architectures. Unlike conventional garbage collectors, VGC employs a dual layer architecture consisting of Active VGC and Passive VGC to enable efficient, low overhead memory management. Active VGC dynamically manages runtime objects using a concurrent mark and sweep strategy tailored for parallel workloads, reducing pause times by up to 30 percent compared to generational collectors in multithreaded benchmarks. Passive VGC operates at compile time and optimizes static object allocation through predictive memory mapping, minimizing fragmentation by aligning objects to cache boundaries. This separation of responsibilities ensures predictable memory access patterns, reduces total memory usage by up to 25 percent, and improves scalability for modern parallel applications. By integrating compile time and runtime optimizations, VGC provides a robust and adaptable solution for memory intensive systems across both low level and high level programming environments.

</details>


### [8] [State Space Estimation for DPOR-based Model Checkers](https://arxiv.org/abs/2512.23996)
*A. R. Balasubramanian,Mohammad Hossein Khoshechin Jorshari,Rupak Majumdar,Umang Mathur,Minjian Zhang*

Main category: cs.PL

TL;DR: 本文研究了并发程序的估计问题，提出了一种蒙特卡洛方法来生成无偏估计器，用于计算Mazurkiewicz跟踪等价类的数量。该方法通过将无状态DPOR算法转换为估计器，并结合Knuth的经典估计器，实现了高效的近似。实验表明，该方法在有限预算下能稳定估计，误差通常在20%以内。


<details>
  <summary>Details</summary>
Motivation: 研究动机在于解决并发程序模型检查中的两个实际问题：估计模型检查的运行时间和已覆盖的搜索空间比例。现有的计数问题被证明是难以高效解决的，因此需要开发新的近似方法。

Method: 方法包括将无状态DPOR算法转换为无偏估计器，并将其探索过程视为一个有界深度和宽度的树。通过结合Knuth的经典估计器和随机枚举技术，控制估计的方差。

Result: 实验结果表明，该方法能在有限的预算下生成稳定的估计，误差通常在20%以内，即使状态空间包含10^5到10^6个等价类。此外，该方法还能估计模型检查的成本。

Conclusion: 本文提出的算法是首个可证明的多项式时间无偏估计器，解决了分配模型检查资源时的一个重要问题。实验验证了其高效性和实用性。

Abstract: We study the estimation problem for concurrent programs: given a bounded program $P$, estimate the number of Mazurkiewicz trace-equivalence classes induced by its interleavings. This quantity informs two practical questions for enumeration-based model checking: how long a model checking run is likely to take, and what fraction of the search space has been covered so far. We first show the counting problem is #P-hard even for restricted programs and, unless $P=NP$, inapproximable within any subexponential factor, ruling out efficient exact or randomized approximation algorithms. We give a Monte Carlo approach to obtain a poly-time unbiased estimator: we convert a stateless optimal DPOR algorithm into an unbiased estimator by viewing its exploration as a bounded-depth, bounded-width tree whose leaves are the maximal Mazurkiewicz traces. A classical estimator by Knuth, when run on this tree, yields an unbiased estimate. To control the variance, we apply stochastic enumeration by maintaining a small population of partial paths per depth whose evolution is coupled. We have implemented our estimator in the JMC model checker and evaluated it on shared-memory benchmarks. With modest budgets, our estimator yields stable estimates, typically within a 20% band, within a few hundred trials, even when the state space has $10^5$--$10^6$ classes. We also show how the same machinery estimates model-checking cost by weighting all explored graphs, not only complete traces. Our algorithms provide the first provable poly-time unbiased estimators for counting traces, a problem of considerable importance when allocating model checking resources.

</details>


<div id='cs.GT'></div>

# cs.GT [[Back]](#toc)

### [9] [Personalized Promotions in Practice: Dynamic Allocation and Reference Effects](https://arxiv.org/abs/2512.23781)
*Jackie Baek,Will Ma,Dmitry Mitrofanov*

Main category: cs.GT

TL;DR: 论文提出了一个高效的个性化促销策略，每天为超过2000万客户选择最佳折扣，同时遵守全球分配约束，并在A/B测试中实现了4.5%的收入增长。


<details>
  <summary>Details</summary>
Motivation: 为了解决大规模客户群体个性化促销的问题，同时考虑客户的跨时间行为和参考效应，以提高收入为目标。

Method: 提出了一种高效的策略，每天为每个客户分配促销折扣（10%、12%、15%、17%或20%），并考虑了客户的跨时间状态和参考效应，提出了一个新的组合定价模型。

Result: 策略在实际部署中成功实现了4.5%的收入增长，通过更好地定位促销敏感客户和学习客户的跨时间模式。理论模型也揭示了最优策略的结构：在提供较差促销折扣ℓ次后，提供一次较好的折扣。

Conclusion: 研究证明了高效个性化促销策略的可行性，并通过理论模型揭示了最优促销策略的周期性结构，为实际应用提供了重要指导。

Abstract: Partnering with a large online retailer, we consider the problem of sending daily personalized promotions to a userbase of over 20 million customers. We propose an efficient policy for determining, every day, the promotion that each customer should receive (10%, 12%, 15%, 17%, or 20% off), while respecting global allocation constraints. This policy was successfully deployed to see a 4.5% revenue increase during an A/B test, by better targeting promotion-sensitive customers and also learning intertemporal patterns across customers.
  We also consider theoretically modeling the intertemporal state of the customer. The data suggests a simple new combinatorial model of pricing with reference effects, where the customer remembers the best promotion they saw over the past $\ell$ days as the "reference value", and is more likely to purchase if this value is poor. We tightly characterize the structure of optimal policies for maximizing long-run average revenue under this model -- they cycle between offering poor promotion values $\ell$ times and offering good values once.

</details>


### [10] [Multilevel Fair Allocation](https://arxiv.org/abs/2512.24105)
*Maxime Lucet,Nawal Benabbou,Aurélie Beynier,Nicolas Maudet*

Main category: cs.GT

TL;DR: 本文提出了多级公平资源分配的概念，针对具有树状层次结构的代理关系，设计了两种算法以确保公平性和效率。


<details>
  <summary>Details</summary>
Motivation: 研究动机在于解决具有层次结构的代理关系中资源分配的公平性和效率问题，尤其是在多级环境下如何保持这些特性。

Method: 提出了两种算法：一种是通用的多项式时间顺序算法，以自上而下的方式运行；另一种是扩展的General Yankee Swap算法，适应多级环境。

Result: 第一种算法在效率和公平性方面提供了理论保证，第二种算法虽然在理论上仅保证效率，但在实践中表现出优秀的公平性。

Conclusion: 本文证明了在多级树状结构代理关系中设计高效公平的资源分配算法的可行性，并通过两种具体算法展示了其实际应用效果。

Abstract: We introduce the concept of multilevel fair allocation of resources with tree-structured hierarchical relations among agents. While at each level it is possible to consider the problem locally as an allocation of an agent to its children, the multilevel allocation can be seen as a trace capturing the fact that the process is iterated until the leaves of the tree. In principle, each intermediary node may have its own local allocation mechanism. The main challenge is then to design algorithms which can retain good fairness and efficiency properties. In this paper we propose two original algorithms under the assumption that leaves of the tree have matroid-rank utility functions and the utility of any internal node is the sum of the utilities of its children. The first one is a generic polynomial-time sequential algorithm that comes with theoretical guarantees in terms of efficiency and fairness. It operates in a top-down fashion -- as commonly observed in real-world applications -- and is compatible with various local algorithms. The second one extends the recently proposed General Yankee Swap to the multilevel setting. This extension comes with efficiency guarantees only, but we show that it preserves excellent fairness properties in practice.

</details>


### [11] [On the Difficulty of Measuring Divisiveness of Proposals under Ranked Preferences](https://arxiv.org/abs/2512.24467)
*Ulle Endriss*

Main category: cs.GT

TL;DR: 本文探讨了在公共政策提案中如何识别最具‘分裂性’提案的方法，并揭示了满足基本规范性要求时面临的困难。


<details>
  <summary>Details</summary>
Motivation: 设计支持数字民主倡议的在线参与平台时，需要识别最具分裂性的提案，以引导讨论并分析审议进展。

Method: 采用了社会选择理论中的公理化方法，探索基于人们偏好的分裂性提案选择定义，并分析其规范性要求。

Result: 研究发现，在满足看似温和的规范性要求下，选择最具分裂性提案的任务存在根本性困难。

Conclusion: 在选择最具分裂性提案时，即使满足看似温和的规范性要求，仍存在根本性困难，这对数字民主平台的提案筛选功能提出了挑战。

Abstract: Given the stated preferences of several people over a number of proposals regarding public policy initiatives, some of those proposals might be judged to be more ``divisive'' than others. When designing online participatory platforms to support digital democracy initiatives enabling citizens to deliberate over such proposals, we might wish to equip those platforms with the functionality to retrieve the most divisive proposals currently under discussion. Such a service would be useful for analysing the progress of deliberation and steering discussion towards issues that still require further debate. Guided by this use case, we explore possibilities for providing a clear definition of what it means to select a set of most divisive proposals on the basis of people's stated preferences over proposals. Then, employing the axiomatic method familiar from social choice theory, we show that the task of selecting the most divisive proposals in a manner that satisfies certain seemingly mild normative requirements faces a number of fundamental difficulties.

</details>
