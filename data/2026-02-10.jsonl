{"id": "2602.07369", "categories": ["cs.GR"], "pdf": "https://arxiv.org/pdf/2602.07369", "abs": "https://arxiv.org/abs/2602.07369", "authors": ["Julian Knodt", "Xifeng Gao"], "title": "Convex Primitive Decomposition for Collision Detection", "comment": null, "summary": "Creation of collision objects for 3D models is a time-consuming task, requiring modelers to manually place primitives such as bounding boxes, capsules, spheres, and other convex primitives to approximate complex meshes. While there has been work in automatic approximate convex decompositions of meshes using convex hulls, they are not practical for applications with tight performance budgets such as games due to slower collision detection and inability to manually modify the output while maintaining convexity as compared to manually placed primitives. Rather than convex decomposition with convex hulls, we devise an approach for bottom-up decomposition of an input mesh into convex primitives specifically for rigid body simulation inspired by quadric mesh simplification. This approach fits primitives to complex, real-world meshes that provide plausible simulation performance and are guaranteed to enclose the input surface. We test convex primitive decomposition on over 60 models from Sketchfab, showing the algorithm's effectiveness. On this dataset, convex primitive decomposition has lower one-way mean and median Hausdorff and Chamfer distance from the collider to the input compared to V-HACD and CoACD, with less than one-third of the complexity as measured by total bytes for each collider. On top of that, rigid-body simulation performance measured by wall-clock time is consistently improved across 24 tested models."}
{"id": "2602.07687", "categories": ["cs.GR"], "pdf": "https://arxiv.org/pdf/2602.07687", "abs": "https://arxiv.org/abs/2602.07687", "authors": ["Yue Chang", "Peter Yichen Chen", "Eitan Grinspun", "Maurizio M. Chiaramonte"], "title": "Low-Rank Koopman Deformables with Log-Linear Time Integration", "comment": null, "summary": "We present a low-rank Koopman operator formulation for accelerating deformable subspace simulation. Using a Dynamic Mode Decomposition (DMD) parameterization of the Koopman operator, our method learns the temporal evolution of deformable dynamics and predicts future states through efficient matrix evaluations instead of sequential time integration. This yields log-linear scaling in the number of time steps and allows large portions of the trajectory to be skipped while retaining accuracy. The resulting temporal efficiency is especially advantageous for optimization tasks such as control and initial-state estimation, where the objective often depends largely on the final configuration.\n  To broaden the scope of Koopman-based reduced-order models in graphics, we introduce a discretization-agnostic extension that learns shared dynamic behavior across multiple shapes and mesh resolutions. Prior DMD-based approaches have been restricted to a single shape and discretization, which limits their usefulness for tasks involving geometry variation. Our formulation generalizes across both shape and discretization, which enables fast shape optimization that was previously impractical for DMD models. This expanded capability highlights the potential of Koopman operator learning as a practical tool for efficient deformable simulation and design."}
{"id": "2602.07782", "categories": ["cs.GR"], "pdf": "https://arxiv.org/pdf/2602.07782", "abs": "https://arxiv.org/abs/2602.07782", "authors": ["Floria Gu", "Nicholas Vining", "Alla Sheffer"], "title": "TABI: Tight and Balanced Interactive Atlas Packing", "comment": null, "summary": "Atlas packing is a key step in many computer graphics applications. Packing algorithms seek to arrange a set of charts within a fixed-size atlas with as little downscaling as possible. Many packing applications such as content creation tools, dynamic atlas generation for video games, and texture space shading require on-the-fly interactive atlas packing. Unfortunately, while many methods have been developed for generating tight high-quality packings, they are designed for offline settings and have running times two or more orders of magnitude greater than what is required for interactive performance. While real-time GPU packing methods exist, they significantly downscale packed charts compared to offline methods. We introduce a GPU packing method that targets interactive speeds, provides packing quality approaching that of offline methods, and supports flexible user control over the tradeoff between performance and quality. We observe that current real-time packing methods leave large gaps between charts and often produce asymmetric, or poorly balanced, packings. These artifacts dramatically degrade packing quality. Our Tight And Balanced method eliminates these artifacts while retaining Interactive performance. TABI generates tight packings by compacting empty space between irregularly shaped charts both horizontally and vertically, using two approximations of chart shape that support efficient parallel processing. We balance packing outputs by automatically adjusting atlas row widths and orientations to accommodate varying chart heights. We show that our method significantly reduces chart downscaling compared to existing interactive methods while remaining orders of magnitude faster than offline alternatives."}
{"id": "2602.07853", "categories": ["cs.GR"], "pdf": "https://arxiv.org/pdf/2602.07853", "abs": "https://arxiv.org/abs/2602.07853", "authors": ["Xiang Feng", "Yunuo Chen", "Chang Yu", "Hao Su", "Demetri Terzopoulos", "Yin Yang", "Joe Masterjohn", "Alejandro Castro", "Chenfanfu Jiang"], "title": "MPM Lite: Linear Kernels and Integration without Particles", "comment": "19 pages", "summary": "In this paper, we introduce MPM Lite, a new hybrid Lagrangian/Eulerian method that eliminates the need for particle-based quadrature at solve time. Standard MPM practices suffer from a performance bottleneck where expensive implicit solves are proportional to particle-per-cell (PPC) counts due to the the choices of particle-based quadrature and wide-stencil kernels. In contrast, MPM Lite treats particles primarily as carriers of kinematic state and material history. By conceptualizing the background Cartesian grid as a voxel hexahedral mesh, we resample particle states onto fixed-location quadrature points using efficient, compact linear kernels. This architectural shift allows force assembly and the entire time-integration process to proceed without accessing particles, making the solver complexity no longer relate to particles. At the core of our method is a novel stress transfer and stretch reconstruction strategy. To avoid non-physical averaging of deformation gradients, we resample the extensive Kirchhoff stress and derive a rotation-free deformation reference solution, which naturally supports an optimization-based incremental potential formulation. Consequently, MPM Lite can be implemented as modular resampling units coupled with an FEM-style integration module, enabling the direct use of off-the-shelf nonlinear solvers, preconditioners, and unambiguous boundary conditions. We demonstrate through extensive experiments that MPM Lite preserves the robustness and versatility of traditional MPM across diverse materials while delivering significant speedups in implicit settings and improving explicit settings at the same time. Check our project page at https://mpmlite.github.io."}
{"id": "2602.07385", "categories": ["cs.GT"], "pdf": "https://arxiv.org/pdf/2602.07385", "abs": "https://arxiv.org/abs/2602.07385", "authors": ["Elad Lavi", "Hadas Shachnai", "Inbal Talgam-Cohen"], "title": "Online Contract Design", "comment": null, "summary": "We initiate the study of online contracts, which integrate the game-theoretic considerations of economic contract theory, with the algorithmic and informational challenges of online algorithm design. Our starting point is the classic online setting with preemption of Buchbinder et al. [SODA'15], in which a hiring principal faces a sequence of adversarial agent arrivals. Upon arrival, the principal must decide whether to tentatively accept the agent to their team, and whether to dismiss previous tentative choices. Dismissal is irrevocable, giving the setting its online decision-making flavor. In our setting, the agents are rational players: once the team is finalized, a game is played where the principal offers contracts (performance-based payment schemes), and each agent decides whether or not to work. Working agents reward the principal, and the goal is to choose a team that maximizes the principal's utility. Our main positive result is a 1/2-competitive algorithm when agent rewards are additive, which matches the best-possible competitive ratio. Our algorithm is randomized and this is necessary, as we show that no deterministic algorithm can attain a bounded competitive ratio. Moreover, if agent rewards are allowed to exhibit combinatorial structure known as XOS, even randomized algorithms might fail. En route to our competitive algorithm, we develop the technique of balance points, which can be useful for further exploration of online contracts in the adversarial model."}
{"id": "2602.07324", "categories": ["cs.PL"], "pdf": "https://arxiv.org/pdf/2602.07324", "abs": "https://arxiv.org/abs/2602.07324", "authors": ["Abdullah H. Rasheed"], "title": "Static Analysis Under Non-Deterministic Program Assumptions", "comment": null, "summary": "Static analyses overwhelmingly trade precision for soundness and automation. For this reason, their use-cases are restricted to situations where imprecision isn't prohibitive. In this paper, we propose and specify a static analysis that accepts user-supplied program assumptions that are local to program locations. Such assumptions can be used to counteract imprecision in static analyses, enabling their use in a much wider variety of applications. These assumptions are taken by the analyzer non-deterministically, resulting in a function from sets of accepted assumptions to the resulting analysis under those assumptions. We also demonstrate the utility of such a function in two ways, both of which showcase how it can enable optimization over a search space of assumptions that is otherwise infeasible without the specified analysis."}
{"id": "2602.08094", "categories": ["cs.GR", "math.NA"], "pdf": "https://arxiv.org/pdf/2602.08094", "abs": "https://arxiv.org/abs/2602.08094", "authors": ["Kevin You", "Juntian Zheng", "Minchen Li"], "title": "Energy-Controllable Time Integration for Elastodynamic Contact", "comment": null, "summary": "Dynamic simulation of elastic bodies is a longstanding task in engineering and computer graphics. In graphics, numerical integrators like implicit Euler and BDF2 are preferred due to their stability at large time steps, but they tend to dissipate energy uncontrollably. In contrast, symplectic methods like implicit midpoint can conserve energy but are not unconditionally stable and fail on moderately stiff problems. To address these limitations, we propose a general class of numerical integrators for Hamiltonian problems which are symplectic on linear problems, yet have superior stability on nonlinear problems. With this, we derive a novel energy-controllable time integrator, A-search, a simple modification of implicit Euler that can follow user-specified energy targets, enabling flexible control over energy dissipation or conservation while maintaining stability and physical fidelity. Our method integrates seamlessly with barrier-type energies and allows for inversion-free and penetration-free guarantees, making it well-suited for handling large deformations and complex collisions. Extensive evaluations over a wide range of material parameters and scenes demonstrate that A-search has biases to keep energy in low frequency motion rather than dissipation, and A-search outperforms traditional methods such as BDF2 at similar total running times by maintaining energy and leading to more visually desirable simulations."}
{"id": "2602.08452", "categories": ["cs.GT"], "pdf": "https://arxiv.org/pdf/2602.08452", "abs": "https://arxiv.org/abs/2602.08452", "authors": ["Senthil Rajasekaran", "Moshe Y. Vardi"], "title": "Modeling Concurrent Multi-Agent Systems", "comment": null, "summary": "Recent work in the field of multi-agent systems has sought to use techniques and concepts from the field of formal methods to provide rigorous theoretical analysis and guarantees on complex systems where multiple agents strategically interact, leading to the creation of the field of equilibrium analysis, which studies equilibria concepts from the field of game theory through a complexity-theoretic lens. Multi-agent systems, however, are complex mathematical objects, and, therefore, defining them in a precise mathematical manner is non-trivial. As a result, researchers often considered more restrictive models that are easier to model but lack expressive power or simply omit critical complexity-theoretic results in their analysis. This paper addresses this problem by carefully analyzing and contrasting complexity-theoretic results in the explicit model, a mathematically precise formulation of the models commonly used in the literature, and the circuit-based model, a novel model that addresses the problems found in the literature. The utility of the circuit-based model is demonstrated through a comprehensive analysis that considers upper and lower bounds for the realizability and verification problems, the two most important decision problems in equilibrium analysis, for both models. By conducting this analysis, we see that problematic issues that are endemic to the explicit model and the equilibrium analysis literature as a whole are adequately handled by the circuit-based model."}
{"id": "2602.07455", "categories": ["cs.PL"], "pdf": "https://arxiv.org/pdf/2602.07455", "abs": "https://arxiv.org/abs/2602.07455", "authors": ["Jinhua Wu", "Yuting Wang", "Liukun Yu", "Linglong Meng"], "title": "RustCompCert: A Verified and Verifying Compiler for a Sequential Subset of Rust", "comment": "Submitted to Rust Verify 2026", "summary": "We present our ongoing work on developing an end-to-end verified Rust compiler based on CompCert. It provides two guarantees: one is semantics preservation from Rust to assembly, i.e., the behaviors of source code includes the behaviors of target code, with which the properties verified at the source can be preserved down to the target; the other is memory safety ensured by the verifying compilation -- the borrow checking pass, which can simplify the verification of Rust programs, e.g., by allowing the verification tools focus on the functional correctness."}
{"id": "2602.08642", "categories": ["cs.GR"], "pdf": "https://arxiv.org/pdf/2602.08642", "abs": "https://arxiv.org/abs/2602.08642", "authors": ["Martin Bálint", "Corentin Salaün", "Hans-Peter Seidel", "Karol Myszkowski"], "title": "Forget Superresolution, Sample Adaptively (when Path Tracing)", "comment": null, "summary": "Real-time path tracing increasingly operates under extremely low sampling budgets, often below one sample per pixel, as rendering complexity, resolution, and frame-rate requirements continue to rise. While super-resolution is widely used in production, it uniformly sacrifices spatial detail and cannot exploit variations in noise, reconstruction difficulty, and perceptual importance across the image. Adaptive sampling offers a compelling alternative, but existing end-to-end approaches rely on approximations that break down in sparse regimes.\n  We introduce an end-to-end adaptive sampling and denoising pipeline explicitly designed for the sub-1-spp regime. Our method uses a stochastic formulation of sample placement that enables gradient estimation despite discrete sampling decisions, allowing stable training of a neural sampler at low sampling budgets. To better align optimization with human perception, we propose a tonemapping-aware training pipeline that integrates differentiable filmic operators and a state-of-the-art perceptual loss, preventing oversampling of regions with low visual impact.\n  In addition, we introduce a gather-based pyramidal denoising filter and a learnable generalization of albedo demodulation tailored to sparse sampling. Our results show consistent improvements over uniform sparse sampling, with notably better reconstruction of perceptually critical details such as specular highlights and shadow boundaries, and demonstrate that adaptive sampling remains effective even at minimal budgets."}
{"id": "2602.08504", "categories": ["cs.GT", "cs.AI", "cs.MA"], "pdf": "https://arxiv.org/pdf/2602.08504", "abs": "https://arxiv.org/abs/2602.08504", "authors": ["Piotr Skowron"], "title": "A General Theory of Proportionality with Additive Utilities", "comment": null, "summary": "We consider a model where a subset of candidates must be selected based on voter preferences, subject to general constraints that specify which subsets are feasible. This model generalizes committee elections with diversity constraints, participatory budgeting (including constraints specifying how funds must be allocated to projects from different pools), and public decision-making. Axioms of proportionality have recently been defined for this general model, but the proposed rules apply only to approval ballots, where each voter submits a subset of candidates she finds acceptable. We propose proportional rules for cardinal ballots, where each voter assigns a numerical value to each candidate corresponding to her utility if that candidate is selected. In developing these rules, we also introduce methods that produce proportional rankings, ensuring that every prefix of the ranking satisfies proportionality."}
{"id": "2602.07627", "categories": ["cs.PL"], "pdf": "https://arxiv.org/pdf/2602.07627", "abs": "https://arxiv.org/abs/2602.07627", "authors": ["Xuran Cai", "Amir Goharshady", "S Hitarth", "Chun Kit Lam"], "title": "Series-Parallel-Loop Decompositions of Control-flow Graphs", "comment": null, "summary": "Control-flow graphs (CFGs) of structured programs are well known to exhibit strong sparsity properties. Traditionally, this sparsity has been modeled using graph parameters such as treewidth and pathwidth, enabling the development of faster parameterized algorithms for tasks in compiler optimization, model checking, and program analysis. However, these parameters only approximate the structural constraints of CFGs: although every structured CFG has treewidth at most~7, many graphs with treewidth at most~7 cannot arise as CFGs. As a result, existing parameterized techniques are optimized for a substantially broader class of graphs than those encountered in practice.\n  In this work, we introduce a new grammar-based decomposition framework that characterizes \\emph{exactly} the class of control-flow graphs generated by structured programs. Our decomposition is intuitive, mirrors the syntactic structure of programs, and remains fully compatible with the dynamic-programming paradigm of treewidth-based methods. Using this framework, we design improved algorithms for two classical compiler optimization problems: \\emph{Register Allocation} and \\emph{Lifetime-Optimal Speculative Partial Redundancy Elimination (LOSPRE)}. Extensive experimental evaluation demonstrates significant performance improvements over previous state-of-the-art approaches, highlighting the benefits of using decompositions tailored specifically to CFGs."}
{"id": "2602.08549", "categories": ["cs.GT", "cs.FL"], "pdf": "https://arxiv.org/pdf/2602.08549", "abs": "https://arxiv.org/abs/2602.08549", "authors": ["Véronique Bruyère", "Emmanuel Filiot", "Christophe Grandmont", "Jean-François Raskin"], "title": "An Automata-Based Approach to Games with $ω$-Automatic Preferences", "comment": null, "summary": "This paper studies multiplayer turn-based games on graphs in which player preferences are modeled as $ω$-automatic relations given by deterministic parity automata. This contrasts with most existing work, which focuses on specific reward functions. We conduct a computational analysis of these games, starting with the threshold problem in the antagonistic zero-sum case. As in classical games, we introduce the concept of value, defined here as the set of plays a player can guarantee to improve upon, relative to their preference relation. We show that this set is recognized by an alternating parity automaton APW of polynomial size. We also establish the computational complexity of several problems related to the concepts of value and optimal strategy, taking advantage of the $ω$-automatic characterization of value. Next, we shift to multiplayer games and Nash equilibria, and revisit the threshold problem in this context. Based on an APW construction again, we close complexity gaps left open in the literature, and additionally show that cooperative rational synthesis is $\\mathsf{PSPACE}$-complete, while it becomes undecidable in the non-cooperative case."}
{"id": "2602.07742", "categories": ["cs.PL"], "pdf": "https://arxiv.org/pdf/2602.07742", "abs": "https://arxiv.org/abs/2602.07742", "authors": ["Nat Karmios", "Sacha-Élie Ayoun", "Philippa Gardner"], "title": "Gillian Debugging: Swinging Through the (Compositional Symbolic Execution) Trees, Extended Version", "comment": "24 pages, 11 figures. To be published at TACAS 2026", "summary": "In recent years, compositional symbolic execution (CSE) tools have been growing in prominence and are becoming more and more applicable to real-world codebases. Still to this day, however, debugging the output of these tools remains difficult, even for specialist users. To address this, we introduce a debugging interface for symbolic execution tools, integrated with Visual Studio Code and the Gillian multi-language CSE platform, with strong focus on visualisation, interactivity, and intuitive representation of symbolic execution trees. We take care in making this interface tool-agnostic, easing its transfer to other symbolic analysis tools in future. We empirically evaluate our work with a user study, the results of which show the debugger's usefulness in helping early researchers understand the principles of CSE and verify fundamental data structure algorithms in Gillian."}
{"id": "2602.08714", "categories": ["cs.GT"], "pdf": "https://arxiv.org/pdf/2602.08714", "abs": "https://arxiv.org/abs/2602.08714", "authors": ["Aris Filos-Ratsikas", "Georgios Kalantzis", "Alexandros A. Voudouris"], "title": "Approximate-EFX Allocations with Ordinal and Limited Cardinal Information", "comment": null, "summary": "We study a discrete fair division problem where $n$ agents have additive valuation functions over a set of $m$ goods. We focus on the well-known $α$-EFX fairness criterion, according to which the envy of an agent for another agent is bounded multiplicatively by $α$, after the removal of any good from the envied agent's bundle. The vast majority of the literature has studied $α$-EFX allocations under the assumption that full knowledge of the valuation functions of the agents is available. Motivated by the established literature on the distortion in social choice, we instead consider $α$-EFX algorithms that operate under limited information on these functions. In particular, we assume that the algorithm has access to the ordinal preference rankings, and is allowed to make a small number of queries to obtain further access to the underlying values of the agents for the goods. We show (near-optimal) tradeoffs between the values of $α$ and the number of queries required to achieve those, with a particular focus on constant EFX approximations. We also consider two interesting special cases, namely instances with a constant number of agents, or with two possible values, and provide improved positive results."}
{"id": "2602.08871", "categories": ["cs.GT", "cs.DM", "cs.DS"], "pdf": "https://arxiv.org/pdf/2602.08871", "abs": "https://arxiv.org/abs/2602.08871", "authors": ["Ziyi Cai", "D. D. Gao", "Prasanna Ramakrishnan", "Kangning Wang"], "title": "Distortion of Metric Voting with Bounded Randomness", "comment": null, "summary": "We study the design of voting rules in the metric distortion framework. It is known that any deterministic rule suffers distortion of at least $3$, and that randomized rules can achieve distortion strictly less than $3$, often at the cost of reduced transparency and interpretability. In this work, we explore the trade-off between these paradigms by asking whether it is possible to break the distortion barrier of $3$ using only \"bounded\" randomness. We answer in the affirmative by presenting a voting rule that (1) achieves distortion of at most $3 - \\varepsilon$ for some absolute constant $\\varepsilon > 0$, and (2) selects a winner uniformly at random from a deterministically identified list of constant size. Our analysis builds on new structural results for the distortion and approximation of Maximal Lotteries and Stable Lotteries."}
{"id": "2602.08966", "categories": ["cs.GT"], "pdf": "https://arxiv.org/pdf/2602.08966", "abs": "https://arxiv.org/abs/2602.08966", "authors": ["Hirota Kinoshita", "Ayumi Igarashi"], "title": "Maximin Shares with Lower Quotas", "comment": null, "summary": "We study the fair division of indivisible items among $n$ agents with heterogeneous additive valuations, subject to lower and upper quotas on the number of items allocated to each agent. Such constraints are crucial in various applications, ranging from personnel assignments to computing resource distribution. This paper focuses on the fairness criterion known as maximin shares (MMS) and its approximations. Under arbitrary lower and upper quotas, we show that a $\\left(\\frac{2n}{3n-1}\\right)$-MMS allocation of goods exists and can be computed in polynomial time, while we also present a polynomial-time algorithm for finding a $\\left(\\frac{3n-1}{2n}\\right)$-MMS allocation of chores. Furthermore, we consider the generalized scenario where items are partitioned into multiple categories, each with its own lower and upper quotas. In this setting, our algorithm computes an $\\left(\\frac{n}{2n-1}\\right)$-MMS allocation of goods or a $\\left(\\frac{2n-1}{n}\\right)$-MMS allocation of chores in polynomial time. These results extend previous work on the cardinality constraints, i.e., the special case where only upper quotas are imposed."}
