<div id=toc></div>

# Table of Contents

- [cs.GR](#cs.GR) [Total: 1]
- [cs.GT](#cs.GT) [Total: 13]


<div id='cs.GR'></div>

# cs.GR [[Back]](#toc)

### [1] [Photons x Force: Differentiable Radiation Pressure Modeling](https://arxiv.org/abs/2602.10712)
*Charles Constant,Elizabeth Bates,Santosh Bhattarai,Marek Ziebart,Tobias Ritschel*

Main category: cs.GR

TL;DR: 论文提出了一个系统，用于优化受辐射压力影响的参数化设计，特别是在航天器设计中，通过三种创新方法解决了高计算成本的问题。


<details>
  <summary>Details</summary>
Motivation: 辐射压力在航天器设计中是主要的非保守力机制，但其高保真模拟的高计算成本限制了其在大规模设计、优化和空间态势感知中的应用。

Method: 提出三种创新方法：1) 基于计算机图形学的蒙特卡洛模拟；2) 使用神经网络作为设计参数的力表示；3) 优化逆辐射压力设计。

Result: 这些方法显著提高了计算效率，使优化设计更加可行，能够快速查询力和优化多种参数。

Conclusion: 该系统通过创新的模拟、表示和优化方法，为航天器设计中的辐射压力问题提供了高效且实用的解决方案。

Abstract: We propose a system to optimize parametric designs subject to radiation pressure, \ie the effect of light on the motion of objects. This is most relevant in the design of spacecraft, where radiation pressure presents the dominant non-conservative forcing mechanism, which is the case beyond approximately 800 km altitude. Despite its importance, the high computational cost of high-fidelity radiation pressure modeling has limited its use in large-scale spacecraft design, optimization, and space situational awareness applications. We enable this by offering three innovations in the simulation, in representation and in optimization: First, a practical computer graphics-inspired Monte-Carlo (MC) simulation of radiation pressure. The simulation is highly parallel, uses importance sampling and next-event estimation to reduce variance and allows simulating an entire family of designs instead of a single spacecraft as in previous work. Second, we introduce neural networks as a representation of forces from design parameters. This neural proxy model, learned from simulations, is inherently differentiable and can query forces orders of magnitude faster than a full MC simulation. Third, and finally, we demonstrate optimizing inverse radiation pressure designs, such as finding geometry, material or operation parameters that minimizes travel time, maximizes proximity given a desired end-point, minimize thruster fuel, trains mission control policies or allocated compute budget in extraterrestrial compute.

</details>


<div id='cs.GT'></div>

# cs.GT [[Back]](#toc)

### [2] [The Complexity of Strategic Behavior in Primary Elections](https://arxiv.org/abs/2602.10290)
*Colin Cleveland,Bart de Keijzer,Maria Polukarov*

Main category: cs.GT

TL;DR: 研究初选中的战略行为计算复杂性，发现纯纳什均衡存在性判定为$Σ_2^{\mathbf P}$完全问题，最佳响应计算为NP完全问题，顺序初选中子博弈完美均衡存在性判定为PSPACE完全问题。


<details>
  <summary>Details</summary>
Motivation: 探讨初选系统相较于直接投票系统在多阶段过程中的战略行为计算复杂性。

Method: 建立一个初选模型，采用第一名当选制和固定平局解决规则，分析选民的战略行为。

Result: 发现初选系统中战略推理的计算难度显著增加，涉及$Σ_2^{\mathbf P}$完全、NP完全和PSPACE完全问题。

Conclusion: 初选系统为计算社会选择中的复杂性理论挑战提供了丰富的研究来源。

Abstract: We study the computational complexity of strategic behaviour in primary elections. Unlike direct voting systems, primaries introduce a multi-stage process in which voters first influence intra-party nominees before a general election determines the final winner. While previous work has evaluated primaries via welfare distortion, we instead examine their game-theoretic properties. We formalise a model of primaries under first-past-the-post with fixed tie-breaking and analyse voters' strategic behaviour. We show that determining whether a pure Nash equilibrium exists is $Σ_2^{\mathbf P}$-complete, computing a best response is NP-complete, and deciding the existence of subgame-perfect equilibria in sequential primaries is PSPACE-complete. These results reveal that primaries fundamentally increase the computational difficulty of strategic reasoning, situating them as a rich source of complexity-theoretic challenges within computational social choice.

</details>


### [3] [Informal and Privatized Transit: Incentives, Efficiency and Coordination](https://arxiv.org/abs/2602.10456)
*Devansh Jalota,Matthew Tsao*

Main category: cs.GT

TL;DR: 该论文提出了一种分析非正式和私营交通服务激励机制的框架，并通过博弈论模型证明分散的利润最大化行为可能导致效率损失，同时提出了针对性干预措施。


<details>
  <summary>Details</summary>
Motivation: 非正式和私营交通服务在大都市中至关重要，但政府在规划交通系统时往往忽视其激励机制，导致系统效率低下。

Method: 引入了一种新颖的博弈论模型，分析完全私营的非正式交通系统中司机和乘客的决策行为，并通过价格无政府界限评估效率损失。

Result: 研究发现分散的利润最大化行为会导致显著的效率损失，但通过针对性干预（如Stackelberg路由机制和跨补贴方案）可以缓解这些问题。

Conclusion: 论文强调政府可以通过特定干预措施改善非正式交通系统的效率，尤其在私营运营商与公共交通共存的环境中。

Abstract: Informal and privatized transit services, such as minibuses and shared auto-rickshaws, are integral to daily travel in large urban metropolises, providing affordable commutes where a formal public transport system is inadequate and other options are unaffordable. Despite the crucial role that these services play in meeting mobility needs, governments often do not account for these services or their underlying incentives when planning transit systems, which can significantly compromise system efficiency.
  Against this backdrop, we develop a framework to analyze the incentives underlying informal and privatized transit systems, while proposing mechanisms to guide public transit operation and incentive design when a substantial share of mobility is provided by such profit-driven private operators. We introduce a novel, analytically tractable game-theoretic model of a fully privatized informal transit system with a fixed menu of routes, in which profit-maximizing informal operators (drivers) decide where to provide service and cost-minimizing commuters (riders) decide whether to use these services. Within this framework, we establish tight price of anarchy bounds which demonstrate that decentralized, profit-maximizing driver behavior can lead to bounded yet substantial losses in cumulative driver profit and rider demand served. We further show that these performance losses can be mitigated through targeted interventions, including Stackelberg routing mechanisms in which a modest share of drivers are centrally controlled, reflecting environments where informal operators coexist with public transit, and cross-subsidization schemes that use route-specific tolls or subsidies to incentivize drivers to operate on particular routes. Finally, we reinforce these findings through numerical experiments based on a real-world informal transit system in Nalasopara, India.

</details>


### [4] [Online Generalized-mean Welfare Maximization: Achieving Near-Optimal Regret from Samples](https://arxiv.org/abs/2602.10469)
*Zongjun Yang,Rachitesh Kumar,Christian Kroer*

Main category: cs.GT

TL;DR: 研究在线公平分配问题，目标是最大化广义平均福利，提出了贪婪算法和历史样本重解法，实现了最优平均遗憾率。


<details>
  <summary>Details</summary>
Motivation: 解决在线公平分配中异构偏好下的广义平均福利最大化问题，尤其是在非平稳环境下如何利用有限历史样本实现最优性能。

Method: 采用贪婪算法和历史样本重解法，前者基于即时福利最大化分配，后者假设剩余物品与历史样本一致并求解最优分配。

Result: 贪婪算法在i.i.d.模型下实现了$\widetilde{O}(1/T)$平均遗憾率；历史样本重解法在非平稳模型下同样实现了最优遗憾率。

Conclusion: 即使在非平稳和分布偏移的情况下，仅需有限历史样本即可实现最优公平分配性能。

Abstract: We study online fair allocation of $T$ sequentially arriving items among $n$ agents with heterogeneous preferences, with the objective of maximizing generalized-mean welfare, defined as the $p$-mean of agents' time-averaged utilities, with $p\in (-\infty, 1)$. We first consider the i.i.d. arrival model and show that the pure greedy algorithm -- which myopically chooses the welfare-maximizing integral allocation -- achieves $\widetilde{O}(1/T)$ average regret. Importantly, in contrast to prior work, our algorithm does not require distributional knowledge and achieves the optimal regret rate using only the online samples.
  We then go beyond i.i.d. arrivals and investigate a nonstationary model with time-varying independent distributions. In the absence of additional data about the distributions, it is known that every online algorithm must suffer $Ω(1)$ average regret. We show that only a single historical sample from each distribution is sufficient to recover the optimal $\widetilde{O}(1/T)$ average regret rate, even in the face of arbitrary non-stationarity. Our algorithms are based on the re-solving paradigm: they assume that the remaining items will be the ones seen historically in those periods and solve the resulting welfare-maximization problem to determine the decision in every period. Finally, we also account for distribution shifts that may distort the fidelity of historical samples and show that the performance of our re-solving algorithms is robust to such shifts.

</details>


### [5] [Pricing Query Complexity of Multiplicative Revenue Approximation](https://arxiv.org/abs/2602.10483)
*Wei Tang,Yifan Wang,Mengxiao Zhang*

Main category: cs.GT

TL;DR: 本文研究了在单买家估值分布未知的情况下，卖方通过发布价格和观察二元购买决策来学习最优垄断价格的多倍误差保证的定价查询复杂度。


<details>
  <summary>Details</summary>
Motivation: 由于仅通过定价查询无法学习估值分布的规模，研究在提供‘规模提示’（如单样本提示或价值范围提示）的情况下，如何实现最优收入的多倍近似。

Method: 考虑了两种规模提示模型（单样本提示和价值范围提示），并对多种分布（MHR分布、常规分布和一般分布）建立了紧致的定价查询复杂度界限。

Result: 针对不同类型的提示和分布，建立了紧致（至多对数因子）的定价查询复杂度保证。

Conclusion: 研究表明，在合适的规模提示下，可以实现最优收入的多倍近似，为实际应用提供了理论支持。

Abstract: We study the pricing query complexity of revenue maximization for a single buyer whose private valuation is drawn from an unknown distribution. In this setting, the seller must learn the optimal monopoly price by posting prices and observing only binary purchase decisions, rather than the realized valuations. Prior work has established tight query complexity bounds for learning a near-optimal price with additive error $\varepsilon$ when the valuation distribution is supported on $[0,1]$. However, our understanding of how to learn a near-optimal price that achieves at least a $(1-\varepsilon)$ fraction of the optimal revenue remains limited.
  In this paper, we study the pricing query complexity of the single-buyer revenue maximization problem under such multiplicative error guarantees in several settings. Observe that when pricing queries are the only source of information about the buyer's distribution, no algorithm can achieve a non-trivial approximation, since the scale of the distribution cannot be learned from pricing queries alone. Motivated by this fundamental impossibility, we consider two natural and well-motivated models that provide "scale hints": (i) a one-sample hint, in which the algorithm observes a single realized valuation before making pricing queries; and (ii) a value-range hint, in which the valuation support is known to lie within $[1, H]$. For each type of hint, we establish pricing query complexity guarantees that are tight up to polylogarithmic factors for several classes of distributions, including monotone hazard rate (MHR) distributions, regular distributions, and general distributions.

</details>


### [6] [Characterization and Computation of Normal-Form Proper Equilibria in Extensive-Form Games via the Sequence-Form Representation](https://arxiv.org/abs/2602.10524)
*Yuqing Hou,Yiyin Cao,Chuangyin Dang*

Main category: cs.GT

TL;DR: 本文提出了一种紧凑的序列形式适当均衡，用于解决扩展形式游戏中正常形式适当均衡难以直接计算的问题。


<details>
  <summary>Details</summary>
Motivation: 由于正常形式适当均衡的扰动结构更严格，导致其直接计算在并行信息集数量增加时变得不可行，本文旨在解决这一问题。

Method: 通过重新定义序列上的期望收益，开发了紧凑的序列形式适当均衡，并基于ε-排列多面体定义了扰动游戏，提出了两种可微路径跟踪方法。

Result: 证明了从任意正实现计划出发的平滑均衡路径的存在性，并通过实验验证了方法的有效性和效率。

Conclusion: 本文提出的方法成功地将正常形式适当均衡的计算问题转化为序列形式问题，并通过实验证明了其可行性和效率。

Abstract: Normal-form proper equilibrium, introduced by Myerson as a refinement of normal-form perfect equilibrium, occupies a distinctive position in the equilibrium analysis of extensive-form games because its more stringent perturbation structure entails the sequential rationality. However, the size of the normal-form representation grows exponentially with the number of parallel information sets, making the direct determination of normal-form proper equilibria intractable. To address this challenge, we develop a compact sequence-form proper equilibrium by redefining the expected payoffs over sequences, and we prove that it coincides with the normal-form proper equilibrium via strategic equivalence. To facilitate computation, we further introduce an alternative representation by defining a class of perturbed games based on an $\varepsilon$-permutahedron over sequences. Building on this representation, we introduce two differentiable path-following methods for computing normal-form proper equilibria. These methods rely on artificial sequence-form games whose expected payoff functions incorporate logarithmic or entropy regularization through an auxiliary variable. We prove the existence of a smooth equilibrium path induced by each artificial game, starting from an arbitrary positive realization plan and converging to a normal-form proper equilibrium of the original game as the auxiliary variable approaches zero. Finally, our experimental results demonstrate the effectiveness and efficiency of the proposed methods.

</details>


### [7] [Necessary President in Elections with Parties](https://arxiv.org/abs/2602.10601)
*Katarína Cechlárová,Ildikó Schlotter*

Main category: cs.GT

TL;DR: 研究不同投票规则下‘必要总统’问题的计算复杂性，发现对于Borda、Maximin和Copeland$^α$规则问题可在多项式时间内解决，而对于包括$\ell$-Approval和$\ell$-Veto在内的位置评分规则，则是$\mathsf{coNP}$-完全的。


<details>
  <summary>Details</summary>
Motivation: 探索在各种投票规则下，确定某候选人是否能成为所有可能提名情况下的选举赢家（即‘必要总统’）的计算复杂性。

Method: 通过分析不同投票规则（如Borda、Maximin、Copeland$^α$、$\ell$-Approval和$\ell$-Veto）下的计算复杂性，并使用参数化复杂性理论评估问题的可解性。

Result: 结果表明，某些规则下问题是多项式时间可解的，而其他规则下则是$\mathsf{coNP}$-完全的或参数化难解的，特别是在位置评分规则和Ranked Pairs规则中。

Conclusion: 研究揭示了‘必要总统’问题在不同投票规则下的复杂性差异，为选举系统的设计和分析提供了理论支持。

Abstract: Consider an election where the set of candidates is partitioned into parties, and each party must choose exactly one candidate to nominate for the election held over all nominees. The Necessary President problem asks whether a candidate, if nominated, becomes the winner of the election for all possible nominations from other parties.
  We study the computational complexity of Necessary President for several voting rules. We show that while this problem is solvable in polynomial time for Borda, Maximin, and Copeland$^α$ for every $α\in [0,1]$, it is $\mathsf{coNP}$-complete for general classes of positional scoring rules that include $\ell$-Approval and $\ell$-Veto, even when the maximum size of a party is two. For such positional scoring rules, we show that Necessary President is $\mathsf{W}[2]$-hard when parameterized by the number of parties, but fixed-parameter tractable with respect to the number of voter types. Additionally, we prove that Necessary President for Ranked Pairs is $\mathsf{coNP}$-complete even for maximum party size two, and $\mathsf{W}[1]$-hard with respect to the number of parties; remarkably, both of these results hold even for constant number of voters.

</details>


### [8] [Smart Lotteries in School Choice: Ex-ante Pareto-Improvement with Ex-post Stability](https://arxiv.org/abs/2602.10679)
*Haris Aziz,Péter Biró,Gergely Csáji,Tom Demeulemeester*

Main category: cs.GT

TL;DR: 该论文提出了一种新的"智能抽签"机制，旨在通过随机支配改进学生与学校的分配效率，同时确保分配的稳定性。尽管计算问题是NP难的，但通过整数规划和列生成等优化技术可以解决。实验结果表明，该机制的福利增益显著高于传统方法。


<details>
  <summary>Details</summary>
Motivation: 现有的学校集中录取机制使用延迟接受算法（DA）和随机破断方法来生成稳定解，但这种方法可能会导致效率不足。论文旨在通过改进随机分配，在不牺牲稳定性的前提下提高效率和福利增益。

Method: 论文提出了一种"智能抽签"机制，通过优化技术（如整数规划和列生成）来解决NP难的计算问题，从而改进概率分配的效率。

Result: 计算实验表明，该机制在生成和实际实例中均能显著提高福利增益，优于传统的破断后效率改进方法。

Conclusion: 该"智能抽签"机制在不牺牲稳定性的前提下，通过优化技术实现了效率的显著提升，为学校录取机制提供了更好的解决方案。

Abstract: In a typical school choice application, the students have strict preferences over the schools while the schools have coarse priorities over the students based on their distance and their enrolled siblings. The outcome of a centralized admission mechanism is then usually obtained by the Deferred Acceptance (DA) algorithm with random tie-breaking. Therefore, every possible outcome of this mechanism is a stable solution for the coarse priorities that will arise with certain probability. This implies a probabilistic assignment, where the admission probability for each student-school pair is specified. In this paper, we propose a new efficiency-improving stable `smart lottery' mechanism. We aim to improve the probabilistic assignment ex-ante in a stochastic dominance sense, while ensuring that the improved random matching is still ex-post stable, meaning that it can be decomposed into stable matchings regarding the original coarse priorities. Therefore, this smart lottery mechanism can provide a clear Pareto-improvement in expectation for any cardinal utilities compared to the standard DA with lottery solution, without sacrificing the stability of the final outcome. We show that although the underlying computational problem is NP-hard, we can solve the problem by using advanced optimization techniques such as integer programming with column generation. We conduct computational experiments on generated and real instances. Our results show that the welfare gains by our mechanism are substantially larger than the expected gains by standard methods that realize efficiency improvements after ties have already been broken.

</details>


### [9] [Core-Stable Kidney Exchange via Altruistic Donors](https://arxiv.org/abs/2602.10725)
*Gergely Csáji,Thánh Nguyen*

Main category: cs.GT

TL;DR: 通过增加无私捐献者，恢复肾脏交换计划的核心稳定性，减少激励问题，提高合作范围与效率。


<details>
  <summary>Details</summary>
Motivation: 医院或国家可能保留易匹配的捐赠者-患者对供内部使用，导致核心不稳定，影响合作范围和效率。为此，需解决激励问题。

Method: 提出一种方法，通过在平台中引入无私捐献者来恢复核心稳定性。分析了两种兼容性图模型（随机图和基于类型的模型），并探讨了不同交换条件下的需求。

Result: 在现实场景中，仅需少量无私捐献者即可恢复稳定性。理论表明，所需捐献者的数量在市场规模增长时呈对数增长或与兼容类型数量成正比。

Conclusion: 模拟结果表明，实际所需的无私捐献者数量远低于理论最坏情况下的预测，验证了该方法的可行性。

Abstract: Kidney exchange programs among hospitals in the United States and across European countries improve efficiency by pooling donors and patients on a centralized platform. Sustaining such cooperation requires stability. When the core is empty, hospitals or countries may withhold easily matched pairs for internal use, creating incentive problems that undermine participation and reduce the scope and efficiency of exchange.
  We propose a method to restore core stability by augmenting the platform with altruistic donors. Although the worst-case number of required altruists can be large, we show that in realistic settings only a small number is needed. We analyze two models of the compatibility graph, one based on random graphs and the other on compatibility types. When only pairwise exchanges are allowed, the number of required altruists is bounded by the maximum number of independent odd cycles, defined as disjoint odd cycles with no edges between them. This bound grows logarithmically with market size in the random graph model and is at most one third of the number of compatibility types in the type-based model. When small exchange cycles are allowed, it suffices for each participating organization to receive a number of altruists proportional to the number of compatibility types. Finally, simulations show that far fewer altruists are needed in practice than worst-case theory suggests.

</details>


### [10] [Equity by Design: Fairness-Driven Recommendation in Heterogeneous Two-Sided Markets](https://arxiv.org/abs/2602.10739)
*Dominykas Seputis,Rajeev Verma,Alexander Timans*

Main category: cs.GT

TL;DR: 该论文研究了双边市场中的异质性激励问题，提出了在现实条件下实现双边公平的方法，并通过实验证明适度的公平约束可以提升业务指标。


<details>
  <summary>Details</summary>
Motivation: 双边市场中的生产者和消费者存在不同的激励和目标，现有方法未能充分考虑到消费者偏好、生产者能力和业务目标的复杂性。

Method: 研究通过引入条件风险价值（CVaR）作为消费者端的目标，并将业务约束直接整合到优化中，扩展了以往的单项目分配方法。

Result: 实验表明，在多项目设置中，“免费公平”机制消失，适度的公平约束可以通过分散曝光来提升业务指标，且可扩展的求解器能高效实现公平分配。

Conclusion: 公平性不应被视为平台效率的负担，而应作为维护市场健康的杠杆，研究为大规模公平分配提供了实用解决方案。

Abstract: Two-sided marketplaces embody heterogeneity in incentives: producers seek exposure while consumers seek relevance, and balancing these competing objectives through constrained optimization is now a standard practice. Yet real platforms face finer-grained complexity: consumers differ in preferences and engagement patterns, producers vary in catalog value and capacity, and business objectives impose additional constraints beyond raw relevance. We formalize two-sided fairness under these realistic conditions, extending prior work from soft single-item allocations to discrete multi-item recommendations. We introduce Conditional Value-at-Risk (CVaR) as a consumer-side objective that compresses group-level utility disparities, and integrate business constraints directly into the optimization. Our experiments reveal that the "free fairness" regime, where producer constraints impose no consumer cost, disappears in multi item settings. Strikingly, moderate fairness constraints can improve business metrics by diversifying exposure away from saturated producers. Scalable solvers match exact solutions at a fraction of the runtime, making fairness-aware allocation practical at scale. These findings reframe fairness not as a tax on platform efficiency but as a lever for sustainable marketplace health.

</details>


### [11] [Near-Feasible Stable Matchings: Incentives and Optimality](https://arxiv.org/abs/2602.10851)
*Frederik Glitzner*

Main category: cs.GT

TL;DR: 论文研究了在稳定匹配问题中，通过微调代理容量来解决不可行匹配的方法，并提出了一种分析代理激励和改进稳定性的框架。


<details>
  <summary>Details</summary>
Motivation: 稳定匹配在校园选择或就业市场等应用中非常重要，但传统方法可能无法保证匹配的存在性或代理的完全匹配。近可行性方法通过调整容量来解决这些问题，但未充分研究其在原始实例中的稳定性及代理激励问题。

Method: 基于稳定装置问题模型，提出了一种量化代理激励的框架，并研究了容量调整对稳定性和计算复杂性的影响。论文证明了容量调整可以在个体和整体层面上同时优化，并提供了高效算法。

Result: 研究表明，不同的容量调整策略对稳定性有显著影响，最小调整和最小偏离激励是可兼容的且在一般条件下可高效计算。论文还提供了精确算法和实验结果。

Conclusion: 论文证明了通过合理的容量调整，可以在保持稳定性的同时高效解决匹配问题，为实际应用提供了理论和算法支持。

Abstract: Stable matching is a fundamental area with many practical applications, such as centralised clearinghouses for school choice or job markets. Recent work has introduced the paradigm of near-feasibility in capacitated matching settings, where agent capacities are slightly modified to ensure the existence of desirable outcomes. While useful when no stable matching exists, or some agents are left unmatched, it has not previously been investigated whether near-feasible stable matchings satisfy desirable properties with regard to their stability in the original instance. Furthermore, prior works often leave open deviation incentive issues that arise when the centralised authority modifies agents' capacities.
  We consider these issues in the Stable Fixtures problem model, which generalises many classical models through non-bipartite preferences and capacitated agents. We develop a formal framework to analyse and quantify agent incentives to adhere to computed matchings. Then, we embed near-feasible stable matchings in this framework and study the trade-offs between instability, capacity modifications, and computational complexity. We prove that capacity modifications can be simultaneously optimal at individual and aggregate levels, and provide efficient algorithms to compute them. We show that different modification strategies significantly affect stability, and establish that minimal modifications and minimal deviation incentives are compatible and efficiently computable under general conditions. Finally, we provide exact algorithms and experimental results for tractable and intractable versions of these problems.

</details>


### [12] [The Computational Intractability of Not Worst Responding](https://arxiv.org/abs/2602.10966)
*Mete Şeref Ahunbay,Paul W. Goldberg,Edwin Lock,Panayotis Mertikopoulos,Bary S. R. Pradelski,Bassel Tarbush*

Main category: cs.GT

TL;DR: 研究探讨了纳什均衡的计算复杂性，发现即使将最优性要求弱化为避免最差响应，问题仍然难以计算。


<details>
  <summary>Details</summary>
Motivation: 探索纳什均衡计算复杂性的根源，验证即使在最弱的理性准则下，问题是否仍然难以解决。

Method: 通过分析游戏中的最优性和避免最差响应准则，研究其在一般游戏和潜在游戏中的计算复杂性。

Result: 发现任何满足最小理性保证的解概念都与纯纳什均衡同样难以计算，且在一般游戏中相关问题是NP完全或NP难，潜在游戏中是PLS完全。

Conclusion: 计算复杂性不仅源于最优性要求，还来自最小理性保证。放松后者揭示了理性强度与满足玩家比例之间的权衡。

Abstract: Finding, counting, or determining the existence of Nash equilibria, where players must play optimally given each others' actions, are known to be computational intractable problems. We ask whether weakening optimality to the requirement that each player merely avoid worst responses -- arguably the weakest meaningful rationality criterion -- yields tractable solution concepts. We show that it does not: any solution concept with this minimal guarantee is ``as intractable'' as pure Nash equilibrium. In general games, determining the existence of no-worst-response action profiles is NP-complete, finding one is NP-hard, and counting them is #P-complete. In potential games, where existence is guaranteed, the search problem is PLS-complete. Computational intractability therefore stems not only from the requirement of optimality, but also from the requirement of a minimal rationality guarantee for each player. Moreover, relaxing the latter requirement gives rise to a tractability trade-off between the strength of individual rationality guarantees and the fraction of players satisfying them.

</details>


### [13] [Let Leaders Play Games: Improving Timing in Leader-based Consensus](https://arxiv.org/abs/2602.11147)
*Rasheed M,Parth Desai,Sujit Gujar*

Main category: cs.GT

TL;DR: 该论文提出了一种名为2-Prop的双区块提案机制，旨在减少区块链中的时间博弈行为，从而提高网络的整体效率和公平性。


<details>
  <summary>Details</summary>
Motivation: 区块链中的领导者（提案者）可能会通过延迟提案时间来获取更多奖励（时间博弈），这不仅增加了区块丢失的风险，还影响了网络的效率和公平性。

Method: 论文设计了一种双区块提案机制（2-Prop），每个时隙选择两个提案者提交区块并确认其中一个。还设计了一种基于区块传播速度的奖励分配策略。

Result: 研究表明，在均匀网络环境下，提案者不延迟提交区块是一种纳什均衡；在非均匀环境下，除非另一提案者极其缓慢，否则快速的提案者不会选择延迟。

Conclusion: 2-Prop机制有效减少了时间博弈的影响，提高了区块链网络的效率和公平性。

Abstract: Propagation latency is inherent to any distributed network, including blockchains. Typically, blockchain protocols provide a timing buffer for block propagation across the network. In leader-based blockchains, the leader -- block proposer -- is known in advance for each slot. A fast (or low-latency) proposer may delay the block proposal in anticipation of more rewards from the transactions that would otherwise be included in the subsequent block. Deploying such a strategy by manipulating the timing is known as timing games. It increases the risk of missed blocks due to reduced time for other nodes to vote on the block, affecting the overall efficiency of the blockchain. Moreover, proposers who play timing games essentially appropriate MEV (additional rewards over transaction fees and the block reward) that would otherwise accrue to the next block, making it unfair to subsequent block proposers. We propose a double-block proposal mechanism, 2-Prop, to curtail timing games. 2-Prop selects two proposers per slot to propose blocks and confirms one of them. We design a reward-sharing policy for proposers based on how quickly their blocks propagate to avoid strategic deviations. In the induced game, which we call the Latency Game, we show that it is a Nash Equilibrium for the proposers to propose the block without delay under homogeneous network settings. Under heterogeneous network settings, we study many configurations, and our analysis shows that a faster proposer would prefer not to delay unless the other proposer is extremely slow. Thus, we show the efficacy of 2-Prop in mitigating the effect of timing games.

</details>


### [14] [Utilitarian Distortion Under Probabilistic Voting](https://arxiv.org/abs/2602.11152)
*Hamidreza Alipour,Mohak Goyal*

Main category: cs.GT

TL;DR: 该论文通过概率投票模型（Plackett-Luce模型）解决了经典投票规则中效率损失的问题，证明Copeland和Borda规则的扭曲度与候选人数量无关，而仅与参数β相关，接近于随机规则的下界。


<details>
  <summary>Details</summary>
Motivation: 经典投票规则（如Plurality）在效率损失方面表现不佳，而Copeland和Borda等规范优越的规则则存在无界扭曲问题。论文旨在通过概率投票模型解决这一矛盾。

Method: 研究采用了Plackett-Luce模型，通过参数β模拟投票者的噪声选择行为，分析了Copeland、Borda、Plurality等规则在概率投票中的扭曲度。

Result: Copeland和Borda的扭曲度上限为β(1+e⁻β)/(1-e⁻β)，且与候选人数量m无关；而Plurality等规则的扭曲度与m相关，表现较差。论文还证明了Copeland规则的近乎紧的下界。

Conclusion: 结果表明，一旦考虑现实投票的概率性，扭曲度框架与规范直觉一致，Copeland和Borda在概率投票中表现优越。

Abstract: The utilitarian distortion framework evaluates voting rules by their worst-case efficiency loss when voters have cardinal utilities but express only ordinal rankings. Under the classical model, a longstanding tension exists: Plurality, which suffers from the spoiler effect, achieves optimal $Θ(m^2)$ distortion among deterministic rules, while normatively superior rules like Copeland and Borda have unbounded distortion. We resolve this tension under probabilistic voting with the Plackett-Luce model, where rankings are noisy reflections of utilities governed by an inverse temperature parameter $β$. Copeland and Borda both achieve at most $β\frac{1+e^{-β}}{1-e^{-β}}$ distortion, independent of the number of candidates $m$, and within a factor of 2 of the lower bound for randomized rules satisfying the probabilistic Condorcet loser criterion known from prior work. This improves upon the prior $O(β^2)$ bound for Borda. These upper bounds are nearly tight: prior work establishes a $(1-o(1))β$ lower bound for Borda, and we prove a $(1-ε)β$ lower bound for Copeland for any constant $ε>0$. In contrast, rules that rely only on top-choice information fare worse: Plurality has distortion $Ω(\min(e^β, m))$ and Random Dictator has distortion $Θ(m)$. Additional `veto' information is also insufficient to remove the dependence on $m$; Plurality Veto and Pruned Plurality Veto have distortion $Ω(β\ln m)$. We also prove a lower bound of $(\frac{5}{8}-ε)β$ (for any constant $ε>0$) for all deterministic finite-precision tournament-based rules, a class that includes Copeland and any rule based on pairwise comparison margins rounded to fixed precision. Our results show that the distortion framework aligns with normative intuitions once the probabilistic nature of real-world voting is taken into account.

</details>
