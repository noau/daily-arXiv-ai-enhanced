<div id=toc></div>

# Table of Contents

- [cs.GR](#cs.GR) [Total: 2]
- [cs.PL](#cs.PL) [Total: 2]
- [cs.GT](#cs.GT) [Total: 4]


<div id='cs.GR'></div>

# cs.GR [[Back]](#toc)

### [1] [Morphology-Preserving Remeshing Approach to Particulate Microstructures via Harmonic Decomposition](https://arxiv.org/abs/2509.08855)
*Mahmoud Shaqfa*

Main category: cs.GR

TL;DR: 论文提出了一种基于层次扩散的方法，用于重新采样表面以优化网格质量，解决了传统谐波方法中采样不均的问题。


<details>
  <summary>Details</summary>
Motivation: 传统谐波方法在表面重构和生成微结构时，采样方式未能考虑局部雅可比矩阵的变化，导致网格质量不均，影响数值模拟的精度和效率。

Method: 采用非线性扩散技术对分析域的曲线坐标进行重新采样，类似于热问题中的扩散过程，通过扩大小三角形并缩小大三角形来实现均衡的网格三角化。

Result: 在球体和半球体谐波方法中测试的各向同性和各向异性扩散方案显著提高了表面三角化的质量指标，同时保持了形态、面积和体积。

Conclusion: 该方法在保持表面形态和几何属性的同时，显著提升了网格质量，为大型2D和3D微结构的数字孪生应用提供了高效解决方案。

Abstract: Harmonic decomposition of surfaces, such as spherical and spheroidal
harmonics, is used to analyze morphology, reconstruct, and generate surface
inclusions of particulate microstructures. However, obtaining high-quality
meshes of engineering microstructures using these approaches remains an open
question. In harmonic approaches, we usually reconstruct surfaces by evaluating
the harmonic bases on equidistantly sampled simplicial complexes of the base
domains (e.g., triangular spheroids and disks). However, this traditional
sampling does not account for local changes in the Jacobian of the basis
functions, resulting in nonuniform discretization after reconstruction or
generation. As it impacts the accuracy and time step, high-quality
discretization of microstructures is crucial for efficient numerical
simulations (e.g., finite element and discrete element methods). To circumvent
this issue, we propose an efficient hierarchical diffusion-based approach for
resampling the surface-i.e., performing a reparameterization-to yield an
equalized mesh triangulation. Analogous to heat problems, we use nonlinear
diffusion to resample the curvilinear coordinates of the analysis domain,
thereby enlarging small triangles at the expense of large triangles on
surfaces. We tested isotropic and anisotropic diffusion schemes on the recent
spheroidal and hemispheroidal harmonics methods. The results show a substantial
improvement in the quality metrics for surface triangulation. Unlike
traditional surface reconstruction and meshing techniques, this approach
preserves surface morphology, along with the areas and volumes of surfaces. We
discuss the results and the associated computational costs for large 2D and 3D
microstructures, such as digital twins of concrete and stone masonry, and their
future applications.

</details>


### [2] [CameraVDP: Perceptual Display Assessment with Uncertainty Estimation via Camera and Visual Difference Prediction](https://arxiv.org/abs/2509.08947)
*Yancheng Cai,Robert Wanat,Rafal Mantiuk*

Main category: cs.GR

TL;DR: 提出了一种结合相机重建流程和视觉差异预测器的方法，用于精准测量电子显示器图像，并评估其对人眼的可见性。


<details>
  <summary>Details</summary>
Motivation: 传统的显示测量方法无法捕捉高频和像素级失真，而相机虽然空间分辨率足够，但会引入光学、采样和光度失真。需要结合视觉系统模型来评估失真是否可见。

Method: 采用相机重建流程（包括HDR图像堆叠、MTF反转、渐晕校正、几何校正、单应变换和颜色校正）与视觉差异预测器（VDP）相结合的方法。

Result: 提出的CameraVDP框架在三项应用中验证有效：缺陷像素检测、色彩边缘感知和显示非均匀性评估。框架还能估计缺陷检测的理论上限并为VDP质量评分提供置信区间。

Conclusion: CameraVDP能够将相机转化为精确的显示测量工具，并通过视觉差异预测器评估失真对人眼的可见性，为显示质量评估提供了新方法。

Abstract: Accurate measurement of images produced by electronic displays is critical
for the evaluation of both traditional and computational displays. Traditional
display measurement methods based on sparse radiometric sampling and fitting a
model are inadequate for capturing spatially varying display artifacts, as they
fail to capture high-frequency and pixel-level distortions. While cameras offer
sufficient spatial resolution, they introduce optical, sampling, and
photometric distortions. Furthermore, the physical measurement must be combined
with a model of a visual system to assess whether the distortions are going to
be visible. To enable perceptual assessment of displays, we propose a
combination of a camera-based reconstruction pipeline with a visual difference
predictor, which account for both the inaccuracy of camera measurements and
visual difference prediction. The reconstruction pipeline combines HDR image
stacking, MTF inversion, vignetting correction, geometric undistortion,
homography transformation, and color correction, enabling cameras to function
as precise display measurement instruments. By incorporating a Visual
Difference Predictor (VDP), our system models the visibility of various stimuli
under different viewing conditions for the human visual system. We validate the
proposed CameraVDP framework through three applications: defective pixel
detection, color fringing awareness, and display non-uniformity evaluation. Our
uncertainty analysis framework enables the estimation of the theoretical upper
bound for defect pixel detection performance and provides confidence intervals
for VDP quality scores.

</details>


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [3] [Towards Verified Compilation of Floating-point Optimization in Scientific Computing Programs](https://arxiv.org/abs/2509.09019)
*Mohit Tekriwal,John Sarracino*

Main category: cs.PL

TL;DR: 该论文探讨了在科学计算程序中浮点优化（尤其是快速数学优化）的正确性验证，并利用Rocq定理证明器中的Verified LLVM框架，对Fused-Multiply-Add（FMA）优化进行了初步验证。


<details>
  <summary>Details</summary>
Motivation: 确保科学计算程序中的编译器优化（尤其是浮点优化）在提升性能的同时保持正确性。

Method: 利用Rocq定理证明器中的Verified LLVM框架，针对实现算术表达式$a * b + c$的基本块，验证FMA优化的正确性。

Result: 论文展示了FMA优化正确性的初步验证结果。

Conclusion: 未来研究可扩展更多程序功能和快速数学浮点优化。

Abstract: Scientific computing programs often undergo aggressive compiler optimization
to achieve high performance and efficient resource utilization. While
performance is critical, we also need to ensure that these optimizations are
correct. In this paper, we focus on a specific class of optimizations,
floating-point optimizations, notably due to fast math, at the LLVM IR level.
We present a preliminary work, which leverages the Verified LLVM framework in
the Rocq theorem prover, to prove the correctness of Fused-Multiply-Add (FMA)
optimization for a basic block implementing the arithmetic expression $a * b +
c$ . We then propose ways to extend this preliminary results by adding more
program features and fast math floating-point optimizations.

</details>


### [4] [Dependent-Type-Preserving Memory Allocation](https://arxiv.org/abs/2509.09059)
*Paulette Koronkevich,William J. Bowman*

Main category: cs.PL

TL;DR: 为了解决依赖类型语言在编译过程中类型规范被外部程序破坏的问题，本文提出了一种类型保持编译的方法，包括开发支持依赖内存分配的中间语言和类型保持的编译器过程。


<details>
  <summary>Details</summary>
Motivation: 依赖类型语言（如Coq、Agda）允许程序员编写详细的程序规范并证明其正确性，但这些规范在编译后会被擦除，导致外部链接程序可能违反原始程序的规范。

Method: 本文提出了一种类型保持编译的方法，开发了支持依赖内存分配的中间语言，并设计了一个类型保持的编译器过程，以确保类型规范在编译和链接过程中不被破坏。

Result: 该方法旨在通过类型检查防止链接不符合类型规范的程序，从而确保程序的内存安全性。

Conclusion: 本文通过类型保持编译和依赖内存分配中间语言的开发，为解决依赖类型语言在编译过程中的规范违反问题提供了潜在的解决方案。

Abstract: Dependently typed programming languages such as Coq, Agda, Idris, and F*,
allow programmers to write detailed specifications of their programs and prove
their programs meet these specifications. However, these specifications can be
violated during compilation since they are erased after type checking. External
programs linked with the compiled program can violate the specifications of the
original program and change the behavior of the compiled program -- even when
compiled with a verified compiler. For example, since Coq does not allow
explicitly allocating memory, a programmer might link their Coq program with a
C program that can allocate memory. Even if the Coq program is compiled with a
verified compiler, the external C program can still violate the memory-safe
specification of the Coq program by providing an uninitialized pointer to
memory. This error could be ruled out by type checking in a language expressive
enough to indicate whether memory is initialized versus uninitialized. Linking
with a program with an uninitialized pointer could be considered ill-typed, and
our linking process could prevent linking with ill-typed programs. To
facilitate type checking during linking, we can use type-preserving
compilation, which preserves the types through the compilation process. In this
ongoing work, we develop a typed intermediate language that supports dependent
memory allocation, as well as a dependent-type-preserving compiler pass for
memory allocation.

</details>


<div id='cs.GT'></div>

# cs.GT [[Back]](#toc)

### [5] [Toward a Multi-Echelon Cyber Warfare Theory: A Meta-Game-Theoretic Paradigm for Defense and Dominance](https://arxiv.org/abs/2509.08976)
*Ya-Ting Yang,Quanyan Zhu*

Main category: cs.GT

TL;DR: 本文探讨了网络战作为现代冲突的核心元素，强调需要将防御和进攻技术整合为连贯的战略。


<details>
  <summary>Details</summary>
Motivation: 尽管先前研究强调孤立的战术或分散的技术，但整体理解对于有效资源部署和风险缓解至关重要。

Method: 论文利用博弈论作为统一框架，结合现代AI技术，设计并优化网络战的多层次策略。

Result: 通过合成网络冲突RedCyber的案例研究，展示了博弈论方法如何捕捉网络操作的相互依存关系。

Conclusion: 本章总结了网络战中博弈论的应用价值，并提出了未来在韧性、跨梯队规划和AI作用方面的研究方向。

Abstract: Cyber warfare has become a central element of modern conflict, especially
within multi-domain operations. As both a distinct and critical domain, cyber
warfare requires integrating defensive and offensive technologies into coherent
strategies. While prior research has emphasized isolated tactics or fragmented
technologies, a holistic understanding is essential for effective resource
deployment and risk mitigation. Game theory offers a unifying framework for
this purpose. It not only models attacker-defender interactions but also
provides quantitative tools for equilibrium analysis, risk assessment, and
strategic reasoning. Integrated with modern AI techniques, game-theoretic
models enable the design and optimization of strategies across multiple levels
of cyber warfare, from policy and strategy to operations, tactics, and
technical implementations. These models capture the paradoxical logic of
conflict, where more resources do not always translate into greater advantage,
and where nonlinear dynamics govern outcomes. To illustrate the approach, this
chapter examines RedCyber, a synthetic cyber conflict, demonstrating how
game-theoretic methods capture the interdependencies of cyber operations. The
chapter concludes with directions for future research on resilience,
cros-echelon planning, and the evolving role of AI in cyber warfare.

</details>


### [6] [Persuasion Gains and Losses from Peer Communication](https://arxiv.org/abs/2509.09099)
*Toygar T. Kerman,Anastas P. Tenev,Konstantin Zabarnyi*

Main category: cs.GT

TL;DR: 研究贝叶斯说服情境，分析发送者如何通过部分信息揭示影响关键数量接收者的决策，探讨网络扩展对发送者效用的影响。


<details>
  <summary>Details</summary>
Motivation: 探索在网络通信环境下，发送者通过部分信息揭示影响接收者决策的效果，以及网络密度如何影响其效用。

Method: 采用贝叶斯说服模型，分析不同网络结构下发送者的效用变化，包括网络扩展和信息层级结构的影响。

Result: 研究发现，网络扩展可以提高发送者效用，但效用并非单调递增；某些网络结构可以达到与空网络相同的最高效用。

Conclusion: 研究表明，并非更多的通信总会带来更好的集体结果，网络结构和信息层级对结果有重要影响。

Abstract: We study a Bayesian persuasion setting in which a sender wants to persuade a
critical mass of receivers by revealing partial information about the state to
them. The homogeneous binary-action receivers are located on a communication
network, and each observes the private messages sent to them and their
immediate neighbors. We examine how the sender's expected utility varies with
increased communication among receivers. We show that for general families of
networks, extending the network can strictly benefit the sender. Thus, the
sender's gain from persuasion is not monotonic in network density. Moreover,
many network extensions can achieve the upper bound on the sender's expected
utility among all networks, which corresponds to the payoff in an empty
network. This is the case in networks reflecting a clear informational
hierarchy (e.g., in global corporations), as well as in decentralized networks
in which information originates from multiple sources (e.g., influencers in
social media). Finally, we show that a slight modification to the structure of
some of these networks precludes the possibility of such beneficial extensions.
Overall, our results caution against presuming that more communication
necessarily leads to better collective outcomes.

</details>


### [7] [Mechanism Design with Outliers and Predictions](https://arxiv.org/abs/2509.09561)
*Argyrios Deligkas,Eduard Eiben,Sophie Klumper,Guido Schäfer,Artem Tsikiridis*

Main category: cs.GT

TL;DR: 研究了在存在异常值的情况下机制设计的问题，重点关注线型设施位置设置中如何通过排除最远$z$个代理来优化社会成本，研究发现这一做法在某些情况下反而会降低效率。


<details>
  <summary>Details</summary>
Motivation: 探索在机制设计中如何处理异常值（极端偏好或非典型行为的代理）对社会成本优化的影响。

Method: 在线型设施位置问题中，设计了一种机制，可以排除最远的$z$个代理，并通过确定性策略证明机制来分析其实用主义和平均主义社会成本。

Result: 研究发现，当$z \ge n/2$时，没有策略证明机制能为任一目标提供有界的近似保证；对于平均主义成本，(z+1)阶统计量是策略证明且2-近似的，但这是最优结果；实用主义成本情况下，机制对异常值的利用效果有限。

Conclusion: 研究表明，排除异常值在某些情况下会降低效率，但通过预测最优位置的设计可以在实用主义成本中实现最优的一致性和鲁棒性权衡。

Abstract: We initiate the study of mechanism design with outliers, where the designer
can discard $z$ agents from the social cost objective. This setting is
particularly relevant when some agents exhibit extreme or atypical preferences.
As a natural case study, we consider facility location on the line: $n$
strategic agents report their preferred locations, and a mechanism places a
facility to minimize a social cost function. In our setting, the $z$ agents
farthest from the chosen facility are excluded from the social cost. While it
may seem intuitive that discarding outliers improves efficiency, our results
reveal that the opposite can hold.
  We derive tight bounds for deterministic strategyproof mechanisms under the
two most-studied objectives: utilitarian and egalitarian social cost. Our
results offer a comprehensive view of the impact of outliers. We first show
that when $z \ge n/2$, no strategyproof mechanism can achieve a bounded
approximation for either objective. For egalitarian cost, selecting the $(z +
1)$-th order statistic is strategyproof and 2-approximate. In fact, we show
that this is best possible by providing a matching lower bound. Notably, this
lower bound of 2 persists even when the mechanism has access to a prediction of
the optimal location, in stark contrast to the setting without outliers. For
utilitarian cost, we show that strategyproof mechanisms cannot effectively
exploit outliers, leading to the counterintuitive outcome that approximation
guarantees worsen as the number of outliers increases. However, in this case,
access to a prediction allows us to design a strategyproof mechanism achieving
the best possible trade-off between consistency and robustness. Finally, we
also establish lower bounds for randomized mechanisms that are truthful in
expectation.

</details>


### [8] [Maximizing social welfare among EF1 allocations at the presence of two types of agents](https://arxiv.org/abs/2509.09641)
*Jiaxuan Ma,Yong Chen,Guangting Chen,Mingyang Gong,Guohui Lin,An Zhang*

Main category: cs.GT

TL;DR: 研究了在不可分割物品分配中，如何最大化功利社会福利，同时满足"最多嫉妒一件物品"的公平性标准。提出了一种针对两种标准化效用函数的2-近似算法，改进了之前的最佳比率，并在特殊情况下证实了APX完全性。


<details>
  <summary>Details</summary>
Motivation: 研究不可分割物品的公平分配问题，目标是最大化社会福利并满足特定的公平性标准。重点关注两种效用函数的特殊情形，以提高近似算法的效率。

Method: 提出了一种2-近似算法，适用于两种标准化效用函数的情形。此外，还针对3个代理人的情况，改进了近似比率。

Result: 在两种标准化效用函数的特殊情形下，实现了2-近似算法，改进了之前的最佳比率16√n。对于3个代理人的情况，提出了更优的5/3-近似算法（标准化效用函数）和2-近似算法（非标准化效用函数）。

Conclusion: 本文提出的算法在特殊情形下显著提高了近似比率，验证了APX完全性，并为不可分割物品的公平分配问题提供了更高效的解决方案。

Abstract: We study the fair allocation of indivisible items to $n$ agents to maximize
the utilitarian social welfare, where the fairness criterion is envy-free up to
one item and there are only two different utility functions shared by the
agents. We present a $2$-approximation algorithm when the two utility functions
are normalized, improving the previous best ratio of $16 \sqrt{n}$ shown for
general normalized utility functions; thus this constant ratio approximation
algorithm confirms the APX-completeness in this special case previously shown
APX-hard. When there are only three agents, i.e., $n = 3$, the previous best
ratio is $3$ shown for general utility functions, and we present an improved
and tight $\frac 53$-approximation algorithm when the two utility functions are
normalized, and a best possible and tight $2$-approximation algorithm when the
two utility functions are unnormalized.

</details>
