{"id": "2509.07044", "categories": ["cs.GR"], "pdf": "https://arxiv.org/pdf/2509.07044", "abs": "https://arxiv.org/abs/2509.07044", "authors": ["Pablo Antolin", "Michael Barton", "Georges-Pierre Bonneau", "Annalisa Buffa", "Amaia Calleja-Ochoa", "Gershon Elber", "Stefanie Elgeti", "Gaizka G\u00f3mez Escudero", "Alicia Gonzalez", "Haizea Gonz\u00e1lez Barrio", "Stefanie Hahmann", "Thibaut Hirschler", "Q Youn Honga", "Konstantin Key", "Myung-Soo Kim", "Michael Kofler", "Norberto Lopez de Lacalle", "Silvia de la Maza", "Kanika Rajain", "Jacques Zwar"], "title": "On design, analysis, and hybrid manufacturing of microstructured blade-like geometries", "comment": "14 pages, 23 figures", "summary": "With the evolution of new manufacturing technologies such as multi-material\n3D printing, one can think of new type of objects that consist of considerably\nless, yet heterogeneous, material, consequently being porous, lighter and\ncheaper, while having the very same functionality as the original object when\nmanufactured from one single solid material. We aim at questioning five decades\nof traditional paradigms in geometric CAD and focus at new generation of CAD\nobjects that are not solid, but contain heterogeneous free-form internal\nmicrostructures. We propose a unified manufacturing pipeline that involves all\nstages, namely design, optimization, manufacturing, and inspection of\nmicrostructured free-form geometries. We demonstrate our pipeline on an\nindustrial test case of a blisk blade that sustains the desired pressure\nlimits, yet requires significantly less material when compared to the solid\ncounterpart.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63a2\u8ba8\u4e86\u5229\u7528\u591a\u6750\u65993D\u6253\u5370\u7b49\u65b0\u578b\u5236\u9020\u6280\u672f\uff0c\u8bbe\u8ba1\u51fa\u5177\u6709\u5f02\u8d28\u5185\u90e8\u5fae\u7ed3\u6784\u7684\u8f7b\u91cf\u5316\u3001\u4f4e\u6210\u672c\u4f46\u529f\u80fd\u76f8\u540c\u7684\u975e\u5b9e\u4f53CAD\u5bf9\u8c61\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u6db5\u76d6\u8bbe\u8ba1\u3001\u4f18\u5316\u3001\u5236\u9020\u548c\u68c0\u6d4b\u7684\u7edf\u4e00\u5236\u9020\u6d41\u7a0b\u3002", "motivation": "\u968f\u7740\u591a\u6750\u65993D\u6253\u5370\u7b49\u65b0\u5236\u9020\u6280\u672f\u7684\u53d1\u5c55\uff0c\u4f20\u7edfCAD\u8bbe\u8ba1\u4e2d\u5b9e\u4f53\u5bf9\u8c61\u7684\u8303\u5f0f\u53d7\u5230\u6311\u6218\uff0c\u7814\u7a76\u8005\u5e0c\u671b\u5f00\u53d1\u51fa\u5177\u6709\u5f02\u8d28\u5fae\u7ed3\u6784\u7684\u8f7b\u91cf\u5316\u5bf9\u8c61\uff0c\u4ee5\u51cf\u5c11\u6750\u6599\u7528\u91cf\u5e76\u964d\u4f4e\u6210\u672c\u3002", "method": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7edf\u4e00\u7684\u5236\u9020\u6d41\u7a0b\uff0c\u5305\u62ec\u8bbe\u8ba1\u3001\u4f18\u5316\u3001\u5236\u9020\u548c\u68c0\u6d4b\u5f02\u8d28\u81ea\u7531\u5f62\u5f0f\u5185\u90e8\u5fae\u7ed3\u6784\u7684\u51e0\u4f55\u4f53\u3002", "result": "\u901a\u8fc7\u4e00\u4e2a\u5de5\u4e1a\u6d4b\u8bd5\u6848\u4f8b\u2014\u2014blisk\u53f6\u7247\uff0c\u5c55\u793a\u4e86\u8be5\u65b9\u6cd5\u80fd\u591f\u5728\u6ee1\u8db3\u538b\u529b\u8981\u6c42\u7684\u540c\u65f6\uff0c\u663e\u8457\u51cf\u5c11\u6750\u6599\u7528\u91cf\u3002", "conclusion": "\u7814\u7a76\u8868\u660e\uff0c\u65b0\u578b\u7684\u5f02\u8d28\u5fae\u7ed3\u6784\u8bbe\u8ba1\u53ef\u4ee5\u53d6\u4ee3\u4f20\u7edf\u5b9e\u4f53\u8bbe\u8ba1\uff0c\u5b9e\u73b0\u529f\u80fd\u4e0d\u53d8\u4f46\u6750\u6599\u66f4\u5c11\u3001\u6210\u672c\u66f4\u4f4e\u7684\u76ee\u6807\u3002"}}
{"id": "2509.07127", "categories": ["cs.GR", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2509.07127", "abs": "https://arxiv.org/abs/2509.07127", "authors": ["Leonardo Zini", "Elia Frigieri", "Sebastiano Aloscari", "Marcello Generali", "Lorenzo Dodi", "Robert Dosen", "Lorenzo Baraldi"], "title": "SVGauge: Towards Human-Aligned Evaluation for SVG Generation", "comment": "Accepted at 23rd edition of International Conference on Image\n  Analysis and Processing 2025", "summary": "Generated Scalable Vector Graphics (SVG) images demand evaluation criteria\ntuned to their symbolic and vectorial nature: criteria that existing metrics\nsuch as FID, LPIPS, or CLIPScore fail to satisfy. In this paper, we introduce\nSVGauge, the first human-aligned, reference based metric for text-to-SVG\ngeneration. SVGauge jointly measures (i) visual fidelity, obtained by\nextracting SigLIP image embeddings and refining them with PCA and whitening for\ndomain alignment, and (ii) semantic consistency, captured by comparing\nBLIP-2-generated captions of the SVGs against the original prompts in the\ncombined space of SBERT and TF-IDF. Evaluation on the proposed SHE benchmark\nshows that SVGauge attains the highest correlation with human judgments and\nreproduces system-level rankings of eight zero-shot LLM-based generators more\nfaithfully than existing metrics. Our results highlight the necessity of\nvector-specific evaluation and provide a practical tool for benchmarking future\ntext-to-SVG generation models.", "AI": {"tldr": "SVGauge\u662f\u4e00\u4e2a\u4e13\u4e3a\u8bc4\u4f30\u6587\u672c\u751f\u6210SVG\u56fe\u50cf\u800c\u8bbe\u8ba1\u7684\u6307\u6807\uff0c\u7ed3\u5408\u89c6\u89c9\u4fdd\u771f\u5ea6\u548c\u8bed\u4e49\u4e00\u81f4\u6027\uff0c\u4f18\u4e8e\u73b0\u6709\u6307\u6807\uff0c\u5e76\u4e0e\u4eba\u7c7b\u5224\u65ad\u9ad8\u5ea6\u76f8\u5173\u3002", "motivation": "\u73b0\u6709\u7684\u56fe\u50cf\u8bc4\u4f30\u6307\u6807\uff08\u5982FID\u3001LPIPS\u3001CLIPScore\uff09\u672a\u80fd\u5145\u5206\u8003\u8651\u5230SVG\u56fe\u50cf\u7684\u7b26\u53f7\u548c\u77e2\u91cf\u7279\u6027\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u65b0\u7684\u8bc4\u4f30\u65b9\u6cd5\u3002", "method": "SVGauge\u901a\u8fc7SigLIP\u56fe\u50cf\u5d4c\u5165\u548cPCA\u767d\u5316\u6280\u672f\u8bc4\u4f30\u89c6\u89c9\u4fdd\u771f\u5ea6\uff0c\u5e76\u901a\u8fc7BLIP-2\u751f\u6210\u7684SVG\u63cf\u8ff0\u4e0e\u539f\u59cb\u6587\u672c\u63d0\u793a\u5bf9\u6bd4\u6765\u8bc4\u4f30\u8bed\u4e49\u4e00\u81f4\u6027\u3002", "result": "\u5728SHE\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cSVGauge\u4e0e\u4eba\u7c7b\u5224\u65ad\u7684\u76f8\u5173\u6027\u6700\u9ad8\uff0c\u5e76\u4e14\u80fd\u591f\u66f4\u51c6\u786e\u5730\u91cd\u73b0\u516b\u79cd\u57fa\u4e8e\u96f6\u6837\u672cLLM\u751f\u6210\u5668\u7684\u7cfb\u7edf\u7ea7\u6392\u540d\u3002", "conclusion": "SVGauge\u4e3a\u672a\u6765\u6587\u672c\u5230SVG\u751f\u6210\u6a21\u578b\u7684\u8bc4\u4f30\u63d0\u4f9b\u4e86\u5b9e\u7528\u5de5\u5177\uff0c\u5f3a\u8c03\u4e86\u77e2\u91cf\u7279\u5b9a\u8bc4\u4f30\u7684\u5fc5\u8981\u6027\u3002"}}
{"id": "2509.07175", "categories": ["cs.GR"], "pdf": "https://arxiv.org/pdf/2509.07175", "abs": "https://arxiv.org/abs/2509.07175", "authors": ["Yanyang Xiao", "Yao Li", "Juan Cao", "Zhonggui Chen"], "title": "Efficient Computation of Voronoi Diagrams Using Point-in-Cell Tests", "comment": null, "summary": "Since the Voronoi diagram appears in many applications, the topic of\nimproving its computational efficiency remains attractive. We propose a novel\nyet efficient method to compute Voronoi diagrams bounded by a given domain,\ni.e., the clipped or restricted Voronoi diagrams. The intersection of the\ndomain and a Voronoi cell (domain-cell intersection) is generated by removing\nthe part outside the cell from the domain, which can be accomplished by several\nclippings. Different from the existing methods, we present an edge-based search\nscheme to find clipping planes (bisectors). A test called point-in-cell is\nfirst set up to tell whether a space point is in a target Voronoi cell or not.\nThen, for each edge of the intermediate domain-cell intersection, we will\nlaunch a clipping only if its two endpoints are respectively inside and outside\nthe corresponding Voronoi cell, where the bisector for the clipping can be\nfound by using a few times of point-in-cell tests. Therefore, our method only\ninvolves the clippings that contribute to the final results, which is a great\nadvantage over the state-of-the-art methods. Additionally, because each\ndomain-cell intersection can be generated independently, we extend the proposed\nmethod to the GPUs for computing Voronoi diagrams in parallel. The experimental\nresults show the best performance of our method compared to state-of-the-art\nones, regardless of site distribution. This paper was first submitted to\nSIGGRAPH Asia 2025.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u4e14\u9ad8\u6548\u7684\u57fa\u4e8e\u8fb9\u641c\u7d22\u7684\u65b9\u6cd5\u6765\u8ba1\u7b97\u6709\u754cVoronoi\u56fe\uff0c\u901a\u8fc7\u70b9-\u5355\u5143\u6d4b\u8bd5\u548c\u4f18\u5316\u88c1\u526a\u7b56\u7565\u663e\u8457\u63d0\u5347\u4e86\u8ba1\u7b97\u6548\u7387\uff0c\u5e76\u5728GPU\u4e0a\u5b9e\u73b0\u4e86\u5e76\u884c\u5316\u3002", "motivation": "Voronoi\u56fe\u5728\u4f17\u591a\u5e94\u7528\u4e2d\u5e7f\u6cdb\u4f7f\u7528\uff0c\u4f46\u5176\u8ba1\u7b97\u6548\u7387\u4ecd\u662f\u4e00\u4e2a\u5173\u952e\u95ee\u9898\u3002\u4e3a\u4e86\u63d0\u9ad8\u6709\u754cVoronoi\u56fe\u7684\u8ba1\u7b97\u6548\u7387\uff0c\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u65b9\u6cd5\u3002", "method": "\u91c7\u7528\u57fa\u4e8e\u8fb9\u7684\u641c\u7d22\u65b9\u6cd5\u786e\u5b9a\u88c1\u526a\u5e73\u9762\uff08\u53cc\u66f2\u9762\uff09\uff0c\u901a\u8fc7\u70b9-\u5355\u5143\u6d4b\u8bd5\u5224\u65ad\u7a7a\u95f4\u70b9\u662f\u5426\u5728\u76ee\u6807Voronoi\u5355\u5143\u5185\uff0c\u4ec5\u5bf9\u8d21\u732e\u6700\u7ec8\u7ed3\u679c\u7684\u8fb9\u8fdb\u884c\u88c1\u526a\u3002\u6b64\u5916\uff0c\u8be5\u65b9\u6cd5\u5728GPU\u4e0a\u5b9e\u73b0\u4e86\u5e76\u884c\u5316\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u4e0d\u540c\u7ad9\u70b9\u5206\u5e03\u60c5\u51b5\u4e0b\u5747\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u8868\u73b0\u51fa\u6700\u4f73\u6027\u80fd\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u7684\u57fa\u4e8e\u8fb9\u641c\u7d22\u7684Voronoi\u56fe\u8ba1\u7b97\u65b9\u6cd5\u4e0d\u4ec5\u9ad8\u6548\uff0c\u8fd8\u80fd\u901a\u8fc7\u5e76\u884c\u5316\u8fdb\u4e00\u6b65\u63d0\u5347\u6027\u80fd\uff0c\u4e3a\u5b9e\u9645\u5e94\u7528\u63d0\u4f9b\u4e86\u65b0\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.07522", "categories": ["cs.GR", "cs.CV"], "pdf": "https://arxiv.org/pdf/2509.07522", "abs": "https://arxiv.org/abs/2509.07522", "authors": ["Jierui Ren", "Haojie Jin", "Bo Pang", "Yisong Chen", "Guoping Wang", "Sheng Li"], "title": "Neural Cone Radiosity for Interactive Global Illumination with Glossy Materials", "comment": null, "summary": "Modeling of high-frequency outgoing radiance distributions has long been a\nkey challenge in rendering, particularly for glossy material. Such\ndistributions concentrate radiative energy within a narrow lobe and are highly\nsensitive to changes in view direction. However, existing neural radiosity\nmethods, which primarily rely on positional feature encoding, exhibit notable\nlimitations in capturing these high-frequency, strongly view-dependent radiance\ndistributions. To address this, we propose a highly-efficient approach by\nreflectance-aware ray cone encoding based on the neural radiosity framework,\nnamed neural cone radiosity. The core idea is to employ a pre-filtered\nmulti-resolution hash grid to accurately approximate the glossy BSDF lobe,\nembedding view-dependent reflectance characteristics directly into the encoding\nprocess through continuous spatial aggregation. Our design not only\nsignificantly improves the network's ability to model high-frequency reflection\ndistributions but also effectively handles surfaces with a wide range of\nglossiness levels, from highly glossy to low-gloss finishes. Meanwhile, our\nmethod reduces the network's burden in fitting complex radiance distributions,\nallowing the overall architecture to remain compact and efficient.\nComprehensive experimental results demonstrate that our method consistently\nproduces high-quality, noise-free renderings in real time under various\nglossiness conditions, and delivers superior fidelity and realism compared to\nbaseline approaches.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u795e\u7ecf\u8f90\u5c04\u6846\u67b6\u7684\u9ad8\u6548\u65b9\u6cd5\uff0c\u901a\u8fc7\u53cd\u5c04\u611f\u77e5\u7684\u5c04\u7ebf\u9525\u7f16\u7801\u6765\u6355\u6349\u9ad8\u9891\u7387\u3001\u5f3a\u89c6\u89d2\u4f9d\u8d56\u7684\u8f90\u5c04\u5206\u5e03\uff0c\u663e\u8457\u63d0\u5347\u4e86\u7f51\u7edc\u5bf9\u955c\u9762\u53cd\u5c04\u5206\u5e03\u5efa\u6a21\u7684\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u7684\u795e\u7ecf\u8f90\u5c04\u65b9\u6cd5\u4e3b\u8981\u4f9d\u8d56\u4e8e\u4f4d\u7f6e\u7279\u5f81\u7f16\u7801\uff0c\u4f46\u5728\u6355\u6349\u9ad8\u9891\u7387\u3001\u5f3a\u89c6\u89d2\u4f9d\u8d56\u7684\u8f90\u5c04\u5206\u5e03\u65f6\u5b58\u5728\u660e\u663e\u5c40\u9650\u6027\uff0c\u9700\u8981\u4e00\u79cd\u66f4\u9ad8\u6548\u7684\u65b9\u6cd5\u6765\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u91c7\u7528\u53cd\u5c04\u611f\u77e5\u7684\u5c04\u7ebf\u9525\u7f16\u7801\uff08\u795e\u7ecf\u9525\u8f90\u5c04\uff09\uff0c\u5229\u7528\u9884\u6ee4\u6ce2\u7684\u591a\u5206\u8fa8\u7387\u54c8\u5e0c\u7f51\u683c\u8fd1\u4f3c\u955c\u9762BSDF\u53f6\uff0c\u5c06\u89c6\u89d2\u4f9d\u8d56\u6027\u53cd\u5c04\u7279\u5f81\u76f4\u63a5\u5d4c\u5165\u7f16\u7801\u8fc7\u7a0b\uff0c\u5e76\u901a\u8fc7\u8fde\u7eed\u7a7a\u95f4\u805a\u5408\u4f18\u5316\u5efa\u6a21\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u80fd\u591f\u5b9e\u65f6\u751f\u6210\u9ad8\u8d28\u91cf\u3001\u65e0\u566a\u58f0\u7684\u6e32\u67d3\u56fe\u50cf\uff0c\u5728\u4e0d\u540c\u955c\u9762\u7a0b\u5ea6\u4e0b\u5747\u8868\u73b0\u51fa\u5353\u8d8a\u7684\u903c\u771f\u5ea6\u548c\u4fdd\u771f\u5ea6\uff0c\u4f18\u4e8e\u57fa\u51c6\u65b9\u6cd5\u3002", "conclusion": "\u6211\u4eec\u7684\u65b9\u6cd5\u4e0d\u4ec5\u663e\u8457\u63d0\u5347\u4e86\u7f51\u7edc\u5bf9\u9ad8\u9891\u7387\u53cd\u5c04\u5206\u5e03\u7684\u5efa\u6a21\u80fd\u529b\uff0c\u8fd8\u80fd\u9ad8\u6548\u5904\u7406\u4ece\u9ad8\u5149\u5230\u4f4e\u5149\u7684\u591a\u79cd\u955c\u9762\u8868\u9762\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u7ed3\u6784\u7684\u7d27\u51d1\u6027\u548c\u9ad8\u6548\u6027\u3002"}}
{"id": "2509.07003", "categories": ["cs.PL", "cs.DC", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.07003", "abs": "https://arxiv.org/abs/2509.07003", "authors": ["Youjie Li", "Cheng Wan", "Zhiqi Lin", "Hongyu Zhu", "Jiacheng Yang", "Ziang Song", "Xinyi Di", "Jiawei Wu", "Huiyao Shu", "Wenlei Bao", "Yanghua Peng", "Haibin Lin", "Li-Wen Chang"], "title": "veScale: Consistent and Efficient Tensor Programming with Eager-Mode SPMD", "comment": "21 pages, 16 figures, 5 tables", "summary": "Large Language Models (LLMs) have scaled rapidly in size and complexity,\nrequiring increasingly intricate parallelism for distributed training, such as\n3D parallelism. This sophistication motivates a shift toward simpler, more\ndebuggable programming paradigm like Single Program Multiple Data (SPMD).\nHowever, SPMD in eager execution introduces two key challenges: ensuring\nconsistency with single-device execution and achieving high performance at\nscale. In this paper, we introduce veScale, an eager-mode training system that\nfully embraces SPMD paradigm to democratize distributed tensor programming.\nveScale addresses the prevalent issue of inconsistent results in systems like\nPyTorch by introducing a novel algorithm of distributed Random Number\nGeneration (RNG) compatible with arbitrary sharded operators. veScale also\nsignificantly boosts training performance by reducing PyTorch primitive's\noverhead and improving communication efficiency. Evaluations show that veScale\ndelivers up to 2.2x speedup over the state-of-the-art training systems, like\nTorchTitan, and cuts code complexity by 78.4%, while preserving\nsingle-device-equivalent results.", "AI": {"tldr": "veScale\u662f\u4e00\u4e2a\u91c7\u7528SPMD\u8303\u5f0f\u7684\u8bad\u7ec3\u7cfb\u7edf\uff0c\u65e8\u5728\u7b80\u5316\u5206\u5e03\u5f0f\u5927\u8bed\u8a00\u6a21\u578b\u7684\u8bad\u7ec3\u8fc7\u7a0b\uff0c\u89e3\u51b3\u7ed3\u679c\u4e0d\u4e00\u81f4\u6027\u548c\u6027\u80fd\u95ee\u9898\uff0c\u5e76\u5728\u901f\u5ea6\u548c\u4ee3\u7801\u590d\u6742\u5ea6\u4e0a\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u7cfb\u7edf\u3002", "motivation": "\u968f\u7740\u5927\u8bed\u8a00\u6a21\u578b\u89c4\u6a21\u548c\u590d\u6742\u6027\u7684\u589e\u52a0\uff0c\u4f20\u7edf\u5206\u5e03\u5f0f\u8bad\u7ec3\u65b9\u6cd5\u59823D\u5e76\u884c\u53d8\u5f97\u590d\u6742\u4e14\u96be\u4ee5\u8c03\u8bd5\uff0c\u56e0\u6b64\u9700\u8981\u66f4\u7b80\u5355\u3001\u53ef\u8c03\u8bd5\u7684\u7f16\u7a0b\u8303\u5f0f\u5982SPMD\u3002\u7136\u800c\uff0cSPMD\u5728\u5373\u65f6\u6267\u884c\u4e2d\u5b58\u5728\u7ed3\u679c\u4e00\u81f4\u6027\u548c\u6027\u80fd\u7684\u6311\u6218\u3002", "method": "\u8bba\u6587\u63d0\u51fa\u4e86veScale\u7cfb\u7edf\uff0c\u91c7\u7528SPMD\u8303\u5f0f\uff0c\u5e76\u901a\u8fc7\u521b\u65b0\u7684\u5206\u5e03\u5f0f\u968f\u673a\u6570\u751f\u6210\u7b97\u6cd5\u89e3\u51b3\u7ed3\u679c\u4e00\u81f4\u6027\u95ee\u9898\uff0c\u540c\u65f6\u4f18\u5316PyTorch\u539f\u8bed\u5f00\u9500\u548c\u901a\u4fe1\u6548\u7387\u4ee5\u63d0\u5347\u6027\u80fd\u3002", "result": "veScale\u5728\u8bc4\u4f30\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u6bd4\u73b0\u6709\u6700\u4f18\u8bad\u7ec3\u7cfb\u7edf\uff08\u5982TorchTitan\uff09\u63d0\u901f2.2\u500d\uff0c\u540c\u65f6\u51cf\u5c1178.4%\u7684\u4ee3\u7801\u590d\u6742\u5ea6\uff0c\u4e14\u4fdd\u6301\u4e86\u4e0e\u5355\u8bbe\u5907\u7b49\u6548\u7684\u7ed3\u679c\u3002", "conclusion": "veScale\u7cfb\u7edf\u6210\u529f\u7b80\u5316\u4e86\u5206\u5e03\u5f0f\u5927\u8bed\u8a00\u6a21\u578b\u7684\u8bad\u7ec3\uff0c\u89e3\u51b3\u4e86SPMD\u8303\u5f0f\u7684\u5173\u952e\u6311\u6218\uff0c\u5e76\u5728\u6027\u80fd\u548c\u4ee3\u7801\u590d\u6742\u5ea6\u4e0a\u53d6\u5f97\u4e86\u663e\u8457\u63d0\u5347\u3002"}}
{"id": "2509.07520", "categories": ["cs.GT"], "pdf": "https://arxiv.org/pdf/2509.07520", "abs": "https://arxiv.org/abs/2509.07520", "authors": ["Martin Hoefer", "Tim Koglin", "Tolga Tel"], "title": "Persuading Agents in Opinion Formation Games", "comment": "Accepted at SAGT 2025", "summary": "Prominent opinion formation models such as the one by Friedkin and Johnsen\n(FJ) concentrate on the effects of peer pressure on public opinions. In\npractice, opinion formation is also based on information about the state of the\nworld and persuasion efforts. In this paper, we analyze an approach of Bayesian\npersuasion in the FJ model. There is an unknown state of the world that\ninfluences the preconceptions of n agents. A sender S can (partially) reveal\ninformation about the state to all agents. The agents update their\npreconceptions, and an equilibrium of public opinions emerges. We propose\nalgorithms for the sender to reveal information in order to optimize various\naspects of the emerging equilibrium. For many natural sender objectives, we\nshow that there are simple optimal strategies. We then focus on a general class\nof range-based objectives with desired opinion ranges for each agent. We\nprovide efficient algorithms in several cases, e.g., when the matrix of\npreconceptions in all states has constant rank, or when there is only a\npolynomial number of range combinations that lead to positive value for S. This\ngeneralizes, e.g., instances with a constant number of states and/or agents, or\ninstances with a logarithmic number of ranges. In general, we show that\nsubadditive range-based objectives allow a simple n-approximation, and even for\nadditive ones, obtaining an $n^{1-c}$-approximation is NP-hard, for any\nconstant $c > 0$.", "AI": {"tldr": "\u672c\u6587\u5206\u6790\u4e86Bayesian persuasion\u5728Friedkin-Johnsen (FJ) \u610f\u89c1\u5f62\u6210\u6a21\u578b\u4e2d\u7684\u5e94\u7528\uff0c\u7814\u7a76\u5982\u4f55\u901a\u8fc7\u4fe1\u606f\u63ed\u793a\u4f18\u5316\u610f\u89c1\u5e73\u8861\uff0c\u5e76\u63d0\u51fa\u7b97\u6cd5\u4ee5\u5b9e\u73b0\u591a\u79cd\u76ee\u6807\u3002", "motivation": "\u73b0\u6709\u610f\u89c1\u5f62\u6210\u6a21\u578b\u5982FJ\u6a21\u578b\u4e3b\u8981\u5173\u6ce8\u540c\u4f34\u538b\u529b\u7684\u5f71\u54cd\uff0c\u800c\u5b9e\u9645\u4e2d\u8fd8\u53d7\u4e16\u754c\u72b6\u6001\u548c\u8bf4\u670d\u52aa\u529b\u7684\u5f71\u54cd\uff0c\u56e0\u6b64\u672c\u7814\u7a76\u63a2\u8ba8Bayesian persuasion\u7684\u4f5c\u7528\u3002", "method": "\u63d0\u51fa\u4e00\u4e2a\u6846\u67b6\uff0c\u5176\u4e2d\u53d1\u9001\u8005S\u53ef\u4ee5\u90e8\u5206\u63ed\u793a\u4e16\u754c\u72b6\u6001\u7684\u4fe1\u606f\uff0c\u4ee3\u7406\u6839\u636e\u8fd9\u4e9b\u4fe1\u606f\u66f4\u65b0\u9884\u60f3\u5e76\u5f62\u6210\u610f\u89c1\u5e73\u8861\u3002\u8bbe\u8ba1\u4e86\u4f18\u5316\u7b97\u6cd5\u4ee5\u5b9e\u73b0\u53d1\u9001\u8005\u7684\u591a\u79cd\u76ee\u6807\u3002", "result": "\u7814\u7a76\u663e\u793a\uff0c\u5bf9\u8bb8\u591a\u81ea\u7136\u76ee\u6807\u5b58\u5728\u7b80\u5355\u6700\u4f18\u7b56\u7565\u3002\u9488\u5bf9\u57fa\u4e8e\u8303\u56f4\u7684\u76ee\u6807\uff0c\u63d0\u4f9b\u4e86\u9ad8\u6548\u7b97\u6cd5\uff0c\u5982\u5728\u9884\u60f3\u77e9\u9635\u4e3a\u5e38\u6570\u79e9\u6216\u8303\u56f4\u7ec4\u5408\u6570\u91cf\u591a\u9879\u5f0f\u65f6\u3002\u540c\u65f6\u8bc1\u660e\u5b50\u52a0\u6027\u76ee\u6807\u5141\u8bb8\u7b80\u5355n\u8fd1\u4f3c\uff0c\u800c\u52a0\u6027\u76ee\u6807\u7684n^{1-c}\u8fd1\u4f3c\u662fNP\u96be\u95ee\u9898\u3002", "conclusion": "\u7814\u7a76\u6269\u5c55\u4e86FJ\u6a21\u578b\uff0c\u5f15\u5165\u4e86Bayesian persuasion\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u7cfb\u5217\u4f18\u5316\u7b97\u6cd5\uff0c\u8868\u660e\u5728\u67d0\u4e9b\u6761\u4ef6\u4e0b\u53ef\u4ee5\u5b9e\u73b0\u9ad8\u6548\u6c42\u89e3\uff0c\u4f46\u666e\u904d\u60c5\u51b5\u4e0b\u8fd1\u4f3c\u95ee\u9898\u5177\u6709\u6311\u6218\u6027\u3002"}}
{"id": "2509.07653", "categories": ["cs.GR"], "pdf": "https://arxiv.org/pdf/2509.07653", "abs": "https://arxiv.org/abs/2509.07653", "authors": ["Yuheng Jiang", "Chengcheng Guo", "Yize Wu", "Yu Hong", "Shengkun Zhu", "Zhehao Shen", "Yingliang Zhang", "Shaohui Jiao", "Zhuo Su", "Lan Xu", "Marc Habermann", "Christian Theobalt"], "title": "Topology-Aware Optimization of Gaussian Primitives for Human-Centric Volumetric Videos", "comment": "Accepted at SIGGRAPH Asia 2025. Project page:\n  https://guochch.github.io/TaoGS/", "summary": "Volumetric video is emerging as a key medium for digitizing the dynamic\nphysical world, creating the virtual environments with six degrees of freedom\nto deliver immersive user experiences. However, robustly modeling general\ndynamic scenes, especially those involving topological changes while\nmaintaining long-term tracking remains a fundamental challenge. In this paper,\nwe present TaoGS, a novel topology-aware dynamic Gaussian representation that\ndisentangles motion and appearance to support, both, long-range tracking and\ntopological adaptation. We represent scene motion with a sparse set of motion\nGaussians, which are continuously updated by a spatio-temporal tracker and\nphotometric cues that detect structural variations across frames. To capture\nfine-grained texture, each motion Gaussian anchors and dynamically activates a\nset of local appearance Gaussians, which are non-rigidly warped to the current\nframe to provide strong initialization and significantly reduce training time.\nThis activation mechanism enables efficient modeling of detailed textures and\nmaintains temporal coherence, allowing high-fidelity rendering even under\nchallenging scenarios such as changing clothes. To enable seamless integration\ninto codec-based volumetric formats, we introduce a global Gaussian Lookup\nTable that records the lifespan of each Gaussian and organizes attributes into\na lifespan-aware 2D layout. This structure aligns naturally with standard video\ncodecs and supports up to 40 compression. TaoGS provides a unified, adaptive\nsolution for scalable volumetric video under topological variation, capturing\nmoments where \"elegance in motion\" and \"Power in Stillness\", delivering\nimmersive experiences that harmonize with the physical world.", "AI": {"tldr": "TaoGS\u662f\u4e00\u79cd\u65b0\u578b\u62d3\u6251\u611f\u77e5\u52a8\u6001\u9ad8\u65af\u8868\u793a\u65b9\u6cd5\uff0c\u901a\u8fc7\u5206\u79bb\u8fd0\u52a8\u548c\u5916\u89c2\u652f\u6301\u957f\u671f\u8ddf\u8e2a\u548c\u62d3\u6251\u9002\u5e94\uff0c\u4e3a\u62d3\u6251\u53d8\u5316\u4e0b\u7684\u53ef\u6269\u5c55\u4f53\u79ef\u89c6\u9891\u63d0\u4f9b\u7edf\u4e00\u89e3\u51b3\u65b9\u6848\u3002", "motivation": "\u5f53\u524d\u5bf9\u52a8\u6001\u573a\u666f\u7684\u9c81\u68d2\u5efa\u6a21\uff0c\u5c24\u5176\u662f\u6d89\u53ca\u62d3\u6251\u53d8\u5316\u4e14\u9700\u4fdd\u6301\u957f\u671f\u8ddf\u8e2a\u7684\u573a\u666f\uff0c\u4ecd\u662f\u4e00\u4e2a\u57fa\u672c\u6311\u6218\u3002\u8fd9\u7bc7\u8bba\u6587\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "TaoGS\u4f7f\u7528\u7a00\u758f\u7684\u8fd0\u52a8\u9ad8\u65af\u96c6\u8868\u793a\u573a\u666f\u8fd0\u52a8\uff0c\u5e76\u52a8\u6001\u6fc0\u6d3b\u5c40\u90e8\u5916\u89c2\u9ad8\u65af\u4ee5\u6355\u6349\u7ec6\u7c92\u5ea6\u7eb9\u7406\u3002\u6b64\u5916\uff0c\u5f15\u5165\u5168\u5c40\u9ad8\u65af\u67e5\u627e\u8868\u4ee5\u652f\u6301\u538b\u7f29\u6807\u51c6\u3002", "result": "TaoGS\u80fd\u591f\u9ad8\u6548\u5efa\u6a21\u7ec6\u8282\u7eb9\u7406\u5e76\u4fdd\u6301\u65f6\u95f4\u4e00\u81f4\u6027\uff0c\u5373\u4f7f\u5728\u8bf8\u5982\u6362\u8863\u670d\u7b49\u6311\u6218\u6027\u573a\u666f\u4e0b\u4e5f\u80fd\u5b9e\u73b0\u9ad8\u4fdd\u771f\u6e32\u67d3\uff0c\u5e76\u652f\u6301\u9ad8\u8fbe40\u500d\u7684\u538b\u7f29\u3002", "conclusion": "TaoGS\u4e3a\u62d3\u6251\u53d8\u5316\u4e0b\u7684\u53ef\u6269\u5c55\u4f53\u79ef\u89c6\u9891\u63d0\u4f9b\u4e86\u7edf\u4e00\u3001\u81ea\u9002\u5e94\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5728\u8fd0\u52a8\u548c\u9759\u6b62\u4e2d\u6355\u6349\u4f18\u96c5\u4e0e\u529b\u91cf\uff0c\u5b9e\u73b0\u6c89\u6d78\u5f0f\u4f53\u9a8c\u3002"}}
{"id": "2509.07551", "categories": ["cs.PL"], "pdf": "https://arxiv.org/pdf/2509.07551", "abs": "https://arxiv.org/abs/2509.07551", "authors": ["Sean Bocirnea", "William J. Bowman"], "title": "Fast and Extensible Hybrid Embeddings with Micros", "comment": "13 pages", "summary": "Macro embedding is a popular approach to defining extensible shallow\nembeddings of object languages in Scheme like host languages. While macro\nembedding has even been shown to enable implementing extensible typed languages\nin systems like Racket, it comes at a cost: compile-time performance. In this\npaper, we revisit micros - syntax to intermediate representation (IR)\ntransformers, rather than source syntax to source syntax transformers (macros).\nMicro embedding enables stopping at an IR, producing a deep embedding and\nenabling high performance compile-time functions over an efficient IR, before\nshallowly embedding the IR back into source syntax. Combining micros with\nseveral design patterns to enable the IR and functions over it to be\nextensible, we achieve extensible hybrid embedding of statically typed\nlanguages with significantly improved compile-time compared to macro-embedding\napproaches. We describe our design patterns and propose new abstractions\npackaging these patterns.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a\"micro embedding\"\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u5728\u4e2d\u95f4\u8868\u793a\uff08IR\uff09\u9636\u6bb5\u505c\u6b62\u5904\u7406\u5e76\u9ad8\u6548\u5730\u8fdb\u884c\u6269\u5c55\uff0c\u663e\u8457\u63d0\u5347\u4e86\u9759\u6001\u7c7b\u578b\u8bed\u8a00\u5d4c\u5165\u7684\u7f16\u8bd1\u65f6\u6027\u80fd\u3002", "motivation": "\u4f20\u7edf\u7684\u5b8f\u5d4c\u5165\uff08macro embedding\uff09\u867d\u7136\u80fd\u591f\u5b9e\u73b0\u53ef\u6269\u5c55\u7684\u7c7b\u578b\u8bed\u8a00\u5d4c\u5165\uff0c\u4f46\u5176\u7f16\u8bd1\u65f6\u6027\u80fd\u8f83\u5dee\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\uff0c\u672c\u7814\u7a76\u63a2\u7d22\u4e86\u65b0\u7684\u5d4c\u5165\u65b9\u6cd5\u3002", "method": "\u63d0\u51fa\"micro embedding\"\u65b9\u6cd5\uff0c\u5c06\u8bed\u6cd5\u8f6c\u6362\u4e3a\u4e2d\u95f4\u8868\u793a\uff08IR\uff09\uff0c\u800c\u975e\u76f4\u63a5\u8f6c\u6362\u4e3a\u6e90\u8bed\u6cd5\uff0c\u5e76\u7ed3\u5408\u591a\u79cd\u8bbe\u8ba1\u6a21\u5f0f\u4ee5\u5b9e\u73b0IR\u7684\u9ad8\u6548\u6269\u5c55\u3002", "result": "\u76f8\u8f83\u4e8e\u5b8f\u5d4c\u5165\u65b9\u6cd5\uff0cmicro embedding\u663e\u8457\u63d0\u5347\u4e86\u9759\u6001\u7c7b\u578b\u8bed\u8a00\u5d4c\u5165\u7684\u7f16\u8bd1\u65f6\u6027\u80fd\uff0c\u5e76\u901a\u8fc7\u65b0\u7684\u62bd\u8c61\u6a21\u5f0f\u5b9e\u73b0\u4e86\u53ef\u6269\u5c55\u6027\u3002", "conclusion": "micro embedding\u662f\u4e00\u79cd\u9ad8\u6548\u7684\u6df7\u5408\u5d4c\u5165\u65b9\u6cd5\uff0c\u80fd\u591f\u663e\u8457\u63d0\u5347\u7f16\u8bd1\u65f6\u6027\u80fd\uff0c\u540c\u65f6\u652f\u6301\u7075\u6d3b\u6269\u5c55\u3002"}}
{"id": "2509.07557", "categories": ["cs.GT", "math.PR"], "pdf": "https://arxiv.org/pdf/2509.07557", "abs": "https://arxiv.org/abs/2509.07557", "authors": ["Paul G\u00f6lz", "Jan Maly", "Ulrike Schmidt-Kraepelin", "Markus Utke", "Philipp C. Verpoort"], "title": "City Sampling for Citizens' Assemblies", "comment": null, "summary": "In citizens' assemblies, a group of constituents is randomly selected to\nweigh in on policy issues. We study a two-stage sampling problem faced by\npractitioners in countries such as Germany, in which constituents' contact\ninformation is stored at a municipal level. As a result, practitioners can only\nselect constituents from a bounded number of cities ex post, while ensuring\nequal selection probability for constituents ex ante.\n  We develop several algorithms for this problem. Although minimizing the\nnumber of contacted cities is NP-hard, we provide a pseudo-polynomial time\nalgorithm and an additive 1-approximation, both based on separation oracles for\na linear programming formulation. Recognizing that practical objectives go\nbeyond minimizing city count, we further introduce a simple and more\ninterpretable greedy algorithm, which additionally satisfies an ex-post\nmonotonicity property and achieves an additive 2-approximation. Finally, we\nexplore a notion of ex-post proportionality, for which we propose two practical\nalgorithms: an optimal algorithm based on column generation and integer linear\nprogramming and a simple heuristic creating particularly transparent\ndistributions. We evaluate these algorithms on data from Germany, and plan to\ndeploy them in cooperation with a leading nonprofit organization in this space.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u5728\u5fb7\u56fd\u7b49\u56fd\u5bb6\u4e2d\uff0c\u7531\u4e8e\u9009\u6c11\u8054\u7cfb\u4fe1\u606f\u5b58\u50a8\u5728\u5e02\u7ea7\u5c42\u9762\uff0c\u516c\u6c11\u5927\u4f1a\u4e2d\u968f\u673a\u9009\u62e9\u9009\u6c11\u7684\u4e24\u9636\u6bb5\u62bd\u6837\u95ee\u9898\uff0c\u63d0\u51fa\u4e86\u591a\u79cd\u7b97\u6cd5\u5e76\u8bc4\u4f30\u4e86\u5176\u6027\u80fd\u3002", "motivation": "\u516c\u6c11\u5927\u4f1a\u4e2d\u9700\u8981\u968f\u673a\u9009\u62e9\u9009\u6c11\u53c2\u4e0e\u653f\u7b56\u8ba8\u8bba\uff0c\u4f46\u7531\u4e8e\u9009\u6c11\u4fe1\u606f\u5b58\u50a8\u5728\u5e02\u7ea7\u5c42\u9762\uff0c\u5b9e\u8df5\u4e2d\u53ea\u80fd\u5728\u6709\u9650\u6570\u91cf\u7684\u57ce\u5e02\u4e2d\u9009\u62e9\u9009\u6c11\uff0c\u540c\u65f6\u786e\u4fdd\u4e8b\u5148\u7684\u5e73\u7b49\u9009\u62e9\u6982\u7387\u3002", "method": "\u5f00\u53d1\u4e86\u591a\u79cd\u7b97\u6cd5\uff0c\u5305\u62ec\u57fa\u4e8e\u7ebf\u6027\u89c4\u5212\u7684\u4f2a\u591a\u9879\u5f0f\u65f6\u95f4\u7b97\u6cd5\u3001\u8d2a\u5a6a\u7b97\u6cd5\u4ee5\u53ca\u63a2\u7d22\u4e8b\u540e\u6bd4\u4f8b\u6027\u7684\u4e24\u79cd\u5b9e\u7528\u7b97\u6cd5\u3002", "result": "\u63d0\u51fa\u7684\u7b97\u6cd5\u5728\u5fb7\u56fd\u6570\u636e\u4e0a\u8fdb\u884c\u4e86\u8bc4\u4f30\uff0c\u5176\u4e2d\u4e00\u4e9b\u7b97\u6cd5\u8868\u73b0\u51fa\u826f\u597d\u7684\u8fd1\u4f3c\u6027\u80fd\u548c\u5b9e\u7528\u6027\u3002", "conclusion": "\u8fd9\u4e9b\u7b97\u6cd5\u4e0d\u4ec5\u89e3\u51b3\u4e86\u5b9e\u8df5\u4e2d\u7684\u62bd\u6837\u95ee\u9898\uff0c\u8fd8\u4e3a\u8fdb\u4e00\u6b65\u90e8\u7f72\u548c\u5e94\u7528\u63d0\u4f9b\u4e86\u652f\u6301\u3002"}}
{"id": "2509.07609", "categories": ["cs.PL"], "pdf": "https://arxiv.org/pdf/2509.07609", "abs": "https://arxiv.org/abs/2509.07609", "authors": ["Yichen Xu", "Oliver Bra\u010devac", "Cao Nguyen Pham", "Martin Odersky"], "title": "What's in the Box: Ergonomic and Expressive Capture Tracking over Generic Data Structures (Extended Version)", "comment": null, "summary": "Capturing types in Scala unify static effect and resource tracking with\nobject capabilities, enabling lightweight effect polymorphism with minimal\nnotational overhead. However, their expressiveness has been insufficient for\ntracking capabilities embedded in generic data structures, preventing them from\nscaling to the standard collections library -- an essential prerequisite for\nbroader adoption. This limitation stems from the inability to name capabilities\nwithin the system's notion of box types.\n  This paper develops System Capless, a new foundation for capturing types that\nprovides the theoretical basis for reach capabilities (rcaps), a novel\nmechanism for naming \"what's in the box.\" The calculus refines the universal\ncapability notion into a new scheme with existential and universal capture set\nquantification. Intuitively, rcaps witness existentially quantified capture\nsets inside the boxes of generic types in a way that does not require exposing\nexistential capture types in the surface language. We have fully mechanized the\nformal metatheory of System Capless in Lean, including proofs of type soundness\nand scope safety. System Capless supports the same lightweight notation of\ncapturing types plus rcaps, as certified by a type-preserving translation, and\nalso enables fully optional explicit capture-set quantification to increase\nexpressiveness.\n  Finally, we present a full reimplementation of capture checking in Scala 3\nbased on System Capless and migrate the entire Scala collections library and an\nasynchronous programming library to evaluate its practicality and ergonomics.\nOur results demonstrate that reach capabilities enable the adoption of capture\nchecking in production code with minimal changes and minimal-to-zero notational\noverhead in a vast majority of cases.", "AI": {"tldr": "System Capless \u4e3a\u6355\u83b7\u7c7b\u578b\u63d0\u4f9b\u4e86\u65b0\u7684\u7406\u8bba\u57fa\u7840\uff0c\u89e3\u51b3\u4e86 Scala \u4e2d\u901a\u7528\u6570\u636e\u7ed3\u6784\u4e2d\u80fd\u529b\u8ddf\u8e2a\u7684\u9650\u5236\uff0c\u652f\u6301\u8f7b\u91cf\u7ea7\u8868\u793a\u5e76\u6269\u5c55\u5230\u6807\u51c6\u96c6\u5408\u5e93\u3002", "motivation": "\u5f53\u524d Scala \u7684\u6355\u83b7\u7c7b\u578b\u5728\u8ddf\u8e2a\u901a\u7528\u6570\u636e\u7ed3\u6784\u4e2d\u7684\u80fd\u529b\u65f6\u8868\u73b0\u4e0d\u8db3\uff0c\u9650\u5236\u4e86\u5176\u5728\u6807\u51c6\u96c6\u5408\u5e93\u4e2d\u7684\u5e7f\u6cdb\u5e94\u7528\u3002", "method": "\u5f00\u53d1\u4e86 System Capless\uff0c\u5f15\u5165\u4e86 \"rcaps\" \u673a\u5236\uff0c\u901a\u8fc7\u5b58\u5728\u6027\u548c\u901a\u7528\u6355\u83b7\u96c6\u5408\u91cf\u5316\u6765\u547d\u540d \"\u76d2\u5b50\u4e2d\u7684\u5185\u5bb9\"\uff0c\u5e76\u5728 Lean \u4e2d\u5f62\u5f0f\u5316\u4e86\u5176\u7406\u8bba\u3002", "result": "System Capless \u652f\u6301\u8f7b\u91cf\u7ea7\u8868\u793a\uff0c\u5e76\u901a\u8fc7 Scala 3 \u4e2d\u7684\u5b9e\u73b0\u9a8c\u8bc1\u4e86\u5176\u5728\u5b9e\u9645\u4ee3\u7801\u4e2d\u7684\u5e94\u7528\uff0c\u8fc1\u79fb\u7ed3\u679c\u663e\u793a\u5176\u5f00\u9500\u6781\u5c0f\u3002", "conclusion": "System Capless \u63d0\u4f9b\u4e86\u4e00\u79cd\u5b9e\u7528\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u80fd\u591f\u5728\u751f\u4ea7\u4ee3\u7801\u4e2d\u4ee5\u6700\u5c0f\u5f00\u9500\u5b9e\u73b0\u80fd\u529b\u8ddf\u8e2a\uff0c\u63a8\u52a8\u4e86\u6355\u83b7\u68c0\u67e5\u7684\u5e7f\u6cdb\u5e94\u7528\u3002"}}
{"id": "2509.07650", "categories": ["cs.GT"], "pdf": "https://arxiv.org/pdf/2509.07650", "abs": "https://arxiv.org/abs/2509.07650", "authors": ["Victor Villin", "Christos Dimitrakakis"], "title": "Inference of Intrinsic Rewards and Fairness in Multi-Agent Systems", "comment": "EWRL18 (2025) Workshop", "summary": "From altruism to antagonism, fairness plays a central role in social\ninteractions. But can we truly understand how fair someone is, especially\nwithout explicit knowledge of their preferences? We cast this challenge as a\nmulti-agent inverse reinforcement learning problem, explicitly structuring\nrewards to reflect how agents value the welfare of others. We introduce novel\nBayesian strategies, reasoning about the optimality of demonstrations and\ncharacterisation of equilibria in general-sum Markov games. Our experiments,\nspanning randomised environments and a collaborative cooking task, reveal that\ncoherent notions of fairness can be reliably inferred from demonstrations.\nFurthermore, when isolating fairness components, we obtain a disentangled\nunderstanding of agents preferences. Crucially, we unveil that by placing\nagents in different groups, we can force them to exhibit new facets of their\nreward structures, cutting through ambiguity to answer the central question:\nwho is being fair?", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u591a\u667a\u80fd\u4f53\u9006\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\uff0c\u901a\u8fc7\u8d1d\u53f6\u65af\u7b56\u7565\u63a8\u65ad\u667a\u80fd\u4f53\u5bf9\u793e\u4f1a\u798f\u5229\u7684\u504f\u597d\uff0c\u4ece\u800c\u89e3\u51b3\u516c\u5e73\u6027\u8bc4\u4f30\u95ee\u9898\u3002\u5b9e\u9a8c\u8868\u660e\uff0c\u53ef\u4ee5\u4ece\u667a\u80fd\u4f53\u7684\u884c\u4e3a\u4e2d\u53ef\u9760\u5730\u63a8\u65ad\u516c\u5e73\u6027\u6982\u5ff5\uff0c\u5e76\u901a\u8fc7\u5206\u7ec4\u63ed\u793a\u5176\u5956\u52b1\u7ed3\u6784\u7684\u66f4\u591a\u65b9\u9762\u3002", "motivation": "\u516c\u5e73\u6027\u662f\u793e\u4f1a\u4e92\u52a8\u7684\u6838\u5fc3\uff0c\u4f46\u7f3a\u4e4f\u5bf9\u4e2a\u4f53\u504f\u597d\u7684\u660e\u786e\u77e5\u8bc6\u65f6\uff0c\u5982\u4f55\u51c6\u786e\u8bc4\u4f30\u516c\u5e73\u6027\u662f\u4e00\u4e2a\u6311\u6218\u3002", "method": "\u91c7\u7528\u591a\u667a\u80fd\u4f53\u9006\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\uff0c\u901a\u8fc7\u8d1d\u53f6\u65af\u7b56\u7565\u5206\u6790\u667a\u80fd\u4f53\u884c\u4e3a\u7684\u4f18\u5316\u6027\uff0c\u5e76\u7814\u7a76\u4e00\u822c\u548c\u9a6c\u5c14\u53ef\u592b\u535a\u5f08\u4e2d\u7684\u5747\u8861\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u53ef\u4ee5\u901a\u8fc7\u6f14\u793a\u53ef\u9760\u5730\u63a8\u65ad\u516c\u5e73\u6027\u6982\u5ff5\uff0c\u5e76\u901a\u8fc7\u5206\u7ec4\u63ed\u793a\u667a\u80fd\u4f53\u5956\u52b1\u7ed3\u6784\u7684\u65b0\u65b9\u9762\uff0c\u6d88\u9664\u4e86\u516c\u5e73\u6027\u8bc4\u4f30\u7684\u6a21\u7cca\u6027\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e0d\u4ec5\u63d0\u4f9b\u4e86\u5bf9\u667a\u80fd\u4f53\u504f\u597d\u7684\u89e3\u6784\u7406\u89e3\uff0c\u8fd8\u901a\u8fc7\u5206\u7ec4\u7b56\u7565\u8bc1\u660e\u4e86\u516c\u5e73\u6027\u8bc4\u4f30\u7684\u6709\u6548\u6027\u3002"}}
{"id": "2509.07929", "categories": ["cs.GT", "cs.IR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.07929", "abs": "https://arxiv.org/abs/2509.07929", "authors": ["Rohan Garg", "Yongjin Xiao", "Jason", "Yang", "Mandar Rahurkar"], "title": "Smart Fast Finish: Preventing Overdelivery via Daily Budget Pacing at DoorDash", "comment": null, "summary": "We present a budget pacing feature called Smart Fast Finish (SFF). SFF builds\nupon the industry standard Fast Finish (FF) feature in budget pacing systems\nthat depletes remaining advertising budget as quickly as possible towards the\nend of some fixed time period. SFF dynamically updates system parameters such\nas start time and throttle rate depending on historical ad-campaign data. SFF\nis currently in use at DoorDash, one of the largest delivery platforms in the\nUS, and is part of its budget pacing system. We show via online budget-split\nexperimentation data and offline simulations that SFF is a robust solution for\noverdelivery mitigation when pacing budget.", "AI": {"tldr": "Smart Fast Finish (SFF) \u662f\u4e00\u79cd\u57fa\u4e8e\u5386\u53f2\u5e7f\u544a\u6d3b\u52a8\u6570\u636e\u52a8\u6001\u66f4\u65b0\u7cfb\u7edf\u53c2\u6570\uff08\u5982\u5f00\u59cb\u65f6\u95f4\u548c\u8282\u6d41\u7387\uff09\u7684\u9884\u7b97\u8282\u594f\u529f\u80fd\uff0c\u7528\u4e8e\u5728\u56fa\u5b9a\u65f6\u95f4\u6bb5\u7684\u672b\u671f\u5feb\u901f\u6d88\u8017\u5269\u4f59\u5e7f\u544a\u9884\u7b97\u3002", "motivation": "\u4e3a\u4e86\u89e3\u51b3\u5e7f\u544a\u9884\u7b97\u8d85\u652f\u7684\u95ee\u9898\uff0c\u9700\u8981\u5728\u56fa\u5b9a\u65f6\u95f4\u6bb5\u672b\u671f\u9ad8\u6548\u6d88\u8017\u5269\u4f59\u9884\u7b97\uff0c\u540c\u65f6\u907f\u514d\u8d85\u6295\u3002", "method": "SFF \u57fa\u4e8e\u884c\u4e1a\u6807\u51c6\u7684 Fast Finish (FF) \u529f\u80fd\uff0c\u901a\u8fc7\u52a8\u6001\u66f4\u65b0\u7cfb\u7edf\u53c2\u6570\uff08\u5982\u5f00\u59cb\u65f6\u95f4\u548c\u8282\u6d41\u7387\uff09\u6765\u4f18\u5316\u9884\u7b97\u6d88\u8017\u3002", "result": "\u5728 DoorDash \u7684\u5b9e\u9645\u5e94\u7528\u548c\u79bb\u7ebf\u6a21\u62df\u4e2d\uff0cSFF \u88ab\u8bc1\u660e\u662f\u4e00\u79cd\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u80fd\u591f\u7a33\u5065\u5730\u7f13\u89e3\u9884\u7b97\u8d85\u652f\u95ee\u9898\u3002", "conclusion": "SFF \u662f\u4e00\u79cd\u5728\u5e7f\u544a\u9884\u7b97\u8282\u594f\u4e2d\u9ad8\u6548\u4e14\u7a33\u5065\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u9002\u7528\u4e8e\u5927\u89c4\u6a21\u5e73\u53f0\u7684\u5b9e\u9645\u5e94\u7528\u3002"}}
