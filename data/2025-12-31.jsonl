{"id": "2512.22383", "categories": ["cs.PL", "cs.LO", "quant-ph"], "pdf": "https://arxiv.org/pdf/2512.22383", "abs": "https://arxiv.org/abs/2512.22383", "authors": ["Mingsheng Ying"], "title": "Symbolic Specification and Reasoning for Quantum Data and Operations", "comment": null, "summary": "In quantum information and computation research, symbolic methods have been widely used for human specification and reasoning about quantum states and operations. At the same time, they are essential for ensuring the scalability and efficiency of automated reasoning and verification tools for quantum algorithms and programs. However, a formal theory for symbolic specification and reasoning about quantum data and operations is still lacking, which significantly limits the practical applicability of automated verification techniques in quantum computing.\n  In this paper, we present a general logical framework, called Symbolic Operator Logic $\\mathbf{SOL}$, which enables symbolic specification and reasoning about quantum data and operations. Within this framework, a classical first-order logical language is embedded into a language of formal operators used to specify quantum data and operations, including their recursive definitions. This embedding allows reasoning about their properties modulo a chosen theory of the underlying classical data (e.g., Boolean algebra or group theory), thereby leveraging existing automated verification tools developed for classical computing. It should be emphasised that this embedding of classical first-order logic into $\\mathbf{SOL}$ is precisely what makes the symbolic method possible.\n  We envision that this framework can provide a conceptual foundation for the formal verification and automated theorem proving of quantum computation and information in proof assistants such as Lean, Coq, and related systems."}
{"id": "2512.22390", "categories": ["cs.PL"], "pdf": "https://arxiv.org/pdf/2512.22390", "abs": "https://arxiv.org/abs/2512.22390", "authors": ["Yuze Li", "Srinivasan Ramachandra Sharma", "Charitha Saumya", "Ali R. Butt", "Kirshanthan Sundararajah"], "title": "Eliminate Branches by Melding IR Instructions", "comment": null, "summary": "Branch mispredictions cause catastrophic performance penalties in modern processors, leading to performance loss. While hardware predictors and profile-guided techniques exist, data-dependent branches with irregular patterns remain challenging. Traditional if-conversion eliminates branches via software predication but faces limitations on architectures like x86. It often fails on paths containing memory instructions or incurs excessive instruction overhead by fully speculating large branch bodies.\n  This paper presents Melding IR Instructions (MERIT), a compiler transformation that eliminates branches by aligning and melding similar operations from divergent paths at the IR instruction level. By observing that divergent paths often perform structurally similar operations with different operands, MERIT adapts sequence alignment to discover merging opportunities and employs safe operand-level guarding to ensure semantic correctness without hardware predication. Implemented as an LLVM pass and evaluated on 102 programs from four benchmark suites, MERIT achieves a geometric mean speedup of 10.9% with peak improvements of 32x compared to hardware branch predictor, demonstrating the effectiveness with reduced static instruction overhead."}
{"id": "2512.22417", "categories": ["cs.PL"], "pdf": "https://arxiv.org/pdf/2512.22417", "abs": "https://arxiv.org/abs/2512.22417", "authors": ["Vasileios Koutavas", "Yu-Yang Lin", "Nikos Tzevelekos"], "title": "A Bounded Game Semantics Checker for Precise Smart Contract Analysis", "comment": "21 pages, 2 figures, 4 tables", "summary": "We present a new approach to finding smart contract vulnerabilities that is precise (no false positives up to our EVM-Yul interpreter), bounded-complete, and, when instrumented with domain knowledge, scales to real-world contracts. Our method is based on game semantics, modelling computation as an interaction between a contract and its environment, reducing reasoning about unknown or malicious external contracts to trace enumeration. We implement this in a tool we refer to as YulToolkit, a bounded game-semantics checker for Yul, the intermediate language of Solidity. By exploring only feasible interactions, YulToolkit avoids over-approximation, and by relying on the theory of game semantics it achieves bounded completeness. To make exploration tractable, YulToolkit supports instrumentation written in Solidity and propagated to Yul, comparable in effort to creating a test harness. Unlike tests, however, our technique explores all admissible traces within the chosen parameters and bounds. We evaluate YulToolkit on three real-world incidents: The DAO, PredyPool, and Lendf.Me, as well as benchmark contracts. In all cases, YulToolkit detects the known vulnerabilities (producing a violation-triggering trace), and after applying fixes, reports no further violations within bounds. These results show that bounded game semantics exploration is an effective and precise addition to the smart contract analysis toolbox, particularly for vulnerabilities such as reentrancy that are hard to detect precisely in real code."}
{"id": "2512.22684", "categories": ["cs.PL"], "pdf": "https://arxiv.org/pdf/2512.22684", "abs": "https://arxiv.org/abs/2512.22684", "authors": ["José Luis Romero", "Cristóbal Isla", "Matías Toro", "Éric Tanter"], "title": "Compiling Gradual Types with Evidence", "comment": "Submitted to TOPLAS", "summary": "Efficiently supporting sound gradual typing in a language with structural types is challenging. To date, the Grift compiler is the only close-to-the-metal implementation of gradual typing in this setting, exploiting coercions for runtime checks, and further extended with monotonic references for efficient access to statically-typed data structures. On the language design and semantics side, the Abstracting Gradual Typing (AGT) methodology has proven fruitful to elucidate existing designs and to innovate by deriving gradualizations of a wide variety of typing disciplines and language features. Grounded in abstract interpretation, the Curry-Howard inspired runtime semantics of AGT is based on the notion of evidence for consistent judgments that evolve during reduction, monitoring the plausibility of well-typedness. While expressive and versatile, it is unclear whether such evidence-based semantics are a viable route to realize an efficient implementation of gradual typing.\n  In this work, we explore this question by designing, implementing, and evaluating an evidence-based compiler, called GrEv. We explain how to bridge the gap between the formal semantics and the GrEv compiler implementation, and identify novel monotonic semantics. We empirically evaluate the performance of GrEv on the Grift benchmark suite. The results show that an evidence-based compiler can be competitive with, and even faster than, a coercion-based compiler, exhibiting more stability across configurations on the static-to-dynamic spectrum. In addition to enriching the space of gradual typing compilers, this work opens a direct door to exploring efficient implementations of the many advanced gradual typing disciplines formally derived with AGT in the literature."}
{"id": "2512.23696", "categories": ["cs.GR"], "pdf": "https://arxiv.org/pdf/2512.23696", "abs": "https://arxiv.org/abs/2512.23696", "authors": ["Jamie Portsmouth", "Peter Kutz", "Stephen Hill"], "title": "OpenPBR: Novel Features and Implementation Details", "comment": "Part of Physically Based Shading in Theory and Practice, SIGGRAPH 2025 Course", "summary": "OpenPBR is a physically based, standardized uber-shader developed for interoperable material authoring and rendering across VFX, animation, and design visualization workflows. This document serves as a companion to the official specification, offering deeper insight into the model's development and more detailed implementation guidance, including code examples and mathematical derivations.\n  We begin with a description of the model's formal structure and theoretical foundations - covering slab-based layering, statistical mixing, and microfacet theory - before turning to its physical components. These include metallic, dielectric, subsurface, and glossy-diffuse base substrates, followed by thin-film iridescence, coat, and fuzz layers. A special-case mode for rendering thin-walled objects is also described.\n  Additional sections explore technical topics in greater depth, such as the decoupling of specular reflectivity from transmission, the choice of parameterization for subsurface scattering, and the detailed physics of coat darkening and thin-film interference. We also discuss planned extensions, including hazy specular reflection and retroreflection."}
{"id": "2512.22254", "categories": ["cs.GT", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.22254", "abs": "https://arxiv.org/abs/2512.22254", "authors": ["Sarthak Sarkar", "Supratim Das", "Purushottam Saha", "Diganta Mukherjee", "Tridib Mukherjee"], "title": "Analyzing Skill Element in Online Fantasy Cricket", "comment": "12 pages, 2 figures", "summary": "Online fantasy cricket has emerged as large-scale competitive systems in which participants construct virtual teams and compete based on real-world player performances. This massive growth has been accompanied by important questions about whether outcomes are primarily driven by skill or chance. We develop a statistical framework to assess the role of skill in determining success on these platforms. We construct and analyze a range of deterministic and stochastic team selection strategies, based on recent form, historical statistics, statistical optimization, and multi-criteria decision making. Strategy performance is evaluated based on points, ranks, and payoff under two contest structures Mega and 4x or Nothing. An extensive comparison between different strategies is made to find an optimal set of strategies. To capture adaptive behavior, we further introduce a dynamic tournament model in which agent populations evolve through a softmax reweighting mechanism proportional to positive payoff realizations. We demonstrate our work by running extensive numerical experiments on the IPL 2024 dataset. The results provide quantitative evidence in favor of the skill element present in online fantasy cricket platforms."}
{"id": "2512.23496", "categories": ["cs.PL", "cs.DC"], "pdf": "https://arxiv.org/pdf/2512.23496", "abs": "https://arxiv.org/abs/2512.23496", "authors": ["Anna Gallone", "Simon Bliudze", "Sophie Cerf", "Olga Kouchnarenko"], "title": "Fancy Some Chips for Your TeaStore? Modeling the Control of an Adaptable Discrete System", "comment": "In Proceedings WACA 2025, arXiv:2512.22054. This work was supported by the ANR grant ANR-23-CE25-0004 (ADAPT). O. Kouchnarenko was supported by the EIPHI Graduate School (grant number ANR-17-EURE-0002)", "summary": "When designing new web applications, developers must cope with different kinds of constraints relative to the resources they rely on: software, hardware, network, online micro-services, or any combination of the mentioned entities. Together, these entities form a complex system of communicating interdependent processes, physical or logical. It is very desirable that such system ensures its robustness to provide a good quality of service. In this paper we introduce Chips, a language that aims at facilitating the design of models made of various entwined components. It allows the description of applications in the form of functional blocks. Chips mixes notions  from control theory and general purpose programming languages to generate robust component-based models. This paper presents how to use Chips to systematically design, model and analyse a complex system project, using a variation of the Adaptable TeaStore application as running example."}
{"id": "2512.22552", "categories": ["cs.GT", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.22552", "abs": "https://arxiv.org/abs/2512.22552", "authors": ["Chuang-Chieh Lin", "Chi-Jen Lu", "Po-An Chen", "Chih-Chieh Hung"], "title": "Computing Pure-Strategy Nash Equilibria in a Two-Party Policy Competition: Existence and Algorithmic Approaches", "comment": "A full version of the extended abstract in AAMAS 2026", "summary": "We formulate two-party policy competition as a two-player non-cooperative game, generalizing Lin et al.'s work (2021). Each party selects a real-valued policy vector as its strategy from a compact subset of Euclidean space, and a voter's utility for a policy is given by the inner product with their preference vector. To capture the uncertainty in the competition, we assume that a policy's winning probability increases monotonically with its total utility across all voters, and we formalize this via an affine isotonic function. A player's payoff is defined as the expected utility received by its supporters. In this work, we first test and validate the isotonicity hypothesis through voting simulations. Next, we prove the existence of a pure-strategy Nash equilibrium (PSNE) in both one- and multi-dimensional settings. Although we construct a counterexample demonstrating the game's non-monotonicity, our experiments show that a decentralized gradient-based algorithm typically converges rapidly to an approximate PSNE. Finally, we present a grid-based search algorithm that finds an $ε$-approximate PSNE of the game in time polynomial in the input size and $1/ε$."}
{"id": "2512.23497", "categories": ["cs.PL", "cs.SE"], "pdf": "https://arxiv.org/pdf/2512.23497", "abs": "https://arxiv.org/abs/2512.23497", "authors": ["Giuseppe De Palma", "Saverio Giallorenzo", "Ivan Lanese", "Gianluigi Zavattaro"], "title": "Adaptable TeaStore: A Choreographic Approach", "comment": "In Proceedings WACA 2025, arXiv:2512.22054", "summary": "The Adaptable TeaStore has recently been proposed as a reference model for adaptable microservice architectures. It includes different configurations, as well as scenarios requiring to transition between them. We describe an implementation of the Adaptable TeaStore based on AIOCJ, a choreographic language that allows one to program multiparty systems that can adapt at runtime to different conditions. Following the choreographic tradition, AIOCJ ensures by-construction correctness of communications (e.g., no deadlocks) before, during, and after adaptation. Adaptation is dynamic, and the adaptation scenarios need to be fully specified only at runtime. Using AIOCJ to model the Adaptable TeaStore, we showcase the strengths of the approach and its current limitations, providing suggestions for future directions for refining the paradigm (and the AIOCJ language, in particular), to better align it with real-world Cloud architectures."}
{"id": "2512.22873", "categories": ["cs.GT"], "pdf": "https://arxiv.org/pdf/2512.22873", "abs": "https://arxiv.org/abs/2512.22873", "authors": ["Huanjun Wang", "Qizhi Fang", "Wenjing Liu"], "title": "Facility Location Games for Multi-Location Agents with Satisfaction", "comment": null, "summary": "In this paper, we study mechanism design for single-facility location games where each agent has multiple private locations in [0, 1]. The individual objective is a satisfaction function that measures the discrepancy between the optimal facility location for an agent and the location provided by the mechanism. Based on different distance functions from agents to the facility, we consider two types of individual objectives: the sum-variant satisfaction and the max-variant satisfaction. Our goal is to design mechanisms that locate one facility to maximize the sum (or the minimum) of all agents' satisfactions, while incentivizing agents to truthfully report their locations. In this paper, we mainly focus on desirable and obnoxious facility location games. For desirable facility location games, we propose two group strategy-proof mechanisms with approximation ratios of 2 and 5/4 for maximizing the sum of the sum-variant and max-variant satisfaction, respectively. Moreover, another mechanism achieves an approximation ratio of 2 for simultaneously maximizing the minimum of the sum-variant satisfaction and the minimum of the max-variant satisfaction. For obnoxious facility location games, we establish that two group strategy-proof mechanisms are the best possible, providing an approximation ratio of 2 for maximizing the sum of the sum-variant satisfaction and the sum of the max-variant satisfaction, respectively. Additionally, we devise two 4/3-approximation randomized group strategy-proof mechanisms, and provide two lower bounds of 1.0625 and 1.0448 of randomized strategy-proof mechanisms for maximizing the sum of the sum-variant satisfaction and the sum of the max-variant satisfaction, respectively."}
{"id": "2512.23552", "categories": ["cs.PL", "cs.SE"], "pdf": "https://arxiv.org/pdf/2512.23552", "abs": "https://arxiv.org/abs/2512.23552", "authors": ["Martin Sulzmann"], "title": "Beyond Per-Thread Lock Sets: Multi-Thread Critical Sections and Dynamic Deadlock Prediction", "comment": null, "summary": "Lock sets are commonly used for dynamic analysis of deadlocks. The standard per-thread lock set construction only considers locks acquired in the same thread, but is unaware of locks acquired in another thread. This leads to false positives and false negatives. The underlying issue is that the commonly used notion of a critical section on which the lock set construction relies ignores events from other threads. We give a trace-based characterization of critical sections that drops this restriction. Critical sections are no longer restricted to a single thread and can cover multiple threads. Such forms of critical sections exist, are natural, and correct the standard formulation.\n  We show how to soundly approximate the trace-based characterization via partial order relations. Thus, we obtain an improved lock set construction that can still be efficiently computed and allows us to remove false positives reported by the DIRK deadlock predictor and remove false negatives by extending the SPDOffline deadlock predictor. We integrate various lock set constructions with increased precision in an extension of SPDOffline. Our extensions remain sound (no false positives) but are more complete (fewer false negatives) w.r.t. SPDOffline. For an extensive standard benchmark suite we can also show that the performance is not affected."}
{"id": "2512.23386", "categories": ["cs.GT", "econ.EM", "q-fin.TR"], "pdf": "https://arxiv.org/pdf/2512.23386", "abs": "https://arxiv.org/abs/2512.23386", "authors": ["Sunghun Ko", "Jinsuk Park"], "title": "Impact of Volatility on Time-Based Transaction Ordering Policies", "comment": null, "summary": "We study Arbitrum's Express Lane Auction (ELA), an ahead-of-time second-price auction that grants the winner an exclusive latency advantage for one minute. Building on a single-round model with risk-averse bidders, we propose a hypothesis that the value of priority access is discounted relative to risk-neutral valuation due to the difficulty of forecasting short-horizon volatility and bidders' risk aversion. We test these predictions using ELA bid records matched to high-frequency ETH prices and find that the result is consistent with the model."}
{"id": "2512.23665", "categories": ["cs.PL"], "pdf": "https://arxiv.org/pdf/2512.23665", "abs": "https://arxiv.org/abs/2512.23665", "authors": ["Tim Vieira", "Ryan Cotterell", "Jason Eisner"], "title": "Automating the Analysis of Parsing Algorithms (and other Dynamic Programs)", "comment": "2021 manuscript; accepted by TACL but not revised for publication", "summary": "Much algorithmic research in NLP aims to efficiently manipulate rich formal structures. An algorithm designer typically seeks to provide guarantees about their proposed algorithm -- for example, that its running time or space complexity is upper-bounded as a certain function of its input size. They may also wish to determine the necessary properties of the quantities derived by the algorithm to synthesize efficient data structures and verify type errors. In this paper, we develop a system for helping programmers to perform these types of analyses. We apply our system to a number of NLP algorithms and find that it successfully infers types, dead and redundant code, and parametric runtime and space complexity bounds."}
{"id": "2512.23618", "categories": ["cs.GT", "cs.CE"], "pdf": "https://arxiv.org/pdf/2512.23618", "abs": "https://arxiv.org/abs/2512.23618", "authors": ["Jake Hartnell", "Eugenio Battaglia"], "title": "Verifiable Off-Chain Governance", "comment": null, "summary": "Current DAO governance praxis limits organizational expressivity and reduces complex organizational decisions to token-weighted voting due to on-chain computational limits. This paper proposes verifiable off-chain computation (leveraging Verifiable Services, TEEs, and ZK proofs) as a framework to transcend these constraints while maintaining cryptoeconomic security. This paper explores three novel governance mechanisms: (1) attestation-based systems that compute multi-dimensional stakeholder legitimacy, (2) collective intelligence through verifiable preference processing, and (3) autonomous policy execution via Policy-as-Code. The framework provides architectural specifications, security models, and implementation considerations for DAOs seeking higher-resolution expressivity and increased operational efficiency, with validation from pioneering implementations demonstrating practical viability."}
