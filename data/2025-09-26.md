<div id=toc></div>

# Table of Contents

- [cs.GR](#cs.GR) [Total: 9]
- [cs.PL](#cs.PL) [Total: 2]
- [cs.GT](#cs.GT) [Total: 2]


<div id='cs.GR'></div>

# cs.GR [[Back]](#toc)

### [1] [SeHDR: Single-Exposure HDR Novel View Synthesis via 3D Gaussian Bracketing](https://arxiv.org/abs/2509.20400)
*Yiyu Li,Haoyuan Wang,Ke Xu,Gerhard Petrus Hancke,Rynson W. H. Lau*

Main category: cs.GR

TL;DR: SeHDR 是一种新颖的高动态范围3D高斯泼溅方法，可以从单曝光的多视角低动态范围图像生成高动态范围新视角，避免了传统方法需要多曝光图像的繁琐捕获和潜在错误。


<details>
  <summary>Details</summary>
Motivation: 传统的多视角HDR合成方法需要从不同曝光的多视角低动态范围图像中学习，这种捕获过程繁琐且容易因物体运动模糊或标定不准确而失败。本文旨在解决这一问题，提出了一种从单曝光的低动态范围图像中学习HDR场景表示的方法。

Method: SeHDR首先从单曝光的低动态范围图像中学习基础3D高斯，然后通过估计具有相同几何形状但不同线性颜色的3D高斯（基于曝光操作），最后利用可微分神经曝光融合（NeEF）将这些高斯整合为HDR高斯用于新视角渲染。

Result: 大量实验表明，SeHDR在性能上优于现有方法以及精心设计的基线模型。

Conclusion: SeHDR成功地解决了从单曝光的多视角低动态范围图像生成高动态范围新视角的问题，避免了传统方法的局限性，并在实验中表现出优越的性能。

Abstract: This paper presents SeHDR, a novel high dynamic range 3D Gaussian Splatting
(HDR-3DGS) approach for generating HDR novel views given multi-view LDR images.
Unlike existing methods that typically require the multi-view LDR input images
to be captured from different exposures, which are tedious to capture and more
likely to suffer from errors (e.g., object motion blurs and
calibration/alignment inaccuracies), our approach learns the HDR scene
representation from multi-view LDR images of a single exposure. Our key insight
to this ill-posed problem is that by first estimating Bracketed 3D Gaussians
(i.e., with different exposures) from single-exposure multi-view LDR images, we
may then be able to merge these bracketed 3D Gaussians into an HDR scene
representation. Specifically, SeHDR first learns base 3D Gaussians from
single-exposure LDR inputs, where the spherical harmonics parameterize colors
in a linear color space. We then estimate multiple 3D Gaussians with identical
geometry but varying linear colors conditioned on exposure manipulations.
Finally, we propose the Differentiable Neural Exposure Fusion (NeEF) to
integrate the base and estimated 3D Gaussians into HDR Gaussians for novel view
rendering. Extensive experiments demonstrate that SeHDR outperforms existing
methods as well as carefully designed baselines.

</details>


### [2] [SGAligner++: Cross-Modal Language-Aided 3D Scene Graph Alignment](https://arxiv.org/abs/2509.20401)
*Binod Singh,Sayan Deb Sarkar,Iro Armeni*

Main category: cs.GR

TL;DR: SGAligner++是一种跨模态、语言辅助的框架，用于3D场景图对齐，通过联合嵌入空间解决不完全或噪声输入的对齐问题，性能优于现有方法40%。


<details>
  <summary>Details</summary>
Motivation: 现有3D场景图对齐方法依赖单模态点云数据，难以处理不完全或噪声输入。

Method: 采用轻量级单模态编码器和基于注意力的融合，学习统一的联合嵌入空间以实现对齐。

Result: 在真实世界数据集上评估，SGAligner++在噪声条件下表现优于现有方法40%。

Conclusion: SGAligner++提升了跨模态场景理解的准确性和泛化能力，适用于视觉定位、3D重建等任务。

Abstract: Aligning 3D scene graphs is a crucial initial step for several applications
in robot navigation and embodied perception. Current methods in 3D scene graph
alignment often rely on single-modality point cloud data and struggle with
incomplete or noisy input. We introduce SGAligner++, a cross-modal,
language-aided framework for 3D scene graph alignment. Our method addresses the
challenge of aligning partially overlapping scene observations across
heterogeneous modalities by learning a unified joint embedding space, enabling
accurate alignment even under low-overlap conditions and sensor noise. By
employing lightweight unimodal encoders and attention-based fusion, SGAligner++
enhances scene understanding for tasks such as visual localization, 3D
reconstruction, and navigation, while ensuring scalability and minimal
computational overhead. Extensive evaluations on real-world datasets
demonstrate that SGAligner++ outperforms state-of-the-art methods by up to 40%
on noisy real-world reconstructions, while enabling cross-modal generalization.

</details>


### [3] [SceneWeaver: All-in-One 3D Scene Synthesis with an Extensible and Self-Reflective Agent](https://arxiv.org/abs/2509.20414)
*Yandan Yang,Baoxiong Jia,Shujie Zhang,Siyuan Huang*

Main category: cs.GR

TL;DR: SceneWeaver是一个基于语言模型的反射性代理框架，通过工具化迭代优化统一多样化的室内场景合成方法，提升了物理合理性、视觉真实性和语义一致性。


<details>
  <summary>Details</summary>
Motivation: 随着具身AI的兴起，研究需要既视觉真实又物理合理且功能多样的3D环境。现有方法虽提升了视觉真实感，但在固定的场景类别、物体细节和物理一致性方面存在不足，且难以满足复杂用户指令。

Method: SceneWeaver采用基于语言模型的规划器选择多样化的场景生成工具，包括数据驱动的生成模型、视觉和LLM方法，并通过自我评估进行迭代优化。

Result: 实验表明，SceneWeaver在物理、视觉和语义指标上均优于现有方法，并能有效泛化到复杂场景和多样指令中。

Conclusion: SceneWeaver为实现通用3D环境生成迈出了重要一步，展示了通过工具化和迭代优化提升场景合成的潜力。

Abstract: Indoor scene synthesis has become increasingly important with the rise of
Embodied AI, which requires 3D environments that are not only visually
realistic but also physically plausible and functionally diverse. While recent
approaches have advanced visual fidelity, they often remain constrained to
fixed scene categories, lack sufficient object-level detail and physical
consistency, and struggle to align with complex user instructions. In this
work, we present SceneWeaver, a reflective agentic framework that unifies
diverse scene synthesis paradigms through tool-based iterative refinement. At
its core, SceneWeaver employs a language model-based planner to select from a
suite of extensible scene generation tools, ranging from data-driven generative
models to visual- and LLM-based methods, guided by self-evaluation of physical
plausibility, visual realism, and semantic alignment with user input. This
closed-loop reason-act-reflect design enables the agent to identify semantic
inconsistencies, invoke targeted tools, and update the environment over
successive iterations. Extensive experiments on both common and open-vocabulary
room types demonstrate that SceneWeaver not only outperforms prior methods on
physical, visual, and semantic metrics, but also generalizes effectively to
complex scenes with diverse instructions, marking a step toward general-purpose
3D environment generation. Project website: https://scene-weaver.github.io/.

</details>


### [4] [ArtUV: Artist-style UV Unwrapping](https://arxiv.org/abs/2509.20710)
*Yuguang Chen,Xinhai Liu,Yang Li,Victor Cheung,Zhuo Chen,Dongyu Zhang,Chunchao Guo*

Main category: cs.GR

TL;DR: ArtUV是一种全自动的端到端方法，用于生成艺术家风格的UV展开，解决了现有方法在时间消耗、碎片化和缺乏语义性等方面的问题。


<details>
  <summary>Details</summary>
Motivation: 现有UV展开方法存在耗时、碎片化、缺乏语义性和不规则UV岛等问题，限制了其实际应用。艺术家风格的UV图需要满足更高标准，如干净的边界、高效的空间利用和语义一致性。

Method: ArtUV模拟专业UV映射过程，分为两个阶段：表面接缝预测和艺术家风格UV参数化。通过SeamGPT生成语义上有意义的切割接缝，然后使用基于优化的方法和自编码器生成艺术家风格的UV图。

Result: ArtUV在多个基准测试中表现优异，确保了语义一致性和拓扑结构的保留，为二维编辑提供了高质量的UV图。

Conclusion: ArtUV作为一种通用解决方案，既可作为专业渲染工具的插件，也可作为独立系统，用于快速生成高质量的UV图。

Abstract: UV unwrapping is an essential task in computer graphics, enabling various
visual editing operations in rendering pipelines. However, existing UV
unwrapping methods struggle with time-consuming, fragmentation, lack of
semanticity, and irregular UV islands, limiting their practical use. An
artist-style UV map must not only satisfy fundamental criteria, such as
overlap-free mapping and minimal distortion, but also uphold higher-level
standards, including clean boundaries, efficient space utilization, and
semantic coherence. We introduce ArtUV, a fully automated, end-to-end method
for generating artist-style UV unwrapping. We simulates the professional UV
mapping process by dividing it into two stages: surface seam prediction and
artist-style UV parameterization. In the seam prediction stage, SeamGPT is used
to generate semantically meaningful cutting seams. Then, in the
parameterization stage, a rough UV obtained from an optimization-based method,
along with the mesh, is fed into an Auto-Encoder, which refines it into an
artist-style UV map. Our method ensures semantic consistency and preserves
topological structure, making the UV map ready for 2D editing. We evaluate
ArtUV across multiple benchmarks and show that it serves as a versatile
solution, functioning seamlessly as either a plug-in for professional rendering
tools or as a standalone system for rapid, high-quality UV generation.

</details>


### [5] [SeamCrafte: Enhancing Mesh Seam Generation for Artist UV Unwrapping via Reinforcement Learning](https://arxiv.org/abs/2509.20725)
*Duoteng Xu,Yuguang Chen,Jing Li,Xinhai Liu,Xueqi Ma,Zhuo Chen,Dongyu Zhang,Chunchao Guo*

Main category: cs.GR

TL;DR: SeamCrafter是一种基于点云输入的自回归GPT风格接缝生成器，通过双分支点云编码器捕获拓扑和几何信息，并使用直接偏好优化(DPO)进一步提升接缝质量，显著降低了UV变形和碎片化。


<details>
  <summary>Details</summary>
Motivation: 现有的接缝生成方法往往在低变形和少碎片化之间难以平衡，导致纹理合成和艺术家工作流程受阻，因此需要一种更优的接缝生成方法。

Method: 提出SeamCrafter模型，采用双分支点云编码器分离并捕获拓扑和几何信息，并通过直接偏好优化(DPO)在偏好数据集上进行微调。

Result: 实验表明，SeamCrafter生成的接缝在变形和碎片化方面显著优于现有方法，同时保持了拓扑一致性和视觉保真度。

Conclusion: SeamCrafter通过创新的建模和优化方法，解决了接缝生成中的关键问题，为纹理映射和UV参数化提供了高质量解决方案。

Abstract: Mesh seams play a pivotal role in partitioning 3D surfaces for UV
parametrization and texture mapping. Poorly placed seams often result in severe
UV distortion or excessive fragmentation, thereby hindering texture synthesis
and disrupting artist workflows. Existing methods frequently trade one failure
mode for another-producing either high distortion or many scattered islands. To
address this, we introduce SeamCrafter, an autoregressive GPT-style seam
generator conditioned on point cloud inputs. SeamCrafter employs a dual-branch
point-cloud encoder that disentangles and captures complementary topological
and geometric cues during pretraining. To further enhance seam quality, we
fine-tune the model using Direct Preference Optimization (DPO) on a preference
dataset derived from a novel seam-evaluation framework. This framework assesses
seams primarily by UV distortion and fragmentation, and provides pairwise
preference labels to guide optimization. Extensive experiments demonstrate that
SeamCrafter produces seams with substantially lower distortion and
fragmentation than prior approaches, while preserving topological consistency
and visual fidelity.

</details>


### [6] [ARMesh: Autoregressive Mesh Generation via Next-Level-of-Detail Prediction](https://arxiv.org/abs/2509.20824)
*Jiabao Lei,Kewei Shi,Zhihao Liang,Kui Jia*

Main category: cs.GR

TL;DR: 提出了一种基于自回归模型（AR）的渐进式3D网格生成方法，通过从粗到细的方式逐步生成网格，改进了传统按字典序逐面生成的方法，使其更符合人类感知，并实现网格的灵活编辑和应用。


<details>
  <summary>Details</summary>
Motivation: 传统的自回归模型按字典序逐面生成3D网格，无法有效捕捉与人类感知一致的几何结构。受2D渐进式生成模型的启发，提出了一种更自然的渐进式网格生成方法。

Method: 将网格简化的自然过程逆向推广到单纯复形，开发了一种基于Transformer的自回归模型，从单一点开始逐步添加几何细节，支持灵活的拓扑结构。

Result: 实验表明，该方法不仅可以通过提前终止控制生成质量和时间，还能应用于网格的细化和编辑。

Conclusion: 渐进式网格生成方法为3D网格生成提供了更直观的控制和应用灵活性。

Abstract: Directly generating 3D meshes, the default representation for 3D shapes in
the graphics industry, using auto-regressive (AR) models has become popular
these days, thanks to their sharpness, compactness in the generated results,
and ability to represent various types of surfaces. However, AR mesh generative
models typically construct meshes face by face in lexicographic order, which
does not effectively capture the underlying geometry in a manner consistent
with human perception. Inspired by 2D models that progressively refine images,
such as the prevailing next-scale prediction AR models, we propose generating
meshes auto-regressively in a progressive coarse-to-fine manner. Specifically,
we view mesh simplification algorithms, which gradually merge mesh faces to
build simpler meshes, as a natural fine-to-coarse process. Therefore, we
generalize meshes to simplicial complexes and develop a transformer-based AR
model to approximate the reverse process of simplification in the order of
level of detail, constructing meshes initially from a single point and
gradually adding geometric details through local remeshing, where the topology
is not predefined and is alterable. Our experiments show that this novel
progressive mesh generation approach not only provides intuitive control over
generation quality and time consumption by early stopping the auto-regressive
process but also enables applications such as mesh refinement and editing.

</details>


### [7] [ArchGPT: Understanding the World's Architectures with Large Multimodal Models](https://arxiv.org/abs/2509.20858)
*Yuze Wang,Luo Yang,Junyi Wang,Yue Qi*

Main category: cs.GR

TL;DR: ArchGPT是一个多模态建筑视觉问答模型，通过可扩展的数据构建管道生成高质量的建筑特定VQA注释，解决了现有VR/MR/AR系统的可扩展性问题。


<details>
  <summary>Details</summary>
Motivation: 建筑体现了美学、文化和历史价值，但现有的VR/MR/AR系统通常是针对特定案例开发的，缺乏跨多样化建筑环境的可扩展性。

Method: 提出了一种多阶段的管道，包括图像筛选、文本验证和知识蒸馏，生成Arch-300K数据集，并对开源多模态模型ShareGPT4V-7B进行监督微调。

Result: ArchGPT通过Arch-300K数据集实现了高质量的视觉问答能力，并在建筑图像分析和对话生成中表现出色。

Conclusion: ArchGPT为建筑领域的沉浸式探索和教育提供了可扩展的解决方案，并展示了多模态模型在专业领域的潜力。

Abstract: Architecture embodies aesthetic, cultural, and historical values, standing as
a tangible testament to human civilization. Researchers have long leveraged
virtual reality (VR), mixed reality (MR), and augmented reality (AR) to enable
immersive exploration and interpretation of architecture, enhancing
accessibility, public understanding, and creative workflows around architecture
in education, heritage preservation, and professional design practice. However,
existing VR/MR/AR systems are often developed case-by-case, relying on
hard-coded annotations and task-specific interactions that do not scale across
diverse built environments. In this work, we present ArchGPT, a multimodal
architectural visual question answering (VQA) model, together with a scalable
data-construction pipeline for curating high-quality, architecture-specific VQA
annotations. This pipeline yields Arch-300K, a domain-specialized dataset of
approximately 315,000 image-question-answer triplets. Arch-300K is built via a
multi-stage process: first, we curate architectural scenes from Wikimedia
Commons and filter unconstrained tourist photo collections using a novel
coarse-to-fine strategy that integrates 3D reconstruction and semantic
segmentation to select occlusion-free, structurally consistent architectural
images. To mitigate noise and inconsistency in raw textual metadata, we propose
an LLM-guided text verification and knowledge-distillation pipeline to generate
reliable, architecture-specific question-answer pairs. Using these curated
images and refined metadata, we further synthesize formal analysis
annotations-including detailed descriptions and aspect-guided conversations-to
provide richer semantic variety while remaining faithful to the data. We
perform supervised fine-tuning of an open-source multimodal backbone
,ShareGPT4V-7B, on Arch-300K, yielding ArchGPT.

</details>


### [8] [Marching Neurons: Accurate Surface Extraction for Neural Implicit Shapes](https://arxiv.org/abs/2509.21007)
*Christian Stippel,Felix Mujkanovic,Thomas Leimkühler,Pedro Hermosilla*

Main category: cs.GR

TL;DR: 介绍了一种从神经隐式函数中精确提取表面的新方法，通过并行操作和深度优先遍历策略，避免了传统方法因空间离散化带来的误差。


<details>
  <summary>Details</summary>
Motivation: 在3D视觉计算中，准确的表面几何表示至关重要。传统的显式和隐式表示各有优势，但现有方法（如Marching Cubes算法）因空间分解和采样导致精度受限。

Method: 采用并行操作和深度优先遍历策略，直接利用神经元的域划分特性来跟踪隐式函数中的表面，而无需依赖空间离散化。

Result: 生成的网格准确捕捉了网络中的完整几何信息，在多样化形状和网络架构中实现了前所未有的精度，同时保持了高效的速度。

Conclusion: 该方法为神经隐式函数的表面提取提供了一种高效且精确的解决方案，显著优于传统方法。

Abstract: Accurate surface geometry representation is crucial in 3D visual computing.
Explicit representations, such as polygonal meshes, and implicit
representations, like signed distance functions, each have distinct advantages,
making efficient conversions between them increasingly important. Conventional
surface extraction methods for implicit representations, such as the widely
used Marching Cubes algorithm, rely on spatial decomposition and sampling,
leading to inaccuracies due to fixed and limited resolution. We introduce a
novel approach for analytically extracting surfaces from neural implicit
functions. Our method operates natively in parallel and can navigate large
neural architectures. By leveraging the fact that each neuron partitions the
domain, we develop a depth-first traversal strategy to efficiently track the
encoded surface. The resulting meshes faithfully capture the full geometric
information from the network without ad-hoc spatial discretization, achieving
unprecedented accuracy across diverse shapes and network architectures while
maintaining competitive speed.

</details>


### [9] [CHARM: Control-point-based 3D Anime Hairstyle Auto-Regressive Modeling](https://arxiv.org/abs/2509.21114)
*Yuze He,Yanning Zhou,Wang Zhao,Jingwen Ye,Yushi Bai,Kaiwen Xiao,Yong-Jin Liu,Zhongqian Sun,Wei Yang*

Main category: cs.GR

TL;DR: CHARM是一种新颖的参数化表示和生成框架，用于动漫发型建模，通过紧凑的控制点参数化和自回归生成框架实现高效设计和高质量生成。


<details>
  <summary>Details</summary>
Motivation: 传统发型建模方法主要关注真实头发，而动漫发型具有高度风格化、分段结构的几何特征，现有技术难以应对；现有方法依赖密集网格建模或手工样条曲线，效率低下且难以扩展学习。

Method: CHARM采用紧凑且可逆的控制点参数化表示，每个发片由一系列控制点表示，每个点仅用五个几何参数编码；基于此表示，提出自回归生成框架，将动漫发型视为序列化的“发语言”，捕捉局部几何和全局发型拓扑。

Result: 构建了AnimeHair数据集（包含37K高质量动漫发型），实验表明CHARM在重建精度和生成质量上均达到最优性能。

Conclusion: CHARM为动漫发型建模提供了一种表达力强且可扩展的解决方案，支持艺术设计友好和基于学习的生成。

Abstract: We present CHARM, a novel parametric representation and generative framework
for anime hairstyle modeling. While traditional hair modeling methods focus on
realistic hair using strand-based or volumetric representations, anime
hairstyle exhibits highly stylized, piecewise-structured geometry that
challenges existing techniques. Existing works often rely on dense mesh
modeling or hand-crafted spline curves, making them inefficient for editing and
unsuitable for scalable learning. CHARM introduces a compact, invertible
control-point-based parameterization, where a sequence of control points
represents each hair card, and each point is encoded with only five geometric
parameters. This efficient and accurate representation supports both
artist-friendly design and learning-based generation. Built upon this
representation, CHARM introduces an autoregressive generative framework that
effectively generates anime hairstyles from input images or point clouds. By
interpreting anime hairstyles as a sequential "hair language", our
autoregressive transformer captures both local geometry and global hairstyle
topology, resulting in high-fidelity anime hairstyle creation. To facilitate
both training and evaluation of anime hairstyle generation, we construct
AnimeHair, a large-scale dataset of 37K high-quality anime hairstyles with
separated hair cards and processed mesh data. Extensive experiments demonstrate
state-of-the-art performance of CHARM in both reconstruction accuracy and
generation quality, offering an expressive and scalable solution for anime
hairstyle modeling. Project page: https://hyzcluster.github.io/charm/

</details>


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [10] [Dual-Language General-Purpose Self-Hosted Visual Language and new Textual Programming Language for Applications](https://arxiv.org/abs/2509.20426)
*Mahmoud Samir Fayed*

Main category: cs.PL

TL;DR: 本研究设计开发了PWCT2，一种阿拉伯语/英语双语、通用、自宿主视觉编程语言，通过Ring文本语言实现高效开发和代码转换，显著提升了代码生成速度和存储效率。


<details>
  <summary>Details</summary>
Motivation: 由于现有的通用视觉编程语言（如PWCT）需依赖文本编程语言进行改进，限制了其发展。因此，本研究旨在开发一种自宿主的通用视觉编程语言PWCT2，以解决这一问题，并提高开发效率和抽象层次。

Method: 研究先设计了一种轻量级动态类型文本编程语言Ring，支持语法定制和领域特定语言创建。随后利用PWCT开发Ring，通过18,945个组件生成24,743行C代码，实现了Ring编译器和虚拟机。基于Ring，进一步开发了PWCT2，实现了从Ring代码到视觉代码的转换。

Result: PWCT2的代码生成速度提升了约36倍，视觉源文件存储需求减少了20倍。软件在Steam平台上广受欢迎，1,772名用户启动使用，总使用时长超过17,000小时。

Conclusion: PWCT2成功实现了自宿主视觉编程语言的目标，显著提升了开发效率和性能，未来研究和开发将继续基于其优势和用户反馈进行。

Abstract: Most visual programming languages (VPLs) are domain-specific, with few
general-purpose VPLs like Programming Without Coding Technology (PWCT). These
general-purpose VPLs are developed using textual programming languages and
improving them requires textual programming. In this thesis, we designed and
developed PWCT2, a dual-language (Arabic/English), general-purpose,
self-hosting visual programming language. Before doing so, we specifically
designed a textual programming language called Ring for its development. Ring
is a dynamically typed language with a lightweight implementation, offering
syntax customization features. It permits the creation of domain-specific
languages through new features that extend object-oriented programming,
allowing for specialized languages resembling Cascading Style Sheets (CSS) or
Supernova language. The Ring Compiler and Virtual Machine are designed using
the PWCT visual programming language where the visual implementation is
composed of 18,945 components that generate 24,743 lines of C code, which
increases the abstraction level and hides unnecessary details. Using PWCT to
develop Ring allowed us to realize several issues in PWCT, which led to the
development of the PWCT2 visual programming language using the Ring textual
programming language. PWCT2 provides approximately 36 times faster code
generation and requires 20 times less storage for visual source files. It also
allows for the conversion of Ring code into visual code, enabling the creation
of a self-hosting VPL that can be developed using itself. PWCT2 consists of
approximately 92,000 lines of Ring code and comes with 394 visual components.
PWCT2 is distributed to many users through the Steam platform and has received
positive feedback, On Steam, 1772 users have launched the software, and the
total recorded usage time exceeds 17,000 hours, encouraging further research
and development.

</details>


### [11] [Efficient Symbolic Computation vis Hash Consing](https://arxiv.org/abs/2509.20534)
*Bowen Zhu,Aayush Sabharwal,Songchen Tan,Yingbo Ma,Alan Edelman,Christopher Rackauckas*

Main category: cs.PL

TL;DR: 该论文通过在JuliaSymbolics中集成哈希共享技术，显著提升了符号计算的性能和内存效率。


<details>
  <summary>Details</summary>
Motivation: 符号计算系统因存储重复子表达式而导致内存低效，即表达式膨胀问题，影响了传统计算机代数和AI驱动数学工具的性能。

Method: 论文采用全局弱引用哈希表技术，对表达进行规范化并消除重复，从而集成到JuliaSymbolics中，减少内存占用并加速关键操作。

Result: 实验表明，符号计算速度提升最多3.2倍，内存占用减少最多2倍，代码生成速度提升5倍，函数编译速度快10倍，数值计算性能提升高达100倍。

Conclusion: 哈希共享技术对提升符号计算的扩展性至关重要，未来可结合e-graphs进一步优化AI驱动管道中的表达式共享。

Abstract: Symbolic computation systems suffer from memory inefficiencies due to
redundant storage of structurally identical subexpressions, commonly known as
expression swell, which degrades performance in both classical computer algebra
and emerging AI-driven mathematical reasoning tools. In this paper, we present
the first integration of hash consing into JuliaSymbolics, a high-performance
symbolic toolkit in Julia, by employing a global weak-reference hash table that
canonicalizes expressions and eliminates duplication. This approach reduces
memory consumption and accelerates key operations such as differentiation,
simplification, and code generation, while seamlessly integrating with Julia's
metaprogramming and just-in-time compilation infrastructure. Benchmark
evaluations across different computational domains reveal substantial
improvements: symbolic computations are accelerated by up to 3.2 times, memory
usage is reduced by up to 2 times, code generation is up to 5 times faster,
function compilation up to 10 times faster, and numerical evaluation up to 100
times faster for larger models. While certain workloads with fewer duplicate
unknown-variable expressions show more modest gains or even slight overhead in
initial computation stages, downstream processing consistently benefits
significantly. These findings underscore the importance of hash consing in
scaling symbolic computation and pave the way for future work integrating hash
consing with e-graphs for enhanced equivalence-aware expression sharing in
AI-driven pipelines.

</details>


<div id='cs.GT'></div>

# cs.GT [[Back]](#toc)

### [12] [Efficient Kernelized Learning in Polyhedral Games Beyond Full-Information: From Colonel Blotto to Congestion Games](https://arxiv.org/abs/2509.20919)
*Andreas Kontogiannis,Vasilis Pollatos,Gabriele Farina,Panayotis Mertikopoulos,Ioannis Panageas*

Main category: cs.GT

TL;DR: 该论文研究了在多面体游戏中高效学习粗相关均衡（CCE）的问题，提出了基于核化框架的学习算法，显著提升了在部分信息设置下的运行时效率。


<details>
  <summary>Details</summary>
Motivation: 多面体游戏（如Colonel Blotto和拥塞游戏）中，由于动作集规模庞大，学习CCE的计算效率成为主要挑战。现有方法在部分信息设置下运行时复杂度较高，难以实用。

Method: 通过构建基于核化（kernelization）的框架，开发了计算高效的基于收益的学习算法，并应用于Colonel Blotto、图形拟阵和网络拥塞游戏等典型多面体游戏中。

Result: 新算法显著提升了学习CCE的运行时复杂度，优于已有方法，特别是在部分信息设置下表现更优。

Conclusion: 论文提出的核化框架为多面体游戏中学习CCE提供了高效解决方案，显著改进了现有方法的效率，拓展了其在实际应用中的可行性。

Abstract: We examine the problem of efficiently learning coarse correlated equilibria
(CCE) in polyhedral games, that is, normal-form games with an exponentially
large number of actions per player and an underlying combinatorial structure.
Prominent examples of such games are the classical Colonel Blotto and
congestion games. To achieve computational efficiency, the learning algorithms
must exhibit regret and per-iteration complexity that scale polylogarithmically
in the size of the players' action sets. This challenge has recently been
addressed in the full-information setting, primarily through the use of
kernelization. However, in the case of the realistic, but mathematically
challenging, partial-information setting, existing approaches result in
suboptimal and impractical runtime complexity to learn CCE. We tackle this
limitation by building a framework based on the kernelization paradigm. We
apply this framework to prominent examples of polyhedral games -- namely the
Colonel Blotto, graphic matroid and network congestion games -- and provide
computationally efficient payoff-based learning algorithms, which significantly
improve upon prior works in terms of the runtime for learning CCE in these
settings.

</details>


### [13] [A Category Theoretic Approach to Approximate Game Theory](https://arxiv.org/abs/2509.20932)
*Neil Ghani*

Main category: cs.GT

TL;DR: 该论文提出了一种基于范畴论的全新方法来研究近似博弈论，通过选择函数和开放博弈模型探讨了近似均衡的代数性质及其组合结构。


<details>
  <summary>Details</summary>
Motivation: 博弈论研究多智能体系统中的决策问题，但在实际应用中，由于输入的不确定性或计算复杂性，精确解可能难以或无法获得。因此，近似博弈论的重要性和实用性促使了该研究。

Method: 论文首先引入了选择函数，建立了近似均衡的简单而鲁棒的模型，并研究了其代数性质和组合结构。随后，该方法被成功应用于更高级的开放博弈模型。

Result: 研究展示了对选择函数和开放博弈模型中近似均衡的代数性质及其组合结构的深入理解，为近似博弈论提供了新的理论框架。

Conclusion: 通过范畴论的方法，论文成功地为近似博弈论提供了新的理论基础，并为实际应用中复杂问题的近似决策提供了理论支持。

Abstract: This paper uses category theory to develop an entirely new approach to
approximate game theory. Game theory is the study of how different agents
within a multi-agent system take decisions. At its core, game theory asks what
an optimal decision is in a given scenario. Thus approximate game theory asks
what is an approximately optimal decision in a given scenario. This is
important in practice as -- just like in much of computing -- exact answers
maybe too difficult to compute or even impossible to compute given inherent
uncertainty in input.
  We consider first "Selection Functions" which are functions and develop a
simple yet robust model of approximate equilibria. We develop the algebraic
properties of approximation wrt selection functions and also relate
approximation to the compositional structure of selection functions. We then
repeat this process successfully for Open Games -- a more advanced model of
game theory.

</details>
