<div id=toc></div>

# Table of Contents

- [cs.GR](#cs.GR) [Total: 7]
- [cs.PL](#cs.PL) [Total: 8]
- [cs.GT](#cs.GT) [Total: 6]


<div id='cs.GR'></div>

# cs.GR [[Back]](#toc)

### [1] [Real-Time Scene Reconstruction using Light Field Probes](https://arxiv.org/abs/2507.14624)
*Yaru Liu,Derek Nowrouzezahri,Morgan Mcguire*

Main category: cs.GR

TL;DR: 该论文提出了一种无需显式使用场景几何的新视角合成方法，通过稀疏图像重建多尺度隐式表示，显著降低了计算成本。


<details>
  <summary>Details</summary>
Motivation: 解决现有神经渲染方法在处理大规模场景时效率低下的问题，以及传统基于几何重建方法的高昂维护成本。

Method: 利用探针数据结构重建场景，通过稀疏图像生成多尺度隐式几何表示，避免了显式依赖场景几何。

Result: 该方法能够高效重建复杂场景，渲染成本与场景复杂度无关，并适用于VR和AR应用。

Conclusion: 通过隐式表示和探针数据结构，论文验证了一种高效且低成本的大规模场景重建方法，适合未来虚拟和增强现实应用。

Abstract: Reconstructing photo-realistic large-scale scenes from images, for example at
city scale, is a long-standing problem in computer graphics. Neural rendering
is an emerging technique that enables photo-realistic image synthesis from
previously unobserved viewpoints; however, state-of-the-art neural rendering
methods have difficulty efficiently rendering a high complex large-scale scene
because these methods typically trade scene size, fidelity, and rendering speed
for quality. The other stream of techniques utilizes scene geometries for
reconstruction. But the cost of building and maintaining a large set of
geometry data increases as scene size grows. Our work explores novel view
synthesis methods that efficiently reconstruct complex scenes without explicit
use of scene geometries. Specifically, given sparse images of the scene
(captured from the real world), we reconstruct intermediate, multi-scale,
implicit representations of scene geometries. In this way, our method avoids
explicitly relying on scene geometry, significantly reducing the computational
cost of maintaining large 3D data. Unlike current methods, we reconstruct the
scene using a probe data structure. Probe data hold highly accurate depth
information of dense data points, enabling the reconstruction of highly complex
scenes. By reconstructing the scene using probe data, the rendering cost is
independent of the complexity of the scene. As such, our approach combines
geometry reconstruction and novel view synthesis. Moreover, when rendering
large-scale scenes, compressing and streaming probe data is more efficient than
using explicit scene geometry. Therefore, our neural representation approach
can potentially be applied to virtual reality (VR) and augmented reality (AR)
applications.

</details>


### [2] [Towards Geometric and Textural Consistency 3D Scene Generation via Single Image-guided Model Generation and Layout Optimization](https://arxiv.org/abs/2507.14841)
*Xiang Tang,Ruotong Li,Xiaopeng Fan*

Main category: cs.GR

TL;DR: 提出了一种新颖的三阶段框架，通过单RGB图像引导生成3D场景，提升了物体生成质量和场景一致性。


<details>
  <summary>Details</summary>
Motivation: 尽管3D生成技术已取得进展，但从单张RGB图像生成3D场景仍是一大挑战，尤其是在多物体场景中保持生成质量和场景一致性。

Method: 采用三阶段框架：图像实例分割与修复、伪立体视角构建及相机参数估计、模型参数化与布局优化。

Result: 实验表明，该方法在几何准确性和纹理保真度上优于现有技术，且在场景布局合成方面具有显著优势。

Conclusion: 该方法有效解决了单图像生成3D场景中的质量与一致性难题，为未来研究提供了新方向。

Abstract: In recent years, 3D generation has made great strides in both academia and
industry. However, generating 3D scenes from a single RGB image remains a
significant challenge, as current approaches often struggle to ensure both
object generation quality and scene coherence in multi-object scenarios. To
overcome these limitations, we propose a novel three-stage framework for 3D
scene generation with explicit geometric representations and high-quality
textural details via single image-guided model generation and spatial layout
optimization. Our method begins with an image instance segmentation and
inpainting phase, which recovers missing details of occluded objects in the
input images, thereby achieving complete generation of foreground 3D assets.
Subsequently, our approach captures the spatial geometry of reference image by
constructing pseudo-stereo viewpoint for camera parameter estimation and scene
depth inference, while employing a model selection strategy to ensure optimal
alignment between the 3D assets generated in the previous step and the input.
Finally, through model parameterization and minimization of the Chamfer
distance between point clouds in 3D and 2D space, our approach optimizes layout
parameters to produce an explicit 3D scene representation that maintains
precise alignment with input guidance image. Extensive experiments on
multi-object scene image sets have demonstrated that our approach not only
outperforms state-of-the-art methods in terms of geometric accuracy and texture
fidelity of individual generated 3D models, but also has significant advantages
in scene layout synthesis.

</details>


### [3] [Time Series Information Visualization -- A Review of Approaches and Tools](https://arxiv.org/abs/2507.14920)
*Evandro S. Ortigossa,Fábio F. Dias,Diego C. Nascimento,Luis Gustavo Nonato*

Main category: cs.GR

TL;DR: 该论文综述了针对时间序列数据的信息可视化技术，探讨了如何通过视觉化方法提升对复杂时间序列数据的理解，并提出了未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 时间序列数据在各领域广泛存在，且通常包含大量多特征样本。分析这类数据需要复杂的工具和方法，而信息可视化可以为数据科学家提供直观的理解方式。开发能够整合多种分析工具的可视化系统，是将原始数据转化为有用知识的关键。

Method: 论文通过综述现有的时间序列可视化技术和设计方法，探讨了如何将这些技术整合到丰富的可视化系统中，以支持用户通过视觉分析进行知识发现。

Result: 总结了适用于多特征时间序列的可视化技术和设计指南，同时指出了当前可视化技术面临的挑战和未解决的问题。

Conclusion: 信息可视化是理解时间序列数据的有效工具，但仍需进一步研究以应对其在分析复杂时间序列数据时的局限性。

Abstract: Time series data are prevalent across various domains and often encompass
large datasets containing multiple time-dependent features in each sample.
Exploring time-varying data is critical for data science practitioners aiming
to understand dynamic behaviors and discover periodic patterns and trends.
However, the analysis of such data often requires sophisticated procedures and
tools. Information visualization is a communication channel that leverages
human perceptual abilities to transform abstract data into visual
representations. Visualization techniques have been successfully applied in the
context of time series to enhance interpretability by graphically representing
the temporal evolution of data. The challenge for information visualization
developers lies in integrating a wide range of analytical tools into rich
visualization systems that can summarize complex datasets while clearly
describing the impacts of the temporal component. Such systems enable data
scientists to turn raw data into understandable and potentially useful
knowledge. This review examines techniques and approaches designed for handling
time series data, guiding users through knowledge discovery processes based on
visual analysis. We also provide readers with theoretical insights and design
guidelines for considering when developing comprehensive information
visualization approaches for time series, with a particular focus on time
series with multiple features. As a result, we highlight the challenges and
future research directions to address open questions in the visualization of
time-dependent data.

</details>


### [4] [Model Simplification through refinement](https://arxiv.org/abs/2507.15186)
*Dmitry Brodsky,Benjamin Watson*

Main category: cs.GR

TL;DR: 本文提出了一种适用于在交互速度下简化大型多边形模型的快速算法，该算法保证了显示结果的时间限制且质量较高。


<details>
  <summary>Details</summary>
Motivation: 随着建模和可视化应用的普及，需要一种方法能在交互速度下简化大型多边形模型，现有算法要么速度慢要么结果质量差。

Method: 算法灵感来自矢量量化文献中的分割算法，逆向简化模型，从粗糙近似开始逐步细化，通过表面曲率的近似指导简化过程。

Result: 算法速度快，能在给定时间限制内生成可显示的结果，且质量良好。

Conclusion: 该算法为大型模型的实时简化提供了有效解决方案，支持进一步细化已生成的简化模型。

Abstract: As modeling and visualization applications proliferate, there arises a need
to simplify large polygonal models at interactive rates. Unfortunately existing
polygon mesh simplification algorithms are not well suited for this task
because they are either too slow (requiring the simplified model to be
pre-computed) or produce models that are too poor in quality. These
shortcomings become particularly acute when models are extremely large. We
present an algorithm suitable for simplification of large models at interactive
speeds. The algorithm is fast and can guarantee displayable results within a
given time limit. Results also have good quality. Inspired by splitting
algorithms from vector quantization literature, we simplify models in reverse,
beginning with an extremely coarse approximation and refining it.
Approximations of surface curvature guide the simplification process.
Previously produced simplifications can be further refined by using them as
input to the algorithm.

</details>


### [5] [Blended Point Cloud Diffusion for Localized Text-guided Shape Editing](https://arxiv.org/abs/2507.15399)
*Etai Sella,Noam Atia,Ron Mokady,Hadar Averbuch-Elor*

Main category: cs.GR

TL;DR: 本文提出了一种基于修复框架和坐标混合算法的3D点云形状编辑方法，通过结合基础3D扩散模型和部分条件形状指导，实现了局部精细化编辑同时保持全局一致性和形状身份。


<details>
  <summary>Details</summary>
Motivation: 现有方法在进行3D形状的局部精细化编辑时难以保持全局一致性，本文旨在解决这一问题。

Method: 采用基于修复的框架，结合基础3D扩散模型和部分条件形状指导，提出推理时间坐标混合算法以平衡全局形状重建与局部修复。

Result: 实验表明，该方法在多种评估指标上优于其他技术，既能忠实于原始形状，又能准确遵循文本描述。

Conclusion: 该方法通过创新性的坐标混合算法，实现了高效的3D形状编辑，无需依赖计算成本高且准确性低的反演过程。

Abstract: Natural language offers a highly intuitive interface for enabling localized
fine-grained edits of 3D shapes. However, prior works face challenges in
preserving global coherence while locally modifying the input 3D shape. In this
work, we introduce an inpainting-based framework for editing shapes represented
as point clouds. Our approach leverages foundation 3D diffusion models for
achieving localized shape edits, adding structural guidance in the form of a
partial conditional shape, ensuring that other regions correctly preserve the
shape's identity. Furthermore, to encourage identity preservation also within
the local edited region, we propose an inference-time coordinate blending
algorithm which balances reconstruction of the full shape with inpainting at a
progression of noise levels during the inference process. Our coordinate
blending algorithm seamlessly blends the original shape with its edited
version, enabling a fine-grained editing of 3D shapes, all while circumventing
the need for computationally expensive and often inaccurate inversion.
Extensive experiments show that our method outperforms alternative techniques
across a wide range of metrics that evaluate both fidelity to the original
shape and also adherence to the textual description.

</details>


### [6] [ObjectGS: Object-aware Scene Reconstruction and Scene Understanding via Gaussian Splatting](https://arxiv.org/abs/2507.15454)
*Ruijie Zhu,Mulin Yu,Linning Xu,Lihan Jiang,Yixuan Li,Tianzhu Zhang,Jiangmiao Pang,Bo Dai*

Main category: cs.GR

TL;DR: ObjectGS 是一个对象感知框架，将 3D 场景重建与语义理解相结合，通过局部锚点生成神经高斯并共享对象 ID，实现精确的对象级重建。


<details>
  <summary>Details</summary>
Motivation: 尽管 3D Gaussian Splatting 在实时新视角合成和高保真重建方面表现出色，但其缺乏语义理解限制了对象级感知能力。

Method: ObjectGS 将场景建模为局部锚点，动态调整锚点并优化其特征，通过独热 ID 编码和分类损失强制语义约束。

Result: 实验表明，ObjectGS 在开放词汇和全景分割任务上优于现有方法，并可无缝集成网格提取和场景编辑等应用。

Conclusion: ObjectGS 通过对象感知框架成功提升了 3D 场景重建的语义理解和应用潜力。

Abstract: 3D Gaussian Splatting is renowned for its high-fidelity reconstructions and
real-time novel view synthesis, yet its lack of semantic understanding limits
object-level perception. In this work, we propose ObjectGS, an object-aware
framework that unifies 3D scene reconstruction with semantic understanding.
Instead of treating the scene as a unified whole, ObjectGS models individual
objects as local anchors that generate neural Gaussians and share object IDs,
enabling precise object-level reconstruction. During training, we dynamically
grow or prune these anchors and optimize their features, while a one-hot ID
encoding with a classification loss enforces clear semantic constraints. We
show through extensive experiments that ObjectGS not only outperforms
state-of-the-art methods on open-vocabulary and panoptic segmentation tasks,
but also integrates seamlessly with applications like mesh extraction and scene
editing. Project page: https://ruijiezhu94.github.io/ObjectGS_page

</details>


### [7] [Gaussian Splatting with Discretized SDF for Relightable Assets](https://arxiv.org/abs/2507.15629)
*Zuo-Liang Zhu,Jian Yang,Beibei Wang*

Main category: cs.GR

TL;DR: 该方法通过离散化的SDF表示与高斯投影一致性损失，提升了基于高斯渲染的反向渲染质量，无需额外内存或复杂优化。


<details>
  <summary>Details</summary>
Motivation: 3D高斯渲染在反向渲染中因离散特性难以应用几何约束，现有方法引入SDF但增加内存和训练复杂度。本文旨在提出一种更高效的解决方案。

Method: 采用离散化的SDF表示，将其编码到每个高斯的采样值中，并通过投影一致性损失约束离散样本与SDF的一致性。

Result: 实验表明，该方法优于现有基于高斯的反向渲染方法，显著提升了重光照质量。

Conclusion: 离散化SDF结合投影一致性损失，为高斯渲染提供了高效且高质量的反向渲染解决方案。

Abstract: 3D Gaussian splatting (3DGS) has shown its detailed expressive ability and
highly efficient rendering speed in the novel view synthesis (NVS) task. The
application to inverse rendering still faces several challenges, as the
discrete nature of Gaussian primitives makes it difficult to apply geometry
constraints. Recent works introduce the signed distance field (SDF) as an extra
continuous representation to regularize the geometry defined by Gaussian
primitives. It improves the decomposition quality, at the cost of increasing
memory usage and complicating training. Unlike these works, we introduce a
discretized SDF to represent the continuous SDF in a discrete manner by
encoding it within each Gaussian using a sampled value. This approach allows us
to link the SDF with the Gaussian opacity through an SDF-to-opacity
transformation, enabling rendering the SDF via splatting and avoiding the
computational cost of ray marching.The key challenge is to regularize the
discrete samples to be consistent with the underlying SDF, as the discrete
representation can hardly apply the gradient-based constraints (\eg Eikonal
loss). For this, we project Gaussians onto the zero-level set of SDF and
enforce alignment with the surface from splatting, namely a projection-based
consistency loss. Thanks to the discretized SDF, our method achieves higher
relighting quality, while requiring no extra memory beyond GS and avoiding
complex manually designed optimization. The experiments reveal that our method
outperforms existing Gaussian-based inverse rendering methods. Our code is
available at https://github.com/NK-CS-ZZL/DiscretizedSDF.

</details>


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [8] [NPUEval: Optimizing NPU Kernels with LLMs and Open Source Compilers](https://arxiv.org/abs/2507.14403)
*Sarunas Kalade,Graham Schelle*

Main category: cs.PL

TL;DR: 论文介绍了NPUEval，一个用于编写和评估NPU内核的基准测试，包含102个常见机器学习算子，评估了LLM生成的代码在功能性和向量化效率上的表现。


<details>
  <summary>Details</summary>
Motivation: 随着神经处理单元（NPU）在低功耗设备中的普及，编写高效的NPU内核需要领域专业知识和特定架构知识，而NPU编程的碎片化使得LLM辅助编程面临挑战。

Method: 提出了NPUEval基准测试，包括102个常见机器学习算子，利用开源编译器工具评估LLM生成代码在AMD NPU上的功能正确性和向量化效率。

Result: 最新推理模型如DeepSeek R1在某些内核上实现了50%以上的向量化，但数据集的平均得分仅为10%，表明这是一个具有挑战性的任务。

Conclusion: NPUEval数据集和评估代码的开源发布为代码生成和NPU内核优化研究提供了重要基准。

Abstract: Neural processing units (NPUs) are gaining prominence in power-sensitive
devices like client devices, with AI PCs being defined by their inclusion of
these specialized processors. Running AI workloads efficiently on these devices
requires libraries of optimized kernels. Creating efficient kernels demands
expertise in domain-specific C++ with vector intrinsics and in-depth knowledge
of the target architecture. Unlike GPU programming, which has had years to
mature, NPU programming is new, with smaller and more fragmented developer
communities across hardware platforms. This fragmentation poses a challenge
when utilizing LLMs to assist in writing NPU kernels, as domain-specific
optimized code examples are underrepresented in LLM pre-training data.
  In this paper we introduce NPUEval -- a benchmark for writing and evaluating
NPU kernels, consisting of 102 common operators for machine learning workloads.
We evaluate LLM generated code on actual hardware based on both functional
correctness and vectorization efficiency using open source compiler tools
targeting the AMD NPU. We evaluate a range of state-of-the-art LLMs with a mix
of proprietary and open-weight models. Latest reasoning models like DeepSeek
R1, show promising results achieving out-of-the-box 50%+ vectorization on
select kernels. However, the average score across the entire dataset remains
roughly 10% even with compiler feedback and vectorized kernel examples --
showing that this is a challenging dataset even for frontier models. The
dataset and evaluation code will be released with a permissive open source
license, providing an essential benchmark for advancing research in code
generation and NPU kernel optimization.

</details>


### [9] [Timetide: A programming model for logically synchronous distributed systems](https://arxiv.org/abs/2507.14471)
*Logan Kenwright,Partha Roop,Nathan Allen,Călin Caşcaval,Avinash Malik*

Main category: cs.PL

TL;DR: 本文提出了一种新颖的多时钟同步程序语义Timetide，用于解决分布式系统中确定性编程的挑战，无需依赖昂贵的物理时钟同步。


<details>
  <summary>Details</summary>
Motivation: 传统的同步语言主要集中式应用，而分布式系统中的确定性编程仍面临挑战，尤其是在物理时钟同步的成本和可扩展性方面。

Method: 通过开发多时钟同步程序的语义，并提出基于逻辑同步模型的编程模型Timetide，避免了物理时钟同步的需求。

Result: Timetide是一种既适合分布式系统又支持形式化验证的多时钟同步语言，无需物理时钟同步或时钟门控。

Conclusion: Timetide为解决分布式系统中的确定性编程问题提供了有效且可扩展的解决方案。

Abstract: Massive strides in deterministic models have been made using synchronous
languages. They are mainly focused on centralised applications, as the
traditional approach is to compile away the concurrency. Time triggered
languages such as Giotto and Lingua Franca are suitable for distribution albeit
that they rely on expensive physical clock synchronisation, which is both
expensive and may suffer from scalability. Hence, deterministic programming of
distributed systems remains challenging. We address the challenges of
deterministic distribution by developing a novel multiclock semantics of
synchronous programs. The developed semantics is amenable to seamless
distribution. Moreover, our programming model, Timetide, alleviates the need
for physical clock synchronisation by building on the recently proposed logical
synchrony model for distributed systems. We discuss the important aspects of
distributing computation, such as network communication delays, and explore the
formal verification of Timetide programs. To the best of our knowledge,
Timetide is the first multiclock synchronous language that is both amenable to
distribution and formal verification without the need for physical clock
synchronisation or clock gating.

</details>


### [10] [Hear Your Code Fail, Voice-Assisted Debugging for Python](https://arxiv.org/abs/2507.15007)
*Sayed Mahbub Hasan Amiri,Md. Mainul Islam,Mohammad Shakhawat Hossen,Sayed Majhab Hasan Amiri,Mohammad Shawkat Ali Mamun,Sk. Humaun Kabir,Naznin Akter*

Main category: cs.PL

TL;DR: 本研究介绍了一种创新的Python语音辅助调试插件，将静默运行时错误转化为可操作的音频诊断，显著降低认知负载并加快错误识别速度。


<details>
  <summary>Details</summary>
Motivation: 传统调试工具对视觉依赖性强，且认知负载高。该研究旨在通过多模态反馈（听觉和视觉）提升调试效率，特别关注视觉障碍者和新手程序员的可用性。

Method: 采用全局异常钩子架构，结合pyttsx3文本转语音和基于Tkinter的GUI可视化，提供并行的听觉和视觉错误反馈。

Result: 实验结果显示，与传统调试方法相比，认知负载降低37%，错误识别速度提高78%，语音延迟小于1.2秒，CPU开销低于18%。插件兼容Python 3.7+环境，适用于多种操作系统。

Conclusion: 该插件通过听觉和视觉反馈显著提升调试效率和可访问性，未来计划整合GPT修复建议和多语言翻译，进一步推动听觉调试范式的发展。

Abstract: This research introduces an innovative voice-assisted debugging plugin for
Python that transforms silent runtime errors into actionable audible
diagnostics. By implementing a global exception hook architecture with pyttsx3
text-to-speech conversion and Tkinter-based GUI visualization, the solution
delivers multimodal error feedback through parallel auditory and visual
channels. Empirical evaluation demonstrates 37% reduced cognitive load (p<0.01,
n=50) compared to traditional stack-trace debugging, while enabling 78% faster
error identification through vocalized exception classification and
contextualization. The system achieves sub-1.2 second voice latency with under
18% CPU overhead during exception handling, vocalizing error types and
consequences while displaying interactive tracebacks with documentation deep
links. Criteria validate compatibility across Python 3.7+ environments on
Windows, macOS, and Linux platforms. Needing only two lines of integration
code, the plugin significantly boosts availability for aesthetically impaired
designers and supports multitasking workflows through hands-free error medical
diagnosis. Educational applications show particular promise, with pilot studies
indicating 45% faster debugging skill acquisition among novice programmers.
Future development will incorporate GPT-based repair suggestions and real-time
multilingual translation to further advance auditory debugging paradigms. The
solution represents a fundamental shift toward human-centric error diagnostics,
bridging critical gaps in programming accessibility while establishing new
standards for cognitive efficiency in software development workflows.

</details>


### [11] [Invariant Generation for Floating-Point Programs via Constraint Solving](https://arxiv.org/abs/2507.15017)
*Xuran Cai,Liqian Chen,Hongfei Fu*

Main category: cs.PL

TL;DR: 提出了一种基于约束求解方法的理论框架，用于生成浮点程序的不变式，解决了浮点误差累积导致程序失效的问题。


<details>
  <summary>Details</summary>
Motivation: 浮点运算中的舍入误差累积可能导致严重的程序失效，因此需要一种方法来确保浮点程序的正确性，尤其是在生成不变式时考虑浮点误差的影响。

Method: 结合FPTaylor的一阶微分表征与约束求解方法，提出了两种多项式不变式生成算法，分别适用于需要初始不变式和不需初始不变式但限于多项式程序的情况。

Result: 实验结果表明，所提算法在时间效率和生成的不变式精度上均优于现有技术。

Conclusion: 该框架成功解决了浮点程序不变式生成问题，尤其是处理了条件分支这一难点，为浮点程序分析提供了有效工具。

Abstract: In numeric-intensive computations, it is well known that the execution of
floating-point programs is imprecise as floating point arithmetics (e.g.,
addition, subtraction, multiplication, division, etc.) incurs rounding errors.
Albeit the rounding error is small for every single floating-point operation,
the aggregation of such error in multiple operations may be dramatic and cause
catastrophic program failures. Therefore, to ensure the correctness of
floating-point programs, the effect of floating point error needs to be
carefully taken into account. In this work, we consider the invariant
generation for floating point programs, whose aim is to generate tight
invariants under the perturbation of floating point errors. Our main
contribution is a theoretical framework on how to apply constraint solving
methods to address the invariant generation problem. In our framework, we
propose a novel combination between the first-order differential
characterization by FPTaylor (TOPLAS 2018) and constraint solving methods,
aiming to reduce the computational burden of constraint solving. Moreover, we
devise two polynomial invariant generation algorithms to instantiate the
framework. The first algorithm is applicable to a wide range of floating-point
operations but requires an initial (coarse) invariant as external input, while
the second does not require an initial invariant but is limited to polynomial
programs. Furthermore, we show how conditional branches, a difficult issue in
floating-point analysis, can be handled in our framework. Experimental results
show that our algorithms outperform SOTA approaches in both the time efficiency
and the precision of the generated invariants over a variety of benchmarks.

</details>


### [12] [A Few Fit Most: Improving Performance Portability of SGEMM on GPUs using Multi-Versioning](https://arxiv.org/abs/2507.15277)
*Robert Hochgraf,Sreepathi Pai*

Main category: cs.PL

TL;DR: 论文提出了一种名为"可移植性调整"的框架，通过多版本化技术自动生成性能可移植的代码，解决了GPU环境中自动调优的过拟合问题，并在GEMM核上取得了接近理论最大性能的表现。


<details>
  <summary>Details</summary>
Motivation: 手动优化线性代数核以适应不同GPU设备和应用复杂且耗时，而自动调优又容易过拟合，需要重新调优以适应环境变化。因此，需要一种新的方法来实现高性能且无需重新调优的代码生成。

Method: 采用多版本化技术，自动生成多个代码变体，以实现性能的可移植性。框架名为"可移植性调整"，能够在不重新调优的情况下生成高性能代码。

Result: 在CLBlast线性代数库的GEMM核上评估，发现可移植性调整技术优于默认内核，接近理论最大性能的10%以内，并且在新设备上表现良好。

Conclusion: 多版本化技术是一种有效的性能可移植性解决方案，能够在不重新调优的情况下适应不同设备和环境，显著优于传统自动调优方法。

Abstract: Hand-optimizing linear algebra kernels for different GPU devices and
applications is complex and labor-intensive. Instead, many developers use
automatic performance tuning (autotuning) to achieve high performance on a
variety of devices. However, autotuning "overfits", and must be redone if any
part of the environment changes, such as if the device or input characteristics
change.
  In most non-trivial cases, a single compute kernel cannot maintain
near-optimal performance across all environments. Changing the kernel to
specialize it to the current execution environment is possible, but on GPUs,
runtime tuning and compilation can be expensive.
  In this work, we use multi-versioning -- producing several variants of the
same code -- as a way to generate performance portable code. We describe a
framework called portability tuning that can automatically generate
multi-versioned code whose performance is portable, requiring no retuning.
  We evaluate our framework on a dataset of execution times for GEMM kernels
from the CLBlast linear algebra library. We find our portability tuning
techniques outperform CLBlast's default kernels -- often approaching within 10%
of the theoretical maximum performance -- despite CLBlast using autotuning
techniques. Further, we find that our generated programs generalize well to new
and unseen devices, matching the performance of autotuning without ever
portability tuning for those devices.

</details>


### [13] [Bayesian Separation Logic](https://arxiv.org/abs/2507.15530)
*Shing Hin Ho,Nicolas Wu,Azalea Raad*

Main category: cs.PL

TL;DR: Bayesian分离逻辑（BaSL）填补了现有分离逻辑无法处理贝叶斯更新的空白，通过Rokhlin-Simmons分解定理证明了贝叶斯定理的内部版本，并成功建模贝叶斯编程语言的语义。


<details>
  <summary>Details</summary>
Motivation: 现有分离逻辑无法处理贝叶斯编程语言（BPPLs）的核心特征——贝叶斯更新，而BaSL旨在填补这一空白。

Method: BaSL基于Kripke资源monoid的新实例化，使用σ-有限测度空间，并通过Rokhlin-Simmons分解定理证明内部贝叶斯定理。

Result: BaSL成功建模了贝叶斯更新、未归一化分布、条件分布等概念，并验证了统计模型的属性如贝叶斯硬币抛掷的期望值。

Conclusion: BaSL为BPPLs提供了语义基础，并通过模块化方式验证了复杂的统计模型属性，填补了现有理论的空白。

Abstract: Bayesian probabilistic programming languages (BPPLs) let users denote
statistical models as code while the interpreter infers the posterior
distribution. The semantics of BPPLs are usually mathematically complex and
unable to reason about desirable properties such as expected values and
independence of random variables. To reason about these properties in a
non-Bayesian setting, probabilistic separation logics such as PSL and Lilac
interpret separating conjunction as probabilistic independence of random
variables. However, no existing separation logic can handle Bayesian updating,
which is the key distinguishing feature of BPPLs.
  To close this gap, we introduce Bayesian separation logic (BaSL), a
probabilistic separation logic that gives semantics to BPPL. We prove an
internal version of Bayes' theorem using a result in measure theory known as
the Rokhlin-Simmons disintegration theorem. Consequently, BaSL can model
probabilistic programming concepts such as Bayesian updating, unnormalised
distribution, conditional distribution, soft constraint, conjugate prior and
improper prior while maintaining modularity via the frame rule. The model of
BaSL is based on a novel instantiation of Kripke resource monoid via
$\sigma$-finite measure spaces over the Hilbert cube, and the semantics of
Hoare triple is compatible with an existing denotational semantics of BPPL
based on the category of $s$-finite kernels. Using BaSL, we then prove
properties of statistical models such as the expected value of Bayesian coin
flip, correlation of random variables in the collider Bayesian network, and the
posterior distributions of the burglar alarm model, a parameter estimation
algorithm, and the Gaussian mixture model.

</details>


### [14] [Formal Analysis of Networked PLC Controllers Interacting with Physical Environments](https://arxiv.org/abs/2507.15596)
*Jaeseo Lee,Kyungmin Bae*

Main category: cs.PL

TL;DR: 提出了一种统一的框架，将离散PLC语义、网络通信和连续物理行为整合，用于精确分析PLC驱动系统，并通过部分顺序减少状态爆炸问题。


<details>
  <summary>Details</summary>
Motivation: 现有PLC验证技术主要关注单个程序，忽视了与物理环境和网络通信的交互，难以应对现实工业系统中连续动态和通信延迟的重要作用。

Method: 结合离散PLC语义、网络通信和连续物理行为，采用部分顺序减少方法以减少状态爆炸。

Result: 框架成功实现了对具有连续动态和网络通信的PLC驱动系统的精确分析，同时有效减少了状态数量。

Conclusion: 该框架填补了现有技术的不足，为复杂工业自动化系统的验证提供了有效的解决方案。

Abstract: Programmable Logic Controllers (PLCs) are widely used in industrial
automation to control physical systems. As PLC applications become increasingly
complex, ensuring their correctness is crucial. Existing formal verification
techniques focus on individual PLC programs in isolation, often neglecting
interactions with physical environments and network communication between
controllers. This limitation poses significant challenges in analyzing
real-world industrial systems, where continuous dynamics and communication
delays play a critical role. In this paper, we present a unified formal
framework that integrates discrete PLC semantics, networked communication, and
continuous physical behaviors. To mitigate state explosion, we apply partial
order reduction, significantly reducing the number of explored states while
maintaining correctness. Our framework enables precise analysis of PLC-driven
systems with continuous dynamics and networked communication.

</details>


### [15] [Closure Conversion, Flat Environments, and the Complexity of Abstract Machines](https://arxiv.org/abs/2507.15843)
*Beniamino Accattoli,Dan Ghica,Giulio Guerrieri,Cláudio Belo Lourenço,Claudio Sacerdoti Coen*

Main category: cs.PL

TL;DR: 本文研究了闭包转换与抽象机器之间的关系，通过简单的λ演算和元组作为源语言，提出了闭包转换正确性的新证明技术，改进了环境处理方式，并分析了时间复杂性，展示了闭包转换对动态成本和代码大小的影响。


<details>
  <summary>Details</summary>
Motivation: 研究闭包转换与抽象机器之间的关系，探索闭包转换的正确性及其在抽象机器中的应用，以及对时间复杂性的影响。

Method: 采用简单的λ演算与元组作为源语言，分析抽象机器在源语言和目标语言中的表现，重点关注无共享环境的扁平闭包/环境情况。

Result: 提出了闭包转换正确性的新证明技术，设计了改进的环境处理方式，并展示了闭包转换在减少动态成本的同时增加初始代码大小，但总体时间复杂性不变。

Conclusion: 闭包转换虽然改变动态成本和代码大小，但通过改进的环境处理方式和正确性证明技术，其在抽象机器中的应用是可行且有效的，且不影响总体时间复杂性。

Abstract: Closure conversion is a program transformation at work in compilers for
functional languages to turn inner functions into global ones, by building
closures pairing the transformed functions with the environment of their free
variables. Abstract machines rely on similar and yet different concepts of
closures and environments.
  In this paper, we study the relationship between the two approaches. We adopt
a very simple {\lambda}-calculus with tuples as source language and study
abstract machines for both the source language and the target of closure
conversion. Moreover, we focus on the simple case of flat
closures/environments, that is, with no sharing of environments. We provide
three contributions.
  Firstly, a new simple proof technique for the correctness of closure
conversion, inspired by abstract machines.
  Secondly, we show how the closure invariants of the target language allow us
to design a new way of handling environments in abstract machines, not
suffering the shortcomings of other styles.
  Thirdly, we study the machines from the point of view of time complexity,
adapting analyses by Accattoli and co-authors. We show that closure conversion
decreases various dynamic costs while increasing the size of the initial code.
Despite these changes, the overall complexity of the machines before and after
closure conversion turns out to be the same.

</details>


<div id='cs.GT'></div>

# cs.GT [[Back]](#toc)

### [16] [A Formal Model of the Economic Impacts of AI Openness Regulation](https://arxiv.org/abs/2507.14193)
*Tori Qiu,Benjamin Laufer,Jon Kleinberg,Hoda Heidari*

Main category: cs.GT

TL;DR: 论文研究了在监管框架下（如欧盟AI法案）一般用途AI模型开放性的定义及其对经济激励的影响，通过模型分析不同开放性标准对市场均衡的影响。


<details>
  <summary>Details</summary>
Motivation: 尽管监管框架（如欧盟AI法案）鼓励通过法律豁免促进开源模型，但通用AI模型的开源定义仍不明确，需要研究其对开发者经济激励的影响。

Method: 论文通过建模通用模型开发者（generalist）与领域特定任务微调者（specialist）之间的策略互动，分析监管要求下不同开源标准对市场均衡的影响。

Result: 研究发现，模型的基线性能决定了增加监管惩罚与开源阈值对通用模型发布策略的影响，提出了一系列有效的监管措施和开源阈值。

Conclusion: 模型为AI治理中关于开放性的决策提供了理论基础，有助于评估和改进实际开源政策。

Abstract: Regulatory frameworks, such as the EU AI Act, encourage openness of
general-purpose AI models by offering legal exemptions for "open-source"
models. Despite this legislative attention on openness, the definition of
open-source foundation models remains ambiguous. This paper models the
strategic interactions among the creator of a general-purpose model (the
generalist) and the entity that fine-tunes the general-purpose model to a
specialized domain or task (the specialist), in response to regulatory
requirements on model openness. We present a stylized model of the regulator's
choice of an open-source definition to evaluate which AI openness standards
will establish appropriate economic incentives for developers. Our results
characterize market equilibria -- specifically, upstream model release
decisions and downstream fine-tuning efforts -- under various openness
regulations and present a range of effective regulatory penalties and
open-source thresholds. Overall, we find the model's baseline performance
determines when increasing the regulatory penalty vs. the open-source threshold
will significantly alter the generalist's release strategy. Our model provides
a theoretical foundation for AI governance decisions around openness and
enables evaluation and refinement of practical open-source policies.

</details>


### [17] [Strategyproofness and Monotone Allocation of Auction in Social Networks](https://arxiv.org/abs/2507.14472)
*Yuhang Guo,Dong Hao,Bin Li,Mingyu Xiao,Bakh Khoussainov*

Main category: cs.GT

TL;DR: 该论文探讨了网络拍卖中的策略证明性，提出了两种单调分配规则类别（ID-MON和IP-MON），并证明了在这些规则下存在收益最大化的支付规则。


<details>
  <summary>Details</summary>
Motivation: 传统的拍卖理论中的分配规则（如Myerson引理中的价值单调分配）无法直接应用于网络拍卖，且现有研究未能解决策略证明性问题，因此需要新的理论和规则。

Method: 论文提出了两种新的单调分配规则——邀请抑制单调性（ID-MON）和邀请促进单调性（IP-MON），并研究了在这些规则下策略证明性支付规则的存在条件及其计算可行性。

Result: 研究发现，对于ID-MON或IP-MON分配规则，存在收益最大化的支付规则，且这些规则在计算上是可行的，从而解决了单一需求组合网络拍卖的策略证明性问题。

Conclusion: 通过引入ID-MON和IP-MON分配规则，论文首次解决了网络拍卖中的策略证明性问题，并为未来的研究提供了理论基础和实用工具。

Abstract: Strategyproofness in network auctions requires that bidders not only report
their valuations truthfully, but also do their best to invite neighbours from
the social network. In contrast to canonical auctions, where the value-monotone
allocation in Myerson's Lemma is a cornerstone, a general principle of
allocation rules for strategyproof network auctions is still missing. We show
that, due to the absence of such a principle, even extensions to multi-unit
network auctions with single-unit demand present unexpected difficulties, and
all pioneering researches fail to be strategyproof. For the first time in this
field, we identify two categories of monotone allocation rules on networks:
Invitation-Depressed Monotonicity (ID-MON) and Invitation-Promoted Monotonicity
(IP-MON). They encompass all existing allocation rules of network auctions as
specific instances. For any given ID-MON or IP-MON allocation rule, we
characterize the existence and sufficient conditions for the strategyproof
payment rules, and show that among all such payment rules, the
revenue-maximizing one exists and is computationally feasible. With these
results, the obstacle of combinatorial network auction with single-minded
bidders is now resolved.

</details>


### [18] [Probing EFX via PMMS: (Non-)Existence Results in Discrete Fair Division](https://arxiv.org/abs/2507.14957)
*Jarosław Byrka,Franciszek Malinka,Tomasz Ponitka*

Main category: cs.GT

TL;DR: 本文研究了不可分割物品的公平分配问题，重点关注EFX和PMMS问题，证明了PMMS与EFX之间的形式分离，并在三种特殊情况下证明了公平分配的存在性。


<details>
  <summary>Details</summary>
Motivation: 公平分配问题是计算经济学中的核心问题之一，EFX和PMMS是其中的重要概念。本文旨在填补关于EFX与PMMS关系的研究空白，并探索其在特定场景下的应用。

Method: 通过构造实例和分析特定估值类型（如个性化双值估值、二进制估值等），并使用多项式时间算法来证明公平分配的存在性。

Result: 证明了在个性化双值估值、二进制估值以及配对需求估值等特殊情况下，EFX和PMMS分配的存在性，并提供了多项式时间算法。

Conclusion: 本文不仅在理论上分离了EFX与PMMS，还为多种特殊估值类型提供了公平分配的构造算法，拓展了公平分配问题的研究边界。

Abstract: We study the fair division of indivisible items and provide new insights into
the EFX problem, which is widely regarded as the central open question in fair
division, and the PMMS problem, a strictly stronger variant of EFX. Our first
result constructs a three-agent instance with two monotone valuations and one
additive valuation in which no PMMS allocation exists. Since EFX allocations
are known to exist under these assumptions, this establishes a formal
separation between EFX and PMMS.
  We prove existence of fair allocations for three important special cases. We
show that EFX allocations exist for personalized bivalued valuations, where for
each agent $i$ there exist values $a_i > b_i$ such that agent $i$ assigns value
$v_i(\{g\}) \in \{a_i, b_i\}$ to each good $g$. We establish an analogous
existence result for PMMS allocations when $a_i$ is divisible by $b_i$. We also
prove that PMMS allocations exist for binary-valued MMS-feasible valuations,
where each bundle $S$ has value $v_i(S) \in \{0, 1\}$. Notably, this result
holds even without assuming monotonicity of valuations and thus applies to the
fair division of chores and mixed manna. Finally, we study a class of
valuations called pair-demand valuations, which extend the well-studied
unit-demand valuations to the case where each agent derives value from at most
two items, and we show that PMMS allocations exist in this setting. Our proofs
are constructive, and we provide polynomial-time algorithms for all three
existence results.

</details>


### [19] [Strategically Robust Game Theory via Optimal Transport](https://arxiv.org/abs/2507.15325)
*Nicolas Lanzetti,Sylvain Fricker,Saverio Bolognani,Florian Dörfler,Dario Paccagnan*

Main category: cs.GT

TL;DR: 论文提出了一种称为"战略鲁棒均衡"的新概念，通过最坏情况行为决策保护代理免受对手行为不确定性的影响，并在实验中展示了其优势。


<details>
  <summary>Details</summary>
Motivation: 游戏理论中代理面临多重不确定性来源（如信息不完整、有限计算和有限理性），传统方法难以应对这些联合不确定性，因此需要一种新的决策保护机制。

Method: 代理在最坏情况下基于可调大小的模糊集合做出决策，模糊集合以隐含定义的行为为中心。通过最优传输方法实现这种策略，并将其称为"战略鲁棒均衡"。

Result: 战略鲁棒均衡在纳什均衡存在条件下同样存在，且在纳什策略和安全策略之间插值；计算成本与纳什均衡相当，并在实验中显示出更高的均衡收益。

Conclusion: 战略鲁棒均衡能有效应对对手行为的不确定性，并通过"鲁棒化协调"效应提高均衡收益。

Abstract: In many game-theoretic settings, agents are challenged with taking decisions
against the uncertain behavior exhibited by others. Often, this uncertainty
arises from multiple sources, e.g., incomplete information, limited
computation, bounded rationality. While it may be possible to guide the agents'
decisions by modeling each source, their joint presence makes this task
particularly daunting. Toward this goal, it is natural for agents to seek
protection against deviations around the emergent behavior itself, which is
ultimately impacted by all the above sources of uncertainty. To do so, we
propose that each agent takes decisions in face of the worst-case behavior
contained in an ambiguity set of tunable size, centered at the emergent
behavior so implicitly defined. This gives rise to a novel equilibrium notion,
which we call strategically robust equilibrium. Building on its definition, we
show that, when judiciously operationalized via optimal transport,
strategically robust equilibria (i) are guaranteed to exist under the same
assumptions required for Nash equilibria; (ii) interpolate between Nash and
security strategies; (iii) come at no additional computational cost compared to
Nash equilibria. Through a variety of experiments, including bi-matrix games,
congestion games, and Cournot competition, we show that strategic robustness
protects against uncertainty in the opponents' behavior and, surprisingly,
often results in higher equilibrium payoffs - an effect we refer to as
coordination via robustification.

</details>


### [20] [The Root of Revenue Continuity](https://arxiv.org/abs/2507.15735)
*Sergiu Hart,Noam Nisan*

Main category: cs.GT

TL;DR: 本文证明了一个关于买家估值分布变化对收益影响的简洁定理，并提出了一个简单的机制调整方法，使其在Wasserstein距离接近的分布下保持近似最优性。


<details>
  <summary>Details</summary>
Motivation: 研究买家估值分布的小变化如何影响可能的最大收益，为机制设计提供理论支持。

Method: 利用Wasserstein距离衡量两个随机估值分布X和Y之间的差异，并推导出其与收益平方根差的界限。

Result: 证明了sqrt(Rev(X))-sqrt(Rev(Y))<=sqrt(W(X,Y))，并提出通过“统一折扣”方法调整机制，使其在接近的分布下保持最优性。

Conclusion: 该研究为机制设计中处理估值分布变化提供了一种简洁且普适的方法，确保机制在接近的分布下仍能高效运行。

Abstract: In the setup of selling one or more goods, various papers have shown, in
various forms and for various purposes, that a small change in the distribution
of a buyer's valuations may cause only a small change in the possible revenue
that can be extracted. We prove a simple, clean, convenient, and general
statement to this effect: let X and Y be random valuations on k additive goods,
and let W(X,Y) be the Wasserstein (or "earth mover's") distance between them;
then sqrt(Rev(X))-sqrt(Rev(Y)) <= sqrt(W(X,Y)). This further implies that a
simple explicit modification of any optimal mechanism for X, namely, "uniform
discounting", is guaranteed to be almost optimal for any Y that is close to X
in the Wasserstein distance.

</details>


### [21] [General Matching Games](https://arxiv.org/abs/2507.15737)
*Felipe Garrido-Lucero,Rida Laraki*

Main category: cs.GT

TL;DR: 本文将Garrido-Lucero和Laraki的一对一匹配游戏模型扩展为一对多匹配市场和室友模型，提出了两种框架，确保核心稳定且耐重新协商的结果存在并可高效计算。


<details>
  <summary>Details</summary>
Motivation: 现有的匹配游戏模型仅限于一对一的两边市场，无法涵盖更为复杂的一对多匹配市场和室友模型。本文旨在扩展模型以适应更广泛的应用场景。

Method: 通过扩展Garrido-Lucero和Laraki的模型至一對多匹配市场和室友模型，提出两种框架，确保核心稳定且耐重新协商的结果存在。

Result: 在扩展的模型中，证明了核心稳定且耐重新协商的结果存在，并可通过高效算法计算。

Conclusion: 本文成功扩展了匹配游戏模型，为一对多市场和室友模型提供了理论支持，并展示了高效计算方法。

Abstract: Matching games is a one-to-one two sided market model introduced by
Garrido-Lucero and Laraki, in which coupled agents' utilities are endogenously
determined as the outcome of a strategic game. They refine the classical
pairwise stability by requiring robustness to renegotiation and provide general
conditions under which pairwise stable and renegotiation-proof outcomes exist
as the limit of a deferred acceptance with competitions algorithm together with
a renegotiation process. In this article, we extend their model to a general
setting encompassing most of one-to-many matching markets and roommates models
and specify two frameworks under which core stable and renegotiation-proof
outcomes exist and can be efficiently computed.

</details>
