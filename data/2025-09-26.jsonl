{"id": "2509.20426", "categories": ["cs.PL", "cs.SE"], "pdf": "https://arxiv.org/pdf/2509.20426", "abs": "https://arxiv.org/abs/2509.20426", "authors": ["Mahmoud Samir Fayed"], "title": "Dual-Language General-Purpose Self-Hosted Visual Language and new Textual Programming Language for Applications", "comment": "PhD thesis", "summary": "Most visual programming languages (VPLs) are domain-specific, with few\ngeneral-purpose VPLs like Programming Without Coding Technology (PWCT). These\ngeneral-purpose VPLs are developed using textual programming languages and\nimproving them requires textual programming. In this thesis, we designed and\ndeveloped PWCT2, a dual-language (Arabic/English), general-purpose,\nself-hosting visual programming language. Before doing so, we specifically\ndesigned a textual programming language called Ring for its development. Ring\nis a dynamically typed language with a lightweight implementation, offering\nsyntax customization features. It permits the creation of domain-specific\nlanguages through new features that extend object-oriented programming,\nallowing for specialized languages resembling Cascading Style Sheets (CSS) or\nSupernova language. The Ring Compiler and Virtual Machine are designed using\nthe PWCT visual programming language where the visual implementation is\ncomposed of 18,945 components that generate 24,743 lines of C code, which\nincreases the abstraction level and hides unnecessary details. Using PWCT to\ndevelop Ring allowed us to realize several issues in PWCT, which led to the\ndevelopment of the PWCT2 visual programming language using the Ring textual\nprogramming language. PWCT2 provides approximately 36 times faster code\ngeneration and requires 20 times less storage for visual source files. It also\nallows for the conversion of Ring code into visual code, enabling the creation\nof a self-hosting VPL that can be developed using itself. PWCT2 consists of\napproximately 92,000 lines of Ring code and comes with 394 visual components.\nPWCT2 is distributed to many users through the Steam platform and has received\npositive feedback, On Steam, 1772 users have launched the software, and the\ntotal recorded usage time exceeds 17,000 hours, encouraging further research\nand development."}
{"id": "2509.20534", "categories": ["cs.PL", "cs.SC", "68W30", "I.1.1; G.4"], "pdf": "https://arxiv.org/pdf/2509.20534", "abs": "https://arxiv.org/abs/2509.20534", "authors": ["Bowen Zhu", "Aayush Sabharwal", "Songchen Tan", "Yingbo Ma", "Alan Edelman", "Christopher Rackauckas"], "title": "Efficient Symbolic Computation vis Hash Consing", "comment": null, "summary": "Symbolic computation systems suffer from memory inefficiencies due to\nredundant storage of structurally identical subexpressions, commonly known as\nexpression swell, which degrades performance in both classical computer algebra\nand emerging AI-driven mathematical reasoning tools. In this paper, we present\nthe first integration of hash consing into JuliaSymbolics, a high-performance\nsymbolic toolkit in Julia, by employing a global weak-reference hash table that\ncanonicalizes expressions and eliminates duplication. This approach reduces\nmemory consumption and accelerates key operations such as differentiation,\nsimplification, and code generation, while seamlessly integrating with Julia's\nmetaprogramming and just-in-time compilation infrastructure. Benchmark\nevaluations across different computational domains reveal substantial\nimprovements: symbolic computations are accelerated by up to 3.2 times, memory\nusage is reduced by up to 2 times, code generation is up to 5 times faster,\nfunction compilation up to 10 times faster, and numerical evaluation up to 100\ntimes faster for larger models. While certain workloads with fewer duplicate\nunknown-variable expressions show more modest gains or even slight overhead in\ninitial computation stages, downstream processing consistently benefits\nsignificantly. These findings underscore the importance of hash consing in\nscaling symbolic computation and pave the way for future work integrating hash\nconsing with e-graphs for enhanced equivalence-aware expression sharing in\nAI-driven pipelines."}
{"id": "2509.20919", "categories": ["cs.GT"], "pdf": "https://arxiv.org/pdf/2509.20919", "abs": "https://arxiv.org/abs/2509.20919", "authors": ["Andreas Kontogiannis", "Vasilis Pollatos", "Gabriele Farina", "Panayotis Mertikopoulos", "Ioannis Panageas"], "title": "Efficient Kernelized Learning in Polyhedral Games Beyond Full-Information: From Colonel Blotto to Congestion Games", "comment": "Accepted at NeurIPS 2025", "summary": "We examine the problem of efficiently learning coarse correlated equilibria\n(CCE) in polyhedral games, that is, normal-form games with an exponentially\nlarge number of actions per player and an underlying combinatorial structure.\nProminent examples of such games are the classical Colonel Blotto and\ncongestion games. To achieve computational efficiency, the learning algorithms\nmust exhibit regret and per-iteration complexity that scale polylogarithmically\nin the size of the players' action sets. This challenge has recently been\naddressed in the full-information setting, primarily through the use of\nkernelization. However, in the case of the realistic, but mathematically\nchallenging, partial-information setting, existing approaches result in\nsuboptimal and impractical runtime complexity to learn CCE. We tackle this\nlimitation by building a framework based on the kernelization paradigm. We\napply this framework to prominent examples of polyhedral games -- namely the\nColonel Blotto, graphic matroid and network congestion games -- and provide\ncomputationally efficient payoff-based learning algorithms, which significantly\nimprove upon prior works in terms of the runtime for learning CCE in these\nsettings."}
{"id": "2509.20400", "categories": ["cs.GR"], "pdf": "https://arxiv.org/pdf/2509.20400", "abs": "https://arxiv.org/abs/2509.20400", "authors": ["Yiyu Li", "Haoyuan Wang", "Ke Xu", "Gerhard Petrus Hancke", "Rynson W. H. Lau"], "title": "SeHDR: Single-Exposure HDR Novel View Synthesis via 3D Gaussian Bracketing", "comment": "ICCV 2025 accepted paper", "summary": "This paper presents SeHDR, a novel high dynamic range 3D Gaussian Splatting\n(HDR-3DGS) approach for generating HDR novel views given multi-view LDR images.\nUnlike existing methods that typically require the multi-view LDR input images\nto be captured from different exposures, which are tedious to capture and more\nlikely to suffer from errors (e.g., object motion blurs and\ncalibration/alignment inaccuracies), our approach learns the HDR scene\nrepresentation from multi-view LDR images of a single exposure. Our key insight\nto this ill-posed problem is that by first estimating Bracketed 3D Gaussians\n(i.e., with different exposures) from single-exposure multi-view LDR images, we\nmay then be able to merge these bracketed 3D Gaussians into an HDR scene\nrepresentation. Specifically, SeHDR first learns base 3D Gaussians from\nsingle-exposure LDR inputs, where the spherical harmonics parameterize colors\nin a linear color space. We then estimate multiple 3D Gaussians with identical\ngeometry but varying linear colors conditioned on exposure manipulations.\nFinally, we propose the Differentiable Neural Exposure Fusion (NeEF) to\nintegrate the base and estimated 3D Gaussians into HDR Gaussians for novel view\nrendering. Extensive experiments demonstrate that SeHDR outperforms existing\nmethods as well as carefully designed baselines."}
{"id": "2509.20932", "categories": ["cs.GT", "cs.LO", "cs.MA", "cs.SC"], "pdf": "https://arxiv.org/pdf/2509.20932", "abs": "https://arxiv.org/abs/2509.20932", "authors": ["Neil Ghani"], "title": "A Category Theoretic Approach to Approximate Game Theory", "comment": "In Proceedings ACT 2024, arXiv:2509.18357", "summary": "This paper uses category theory to develop an entirely new approach to\napproximate game theory. Game theory is the study of how different agents\nwithin a multi-agent system take decisions. At its core, game theory asks what\nan optimal decision is in a given scenario. Thus approximate game theory asks\nwhat is an approximately optimal decision in a given scenario. This is\nimportant in practice as -- just like in much of computing -- exact answers\nmaybe too difficult to compute or even impossible to compute given inherent\nuncertainty in input.\n  We consider first \"Selection Functions\" which are functions and develop a\nsimple yet robust model of approximate equilibria. We develop the algebraic\nproperties of approximation wrt selection functions and also relate\napproximation to the compositional structure of selection functions. We then\nrepeat this process successfully for Open Games -- a more advanced model of\ngame theory."}
{"id": "2509.20401", "categories": ["cs.GR", "cs.RO"], "pdf": "https://arxiv.org/pdf/2509.20401", "abs": "https://arxiv.org/abs/2509.20401", "authors": ["Binod Singh", "Sayan Deb Sarkar", "Iro Armeni"], "title": "SGAligner++: Cross-Modal Language-Aided 3D Scene Graph Alignment", "comment": null, "summary": "Aligning 3D scene graphs is a crucial initial step for several applications\nin robot navigation and embodied perception. Current methods in 3D scene graph\nalignment often rely on single-modality point cloud data and struggle with\nincomplete or noisy input. We introduce SGAligner++, a cross-modal,\nlanguage-aided framework for 3D scene graph alignment. Our method addresses the\nchallenge of aligning partially overlapping scene observations across\nheterogeneous modalities by learning a unified joint embedding space, enabling\naccurate alignment even under low-overlap conditions and sensor noise. By\nemploying lightweight unimodal encoders and attention-based fusion, SGAligner++\nenhances scene understanding for tasks such as visual localization, 3D\nreconstruction, and navigation, while ensuring scalability and minimal\ncomputational overhead. Extensive evaluations on real-world datasets\ndemonstrate that SGAligner++ outperforms state-of-the-art methods by up to 40%\non noisy real-world reconstructions, while enabling cross-modal generalization."}
{"id": "2509.20414", "categories": ["cs.GR", "cs.CV", "cs.LG", "cs.RO"], "pdf": "https://arxiv.org/pdf/2509.20414", "abs": "https://arxiv.org/abs/2509.20414", "authors": ["Yandan Yang", "Baoxiong Jia", "Shujie Zhang", "Siyuan Huang"], "title": "SceneWeaver: All-in-One 3D Scene Synthesis with an Extensible and Self-Reflective Agent", "comment": "Accepted by NeurIPS 2025, 26 pages", "summary": "Indoor scene synthesis has become increasingly important with the rise of\nEmbodied AI, which requires 3D environments that are not only visually\nrealistic but also physically plausible and functionally diverse. While recent\napproaches have advanced visual fidelity, they often remain constrained to\nfixed scene categories, lack sufficient object-level detail and physical\nconsistency, and struggle to align with complex user instructions. In this\nwork, we present SceneWeaver, a reflective agentic framework that unifies\ndiverse scene synthesis paradigms through tool-based iterative refinement. At\nits core, SceneWeaver employs a language model-based planner to select from a\nsuite of extensible scene generation tools, ranging from data-driven generative\nmodels to visual- and LLM-based methods, guided by self-evaluation of physical\nplausibility, visual realism, and semantic alignment with user input. This\nclosed-loop reason-act-reflect design enables the agent to identify semantic\ninconsistencies, invoke targeted tools, and update the environment over\nsuccessive iterations. Extensive experiments on both common and open-vocabulary\nroom types demonstrate that SceneWeaver not only outperforms prior methods on\nphysical, visual, and semantic metrics, but also generalizes effectively to\ncomplex scenes with diverse instructions, marking a step toward general-purpose\n3D environment generation. Project website: https://scene-weaver.github.io/."}
{"id": "2509.20710", "categories": ["cs.GR", "cs.CV"], "pdf": "https://arxiv.org/pdf/2509.20710", "abs": "https://arxiv.org/abs/2509.20710", "authors": ["Yuguang Chen", "Xinhai Liu", "Yang Li", "Victor Cheung", "Zhuo Chen", "Dongyu Zhang", "Chunchao Guo"], "title": "ArtUV: Artist-style UV Unwrapping", "comment": null, "summary": "UV unwrapping is an essential task in computer graphics, enabling various\nvisual editing operations in rendering pipelines. However, existing UV\nunwrapping methods struggle with time-consuming, fragmentation, lack of\nsemanticity, and irregular UV islands, limiting their practical use. An\nartist-style UV map must not only satisfy fundamental criteria, such as\noverlap-free mapping and minimal distortion, but also uphold higher-level\nstandards, including clean boundaries, efficient space utilization, and\nsemantic coherence. We introduce ArtUV, a fully automated, end-to-end method\nfor generating artist-style UV unwrapping. We simulates the professional UV\nmapping process by dividing it into two stages: surface seam prediction and\nartist-style UV parameterization. In the seam prediction stage, SeamGPT is used\nto generate semantically meaningful cutting seams. Then, in the\nparameterization stage, a rough UV obtained from an optimization-based method,\nalong with the mesh, is fed into an Auto-Encoder, which refines it into an\nartist-style UV map. Our method ensures semantic consistency and preserves\ntopological structure, making the UV map ready for 2D editing. We evaluate\nArtUV across multiple benchmarks and show that it serves as a versatile\nsolution, functioning seamlessly as either a plug-in for professional rendering\ntools or as a standalone system for rapid, high-quality UV generation."}
{"id": "2509.20725", "categories": ["cs.GR", "cs.CV"], "pdf": "https://arxiv.org/pdf/2509.20725", "abs": "https://arxiv.org/abs/2509.20725", "authors": ["Duoteng Xu", "Yuguang Chen", "Jing Li", "Xinhai Liu", "Xueqi Ma", "Zhuo Chen", "Dongyu Zhang", "Chunchao Guo"], "title": "SeamCrafte: Enhancing Mesh Seam Generation for Artist UV Unwrapping via Reinforcement Learning", "comment": null, "summary": "Mesh seams play a pivotal role in partitioning 3D surfaces for UV\nparametrization and texture mapping. Poorly placed seams often result in severe\nUV distortion or excessive fragmentation, thereby hindering texture synthesis\nand disrupting artist workflows. Existing methods frequently trade one failure\nmode for another-producing either high distortion or many scattered islands. To\naddress this, we introduce SeamCrafter, an autoregressive GPT-style seam\ngenerator conditioned on point cloud inputs. SeamCrafter employs a dual-branch\npoint-cloud encoder that disentangles and captures complementary topological\nand geometric cues during pretraining. To further enhance seam quality, we\nfine-tune the model using Direct Preference Optimization (DPO) on a preference\ndataset derived from a novel seam-evaluation framework. This framework assesses\nseams primarily by UV distortion and fragmentation, and provides pairwise\npreference labels to guide optimization. Extensive experiments demonstrate that\nSeamCrafter produces seams with substantially lower distortion and\nfragmentation than prior approaches, while preserving topological consistency\nand visual fidelity."}
{"id": "2509.20824", "categories": ["cs.GR", "cs.CV"], "pdf": "https://arxiv.org/pdf/2509.20824", "abs": "https://arxiv.org/abs/2509.20824", "authors": ["Jiabao Lei", "Kewei Shi", "Zhihao Liang", "Kui Jia"], "title": "ARMesh: Autoregressive Mesh Generation via Next-Level-of-Detail Prediction", "comment": "NeurIPS 2025, Project Page: https://jblei.site/proj/armesh", "summary": "Directly generating 3D meshes, the default representation for 3D shapes in\nthe graphics industry, using auto-regressive (AR) models has become popular\nthese days, thanks to their sharpness, compactness in the generated results,\nand ability to represent various types of surfaces. However, AR mesh generative\nmodels typically construct meshes face by face in lexicographic order, which\ndoes not effectively capture the underlying geometry in a manner consistent\nwith human perception. Inspired by 2D models that progressively refine images,\nsuch as the prevailing next-scale prediction AR models, we propose generating\nmeshes auto-regressively in a progressive coarse-to-fine manner. Specifically,\nwe view mesh simplification algorithms, which gradually merge mesh faces to\nbuild simpler meshes, as a natural fine-to-coarse process. Therefore, we\ngeneralize meshes to simplicial complexes and develop a transformer-based AR\nmodel to approximate the reverse process of simplification in the order of\nlevel of detail, constructing meshes initially from a single point and\ngradually adding geometric details through local remeshing, where the topology\nis not predefined and is alterable. Our experiments show that this novel\nprogressive mesh generation approach not only provides intuitive control over\ngeneration quality and time consumption by early stopping the auto-regressive\nprocess but also enables applications such as mesh refinement and editing."}
{"id": "2509.20858", "categories": ["cs.GR", "cs.CV", "cs.MM"], "pdf": "https://arxiv.org/pdf/2509.20858", "abs": "https://arxiv.org/abs/2509.20858", "authors": ["Yuze Wang", "Luo Yang", "Junyi Wang", "Yue Qi"], "title": "ArchGPT: Understanding the World's Architectures with Large Multimodal Models", "comment": null, "summary": "Architecture embodies aesthetic, cultural, and historical values, standing as\na tangible testament to human civilization. Researchers have long leveraged\nvirtual reality (VR), mixed reality (MR), and augmented reality (AR) to enable\nimmersive exploration and interpretation of architecture, enhancing\naccessibility, public understanding, and creative workflows around architecture\nin education, heritage preservation, and professional design practice. However,\nexisting VR/MR/AR systems are often developed case-by-case, relying on\nhard-coded annotations and task-specific interactions that do not scale across\ndiverse built environments. In this work, we present ArchGPT, a multimodal\narchitectural visual question answering (VQA) model, together with a scalable\ndata-construction pipeline for curating high-quality, architecture-specific VQA\nannotations. This pipeline yields Arch-300K, a domain-specialized dataset of\napproximately 315,000 image-question-answer triplets. Arch-300K is built via a\nmulti-stage process: first, we curate architectural scenes from Wikimedia\nCommons and filter unconstrained tourist photo collections using a novel\ncoarse-to-fine strategy that integrates 3D reconstruction and semantic\nsegmentation to select occlusion-free, structurally consistent architectural\nimages. To mitigate noise and inconsistency in raw textual metadata, we propose\nan LLM-guided text verification and knowledge-distillation pipeline to generate\nreliable, architecture-specific question-answer pairs. Using these curated\nimages and refined metadata, we further synthesize formal analysis\nannotations-including detailed descriptions and aspect-guided conversations-to\nprovide richer semantic variety while remaining faithful to the data. We\nperform supervised fine-tuning of an open-source multimodal backbone\n,ShareGPT4V-7B, on Arch-300K, yielding ArchGPT."}
{"id": "2509.21007", "categories": ["cs.GR", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2509.21007", "abs": "https://arxiv.org/abs/2509.21007", "authors": ["Christian Stippel", "Felix Mujkanovic", "Thomas Leimkühler", "Pedro Hermosilla"], "title": "Marching Neurons: Accurate Surface Extraction for Neural Implicit Shapes", "comment": "SIGGRAPH Asia 2025 (Journal Track)", "summary": "Accurate surface geometry representation is crucial in 3D visual computing.\nExplicit representations, such as polygonal meshes, and implicit\nrepresentations, like signed distance functions, each have distinct advantages,\nmaking efficient conversions between them increasingly important. Conventional\nsurface extraction methods for implicit representations, such as the widely\nused Marching Cubes algorithm, rely on spatial decomposition and sampling,\nleading to inaccuracies due to fixed and limited resolution. We introduce a\nnovel approach for analytically extracting surfaces from neural implicit\nfunctions. Our method operates natively in parallel and can navigate large\nneural architectures. By leveraging the fact that each neuron partitions the\ndomain, we develop a depth-first traversal strategy to efficiently track the\nencoded surface. The resulting meshes faithfully capture the full geometric\ninformation from the network without ad-hoc spatial discretization, achieving\nunprecedented accuracy across diverse shapes and network architectures while\nmaintaining competitive speed."}
{"id": "2509.21114", "categories": ["cs.GR", "cs.CV"], "pdf": "https://arxiv.org/pdf/2509.21114", "abs": "https://arxiv.org/abs/2509.21114", "authors": ["Yuze He", "Yanning Zhou", "Wang Zhao", "Jingwen Ye", "Yushi Bai", "Kaiwen Xiao", "Yong-Jin Liu", "Zhongqian Sun", "Wei Yang"], "title": "CHARM: Control-point-based 3D Anime Hairstyle Auto-Regressive Modeling", "comment": "SIGGRAPH Asia 2025. 17 pages, 15 figures", "summary": "We present CHARM, a novel parametric representation and generative framework\nfor anime hairstyle modeling. While traditional hair modeling methods focus on\nrealistic hair using strand-based or volumetric representations, anime\nhairstyle exhibits highly stylized, piecewise-structured geometry that\nchallenges existing techniques. Existing works often rely on dense mesh\nmodeling or hand-crafted spline curves, making them inefficient for editing and\nunsuitable for scalable learning. CHARM introduces a compact, invertible\ncontrol-point-based parameterization, where a sequence of control points\nrepresents each hair card, and each point is encoded with only five geometric\nparameters. This efficient and accurate representation supports both\nartist-friendly design and learning-based generation. Built upon this\nrepresentation, CHARM introduces an autoregressive generative framework that\neffectively generates anime hairstyles from input images or point clouds. By\ninterpreting anime hairstyles as a sequential \"hair language\", our\nautoregressive transformer captures both local geometry and global hairstyle\ntopology, resulting in high-fidelity anime hairstyle creation. To facilitate\nboth training and evaluation of anime hairstyle generation, we construct\nAnimeHair, a large-scale dataset of 37K high-quality anime hairstyles with\nseparated hair cards and processed mesh data. Extensive experiments demonstrate\nstate-of-the-art performance of CHARM in both reconstruction accuracy and\ngeneration quality, offering an expressive and scalable solution for anime\nhairstyle modeling. Project page: https://hyzcluster.github.io/charm/"}
