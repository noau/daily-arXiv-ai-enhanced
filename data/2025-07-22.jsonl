{"id": "2507.14624", "categories": ["cs.GR", "cs.CV"], "pdf": "https://arxiv.org/pdf/2507.14624", "abs": "https://arxiv.org/abs/2507.14624", "authors": ["Yaru Liu", "Derek Nowrouzezahri", "Morgan Mcguire"], "title": "Real-Time Scene Reconstruction using Light Field Probes", "comment": null, "summary": "Reconstructing photo-realistic large-scale scenes from images, for example at\ncity scale, is a long-standing problem in computer graphics. Neural rendering\nis an emerging technique that enables photo-realistic image synthesis from\npreviously unobserved viewpoints; however, state-of-the-art neural rendering\nmethods have difficulty efficiently rendering a high complex large-scale scene\nbecause these methods typically trade scene size, fidelity, and rendering speed\nfor quality. The other stream of techniques utilizes scene geometries for\nreconstruction. But the cost of building and maintaining a large set of\ngeometry data increases as scene size grows. Our work explores novel view\nsynthesis methods that efficiently reconstruct complex scenes without explicit\nuse of scene geometries. Specifically, given sparse images of the scene\n(captured from the real world), we reconstruct intermediate, multi-scale,\nimplicit representations of scene geometries. In this way, our method avoids\nexplicitly relying on scene geometry, significantly reducing the computational\ncost of maintaining large 3D data. Unlike current methods, we reconstruct the\nscene using a probe data structure. Probe data hold highly accurate depth\ninformation of dense data points, enabling the reconstruction of highly complex\nscenes. By reconstructing the scene using probe data, the rendering cost is\nindependent of the complexity of the scene. As such, our approach combines\ngeometry reconstruction and novel view synthesis. Moreover, when rendering\nlarge-scale scenes, compressing and streaming probe data is more efficient than\nusing explicit scene geometry. Therefore, our neural representation approach\ncan potentially be applied to virtual reality (VR) and augmented reality (AR)\napplications."}
{"id": "2507.14841", "categories": ["cs.GR", "cs.CV"], "pdf": "https://arxiv.org/pdf/2507.14841", "abs": "https://arxiv.org/abs/2507.14841", "authors": ["Xiang Tang", "Ruotong Li", "Xiaopeng Fan"], "title": "Towards Geometric and Textural Consistency 3D Scene Generation via Single Image-guided Model Generation and Layout Optimization", "comment": "15 pages, 8 figures, Project page: https://xdlbw.github.io/sing3d/", "summary": "In recent years, 3D generation has made great strides in both academia and\nindustry. However, generating 3D scenes from a single RGB image remains a\nsignificant challenge, as current approaches often struggle to ensure both\nobject generation quality and scene coherence in multi-object scenarios. To\novercome these limitations, we propose a novel three-stage framework for 3D\nscene generation with explicit geometric representations and high-quality\ntextural details via single image-guided model generation and spatial layout\noptimization. Our method begins with an image instance segmentation and\ninpainting phase, which recovers missing details of occluded objects in the\ninput images, thereby achieving complete generation of foreground 3D assets.\nSubsequently, our approach captures the spatial geometry of reference image by\nconstructing pseudo-stereo viewpoint for camera parameter estimation and scene\ndepth inference, while employing a model selection strategy to ensure optimal\nalignment between the 3D assets generated in the previous step and the input.\nFinally, through model parameterization and minimization of the Chamfer\ndistance between point clouds in 3D and 2D space, our approach optimizes layout\nparameters to produce an explicit 3D scene representation that maintains\nprecise alignment with input guidance image. Extensive experiments on\nmulti-object scene image sets have demonstrated that our approach not only\noutperforms state-of-the-art methods in terms of geometric accuracy and texture\nfidelity of individual generated 3D models, but also has significant advantages\nin scene layout synthesis."}
{"id": "2507.14920", "categories": ["cs.GR"], "pdf": "https://arxiv.org/pdf/2507.14920", "abs": "https://arxiv.org/abs/2507.14920", "authors": ["Evandro S. Ortigossa", "Fábio F. Dias", "Diego C. Nascimento", "Luis Gustavo Nonato"], "title": "Time Series Information Visualization -- A Review of Approaches and Tools", "comment": "Preprint. Under review", "summary": "Time series data are prevalent across various domains and often encompass\nlarge datasets containing multiple time-dependent features in each sample.\nExploring time-varying data is critical for data science practitioners aiming\nto understand dynamic behaviors and discover periodic patterns and trends.\nHowever, the analysis of such data often requires sophisticated procedures and\ntools. Information visualization is a communication channel that leverages\nhuman perceptual abilities to transform abstract data into visual\nrepresentations. Visualization techniques have been successfully applied in the\ncontext of time series to enhance interpretability by graphically representing\nthe temporal evolution of data. The challenge for information visualization\ndevelopers lies in integrating a wide range of analytical tools into rich\nvisualization systems that can summarize complex datasets while clearly\ndescribing the impacts of the temporal component. Such systems enable data\nscientists to turn raw data into understandable and potentially useful\nknowledge. This review examines techniques and approaches designed for handling\ntime series data, guiding users through knowledge discovery processes based on\nvisual analysis. We also provide readers with theoretical insights and design\nguidelines for considering when developing comprehensive information\nvisualization approaches for time series, with a particular focus on time\nseries with multiple features. As a result, we highlight the challenges and\nfuture research directions to address open questions in the visualization of\ntime-dependent data."}
{"id": "2507.15186", "categories": ["cs.GR"], "pdf": "https://arxiv.org/pdf/2507.15186", "abs": "https://arxiv.org/abs/2507.15186", "authors": ["Dmitry Brodsky", "Benjamin Watson"], "title": "Model Simplification through refinement", "comment": null, "summary": "As modeling and visualization applications proliferate, there arises a need\nto simplify large polygonal models at interactive rates. Unfortunately existing\npolygon mesh simplification algorithms are not well suited for this task\nbecause they are either too slow (requiring the simplified model to be\npre-computed) or produce models that are too poor in quality. These\nshortcomings become particularly acute when models are extremely large. We\npresent an algorithm suitable for simplification of large models at interactive\nspeeds. The algorithm is fast and can guarantee displayable results within a\ngiven time limit. Results also have good quality. Inspired by splitting\nalgorithms from vector quantization literature, we simplify models in reverse,\nbeginning with an extremely coarse approximation and refining it.\nApproximations of surface curvature guide the simplification process.\nPreviously produced simplifications can be further refined by using them as\ninput to the algorithm."}
{"id": "2507.14193", "categories": ["cs.GT", "cs.AI", "cs.CY"], "pdf": "https://arxiv.org/pdf/2507.14193", "abs": "https://arxiv.org/abs/2507.14193", "authors": ["Tori Qiu", "Benjamin Laufer", "Jon Kleinberg", "Hoda Heidari"], "title": "A Formal Model of the Economic Impacts of AI Openness Regulation", "comment": null, "summary": "Regulatory frameworks, such as the EU AI Act, encourage openness of\ngeneral-purpose AI models by offering legal exemptions for \"open-source\"\nmodels. Despite this legislative attention on openness, the definition of\nopen-source foundation models remains ambiguous. This paper models the\nstrategic interactions among the creator of a general-purpose model (the\ngeneralist) and the entity that fine-tunes the general-purpose model to a\nspecialized domain or task (the specialist), in response to regulatory\nrequirements on model openness. We present a stylized model of the regulator's\nchoice of an open-source definition to evaluate which AI openness standards\nwill establish appropriate economic incentives for developers. Our results\ncharacterize market equilibria -- specifically, upstream model release\ndecisions and downstream fine-tuning efforts -- under various openness\nregulations and present a range of effective regulatory penalties and\nopen-source thresholds. Overall, we find the model's baseline performance\ndetermines when increasing the regulatory penalty vs. the open-source threshold\nwill significantly alter the generalist's release strategy. Our model provides\na theoretical foundation for AI governance decisions around openness and\nenables evaluation and refinement of practical open-source policies."}
{"id": "2507.14403", "categories": ["cs.PL"], "pdf": "https://arxiv.org/pdf/2507.14403", "abs": "https://arxiv.org/abs/2507.14403", "authors": ["Sarunas Kalade", "Graham Schelle"], "title": "NPUEval: Optimizing NPU Kernels with LLMs and Open Source Compilers", "comment": null, "summary": "Neural processing units (NPUs) are gaining prominence in power-sensitive\ndevices like client devices, with AI PCs being defined by their inclusion of\nthese specialized processors. Running AI workloads efficiently on these devices\nrequires libraries of optimized kernels. Creating efficient kernels demands\nexpertise in domain-specific C++ with vector intrinsics and in-depth knowledge\nof the target architecture. Unlike GPU programming, which has had years to\nmature, NPU programming is new, with smaller and more fragmented developer\ncommunities across hardware platforms. This fragmentation poses a challenge\nwhen utilizing LLMs to assist in writing NPU kernels, as domain-specific\noptimized code examples are underrepresented in LLM pre-training data.\n  In this paper we introduce NPUEval -- a benchmark for writing and evaluating\nNPU kernels, consisting of 102 common operators for machine learning workloads.\nWe evaluate LLM generated code on actual hardware based on both functional\ncorrectness and vectorization efficiency using open source compiler tools\ntargeting the AMD NPU. We evaluate a range of state-of-the-art LLMs with a mix\nof proprietary and open-weight models. Latest reasoning models like DeepSeek\nR1, show promising results achieving out-of-the-box 50%+ vectorization on\nselect kernels. However, the average score across the entire dataset remains\nroughly 10% even with compiler feedback and vectorized kernel examples --\nshowing that this is a challenging dataset even for frontier models. The\ndataset and evaluation code will be released with a permissive open source\nlicense, providing an essential benchmark for advancing research in code\ngeneration and NPU kernel optimization."}
{"id": "2507.15399", "categories": ["cs.GR", "cs.CV"], "pdf": "https://arxiv.org/pdf/2507.15399", "abs": "https://arxiv.org/abs/2507.15399", "authors": ["Etai Sella", "Noam Atia", "Ron Mokady", "Hadar Averbuch-Elor"], "title": "Blended Point Cloud Diffusion for Localized Text-guided Shape Editing", "comment": "Accepted to ICCV 2025. Project Page:\n  https://tau-vailab.github.io/BlendedPC/", "summary": "Natural language offers a highly intuitive interface for enabling localized\nfine-grained edits of 3D shapes. However, prior works face challenges in\npreserving global coherence while locally modifying the input 3D shape. In this\nwork, we introduce an inpainting-based framework for editing shapes represented\nas point clouds. Our approach leverages foundation 3D diffusion models for\nachieving localized shape edits, adding structural guidance in the form of a\npartial conditional shape, ensuring that other regions correctly preserve the\nshape's identity. Furthermore, to encourage identity preservation also within\nthe local edited region, we propose an inference-time coordinate blending\nalgorithm which balances reconstruction of the full shape with inpainting at a\nprogression of noise levels during the inference process. Our coordinate\nblending algorithm seamlessly blends the original shape with its edited\nversion, enabling a fine-grained editing of 3D shapes, all while circumventing\nthe need for computationally expensive and often inaccurate inversion.\nExtensive experiments show that our method outperforms alternative techniques\nacross a wide range of metrics that evaluate both fidelity to the original\nshape and also adherence to the textual description."}
{"id": "2507.14472", "categories": ["cs.GT", "cs.AI", "cs.MA", "econ.TH"], "pdf": "https://arxiv.org/pdf/2507.14472", "abs": "https://arxiv.org/abs/2507.14472", "authors": ["Yuhang Guo", "Dong Hao", "Bin Li", "Mingyu Xiao", "Bakh Khoussainov"], "title": "Strategyproofness and Monotone Allocation of Auction in Social Networks", "comment": "Accepted by IJCAI 2025", "summary": "Strategyproofness in network auctions requires that bidders not only report\ntheir valuations truthfully, but also do their best to invite neighbours from\nthe social network. In contrast to canonical auctions, where the value-monotone\nallocation in Myerson's Lemma is a cornerstone, a general principle of\nallocation rules for strategyproof network auctions is still missing. We show\nthat, due to the absence of such a principle, even extensions to multi-unit\nnetwork auctions with single-unit demand present unexpected difficulties, and\nall pioneering researches fail to be strategyproof. For the first time in this\nfield, we identify two categories of monotone allocation rules on networks:\nInvitation-Depressed Monotonicity (ID-MON) and Invitation-Promoted Monotonicity\n(IP-MON). They encompass all existing allocation rules of network auctions as\nspecific instances. For any given ID-MON or IP-MON allocation rule, we\ncharacterize the existence and sufficient conditions for the strategyproof\npayment rules, and show that among all such payment rules, the\nrevenue-maximizing one exists and is computationally feasible. With these\nresults, the obstacle of combinatorial network auction with single-minded\nbidders is now resolved."}
{"id": "2507.14471", "categories": ["cs.PL", "cs.DC"], "pdf": "https://arxiv.org/pdf/2507.14471", "abs": "https://arxiv.org/abs/2507.14471", "authors": ["Logan Kenwright", "Partha Roop", "Nathan Allen", "Călin Caşcaval", "Avinash Malik"], "title": "Timetide: A programming model for logically synchronous distributed systems", "comment": "25 Pages, 21 Figures", "summary": "Massive strides in deterministic models have been made using synchronous\nlanguages. They are mainly focused on centralised applications, as the\ntraditional approach is to compile away the concurrency. Time triggered\nlanguages such as Giotto and Lingua Franca are suitable for distribution albeit\nthat they rely on expensive physical clock synchronisation, which is both\nexpensive and may suffer from scalability. Hence, deterministic programming of\ndistributed systems remains challenging. We address the challenges of\ndeterministic distribution by developing a novel multiclock semantics of\nsynchronous programs. The developed semantics is amenable to seamless\ndistribution. Moreover, our programming model, Timetide, alleviates the need\nfor physical clock synchronisation by building on the recently proposed logical\nsynchrony model for distributed systems. We discuss the important aspects of\ndistributing computation, such as network communication delays, and explore the\nformal verification of Timetide programs. To the best of our knowledge,\nTimetide is the first multiclock synchronous language that is both amenable to\ndistribution and formal verification without the need for physical clock\nsynchronisation or clock gating."}
{"id": "2507.15454", "categories": ["cs.GR", "cs.AI", "cs.CV", "cs.HC"], "pdf": "https://arxiv.org/pdf/2507.15454", "abs": "https://arxiv.org/abs/2507.15454", "authors": ["Ruijie Zhu", "Mulin Yu", "Linning Xu", "Lihan Jiang", "Yixuan Li", "Tianzhu Zhang", "Jiangmiao Pang", "Bo Dai"], "title": "ObjectGS: Object-aware Scene Reconstruction and Scene Understanding via Gaussian Splatting", "comment": "Accepted by ICCV 2025", "summary": "3D Gaussian Splatting is renowned for its high-fidelity reconstructions and\nreal-time novel view synthesis, yet its lack of semantic understanding limits\nobject-level perception. In this work, we propose ObjectGS, an object-aware\nframework that unifies 3D scene reconstruction with semantic understanding.\nInstead of treating the scene as a unified whole, ObjectGS models individual\nobjects as local anchors that generate neural Gaussians and share object IDs,\nenabling precise object-level reconstruction. During training, we dynamically\ngrow or prune these anchors and optimize their features, while a one-hot ID\nencoding with a classification loss enforces clear semantic constraints. We\nshow through extensive experiments that ObjectGS not only outperforms\nstate-of-the-art methods on open-vocabulary and panoptic segmentation tasks,\nbut also integrates seamlessly with applications like mesh extraction and scene\nediting. Project page: https://ruijiezhu94.github.io/ObjectGS_page"}
{"id": "2507.14957", "categories": ["cs.GT", "cs.AI", "cs.DS"], "pdf": "https://arxiv.org/pdf/2507.14957", "abs": "https://arxiv.org/abs/2507.14957", "authors": ["Jarosław Byrka", "Franciszek Malinka", "Tomasz Ponitka"], "title": "Probing EFX via PMMS: (Non-)Existence Results in Discrete Fair Division", "comment": "27 pages, 4 figures", "summary": "We study the fair division of indivisible items and provide new insights into\nthe EFX problem, which is widely regarded as the central open question in fair\ndivision, and the PMMS problem, a strictly stronger variant of EFX. Our first\nresult constructs a three-agent instance with two monotone valuations and one\nadditive valuation in which no PMMS allocation exists. Since EFX allocations\nare known to exist under these assumptions, this establishes a formal\nseparation between EFX and PMMS.\n  We prove existence of fair allocations for three important special cases. We\nshow that EFX allocations exist for personalized bivalued valuations, where for\neach agent $i$ there exist values $a_i > b_i$ such that agent $i$ assigns value\n$v_i(\\{g\\}) \\in \\{a_i, b_i\\}$ to each good $g$. We establish an analogous\nexistence result for PMMS allocations when $a_i$ is divisible by $b_i$. We also\nprove that PMMS allocations exist for binary-valued MMS-feasible valuations,\nwhere each bundle $S$ has value $v_i(S) \\in \\{0, 1\\}$. Notably, this result\nholds even without assuming monotonicity of valuations and thus applies to the\nfair division of chores and mixed manna. Finally, we study a class of\nvaluations called pair-demand valuations, which extend the well-studied\nunit-demand valuations to the case where each agent derives value from at most\ntwo items, and we show that PMMS allocations exist in this setting. Our proofs\nare constructive, and we provide polynomial-time algorithms for all three\nexistence results."}
{"id": "2507.15007", "categories": ["cs.PL", "cs.CL"], "pdf": "https://arxiv.org/pdf/2507.15007", "abs": "https://arxiv.org/abs/2507.15007", "authors": ["Sayed Mahbub Hasan Amiri", "Md. Mainul Islam", "Mohammad Shakhawat Hossen", "Sayed Majhab Hasan Amiri", "Mohammad Shawkat Ali Mamun", "Sk. Humaun Kabir", "Naznin Akter"], "title": "Hear Your Code Fail, Voice-Assisted Debugging for Python", "comment": "35 pages, 20 figures", "summary": "This research introduces an innovative voice-assisted debugging plugin for\nPython that transforms silent runtime errors into actionable audible\ndiagnostics. By implementing a global exception hook architecture with pyttsx3\ntext-to-speech conversion and Tkinter-based GUI visualization, the solution\ndelivers multimodal error feedback through parallel auditory and visual\nchannels. Empirical evaluation demonstrates 37% reduced cognitive load (p<0.01,\nn=50) compared to traditional stack-trace debugging, while enabling 78% faster\nerror identification through vocalized exception classification and\ncontextualization. The system achieves sub-1.2 second voice latency with under\n18% CPU overhead during exception handling, vocalizing error types and\nconsequences while displaying interactive tracebacks with documentation deep\nlinks. Criteria validate compatibility across Python 3.7+ environments on\nWindows, macOS, and Linux platforms. Needing only two lines of integration\ncode, the plugin significantly boosts availability for aesthetically impaired\ndesigners and supports multitasking workflows through hands-free error medical\ndiagnosis. Educational applications show particular promise, with pilot studies\nindicating 45% faster debugging skill acquisition among novice programmers.\nFuture development will incorporate GPT-based repair suggestions and real-time\nmultilingual translation to further advance auditory debugging paradigms. The\nsolution represents a fundamental shift toward human-centric error diagnostics,\nbridging critical gaps in programming accessibility while establishing new\nstandards for cognitive efficiency in software development workflows."}
{"id": "2507.15629", "categories": ["cs.GR", "cs.CV"], "pdf": "https://arxiv.org/pdf/2507.15629", "abs": "https://arxiv.org/abs/2507.15629", "authors": ["Zuo-Liang Zhu", "Jian Yang", "Beibei Wang"], "title": "Gaussian Splatting with Discretized SDF for Relightable Assets", "comment": null, "summary": "3D Gaussian splatting (3DGS) has shown its detailed expressive ability and\nhighly efficient rendering speed in the novel view synthesis (NVS) task. The\napplication to inverse rendering still faces several challenges, as the\ndiscrete nature of Gaussian primitives makes it difficult to apply geometry\nconstraints. Recent works introduce the signed distance field (SDF) as an extra\ncontinuous representation to regularize the geometry defined by Gaussian\nprimitives. It improves the decomposition quality, at the cost of increasing\nmemory usage and complicating training. Unlike these works, we introduce a\ndiscretized SDF to represent the continuous SDF in a discrete manner by\nencoding it within each Gaussian using a sampled value. This approach allows us\nto link the SDF with the Gaussian opacity through an SDF-to-opacity\ntransformation, enabling rendering the SDF via splatting and avoiding the\ncomputational cost of ray marching.The key challenge is to regularize the\ndiscrete samples to be consistent with the underlying SDF, as the discrete\nrepresentation can hardly apply the gradient-based constraints (\\eg Eikonal\nloss). For this, we project Gaussians onto the zero-level set of SDF and\nenforce alignment with the surface from splatting, namely a projection-based\nconsistency loss. Thanks to the discretized SDF, our method achieves higher\nrelighting quality, while requiring no extra memory beyond GS and avoiding\ncomplex manually designed optimization. The experiments reveal that our method\noutperforms existing Gaussian-based inverse rendering methods. Our code is\navailable at https://github.com/NK-CS-ZZL/DiscretizedSDF."}
{"id": "2507.15325", "categories": ["cs.GT", "math.OC"], "pdf": "https://arxiv.org/pdf/2507.15325", "abs": "https://arxiv.org/abs/2507.15325", "authors": ["Nicolas Lanzetti", "Sylvain Fricker", "Saverio Bolognani", "Florian Dörfler", "Dario Paccagnan"], "title": "Strategically Robust Game Theory via Optimal Transport", "comment": null, "summary": "In many game-theoretic settings, agents are challenged with taking decisions\nagainst the uncertain behavior exhibited by others. Often, this uncertainty\narises from multiple sources, e.g., incomplete information, limited\ncomputation, bounded rationality. While it may be possible to guide the agents'\ndecisions by modeling each source, their joint presence makes this task\nparticularly daunting. Toward this goal, it is natural for agents to seek\nprotection against deviations around the emergent behavior itself, which is\nultimately impacted by all the above sources of uncertainty. To do so, we\npropose that each agent takes decisions in face of the worst-case behavior\ncontained in an ambiguity set of tunable size, centered at the emergent\nbehavior so implicitly defined. This gives rise to a novel equilibrium notion,\nwhich we call strategically robust equilibrium. Building on its definition, we\nshow that, when judiciously operationalized via optimal transport,\nstrategically robust equilibria (i) are guaranteed to exist under the same\nassumptions required for Nash equilibria; (ii) interpolate between Nash and\nsecurity strategies; (iii) come at no additional computational cost compared to\nNash equilibria. Through a variety of experiments, including bi-matrix games,\ncongestion games, and Cournot competition, we show that strategic robustness\nprotects against uncertainty in the opponents' behavior and, surprisingly,\noften results in higher equilibrium payoffs - an effect we refer to as\ncoordination via robustification."}
{"id": "2507.15017", "categories": ["cs.PL"], "pdf": "https://arxiv.org/pdf/2507.15017", "abs": "https://arxiv.org/abs/2507.15017", "authors": ["Xuran Cai", "Liqian Chen", "Hongfei Fu"], "title": "Invariant Generation for Floating-Point Programs via Constraint Solving", "comment": null, "summary": "In numeric-intensive computations, it is well known that the execution of\nfloating-point programs is imprecise as floating point arithmetics (e.g.,\naddition, subtraction, multiplication, division, etc.) incurs rounding errors.\nAlbeit the rounding error is small for every single floating-point operation,\nthe aggregation of such error in multiple operations may be dramatic and cause\ncatastrophic program failures. Therefore, to ensure the correctness of\nfloating-point programs, the effect of floating point error needs to be\ncarefully taken into account. In this work, we consider the invariant\ngeneration for floating point programs, whose aim is to generate tight\ninvariants under the perturbation of floating point errors. Our main\ncontribution is a theoretical framework on how to apply constraint solving\nmethods to address the invariant generation problem. In our framework, we\npropose a novel combination between the first-order differential\ncharacterization by FPTaylor (TOPLAS 2018) and constraint solving methods,\naiming to reduce the computational burden of constraint solving. Moreover, we\ndevise two polynomial invariant generation algorithms to instantiate the\nframework. The first algorithm is applicable to a wide range of floating-point\noperations but requires an initial (coarse) invariant as external input, while\nthe second does not require an initial invariant but is limited to polynomial\nprograms. Furthermore, we show how conditional branches, a difficult issue in\nfloating-point analysis, can be handled in our framework. Experimental results\nshow that our algorithms outperform SOTA approaches in both the time efficiency\nand the precision of the generated invariants over a variety of benchmarks."}
{"id": "2507.15735", "categories": ["cs.GT"], "pdf": "https://arxiv.org/pdf/2507.15735", "abs": "https://arxiv.org/abs/2507.15735", "authors": ["Sergiu Hart", "Noam Nisan"], "title": "The Root of Revenue Continuity", "comment": null, "summary": "In the setup of selling one or more goods, various papers have shown, in\nvarious forms and for various purposes, that a small change in the distribution\nof a buyer's valuations may cause only a small change in the possible revenue\nthat can be extracted. We prove a simple, clean, convenient, and general\nstatement to this effect: let X and Y be random valuations on k additive goods,\nand let W(X,Y) be the Wasserstein (or \"earth mover's\") distance between them;\nthen sqrt(Rev(X))-sqrt(Rev(Y)) <= sqrt(W(X,Y)). This further implies that a\nsimple explicit modification of any optimal mechanism for X, namely, \"uniform\ndiscounting\", is guaranteed to be almost optimal for any Y that is close to X\nin the Wasserstein distance."}
{"id": "2507.15277", "categories": ["cs.PL", "D.3.4"], "pdf": "https://arxiv.org/pdf/2507.15277", "abs": "https://arxiv.org/abs/2507.15277", "authors": ["Robert Hochgraf", "Sreepathi Pai"], "title": "A Few Fit Most: Improving Performance Portability of SGEMM on GPUs using Multi-Versioning", "comment": "13 pages, 8 figures", "summary": "Hand-optimizing linear algebra kernels for different GPU devices and\napplications is complex and labor-intensive. Instead, many developers use\nautomatic performance tuning (autotuning) to achieve high performance on a\nvariety of devices. However, autotuning \"overfits\", and must be redone if any\npart of the environment changes, such as if the device or input characteristics\nchange.\n  In most non-trivial cases, a single compute kernel cannot maintain\nnear-optimal performance across all environments. Changing the kernel to\nspecialize it to the current execution environment is possible, but on GPUs,\nruntime tuning and compilation can be expensive.\n  In this work, we use multi-versioning -- producing several variants of the\nsame code -- as a way to generate performance portable code. We describe a\nframework called portability tuning that can automatically generate\nmulti-versioned code whose performance is portable, requiring no retuning.\n  We evaluate our framework on a dataset of execution times for GEMM kernels\nfrom the CLBlast linear algebra library. We find our portability tuning\ntechniques outperform CLBlast's default kernels -- often approaching within 10%\nof the theoretical maximum performance -- despite CLBlast using autotuning\ntechniques. Further, we find that our generated programs generalize well to new\nand unseen devices, matching the performance of autotuning without ever\nportability tuning for those devices."}
{"id": "2507.15737", "categories": ["cs.GT"], "pdf": "https://arxiv.org/pdf/2507.15737", "abs": "https://arxiv.org/abs/2507.15737", "authors": ["Felipe Garrido-Lucero", "Rida Laraki"], "title": "General Matching Games", "comment": null, "summary": "Matching games is a one-to-one two sided market model introduced by\nGarrido-Lucero and Laraki, in which coupled agents' utilities are endogenously\ndetermined as the outcome of a strategic game. They refine the classical\npairwise stability by requiring robustness to renegotiation and provide general\nconditions under which pairwise stable and renegotiation-proof outcomes exist\nas the limit of a deferred acceptance with competitions algorithm together with\na renegotiation process. In this article, we extend their model to a general\nsetting encompassing most of one-to-many matching markets and roommates models\nand specify two frameworks under which core stable and renegotiation-proof\noutcomes exist and can be efficiently computed."}
{"id": "2507.15530", "categories": ["cs.PL", "cs.LO", "F.3.1; F.3.2"], "pdf": "https://arxiv.org/pdf/2507.15530", "abs": "https://arxiv.org/abs/2507.15530", "authors": ["Shing Hin Ho", "Nicolas Wu", "Azalea Raad"], "title": "Bayesian Separation Logic", "comment": null, "summary": "Bayesian probabilistic programming languages (BPPLs) let users denote\nstatistical models as code while the interpreter infers the posterior\ndistribution. The semantics of BPPLs are usually mathematically complex and\nunable to reason about desirable properties such as expected values and\nindependence of random variables. To reason about these properties in a\nnon-Bayesian setting, probabilistic separation logics such as PSL and Lilac\ninterpret separating conjunction as probabilistic independence of random\nvariables. However, no existing separation logic can handle Bayesian updating,\nwhich is the key distinguishing feature of BPPLs.\n  To close this gap, we introduce Bayesian separation logic (BaSL), a\nprobabilistic separation logic that gives semantics to BPPL. We prove an\ninternal version of Bayes' theorem using a result in measure theory known as\nthe Rokhlin-Simmons disintegration theorem. Consequently, BaSL can model\nprobabilistic programming concepts such as Bayesian updating, unnormalised\ndistribution, conditional distribution, soft constraint, conjugate prior and\nimproper prior while maintaining modularity via the frame rule. The model of\nBaSL is based on a novel instantiation of Kripke resource monoid via\n$\\sigma$-finite measure spaces over the Hilbert cube, and the semantics of\nHoare triple is compatible with an existing denotational semantics of BPPL\nbased on the category of $s$-finite kernels. Using BaSL, we then prove\nproperties of statistical models such as the expected value of Bayesian coin\nflip, correlation of random variables in the collider Bayesian network, and the\nposterior distributions of the burglar alarm model, a parameter estimation\nalgorithm, and the Gaussian mixture model."}
{"id": "2507.15596", "categories": ["cs.PL", "cs.LO"], "pdf": "https://arxiv.org/pdf/2507.15596", "abs": "https://arxiv.org/abs/2507.15596", "authors": ["Jaeseo Lee", "Kyungmin Bae"], "title": "Formal Analysis of Networked PLC Controllers Interacting with Physical Environments", "comment": "To appear in Proceedings of the Static Analysis Symposium (SAS) 2025", "summary": "Programmable Logic Controllers (PLCs) are widely used in industrial\nautomation to control physical systems. As PLC applications become increasingly\ncomplex, ensuring their correctness is crucial. Existing formal verification\ntechniques focus on individual PLC programs in isolation, often neglecting\ninteractions with physical environments and network communication between\ncontrollers. This limitation poses significant challenges in analyzing\nreal-world industrial systems, where continuous dynamics and communication\ndelays play a critical role. In this paper, we present a unified formal\nframework that integrates discrete PLC semantics, networked communication, and\ncontinuous physical behaviors. To mitigate state explosion, we apply partial\norder reduction, significantly reducing the number of explored states while\nmaintaining correctness. Our framework enables precise analysis of PLC-driven\nsystems with continuous dynamics and networked communication."}
{"id": "2507.15843", "categories": ["cs.PL", "D.3.1; F.3.1; F.3.2; D.2.4"], "pdf": "https://arxiv.org/pdf/2507.15843", "abs": "https://arxiv.org/abs/2507.15843", "authors": ["Beniamino Accattoli", "Dan Ghica", "Giulio Guerrieri", "Cláudio Belo Lourenço", "Claudio Sacerdoti Coen"], "title": "Closure Conversion, Flat Environments, and the Complexity of Abstract Machines", "comment": null, "summary": "Closure conversion is a program transformation at work in compilers for\nfunctional languages to turn inner functions into global ones, by building\nclosures pairing the transformed functions with the environment of their free\nvariables. Abstract machines rely on similar and yet different concepts of\nclosures and environments.\n  In this paper, we study the relationship between the two approaches. We adopt\na very simple {\\lambda}-calculus with tuples as source language and study\nabstract machines for both the source language and the target of closure\nconversion. Moreover, we focus on the simple case of flat\nclosures/environments, that is, with no sharing of environments. We provide\nthree contributions.\n  Firstly, a new simple proof technique for the correctness of closure\nconversion, inspired by abstract machines.\n  Secondly, we show how the closure invariants of the target language allow us\nto design a new way of handling environments in abstract machines, not\nsuffering the shortcomings of other styles.\n  Thirdly, we study the machines from the point of view of time complexity,\nadapting analyses by Accattoli and co-authors. We show that closure conversion\ndecreases various dynamic costs while increasing the size of the initial code.\nDespite these changes, the overall complexity of the machines before and after\nclosure conversion turns out to be the same."}
