{"id": "2510.09814", "categories": ["cs.GT"], "pdf": "https://arxiv.org/pdf/2510.09814", "abs": "https://arxiv.org/abs/2510.09814", "authors": ["Emile Martinez", "Felipe Garrido-Lucero", "Umberto Grandi"], "title": "Stability in Online Assignment Games", "comment": null, "summary": "The assignment game models a housing market where buyers and sellers are\nmatched, and transaction prices are set so that the resulting allocation is\nstable. Shapley and Shubik showed that every stable allocation is necessarily\nbuilt on a maximum social welfare matching. In practice, however, stable\nallocations are rarely attainable, as matchings are often sub-optimal,\nparticularly in online settings where eagents arrive sequentially to the\nmarket. In this paper, we introduce and compare two complementary measures of\ninstability for allocations with sub-optimal matchings, establish their\nconnections to the optimality ratio of the underlying matching, and use this\nframework to study the stability performances of randomized algorithms in\nonline assignment games."}
{"id": "2510.10335", "categories": ["cs.GT"], "pdf": "https://arxiv.org/pdf/2510.10335", "abs": "https://arxiv.org/abs/2510.10335", "authors": ["Jugal Garg", "Eklavya Sharma", "Xiaowei Wu"], "title": "Proportional and Pareto-Optimal Allocation of Chores with Subsidy", "comment": null, "summary": "We consider the problem of allocating $m$ indivisible chores among $n$ agents\nwith possibly different weights, aiming for a solution that is both fair and\nefficient. Specifically, we focus on the classic fairness notion of\nproportionality and efficiency notion of Pareto-optimality. Since proportional\nallocations may not always exist in this setting, we allow the use of subsidies\n(monetary compensation to agents) to ensure agents are\nproportionally-satisfied, and aim to minimize the total subsidy required. Wu\nand Zhou (WINE 2024) showed that when each chore has disutility at most 1, a\ntotal subsidy of at most $n/3 - 1/6$ is sufficient to guarantee\nproportionality. However, their approach is based on a complex technique, which\ndoes not guarantee economic efficiency - a key desideratum in fair division.\n  In this work, we give a polynomial-time algorithm that achieves the same\nsubsidy bound while also ensuring Pareto-optimality. Moreover, both our\nalgorithm and its analysis are significantly simpler than those of Wu and Zhou\n(WINE 2024). Our approach first computes a proportionally-fair competitive\nequilibrium, and then applies a rounding procedure guided by\nminimum-pain-per-buck edges."}
{"id": "2510.10423", "categories": ["cs.GT"], "pdf": "https://arxiv.org/pdf/2510.10423", "abs": "https://arxiv.org/abs/2510.10423", "authors": ["Ehsan Heidari", "Alireza Kaviani", "Masoud Seddighin", "AmirMohammad Shahrezaei"], "title": "Improved Maximin Share Guarantee for Additive Valuations", "comment": null, "summary": "The maximin share ($\\textsf{MMS}$) is the most prominent share-based fairness\nnotion in the fair allocation of indivisible goods. Recent years have seen\nsignificant efforts to improve the approximation guarantees for $\\textsf{MMS}$\nfor different valuation classes, particularly for additive valuations. For the\nadditive setting, it has been shown that for some instances, no allocation can\nguarantee a factor better than $1-\\tfrac{1}{n^4}$ of maximin share value to all\nagents. However, the best currently known algorithm achieves an approximation\nguarantee of $\\tfrac{3}{4} + \\tfrac{3}{3836}$ for $\\textsf{MMS}$. In this work,\nwe narrow this gap and improve the best-known approximation guarantee for\n$\\textsf{MMS}$ to $\\tfrac{10}{13}$."}
{"id": "2510.10698", "categories": ["cs.GT"], "pdf": "https://arxiv.org/pdf/2510.10698", "abs": "https://arxiv.org/abs/2510.10698", "authors": ["Masoud Seddighin", "Saeed Seddighin"], "title": "Fair Assignment of Indivisible Chores to Asymmetric Agents", "comment": null, "summary": "We consider the problem of assigning indivisible chores to agents with\ndifferent entitlements in the maximin share value (\\MMS) context. While\nconstant-\\MMS\\ allocations/assignments are guaranteed to exist for both goods\nand chores in the symmetric setting, the situation becomes much more complex\nwhen agents have different entitlements. For the allocation of indivisible\ngoods, it has been proven that an $n$-\\WMMS\\ (weighted \\MMS) guarantee is the\nbest one can hope for. For indivisible chores, however, it was recently\ndiscovered that an $O(\\log n)$-\\WMMS\\ assignment is guaranteed to exist. In\nthis work, we improve this upper bound to a constant-\\WMMS\\\nguarantee.\\footnote{We prove the existence of a 20-\\WMMS\\ assignment, but we\ndid not attempt to optimize the constant factor. We believe our methods already\nyield a slightly better bound with a tighter analysis.}"}
{"id": "2510.09997", "categories": ["cs.GR", "cs.CV"], "pdf": "https://arxiv.org/pdf/2510.09997", "abs": "https://arxiv.org/abs/2510.09997", "authors": ["Zhigang Cheng", "Mingchao Sun", "Yu Liu", "Zengye Ge", "Luyang Tang", "Mu Xu", "Yangyan Li", "Peng Pan"], "title": "CLoD-GS: Continuous Level-of-Detail via 3D Gaussian Splatting", "comment": null, "summary": "Level of Detail (LoD) is a fundamental technique in real-time computer\ngraphics for managing the rendering costs of complex scenes while preserving\nvisual fidelity. Traditionally, LoD is implemented using discrete levels\n(DLoD), where multiple, distinct versions of a model are swapped out at\ndifferent distances. This long-standing paradigm, however, suffers from two\nmajor drawbacks: it requires significant storage for multiple model copies and\ncauses jarring visual ``popping\" artifacts during transitions, degrading the\nuser experience. We argue that the explicit, primitive-based nature of the\nemerging 3D Gaussian Splatting (3DGS) technique enables a more ideal paradigm:\nContinuous LoD (CLoD). A CLoD approach facilitates smooth, seamless quality\nscaling within a single, unified model, thereby circumventing the core problems\nof DLOD. To this end, we introduce CLoD-GS, a framework that integrates a\ncontinuous LoD mechanism directly into a 3DGS representation. Our method\nintroduces a learnable, distance-dependent decay parameter for each Gaussian\nprimitive, which dynamically adjusts its opacity based on viewpoint proximity.\nThis allows for the progressive and smooth filtering of less significant\nprimitives, effectively creating a continuous spectrum of detail within one\nmodel. To train this model to be robust across all distances, we introduce a\nvirtual distance scaling mechanism and a novel coarse-to-fine training strategy\nwith rendered point count regularization. Our approach not only eliminates the\nstorage overhead and visual artifacts of discrete methods but also reduces the\nprimitive count and memory footprint of the final model. Extensive experiments\ndemonstrate that CLoD-GS achieves smooth, quality-scalable rendering from a\nsingle model, delivering high-fidelity results across a wide range of\nperformance targets."}
{"id": "2510.09726", "categories": ["cs.PL", "cs.AI", "cs.SE"], "pdf": "https://arxiv.org/pdf/2510.09726", "abs": "https://arxiv.org/abs/2510.09726", "authors": ["Tilman Hinnerichs", "Reuben Gardos Reid", "Jaap de Jong", "Bart Swinkels", "Pamela Wochner", "Nicolae Filat", "Tudor Magurescu", "Issa Hanou", "Sebastijan Dumancic"], "title": "Herb.jl: A Unifying Program Synthesis Library", "comment": null, "summary": "Program synthesis -- the automatic generation of code given a specification\n-- is one of the most fundamental tasks in artificial intelligence (AI) and\nmany programmers' dream. Numerous synthesizers have been developed to tackle\nprogram synthesis, manifesting different ideas to approach the exponentially\ngrowing program space. While numerous smart program synthesis tools exist,\nreusing and remixing previously developed methods is tedious and\ntime-consuming. We propose Herb.jl, a unifying program synthesis library\nwritten in the Julia programming language, to address these issues. Since\ncurrent methods rely on similar building blocks, we aim to modularize the\nunderlying synthesis algorithm into communicating and fully extendable\nsub-compartments, allowing for straightforward reapplication of these modules.\nTo demonstrate the benefits of using Herb.jl, we show three common use cases:\n1. how to implement a simple problem and grammar, and how to solve it, 2. how\nto implement a previously developed synthesizer with just a few lines of code,\nand 3. how to run a synthesizer against a benchmark."}
{"id": "2510.10929", "categories": ["cs.GT"], "pdf": "https://arxiv.org/pdf/2510.10929", "abs": "https://arxiv.org/abs/2510.10929", "authors": ["Junjie Luo", "Changjun Wang"], "title": "Achieving Coordination in Non-Cooperative Joint Replenishment Games", "comment": null, "summary": "We analyze an infinite-horizon deterministic joint replenishment model from a\nnon-cooperative game-theoretical approach. In this model, a group of retailers\ncan choose to jointly place an order, which incurs a major setup cost\nindependent of the group, and a minor setup cost for each retailer.\nAdditionally, each retailer is associated with a holding cost. Our objective is\nto design cost allocation rules that minimize the long-run average system cost\nwhile accounting for the fact that each retailer independently selects its\nreplenishment interval to minimize its own cost. We introduce a class of cost\nallocation rules that distribute the major setup cost among the associated\nretailers in proportion to their predefined weights. For these rules, we\nestablish a monotonicity property of agent better responses, which enables us\nto prove the existence of a payoff dominant pure Nash equilibrium that can also\nbe computed efficiently. We then analyze the efficiency of these equilibria by\nexamining the price of stability (PoS), the ratio of the best Nash\nequilibrium's system cost to the social optimum, across different information\nsettings. In particular, our analysis reveals that one rule, which leverages\nretailers' own holding cost rates, achieves a near-optimal PoS of 1.25, while\nanother rule that does not require access to retailers' private information\nalso yields a favorable PoS."}
{"id": "2510.10218", "categories": ["cs.GR"], "pdf": "https://arxiv.org/pdf/2510.10218", "abs": "https://arxiv.org/abs/2510.10218", "authors": ["Gaurav Rai", "Ojaswa Sharma"], "title": "Sketch Animation: State-of-the-art Report", "comment": null, "summary": "Sketch animation has emerged as a transformative technology, bridging art and\nscience to create dynamic visual narratives across various fields such as\nentertainment, education, healthcare, and virtual reality. This survey explores\nrecent trends and innovations in sketch animation, with a focus on methods that\nhave advanced the state of the art. The paper categorizes and evaluates key\nmethodologies, including keyframe interpolation, physics-based animation,\ndata-driven, motion capture, and deep learning approaches. We examine the\nintegration of artificial intelligence, real-time rendering, and cloud-based\nsolutions, highlighting their impact on enhancing realism, scalability, and\ninteractivity. Additionally, the survey delves into the challenges of\ncomputational complexity, scalability, and user-friendly interfaces, as well as\nemerging opportunities within metaverse applications and human-machine\ninteraction. By synthesizing insights from a wide array of research, this\nsurvey aims to provide a comprehensive understanding of the current landscape\nand future directions of sketch animation, serving as a resource for both\nacademics and industry professionals seeking to innovate in this dynamic field."}
{"id": "2510.09932", "categories": ["cs.PL", "cs.AR"], "pdf": "https://arxiv.org/pdf/2510.09932", "abs": "https://arxiv.org/abs/2510.09932", "authors": ["Devansh Jain", "Akash Pardeshi", "Marco Frigo", "Krut Patel", "Kaustubh Khulbe", "Jai Arora", "Charith Mendis"], "title": "ACT: Automatically Generating Compiler Backends from Tensor Accelerator ISA Descriptions", "comment": null, "summary": "Tensor compilers play a key role in enabling high-performance implementations\nof deep learning workloads. These compilers rely on existing CPU and GPU code\ngeneration backends to generate device-specific code. Recently, many tensor\naccelerators (neural processing units) have been proposed to further accelerate\nthese workloads. Compared to commodity hardware, however, most of the proposed\ntensor accelerators do not have compiler backends with code generation support.\nMoreover, the accelerator designs are subject to fast iteration cycles, making\nit difficult to manually develop compiler backends similar to commodity\nhardware platforms. Therefore, to increase adoption and enable faster software\ndevelopment cycles for novel tensor accelerator designs, we need to make the\ncompiler backend construction process more agile.\n  To address this gap, we introduce ACT, a compiler backend generator that\nautomatically generates compiler backends for tensor accelerators, given just\nthe instruction set architecture (ISA) descriptions. We first formally specify\nthe compiler backend generation problem that introduces a novel specification\nfor describing tensor accelerator ISAs. Next, we design ACT such that it\nsupports user-programmable memories and complex parameterized instructions that\nare prevalent in tensor accelerators. ACT uses a novel parameterized equality\nsaturation-based instruction selection phase and a constraint programming-based\nmemory allocation phase. We prove that compiler backends generated by ACT are\nsound and complete. Finally, we generate compiler backends for three\naccelerator platforms from industry and academia, and show that they match or\noutperform code written using hand-optimized kernel libraries while maintaining\nlow compilation overheads."}
{"id": "2510.11253", "categories": ["cs.GT"], "pdf": "https://arxiv.org/pdf/2510.11253", "abs": "https://arxiv.org/abs/2510.11253", "authors": ["Sayantika Mandal", "Harman Agrawal", "Swaprava Nath"], "title": "Likes, Budgets, and Equilibria: Designing Contests for Socially Optimal Advertising", "comment": "26 pages, under review", "summary": "Firms (businesses, service providers, entertainment organizations, political\nparties, etc.) advertise on social networks to draw people's attention and\nimprove their awareness of the brands of the firms. In all such cases, the\ncompetitive nature of their engagements gives rise to a game where the firms\nneed to decide how to distribute their budget over the agents on a network to\nmaximize their brand's awareness. The firms (players) therefore need to\noptimize how much budget they should put on the vertices of the network so that\nthe spread improves via direct (via advertisements or free promotional offers)\nand indirect marketing (words-of-mouth). We propose a two-timescale model of\ndecisions where the communication between the vertices happen in a faster\ntimescale and the strategy update of the firms happen in a slower timescale. We\nshow that under fairly standard conditions, the best response dynamics of the\nfirms converge to a pure strategy Nash equilibrium. However, such equilibria\ncan be away from a socially optimal one. We provide a characterization of the\ncontest success functions and provide examples for the designers of such\ncontests (e.g., regulators, social network providers, etc.) such that the Nash\nequilibrium becomes unique and social welfare maximizing. Our experiments show\nthat for realistic scenarios, such contest success functions perform fairly\nwell."}
{"id": "2510.10256", "categories": ["cs.GR"], "pdf": "https://arxiv.org/pdf/2510.10256", "abs": "https://arxiv.org/abs/2510.10256", "authors": ["Gonzalo Gomez-Nogales", "Zhen Chen", "Rosalie Martin", "Elena Garces", "Danny M. Kaufman"], "title": "Unlocking Thickness Modeling for Codimensional Contact Simulation", "comment": null, "summary": "In this work we analyze and address a fundamental restriction that blocks the\nreliable application of codimensional yarn-level and shell models with\nthickness, to simulate real-world woven and knit fabrics. As discretizations\nrefine toward practical and accurate physical modeling, such models can\ngenerate non-physical contact forces with stencil-neighboring elements in the\nsimulation mesh, leading to severe locking artifacts. While not well-documented\nin the literature, this restriction has so far been addressed with two\nalternatives with undesirable tradeoffs. One option is to restrict the mesh to\ncoarse resolutions, however, this eliminates the possibility of accurate (and\nconsistent) resolution simulations across real-world material variations. A\nsecond alternative instead seeks to cull contact pairs that can create such\nlocking forces in the first place. This relaxes resolution restrictions but\ncompromise robustness. Culling can and will generate unacceptable and\nunpredictable geometric intersections and tunneling that destroys weaving and\nknitting structures and cause unrecoverable pull-throughs. We address these\nchallenges to simulating real-world materials with a new and practical\ncontact-processing model for thickened codimensional simulation, that removes\nresolution restrictions, while guaranteeing contact-locking-free,\nnon-intersecting simulations. We demonstrate the application of our model\nacross a wide range of previously unavailable simulation scenarios, with\nreal-world material yarn and fabric parameters and patterns, challenging\nsimulation conditions and mesh resolutions, and both rod and shell models,\nintegrated with the IPC barrier."}
{"id": "2510.10015", "categories": ["cs.PL"], "pdf": "https://arxiv.org/pdf/2510.10015", "abs": "https://arxiv.org/abs/2510.10015", "authors": ["Jinhua Wu", "Yuting Wang", "Liukun Yu", "Linglong Meng"], "title": "End-to-end Compositional Verification of Program Safety through Verified and Verifying Compilation", "comment": null, "summary": "Program safety (i.e., absence of undefined behaviors) is critical for correct\noperation of computer systems. It is usually verified at the source level\n(e.g., by separation logics) and preserved to the target by verified compilers\n(e.g., CompCert), thereby achieving end-to-end verification of safety. However,\nmodern safe programming languages like Rust pose new problems in achieving\nend-to-end safety. Because not all functionalities can be implemented in the\nsafe language, mixing safe and unsafe modules is needed. Therefore, verified\ncompilation must preserve a modular notion of safety which can be composed at\nthe target level. Furthermore, certain classes of errors (e.g., memory errors)\nare automatically excluded by verifying compilation (e.g., borrow checking) for\nmodules written in safe languages. As a result, verified compilation needs to\ncooperate with verifying compilation to ensure end-to-end safety.\n  To address the above problems, we propose a modular and generic definition of\nsafety called open safety based on program semantics described as open labeled\ntransition systems (LTS). Open safety is composable at the boundary of modules\nand can be modularly preserved by verified compositional compilation. Those\nproperties enable separate verification of safety for heterogeneous modules and\ncomposition of the safety results at the target level. Open safety can be\ngeneralized to partial safety (i.e., only a certain class of errors can occur).\nBy this we formalized the correctness of verifying compilation as derivation of\ntotal safety from partial safety. We demonstrate how our framework can combine\nverified and verifying compilation by developing a verified compiler for an\nownership language (called Owlang) inspired by Rust. We evaluate our approach\non the compositional safety verification using a hash map implemented by Owlang\nand C."}
{"id": "2510.11255", "categories": ["cs.GT"], "pdf": "https://arxiv.org/pdf/2510.11255", "abs": "https://arxiv.org/abs/2510.11255", "authors": ["Ashwin Goyal", "Drashthi Doshi", "Swaprava Nath"], "title": "Temporal Cooperative Games", "comment": "21 pages, under review", "summary": "Classical cooperative game theory assumes that the worth of a coalition\ndepends only on the set of agents involved, but in practice, it may also depend\non the order in which agents arrive. Motivated by such scenarios, we introduce\ntemporal cooperative games (TCG), where the worth $v$ becomes a function of the\nsequence of agents $\\pi$ rather than just the set $S$. This shift calls for\nrethinking the underlying axioms. A key property in this temporal framework is\nthe incentive for optimal arrival (I4OA), which encourages agents to join in\nthe order maximizing total worth. Alongside, we define two additional\nproperties: online individual rationality (OIR), incentivizing earlier agents\nto invite more participants, and sequential efficiency (SE), ensuring that the\ntotal worth of any sequence is fully distributed among its agents. We identify\na class of reward-sharing mechanisms uniquely characterized by these three\nproperties. The classical Shapley value does not directly apply here, so we\nconstruct its natural analogs in two variants: the sequential world, where\nrewards are defined for each sequence-player pair, and the extended world,\nwhere rewards are defined for each player alone. Properties of efficiency,\nadditivity, and null player uniquely determine these Shapley analogs in both\nworlds. Importantly, the Shapley analogs are disjoint from mechanisms\nsatisfying I4OA, OIR, and SE, and this conflict persists even for restricted\nclasses such as convex and simple TCGs. Our findings thus uncover a fundamental\ntension: when players arrive sequentially, reward-sharing mechanisms satisfying\ndesirable temporal properties must inherently differ from Shapley-inspired\nones, opening new questions for defining fair and efficient solution concepts\nin TCGs."}
{"id": "2510.10581", "categories": ["cs.GR"], "pdf": "https://arxiv.org/pdf/2510.10581", "abs": "https://arxiv.org/abs/2510.10581", "authors": ["Heng Zhang", "Yuling Shi", "Xiaodong Gu", "Haochen You", "Zijian Zhang", "Lubin Gan", "Yilei Yuan", "Jin Huang"], "title": "GraphTracer: Graph-Guided Failure Tracing in LLM Agents for Robust Multi-Turn Deep Search", "comment": null, "summary": "Multi-agent systems powered by Large Language Models excel at complex tasks\nthrough coordinated collaboration, yet they face high failure rates in\nmulti-turn deep search scenarios. Existing temporal attribution methods\nstruggle to accurately diagnose root causes, particularly when errors propagate\nacross multiple agents. Attempts to automate failure attribution by analyzing\naction sequences remain ineffective due to their inability to account for\ninformation dependencies that span agents. This paper identifies two core\nchallenges: \\textit{(i) distinguishing symptoms from root causes in multi-agent\nerror propagation}, and \\textit{(ii) tracing information dependencies beyond\ntemporal order}. To address these issues, we introduce \\textbf{GraphTracer}, a\nframework that redefines failure attribution through information flow analysis.\nGraphTracer constructs Information Dependency Graphs (IDGs) to explicitly\ncapture how agents reference and build on prior outputs. It localizes root\ncauses by tracing through these dependency structures instead of relying on\ntemporal sequences. GraphTracer also uses graph-aware synthetic data generation\nto target critical nodes, creating realistic failure scenarios. Evaluations on\nthe Who\\&When benchmark and integration into production systems demonstrate\nthat GraphTracer-8B achieves up to 18.18\\% higher attribution accuracy compared\nto state-of-the-art models and enables 4.8\\% to 14.2\\% performance improvements\nin deployed multi-agent frameworks, establishing a robust solution for\nmulti-agent system debugging."}
{"id": "2510.10209", "categories": ["cs.PL", "cs.LG", "cs.PF"], "pdf": "https://arxiv.org/pdf/2510.10209", "abs": "https://arxiv.org/abs/2510.10209", "authors": ["Massinissa Merouani", "Afif Boudaoud", "Riyadh Baghdadi"], "title": "LOOPerSet: A Large-Scale Dataset for Data-Driven Polyhedral Compiler Optimization", "comment": null, "summary": "The advancement of machine learning for compiler optimization, particularly\nwithin the polyhedral model, is constrained by the scarcity of large-scale,\npublic performance datasets. This data bottleneck forces researchers to\nundertake costly data generation campaigns, slowing down innovation and\nhindering reproducible research learned code optimization. To address this gap,\nwe introduce LOOPerSet, a new public dataset containing 28 million labeled data\npoints derived from 220,000 unique, synthetically generated polyhedral\nprograms. Each data point maps a program and a complex sequence of\nsemantics-preserving transformations (such as fusion, skewing, tiling, and\nparallelism)to a ground truth performance measurement (execution time). The\nscale and diversity of LOOPerSet make it a valuable resource for training and\nevaluating learned cost models, benchmarking new model architectures, and\nexploring the frontiers of automated polyhedral scheduling. The dataset is\nreleased under a permissive license to foster reproducible research and lower\nthe barrier to entry for data-driven compiler optimization."}
{"id": "2510.11550", "categories": ["cs.GT", "cs.CC"], "pdf": "https://arxiv.org/pdf/2510.11550", "abs": "https://arxiv.org/abs/2510.11550", "authors": ["Kristoffer Arnsfelt Hansen", "Xinhao Nie"], "title": "On the Complexity of Stationary Nash Equilibria in Discounted Perfect Information Stochastic Games", "comment": null, "summary": "We study the problem of computing stationary Nash equilibria in discounted\nperfect information stochastic games from the viewpoint of computational\ncomplexity. For two-player games we prove the problem to be in PPAD, which\ntogether with a previous PPAD-hardness result precisely classifies the problem\nas PPAD-complete. In addition to this we give an improved and simpler\nPPAD-hardness proof for computing a stationary epsilon-Nash equilibrium. For\n3-player games we construct games showing that rational-valued stationary Nash\nequilibria are not guaranteed to exist, and we use these to prove\nSqrtSum-hardness of computing a stationary Nash equilibrium in 4-player games."}
{"id": "2510.10585", "categories": ["cs.GR"], "pdf": "https://arxiv.org/pdf/2510.10585", "abs": "https://arxiv.org/abs/2510.10585", "authors": ["Heng Zhang", "Yuling Shi", "Xiaodong Gu", "Haochen You", "Zijian Zhang", "Lubin Gan", "Yilei Yuan", "Jin Huang"], "title": "D3MAS: Decompose, Deduce, and Distribute for Enhanced Knowledge Sharing in Multi-Agent Systems", "comment": null, "summary": "Multi-agent systems powered by large language models exhibit strong\ncapabilities in collaborative problem-solving. However, these systems suffer\nfrom substantial knowledge redundancy. Agents duplicate efforts in retrieval\nand reasoning processes. This inefficiency stems from a deeper issue: current\narchitectures lack mechanisms to ensure agents share minimal sufficient\ninformation at each operational stage. Empirical analysis reveals an average\nknowledge duplication rate of 47.3\\% across agent communications. We propose\nD3MAS (Decompose, Deduce, and Distribute), a hierarchical coordination\nframework addressing redundancy through structural design rather than explicit\noptimization. The framework organizes collaboration across three coordinated\nlayers. Task decomposition filters irrelevant sub-problems early. Collaborative\nreasoning captures complementary inference paths across agents. Distributed\nmemory provides access to non-redundant knowledge. These layers coordinate\nthrough structured message passing in a unified heterogeneous graph. This\ncross-layer alignment ensures information remains aligned with actual task\nneeds. Experiments on four challenging datasets show that D3MAS consistently\nimproves reasoning accuracy by 8.7\\% to 15.6\\% and reduces knowledge redundancy\nby 46\\% on average."}
{"id": "2510.10216", "categories": ["cs.PL", "cs.AI", "cs.SE"], "pdf": "https://arxiv.org/pdf/2510.10216", "abs": "https://arxiv.org/abs/2510.10216", "authors": ["Zhechong Huang", "Zhao Zhang", "Ruyi Ji", "Tingxuan Xia", "Qihao Zhu", "Qinxiang Cao", "Zeyu Sun", "Yingfei Xiong"], "title": "Learning to Guarantee Type Correctness in Code Generation through Type-Guided Program Synthesis", "comment": null, "summary": "Language models have shown remarkable proficiency in code generation;\nnevertheless, ensuring type correctness remains a challenge. Although\ntraditional methods, such as constrained decoding, alleviate this problem by\nexternally rejecting untypable code, the model itself does not effectively\nlearn type reasoning internally, which ultimately limits its overall\nperformance. This paper introduces TyFlow, a novel system that internalizes\ntype reasoning within code generation to guide the model to learn the type\nsystem. The core of our approach is a novel type-guided program synthesis\nsystem that maintains an isomorphism between type derivation trees and\nsynthesis derivation trees, enabling a new code representation based on\nsynthesis decision sequences rather than traditional text-based token\nsequences. By offloading the complexity of type system learning to the\nrepresentation itself, models can redirect their computational resources toward\nhigher-level program semantics. Our evaluation shows that TyFlow not only\neliminates type errors but also significantly improves functional correctness,\nhighlighting the importance of aligning LMs with type systems internally."}
{"id": "2510.11625", "categories": ["cs.GT"], "pdf": "https://arxiv.org/pdf/2510.11625", "abs": "https://arxiv.org/abs/2510.11625", "authors": ["Drew Springham", "Edith Elkind", "Bart de Keijzer", "Maria Polukarov"], "title": "Multiwinner Voting with Interval Preferences under Incomplete Information", "comment": "19 pages, 5 figures", "summary": "In multiwinner approval elections with many candidates, voters may struggle\nto determine their preferences over the entire slate of candidates. It is\ntherefore of interest to explore which (if any) fairness guarantees can be\nprovided under reduced communication. In this paper, we consider voters with\none-dimensional preferences: voters and candidates are associated with points\nin $\\mathbb R$, and each voter's approval set forms an interval of $\\mathbb R$.\nWe put forward a probabilistic preference model, where the voter set consists\nof $\\sigma$ different groups; each group is associated with a distribution over\nan interval of $\\mathbb R$, so that each voter draws the endpoints of her\napproval interval from the distribution associated with her group. We present\nan algorithm for computing committees that provide Proportional Justified\nRepresentation + (PJR+), which proceeds by querying voters' preferences, and\nshow that, in expectation, it makes $\\mathcal{O}(\\log( \\sigma\\cdot k))$ queries\nper voter, where $k$ is the desired committee size."}
{"id": "2510.10715", "categories": ["cs.GR", "cs.CV"], "pdf": "https://arxiv.org/pdf/2510.10715", "abs": "https://arxiv.org/abs/2510.10715", "authors": ["Shelly Golan", "Yotam Nitzan", "Zongze Wu", "Or Patashnik"], "title": "VLM-Guided Adaptive Negative Prompting for Creative Generation", "comment": "Project page at:\n  https://shelley-golan.github.io/VLM-Guided-Creative-Generation/", "summary": "Creative generation is the synthesis of new, surprising, and valuable samples\nthat reflect user intent yet cannot be envisioned in advance. This task aims to\nextend human imagination, enabling the discovery of visual concepts that exist\nin the unexplored spaces between familiar domains. While text-to-image\ndiffusion models excel at rendering photorealistic scenes that faithfully match\nuser prompts, they still struggle to generate genuinely novel content. Existing\napproaches to enhance generative creativity either rely on interpolation of\nimage features, which restricts exploration to predefined categories, or\nrequire time-intensive procedures such as embedding optimization or model\nfine-tuning. We propose VLM-Guided Adaptive Negative-Prompting, a\ntraining-free, inference-time method that promotes creative image generation\nwhile preserving the validity of the generated object. Our approach utilizes a\nvision-language model (VLM) that analyzes intermediate outputs of the\ngeneration process and adaptively steers it away from conventional visual\nconcepts, encouraging the emergence of novel and surprising outputs. We\nevaluate creativity through both novelty and validity, using statistical\nmetrics in the CLIP embedding space. Through extensive experiments, we show\nconsistent gains in creative novelty with negligible computational overhead.\nMoreover, unlike existing methods that primarily generate single objects, our\napproach extends to complex scenarios, such as generating coherent sets of\ncreative objects and preserving creativity within elaborate compositional\nprompts. Our method integrates seamlessly into existing diffusion pipelines,\noffering a practical route to producing creative outputs that venture beyond\nthe constraints of textual descriptions."}
{"id": "2510.10219", "categories": ["cs.PL"], "pdf": "https://arxiv.org/pdf/2510.10219", "abs": "https://arxiv.org/abs/2510.10219", "authors": ["Ruihao Li", "Lizy K. John", "Neeraja J. Yadwadkar"], "title": "Old is Gold: Optimizing Single-threaded Applications with Exgen-Malloc", "comment": null, "summary": "Memory allocators hide beneath nearly every application stack, yet their\nperformance footprint extends far beyond their code size. Even small\ninefficiencies in the allocators ripple through caches and the rest of the\nmemory hierarchy, collectively imposing what operators often call a \"datacenter\ntax\". At hyperscale, even a 1% improvement in allocator efficiency can unlock\nmillions of dollars in savings and measurable reductions in datacenter energy\nconsumption. Modern memory allocators are designed to optimize allocation speed\nand memory fragmentation in multi-threaded environments, relying on complex\nmetadata and control logic to achieve high performance. However, the overhead\nintroduced by this complexity prompts a reevaluation of allocator design.\nNotably, such overhead can be avoided in single-threaded scenarios, which\ncontinue to be widely used across diverse application domains.\n  In this paper, we introduce Exgen-Malloc, a memory allocator purpose-built\nfor single-threaded applications. By specializing for single-threaded\nexecution, Exgen-Malloc eliminates unnecessary metadata, simplifies the control\nflow, thereby reducing overhead and improving allocation efficiency. Its core\ndesign features include a centralized heap, a single free-block list, and a\nbalanced strategy for memory commitment and relocation. Additionally,\nExgen-Malloc incorporates design principles in modern multi-threaded\nallocators, which do not exist in legacy single-threaded allocators such as\ndlmalloc. We evaluate Exgen-Malloc on two Intel Xeon platforms. Across both\nsystems, Exgen-Malloc achieves a speedup of 1.17x, 1.10x, and 1.93x over\ndlmalloc on SPEC CPU2017, redis-benchmark, and mimalloc-bench, respectively. In\naddition to performance, Exgen-Malloc achieves 6.2%, 0.1%, and 25.2% memory\nsavings over mimalloc on SPEC CPU2017, redis-benchmark, and mimalloc-bench,\nrespectively."}
{"id": "2510.10751", "categories": ["cs.GR"], "pdf": "https://arxiv.org/pdf/2510.10751", "abs": "https://arxiv.org/abs/2510.10751", "authors": ["Ningna Wang", "Rui Xu", "Yibo Yin", "Zichun Zhong", "Taku Komura", "Wenping Wang", "Xiaohu Guo"], "title": "MATStruct: High-Quality Medial Mesh Computation via Structure-aware Variational Optimization", "comment": null, "summary": "We propose a novel optimization framework for computing the medial axis\ntransform that simultaneously preserves the medial structure and ensures high\nmedial mesh quality. The medial structure, consisting of interconnected sheets,\nseams, and junctions, provides a natural volumetric decomposition of a 3D\nshape. Our method introduces a structure-aware, particle-based optimization\npipeline guided by the restricted power diagram (RPD), which partitions the\ninput volume into convex cells whose dual encodes the connectivity of the\nmedial mesh. Structure-awareness is enforced through a spherical quadratic\nerror metric (SQEM) projection that constrains the movement of medial spheres,\nwhile a Gaussian kernel energy encourages an even spatial distribution.\nCompared to feature-preserving methods such as MATFP and MATTopo, our approach\nproduces cleaner and more accurate medial structures with significantly\nimproved mesh quality. In contrast to voxel-based, point-cloud-based, and\nvariational methods, our framework is the first to integrate structural\nawareness into the optimization process, yielding medial meshes with superior\ngeometric fidelity, topological correctness, and explicit structural\ndecomposition."}
{"id": "2510.10410", "categories": ["cs.PL", "cs.SE"], "pdf": "https://arxiv.org/pdf/2510.10410", "abs": "https://arxiv.org/abs/2510.10410", "authors": ["Hui Xu"], "title": "A Trace-based Approach for Code Safety Analysis", "comment": null, "summary": "Rust is a memory-safe programming language that disallows undefined behavior.\nIts safety guarantees have been extensively examined by the community through\nempirical studies, which has led to its remarkable success. However, unsafe\ncode remains a critical concern in Rust. By reviewing the safety design of Rust\nand analyzing real-world Rust projects, this paper establishes a systematic\nframework for understanding unsafe code and undefined behavior, and summarizes\nthe soundness criteria for Rust code. It further derives actionable guidance\nfor achieving sound encapsulation."}
{"id": "2510.10841", "categories": ["cs.GR"], "pdf": "https://arxiv.org/pdf/2510.10841", "abs": "https://arxiv.org/abs/2510.10841", "authors": ["Chen Wang", "Mengtan Lin"], "title": "The Fire We Share", "comment": "Accepted to VISAP'25 (IEEE VIS Arts Program), held alongside IEEE VIS\n  2025", "summary": "The Fire We Share proposes a care-centered, consequence-aware visualization\nframework for engaging with wildfire data not as static metrics, but as living\narchives of ecological and social entanglement. By combining plants-inspired\ndata forms, event-based mapping, and narrative layering, the project\nforegrounds fire as a shared temporal condition-one that cuts across natural\ncycles and human systems. Rather than simplifying wildfire data into digestible\nvisuals, The Fire We Share reimagines it as a textured, wounded\narchive-embodied, relational, and radically ethical."}
{"id": "2510.10517", "categories": ["cs.PL", "cs.AI", "cs.SE"], "pdf": "https://arxiv.org/pdf/2510.10517", "abs": "https://arxiv.org/abs/2510.10517", "authors": ["Su-Hyeon Kim", "Joonghyuk Hahn", "Sooyoung Cha", "Yo-Sub Han"], "title": "ECO: Enhanced Code Optimization via Performance-Aware Prompting for Code-LLMs", "comment": null, "summary": "Code runtime optimization-the task of rewriting a given code to a faster\none-remains challenging, as it requires reasoning about performance trade-offs\ninvolving algorithmic and structural choices. Recent approaches employ\ncode-LLMs with slow-fast code pairs provided as optimization guidance, but such\npair-based methods obscure the causal factors of performance gains and often\nlead to superficial pattern imitation rather than genuine performance\nreasoning. We introduce ECO, a performance-aware prompting framework for code\noptimization. ECO first distills runtime optimization instructions (ROIs) from\nreference slow-fast code pairs; Each ROI describes root causes of inefficiency\nand the rationales that drive performance improvements. For a given input code,\nECO in parallel employs (i) a symbolic advisor to produce a bottleneck\ndiagnosis tailored to the code, and (ii) an ROI retriever to return related\nROIs. These two outputs are then composed into a performance-aware prompt,\nproviding actionable guidance for code-LLMs. ECO's prompts are model-agnostic,\nrequire no fine-tuning, and can be easily prepended to any code-LLM prompt. Our\nempirical studies highlight that ECO prompting significantly improves\ncode-LLMs' ability to generate efficient code, achieving speedups of up to\n7.81x while minimizing correctness loss."}
{"id": "2510.10531", "categories": ["cs.PL", "cs.DC", "cs.LO", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2510.10531", "abs": "https://arxiv.org/abs/2510.10531", "authors": ["Guillaume Ambal", "George Hodgkins", "Mark Madler", "Gregory Chockler", "Brijesh Dongol", "Joseph Izraelevitz", "Azalea Raad", "Viktor Vafeiadis"], "title": "A Verified High-Performance Composable Object Library for Remote Direct Memory Access (Extended Version)", "comment": null, "summary": "Remote Direct Memory Access (RDMA) is a memory technology that allows remote\ndevices to directly write to and read from each other's memory, bypassing\ncomponents such as the CPU and operating system. This enables low-latency\nhigh-throughput networking, as required for many modern data centres, HPC\napplications and AI/ML workloads. However, baseline RDMA comprises a highly\npermissive weak memory model that is difficult to use in practice and has only\nrecently been formalised. In this paper, we introduce the Library of Composable\nObjects (LOCO), a formally verified library for building multi-node objects on\nRDMA, filling the gap between shared memory and distributed system programming.\nLOCO objects are well-encapsulated and take advantage of the strong locality\nand the weak consistency characteristics of RDMA. They have performance\ncomparable to custom RDMA systems (e.g. distributed maps), but with a far\nsimpler programming model amenable to formal proofs of correctness. To support\nverification, we develop a novel modular declarative verification framework,\ncalled Mowgli, that is flexible enough to model multinode objects and is\nindependent of a memory consistency model. We instantiate Mowgli with the RDMA\nmemory model, and use it to verify correctness of LOCO libraries."}
{"id": "2510.11007", "categories": ["cs.PL", "cs.FL"], "pdf": "https://arxiv.org/pdf/2510.11007", "abs": "https://arxiv.org/abs/2510.11007", "authors": ["Antonina Nepeivoda", "Ilya Afanasyev"], "title": "Abstract String Domain Defined with Word Equations as a Reduced Product (Extended Version)", "comment": null, "summary": "We introduce a string-interval abstract domain, where string intervals are\ncharacterized by systems of word equations (encoding lower bounds on string\nvalues) and word disequalities (encoding upper bounds). Building upon the\nlattice structure of string intervals, we define an abstract string object as a\nreduced product on a string property semilattice, determined by\nlength-non-increasing morphisms. We consider several reduction strategies for\nabstract string objects and show that upon these strategies the string object\ndomain forms a lattice. We define basic abstract string operations on the\ndomain, aiming to minimize computational overheads on the reduction, and show\nhow the domain can be used to analyse properties of JavaScript string\nmanipulating programs."}
{"id": "2510.11420", "categories": ["cs.PL", "quant-ph"], "pdf": "https://arxiv.org/pdf/2510.11420", "abs": "https://arxiv.org/abs/2510.11420", "authors": ["Mark Koch", "Agust√≠n Borgna", "Seyon Sivarajah", "Alan Lawrence", "Alec Edgington", "Douglas Wilson", "Craig Roy", "Luca Mondada", "Lukas Heidemann", "Ross Duncan"], "title": "HUGR: A Quantum-Classical Intermediate Representation", "comment": "8 pages, extended abstract submitted to PlanQC25", "summary": "We introduce the Hierarchical Unified Graph Representation (HUGR): a novel\ngraph based intermediate representation for mixed quantum-classical programs.\nHUGR's design features high expressivity and extensibility to capture the\ncapabilities of near-term and forthcoming quantum computing devices, as well as\nnew and evolving abstractions from novel quantum programming paradigms. The\ngraph based structure is machine-friendly and supports powerful pattern\nmatching based compilation techniques. Inspired by MLIR, HUGR's extensibility\nfurther allows compilation tooling to reason about programs at multiple levels\nof abstraction, lowering smoothly between them. Safety guarantees in the\nstructure including strict, static typing and linear quantum types allow rapid\ndevelopment of compilation tooling without fear of program invalidation. A full\nspecification of HUGR and reference implementation are open-source and\navailable online."}
{"id": "2510.11573", "categories": ["cs.PL"], "pdf": "https://arxiv.org/pdf/2510.11573", "abs": "https://arxiv.org/abs/2510.11573", "authors": ["Santiago Arranz-Olmos", "Gilles Barthe", "Lionel Blatter", "Xingyu Xie", "Zhiyuan Zhang"], "title": "(Dis)Proving Spectre Security with Speculation-Passing Style", "comment": null, "summary": "Constant-time (CT) verification tools are commonly used for detecting\npotential side-channel vulnerabilities in cryptographic libraries. Recently, a\nnew class of tools, called speculative constant-time (SCT) tools, has also been\nused for detecting potential Spectre vulnerabilities. In many cases, these SCT\ntools have emerged as liftings of CT tools. However, these liftings are seldom\ndefined precisely and are almost never analyzed formally. The goal of this\npaper is to address this gap, by developing formal foundations for these\nliftings, and to demonstrate that these foundations can yield practical\nbenefits.\n  Concretely, we introduce a program transformation, coined Speculation-Passing\nStyle (SPS), for reducing SCT verification to CT verification. Essentially, the\ntransformation instruments the program with a new input that corresponds to\nattacker-controlled predictions and modifies the program to follow them. This\napproach is sound and complete, in the sense that a program is SCT if and only\nif its SPS transform is CT. Thus, we can leverage existing CT verification\ntools to prove SCT; we illustrate this by combining SPS with three standard\nmethodologies for CT verification, namely reducing it to non-interference,\nassertion safety and dynamic taint analysis. We realize these combinations with\nthree existing tools, EasyCrypt, BINSEC, and ctgrind, and we evaluate them on\nKocher's benchmarks for Spectre-v1. Our results focus on Spectre-v1 in the\nstandard CT leakage model; however, we also discuss applications of our method\nto other variants of Spectre and other leakage models."}
