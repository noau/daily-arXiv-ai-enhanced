{"id": "2507.08759", "categories": ["cs.PL", "cs.LO"], "pdf": "https://arxiv.org/pdf/2507.08759", "abs": "https://arxiv.org/abs/2507.08759", "authors": ["Maximilian Dor\u00e9"], "title": "Dependent Multiplicities in Dependent Linear Type Theory", "comment": null, "summary": "We present a novel dependent linear type theory in which the multiplicity of\nsome variable - i.e., the number of times the variable can be used in a program\n- can depend on other variables. This allows us to give precise resource\nannotations to many higher-order functions that cannot be adequately typed in\nany other system. Inspired by the Dialectica translation, our typing discipline\nis obtained by embedding linear logic into dependent type theory and specifying\nhow the embedded logic interacts with the host theory. We can then use a\nstandard natural numbers type to obtain a quantitative typing system with\ndependent multiplicities. We characterise the semantics for our theory as a\ncombination of standard models of dependent type theory and linear logic. Our\nsystem can be added to any dependently typed language, which we demonstrate\nwith an implementation in Agda.", "AI": {"tldr": "\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u4f9d\u8d56\u7ebf\u6027\u7c7b\u578b\u7406\u8bba\uff0c\u5176\u4e2d\u53d8\u91cf\u7684\u591a\u91cd\u6027\uff08\u5373\u53d8\u91cf\u5728\u7a0b\u5e8f\u4e2d\u53ef\u4ee5\u88ab\u4f7f\u7528\u7684\u6b21\u6570\uff09\u53ef\u4ee5\u4f9d\u8d56\u4e8e\u5176\u4ed6\u53d8\u91cf\u3002\u8fd9\u4f7f\u6211\u4eec\u80fd\u591f\u4e3a\u8bb8\u591a\u5728\u73b0\u6709\u7cfb\u7edf\u4e2d\u65e0\u6cd5\u5145\u5206\u7c7b\u578b\u5316\u7684\u9ad8\u9636\u51fd\u6570\u63d0\u4f9b\u7cbe\u786e\u7684\u8d44\u6e90\u6ce8\u91ca\u3002", "motivation": "\u73b0\u6709\u7cfb\u7edf\u4e2d\u65e0\u6cd5\u4e3a\u67d0\u4e9b\u9ad8\u9636\u51fd\u6570\u63d0\u4f9b\u7cbe\u786e\u7684\u8d44\u6e90\u6ce8\u91ca\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u80fd\u591f\u652f\u6301\u4f9d\u8d56\u591a\u91cd\u6027\u7684\u7c7b\u578b\u7406\u8bba\u3002", "method": "\u901a\u8fc7\u5c06\u7ebf\u6027\u903b\u8f91\u5d4c\u5165\u4f9d\u8d56\u7c7b\u578b\u7406\u8bba\uff0c\u5e76\u6307\u5b9a\u5d4c\u5165\u903b\u8f91\u4e0e\u5bbf\u4e3b\u7406\u8bba\u7684\u4ea4\u4e92\u65b9\u5f0f\uff0c\u6211\u4eec\u8bbe\u8ba1\u4e86\u4e00\u79cd\u65b0\u7684\u7c7b\u578b\u89c4\u5219\u3002\u5229\u7528\u6807\u51c6\u7684\u81ea\u7136\u6570\u7c7b\u578b\uff0c\u6211\u4eec\u5b9e\u73b0\u4e86\u4e00\u4e2a\u5177\u6709\u4f9d\u8d56\u591a\u91cd\u6027\u7684\u5b9a\u91cf\u7c7b\u578b\u7cfb\u7edf\u3002", "result": "\u6211\u4eec\u7684\u7406\u8bba\u7ed3\u5408\u4e86\u4f9d\u8d56\u7c7b\u578b\u7406\u8bba\u548c\u7ebf\u6027\u903b\u8f91\u7684\u6807\u51c6\u6a21\u578b\uff0c\u5e76\u63d0\u4f9b\u4e86\u4e00\u4e2a\u5728Agda\u4e2d\u5b9e\u73b0\u7684\u793a\u4f8b\u3002", "conclusion": "\u8be5\u4f9d\u8d56\u7ebf\u6027\u7c7b\u578b\u7406\u8bba\u4e3a\u9ad8\u9636\u51fd\u6570\u7684\u8d44\u6e90\u7ba1\u7406\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5e76\u53ef\u96c6\u6210\u5230\u4efb\u4f55\u4f9d\u8d56\u7c7b\u578b\u8bed\u8a00\u4e2d\u3002"}}
{"id": "2507.08796", "categories": ["cs.PL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.08796", "abs": "https://arxiv.org/abs/2507.08796", "authors": ["Owen Lewis", "Neil Ghani", "Andrew Dudzik", "Christos Perivolaropoulos", "Razvan Pascanu", "Petar Veli\u010dkovi\u0107"], "title": "Filter Equivariant Functions: A symmetric account of length-general extrapolation on lists", "comment": "18 pages, 2 figures", "summary": "What should a function that extrapolates beyond known input/output examples\nlook like? This is a tricky question to answer in general, as any function\nmatching the outputs on those examples can in principle be a correct\nextrapolant. We argue that a \"good\" extrapolant should follow certain kinds of\nrules, and here we study a particularly appealing criterion for rule-following\nin list functions: that the function should behave predictably even when\ncertain elements are removed. In functional programming, a standard way to\nexpress such removal operations is by using a filter function. Accordingly, our\npaper introduces a new semantic class of functions -- the filter equivariant\nfunctions. We show that this class contains interesting examples, prove some\nbasic theorems about it, and relate it to the well-known class of map\nequivariant functions. We also present a geometric account of filter\nequivariants, showing how they correspond naturally to certain simplicial\nstructures. Our highlight result is the amalgamation algorithm, which\nconstructs any filter-equivariant function's output by first studying how it\nbehaves on sublists of the input, in a way that extrapolates perfectly.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u51fd\u6570\u8bed\u4e49\u7c7b\u522b\u2014\u2014\u6ee4\u6ce2\u5668\u7b49\u53d8\u51fd\u6570\uff0c\u5e76\u901a\u8fc7\u51e0\u4f55\u89e3\u91ca\u548c\u7b97\u6cd5\u5c55\u793a\u4e86\u5176\u5728\u5217\u8868\u51fd\u6570\u4e2d\u7684\u9884\u6d4b\u884c\u4e3a\u3002", "motivation": "\u7814\u7a76\u5982\u4f55\u5b9a\u4e49\u5728\u5df2\u77e5\u8f93\u5165/\u8f93\u51fa\u793a\u4f8b\u4e4b\u5916\u5177\u6709\u826f\u597d\u63a8\u65ad\u80fd\u529b\u7684\u51fd\u6570\uff0c\u5c24\u5176\u662f\u9075\u5faa\u67d0\u4e9b\u89c4\u5219\u7684\u51fd\u6570\u3002", "method": "\u5f15\u5165\u6ee4\u6ce2\u5668\u7b49\u53d8\u51fd\u6570\u7684\u6982\u5ff5\uff0c\u901a\u8fc7\u6ee4\u6ce2\u5668\u51fd\u6570\u8868\u8fbe\u5143\u7d20\u79fb\u9664\u64cd\u4f5c\uff0c\u5e76\u7814\u7a76\u5176\u6027\u8d28\u53ca\u5176\u4e0e\u6620\u5c04\u7b49\u53d8\u51fd\u6570\u7684\u5173\u7cfb\u3002", "result": "\u8bc1\u660e\u4e86\u6ee4\u6ce2\u5668\u7b49\u53d8\u51fd\u6570\u7c7b\u7684\u6709\u8da3\u793a\u4f8b\u548c\u57fa\u672c\u5b9a\u7406\uff0c\u63d0\u51fa\u4e86\u51e0\u4f55\u89e3\u91ca\uff0c\u5e76\u5f00\u53d1\u4e86\u5b8c\u7f8e\u63a8\u65ad\u7684\u5408\u5e76\u7b97\u6cd5\u3002", "conclusion": "\u6ee4\u6ce2\u5668\u7b49\u53d8\u51fd\u6570\u4e3a\u51fd\u6570\u63a8\u65ad\u63d0\u4f9b\u4e86\u4e00\u4e2a\u6709\u5438\u5f15\u529b\u7684\u89c4\u5219\u9075\u5faa\u6807\u51c6\uff0c\u4e14\u5728\u7406\u8bba\u548c\u7b97\u6cd5\u4e0a\u5747\u5177\u6709\u5b9e\u7528\u6027\u3002"}}
{"id": "2507.08285", "categories": ["cs.GR", "cs.CV"], "pdf": "https://arxiv.org/pdf/2507.08285", "abs": "https://arxiv.org/abs/2507.08285", "authors": ["Gwanhyeong Koo", "Sunjae Yoon", "Younghwan Lee", "Ji Woo Hong", "Chang D. Yoo"], "title": "FlowDrag: 3D-aware Drag-based Image Editing with Mesh-guided Deformation Vector Flow Fields", "comment": "ICML 2025 Spotlight", "summary": "Drag-based editing allows precise object manipulation through point-based\ncontrol, offering user convenience. However, current methods often suffer from\na geometric inconsistency problem by focusing exclusively on matching\nuser-defined points, neglecting the broader geometry and leading to artifacts\nor unstable edits. We propose FlowDrag, which leverages geometric information\nfor more accurate and coherent transformations. Our approach constructs a 3D\nmesh from the image, using an energy function to guide mesh deformation based\non user-defined drag points. The resulting mesh displacements are projected\ninto 2D and incorporated into a UNet denoising process, enabling precise\nhandle-to-target point alignment while preserving structural integrity.\nAdditionally, existing drag-editing benchmarks provide no ground truth, making\nit difficult to assess how accurately the edits match the intended\ntransformations. To address this, we present VFD (VidFrameDrag) benchmark\ndataset, which provides ground-truth frames using consecutive shots in a video\ndataset. FlowDrag outperforms existing drag-based editing methods on both VFD\nBench and DragBench.", "AI": {"tldr": "FlowDrag\u901a\u8fc7\u7ed3\u5408\u51e0\u4f55\u4fe1\u606f\u548c\u7528\u6237\u5b9a\u4e49\u7684\u70b9\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u66f4\u51c6\u786e\u548c\u4e00\u81f4\u7684\u7269\u4f53\u7f16\u8f91\u65b9\u6cd5\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u4e2d\u51e0\u4f55\u4e0d\u4e00\u81f4\u7684\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u7684\u62d6\u62fd\u7f16\u8f91\u65b9\u6cd5\u4ec5\u5173\u6ce8\u5339\u914d\u7528\u6237\u5b9a\u4e49\u7684\u70b9\uff0c\u5ffd\u89c6\u4e86\u66f4\u5e7f\u6cdb\u7684\u51e0\u4f55\u7ed3\u6784\uff0c\u5bfc\u81f4\u7f16\u8f91\u65f6\u51fa\u73b0\u4f2a\u5f71\u6216\u4e0d\u7a33\u5b9a\u7684\u7ed3\u679c\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u66f4\u51c6\u786e\u7684\u65b9\u6cd5\u3002", "method": "FlowDrag\u901a\u8fc7\u4ece\u56fe\u50cf\u6784\u5efa3D\u7f51\u683c\uff0c\u5e76\u4f7f\u7528\u80fd\u91cf\u51fd\u6570\u6307\u5bfc\u57fa\u4e8e\u7528\u6237\u5b9a\u4e49\u70b9\u7684\u7f51\u683c\u53d8\u5f62\uff0c\u7136\u540e\u5c06\u4f4d\u79fb\u6295\u5f71\u52302D\u5e76\u5f15\u5165UNet\u53bb\u566a\u8fc7\u7a0b\uff0c\u4ee5\u5b9e\u73b0\u7cbe\u786e\u7684\u7f16\u8f91\u3002", "result": "FlowDrag\u5728VFD Bench\u548cDragBench\u4e0a\u4f18\u4e8e\u73b0\u6709\u7684\u62d6\u62fd\u7f16\u8f91\u65b9\u6cd5\uff0c\u4e14\u63d0\u51fa\u7684VFD\u57fa\u51c6\u6570\u636e\u96c6\u63d0\u4f9b\u4e86\u771f\u5b9e\u7684\u7f16\u8f91\u57fa\u51c6\u3002", "conclusion": "FlowDrag\u901a\u8fc7\u5229\u7528\u51e0\u4f55\u4fe1\u606f\u6539\u8fdb\u4e86\u62d6\u62fd\u7f16\u8f91\u7684\u51c6\u786e\u6027\uff0c\u5e76\u901a\u8fc7VFD\u57fa\u51c6\u6570\u636e\u96c6\u586b\u8865\u4e86\u8bc4\u4f30\u7a7a\u767d\uff0c\u4e3a\u672a\u6765\u7684\u7814\u7a76\u63d0\u4f9b\u4e86\u65b9\u5411\u3002"}}
{"id": "2507.08513", "categories": ["cs.GR", "cs.CV"], "pdf": "https://arxiv.org/pdf/2507.08513", "abs": "https://arxiv.org/abs/2507.08513", "authors": ["Liu He", "Xiao Zeng", "Yizhi Song", "Albert Y. C. Chen", "Lu Xia", "Shashwat Verma", "Sankalp Dayal", "Min Sun", "Cheng-Hao Kuo", "Daniel Aliaga"], "title": "Advancing Multimodal LLMs by Large-Scale 3D Visual Instruction Dataset Generation", "comment": null, "summary": "Multimodal Large Language Models (MLLMs) struggle with accurately capturing\ncamera-object relations, especially for object orientation, camera viewpoint,\nand camera shots. This stems from the fact that existing MLLMs are trained on\nimages with limited diverse camera-object relations and corresponding textual\ndescriptions. To address this, we propose a synthetic generation pipeline to\ncreate large-scale 3D visual instruction datasets. Our framework takes 3D\nassets as input and uses rendering and diffusion-based image generation models\nto create photorealistic images preserving precise camera-object relations.\nAdditionally, large language models (LLMs) are used to generate text prompts\nfor guiding visual instruction tuning and controlling image generation. We\ncreate Ultimate3D, a dataset of 240K VQAs with precise camera-object\nannotations, and corresponding benchmark. MLLMs fine-tuned on our proposed\ndataset outperform commercial models by a large margin, achieving an average\naccuracy improvement of 33.4% on camera-object relation recognition tasks. Our\ncode, dataset, and benchmark will contribute to broad MLLM applications.", "AI": {"tldr": "\u4e3a\u4e86\u89e3\u51b3\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff08MLLMs\uff09\u5728\u6355\u6349\u76f8\u673a-\u7269\u4f53\u5173\u7cfb\u65b9\u9762\u7684\u4e0d\u8db3\uff0c\u7814\u7a76\u4eba\u5458\u63d0\u51fa\u4e86\u4e00\u79cd\u5408\u6210\u751f\u6210\u6d41\u6c34\u7ebf\u6765\u521b\u5efa\u5927\u89c4\u6a213D\u89c6\u89c9\u6307\u4ee4\u6570\u636e\u96c6\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6a21\u578b\u5728\u76f8\u5173\u4efb\u52a1\u4e0a\u7684\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u7684\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff08MLLMs\uff09\u5728\u51c6\u786e\u6355\u6349\u76f8\u673a-\u7269\u4f53\u5173\u7cfb\uff08\u5982\u7269\u4f53\u65b9\u5411\u3001\u76f8\u673a\u89c6\u89d2\u548c\u62cd\u6444\u89d2\u5ea6\uff09\u65b9\u9762\u8868\u73b0\u4e0d\u4f73\uff0c\u4e3b\u8981\u662f\u56e0\u4e3a\u8bad\u7ec3\u6570\u636e\u4e2d\u7f3a\u4e4f\u591a\u6837\u5316\u7684\u76f8\u673a-\u7269\u4f53\u5173\u7cfb\u548c\u76f8\u5e94\u7684\u6587\u672c\u63cf\u8ff0\u3002", "method": "\u7814\u7a76\u4eba\u5458\u63d0\u51fa\u4e86\u4e00\u79cd\u5408\u6210\u751f\u6210\u6d41\u6c34\u7ebf\uff0c\u4f7f\u75283D\u8d44\u4ea7\u4f5c\u4e3a\u8f93\u5165\uff0c\u901a\u8fc7\u6e32\u67d3\u548c\u57fa\u4e8e\u6269\u6563\u7684\u56fe\u50cf\u751f\u6210\u6a21\u578b\u521b\u5efa\u4fdd\u7559\u7cbe\u786e\u76f8\u673a-\u7269\u4f53\u5173\u7cfb\u7684\u903c\u771f\u56fe\u50cf\uff0c\u540c\u65f6\u5229\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u751f\u6210\u6587\u672c\u63d0\u793a\u4ee5\u6307\u5bfc\u89c6\u89c9\u6307\u4ee4\u8c03\u6574\u548c\u63a7\u5236\u56fe\u50cf\u751f\u6210\u3002", "result": "\u6784\u5efa\u7684Ultimate3D\u6570\u636e\u96c6\u5305\u542b24\u4e07\u4e2a\u89c6\u89c9\u95ee\u7b54\uff08VQA\uff09\u6837\u672c\uff0c\u5177\u6709\u7cbe\u786e\u7684\u76f8\u673a-\u7269\u4f53\u5173\u7cfb\u6807\u6ce8\u3002\u5b9e\u9a8c\u8868\u660e\uff0c\u57fa\u4e8e\u8be5\u6570\u636e\u96c6\u5fae\u8c03\u7684MLLMs\u5728\u76f8\u673a-\u7269\u4f53\u5173\u7cfb\u8bc6\u522b\u4efb\u52a1\u4e0a\u7684\u5e73\u5747\u51c6\u786e\u7387\u63d0\u5347\u4e8633.4%\uff0c\u663e\u8457\u4f18\u4e8e\u5546\u4e1a\u6a21\u578b\u3002", "conclusion": "\u8be5\u7814\u7a76\u901a\u8fc7\u521b\u5efa\u9ad8\u8d28\u91cf\u7684\u5408\u6210\u6570\u636e\u96c6\uff0c\u663e\u8457\u63d0\u5347\u4e86MLLMs\u5728\u76f8\u673a-\u7269\u4f53\u5173\u7cfb\u4efb\u52a1\u4e0a\u7684\u6027\u80fd\uff0c\u5176\u4ee3\u7801\u3001\u6570\u636e\u96c6\u548c\u57fa\u51c6\u5c06\u4e3a\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u7684\u5e7f\u6cdb\u5e94\u7528\u505a\u51fa\u8d21\u732e\u3002"}}
