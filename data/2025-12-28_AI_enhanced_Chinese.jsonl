{"id": "2512.20904", "categories": ["cs.GR"], "pdf": "https://arxiv.org/pdf/2512.20904", "abs": "https://arxiv.org/abs/2512.20904", "authors": ["Wei Du", "Qing Fang", "Ligang Liu", "Xiao-Ming Fu"], "title": "Efficient Computation of Integer-constrained Cones for Conformal Parameterizations", "comment": "15 pages; under review", "summary": "We propose an efficient method to compute a small set of integer-constrained cone singularities, which induce a rotationally seamless conformal parameterization with low distortion. Since the problem only involves discrete variables, i.e., vertex-constrained positions, integer-constrained angles, and the number of cones, we alternately optimize these three types of variables to achieve tractable convergence. Central to high efficiency is an explicit construction algorithm that reduces the optimization problem scale to be slightly greater than the number of integer variables for determining the optimal angles with fixed positions and numbers, even for high-genus surfaces. In addition, we derive a new derivative formula that allows us to move the cones, effectively reducing distortion until convergence. Combined with other strategies, including repositioning and adding cones to decrease distortion, adaptively selecting a constrained number of integer variables for efficient optimization, and pairing cones to reduce the number, we quickly achieve a favorable tradeoff between the number of cones and the parameterization distortion. We demonstrate the effectiveness and practicability of our cones by using them to generate rotationally seamless and low-distortion parameterizations on a massive test data set. Our method demonstrates an order-of-magnitude speedup (30$\\times$ faster on average) compared to state-of-the-art approaches while maintaining comparable cone numbers and parameterization distortion.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u9ad8\u6548\u65b9\u6cd5\uff0c\u8ba1\u7b97\u6574\u6570\u7ea6\u675f\u7684\u9525\u5947\u5f02\u70b9\u96c6\uff0c\u751f\u6210\u65cb\u8f6c\u65e0\u7f1d\u4e14\u4f4e\u53d8\u5f62\u7684\u5171\u5f62\u53c2\u6570\u5316\u3002", "motivation": "\u4f20\u7edf\u65b9\u6cd5\u5728\u5904\u7406\u9ad8\u4e8f\u683c\u8868\u9762\u7684\u5171\u5f62\u53c2\u6570\u5316\u65f6\u6548\u7387\u8f83\u4f4e\uff0c\u672c\u6587\u65e8\u5728\u901a\u8fc7\u4f18\u5316\u79bb\u6563\u53d8\u91cf\uff08\u9876\u70b9\u7ea6\u675f\u4f4d\u7f6e\u3001\u6574\u6570\u7ea6\u675f\u89d2\u5ea6\u548c\u9525\u6570\uff09\u5b9e\u73b0\u9ad8\u6548\u4e14\u4f4e\u53d8\u5f62\u7684\u53c2\u6570\u5316\u3002", "method": "\u4ea4\u66ff\u4f18\u5316\u4e09\u79cd\u79bb\u6563\u53d8\u91cf\uff08\u9876\u70b9\u4f4d\u7f6e\u3001\u89d2\u5ea6\u7ea6\u675f\u548c\u9525\u6570\uff09\uff0c\u91c7\u7528\u663e\u5f0f\u6784\u9020\u7b97\u6cd5\u51cf\u5c0f\u4f18\u5316\u89c4\u6a21\uff0c\u5e76\u5f15\u5165\u65b0\u5bfc\u6570\u516c\u5f0f\u79fb\u52a8\u9525\u4ee5\u964d\u4f4e\u53d8\u5f62\u3002", "result": "\u5728\u5927\u91cf\u6d4b\u8bd5\u6570\u636e\u4e0a\u751f\u6210\u65cb\u8f6c\u65e0\u7f1d\u4e14\u4f4e\u53d8\u5f62\u7684\u53c2\u6570\u5316\uff0c\u5e73\u5747\u901f\u5ea6\u6bd4\u73b0\u6709\u65b9\u6cd5\u5feb30\u500d\uff0c\u540c\u65f6\u4fdd\u6301\u76f8\u4f3c\u7684\u9525\u6570\u548c\u53d8\u5f62\u6c34\u5e73\u3002", "conclusion": "\u672c\u6587\u65b9\u6cd5\u5728\u6548\u7387\u4e0a\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u6280\u672f\uff0c\u9002\u7528\u4e8e\u9ad8\u4e8f\u683c\u8868\u9762\u7684\u5171\u5f62\u53c2\u6570\u5316\u4efb\u52a1\u3002"}}
{"id": "2512.20943", "categories": ["cs.GR", "cs.DC", "cs.LG", "cs.MM", "cs.NI", "eess.IV"], "pdf": "https://arxiv.org/pdf/2512.20943", "abs": "https://arxiv.org/abs/2512.20943", "authors": ["Zhe Wang", "Jinghang Li", "Yifei Zhu"], "title": "AirGS: Real-Time 4D Gaussian Streaming for Free-Viewpoint Video Experiences", "comment": "This paper is accepted by IEEE International Conference on Computer Communications (INFOCOM), 2026", "summary": "Free-viewpoint video (FVV) enables immersive viewing experiences by allowing users to view scenes from arbitrary perspectives. As a prominent reconstruction technique for FVV generation, 4D Gaussian Splatting (4DGS) models dynamic scenes with time-varying 3D Gaussian ellipsoids and achieves high-quality rendering via fast rasterization. However, existing 4DGS approaches suffer from quality degradation over long sequences and impose substantial bandwidth and storage overhead, limiting their applicability in real-time and wide-scale deployments. Therefore, we present AirGS, a streaming-optimized 4DGS framework that rearchitects the training and delivery pipeline to enable high-quality, low-latency FVV experiences. AirGS converts Gaussian video streams into multi-channel 2D formats and intelligently identifies keyframes to enhance frame reconstruction quality. It further combines temporal coherence with inflation loss to reduce training time and representation size. To support communication-efficient transmission, AirGS models 4DGS delivery as an integer linear programming problem and design a lightweight pruning level selection algorithm to adaptively prune the Gaussian updates to be transmitted, balancing reconstruction quality and bandwidth consumption. Extensive experiments demonstrate that AirGS reduces quality deviation in PSNR by more than 20% when scene changes, maintains frame-level PSNR consistently above 30, accelerates training by 6 times, reduces per-frame transmission size by nearly 50% compared to the SOTA 4DGS approaches.", "AI": {"tldr": "AirGS\u662f\u4e00\u4e2a\u4f18\u5316\u76844D\u9ad8\u65af\u6cfc\u6e85\u6846\u67b6\uff0c\u901a\u8fc7\u91cd\u65b0\u8bbe\u8ba1\u8bad\u7ec3\u548c\u4ea4\u4ed8\u6d41\u7a0b\uff0c\u63d0\u4f9b\u9ad8\u8d28\u91cf\u3001\u4f4e\u5ef6\u8fdf\u7684\u81ea\u7531\u89c6\u70b9\u89c6\u9891\u4f53\u9a8c\uff0c\u663e\u8457\u63d0\u5347\u4e86\u957f\u671f\u5e8f\u5217\u7684\u8d28\u91cf\u548c\u5e26\u5bbd\u6548\u7387\u3002", "motivation": "\u73b0\u6709\u76844D\u9ad8\u65af\u6cfc\u6e85\u6280\u672f\u5728\u957f\u5e8f\u5217\u4e2d\u8d28\u91cf\u4e0b\u964d\uff0c\u4e14\u5e26\u5bbd\u548c\u5b58\u50a8\u5f00\u9500\u5927\uff0c\u9650\u5236\u4e86\u5176\u5728\u5b9e\u65f6\u548c\u5927\u89c4\u6a21\u90e8\u7f72\u4e2d\u7684\u5e94\u7528\u3002", "method": "AirGS\u5c06\u9ad8\u65af\u89c6\u9891\u6d41\u8f6c\u6362\u4e3a\u591a\u901a\u90532D\u683c\u5f0f\uff0c\u667a\u80fd\u8bc6\u522b\u5173\u952e\u5e27\uff0c\u7ed3\u5408\u65f6\u95f4\u76f8\u5e72\u6027\u548c\u81a8\u80c0\u635f\u5931\u4ee5\u51cf\u5c11\u8bad\u7ec3\u65f6\u95f4\u548c\u8868\u793a\u5927\u5c0f\uff0c\u5e76\u901a\u8fc7\u8f7b\u91cf\u7ea7\u526a\u679d\u7b97\u6cd5\u4f18\u5316\u4f20\u8f93\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cAirGS\u5728\u573a\u666f\u53d8\u5316\u65f6PSNR\u8d28\u91cf\u504f\u5dee\u964d\u4f4e20%\u4ee5\u4e0a\uff0c\u5e27\u7ea7PSNR\u7a33\u5b9a\u9ad8\u4e8e30\uff0c\u8bad\u7ec3\u901f\u5ea6\u63d0\u53476\u500d\uff0c\u6bcf\u5e27\u4f20\u8f93\u5927\u5c0f\u51cf\u5c11\u8fd150%\u3002", "conclusion": "AirGS\u901a\u8fc7\u9ad8\u6548\u7684\u8bbe\u8ba1\u548c\u4f18\u5316\uff0c\u663e\u8457\u63d0\u5347\u4e864DGS\u7684\u6027\u80fd\u548c\u9002\u7528\u6027\uff0c\u4e3a\u5b9e\u65f6\u548c\u5927\u89c4\u6a21\u81ea\u7531\u89c6\u70b9\u89c6\u9891\u5e94\u7528\u63d0\u4f9b\u4e86\u53ef\u884c\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2512.21099", "categories": ["cs.GR", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2512.21099", "abs": "https://arxiv.org/abs/2512.21099", "authors": ["Jaeseong Lee", "Junyeong Ahn", "Taewoong Kang", "Jaegul Choo"], "title": "TexAvatars : Hybrid Texel-3D Representations for Stable Rigging of Photorealistic Gaussian Head Avatars", "comment": "3DV 2026, Project page with videos: https://summertight.github.io/TexAvatars/", "summary": "Constructing drivable and photorealistic 3D head avatars has become a central task in AR/XR, enabling immersive and expressive user experiences. With the emergence of high-fidelity and efficient representations such as 3D Gaussians, recent works have pushed toward ultra-detailed head avatars. Existing approaches typically fall into two categories: rule-based analytic rigging or neural network-based deformation fields. While effective in constrained settings, both approaches often fail to generalize to unseen expressions and poses, particularly in extreme reenactment scenarios. Other methods constrain Gaussians to the global texel space of 3DMMs to reduce rendering complexity. However, these texel-based avatars tend to underutilize the underlying mesh structure. They apply minimal analytic deformation and rely heavily on neural regressors and heuristic regularization in UV space, which weakens geometric consistency and limits extrapolation to complex, out-of-distribution deformations. To address these limitations, we introduce TexAvatars, a hybrid avatar representation that combines the explicit geometric grounding of analytic rigging with the spatial continuity of texel space. Our approach predicts local geometric attributes in UV space via CNNs, but drives 3D deformation through mesh-aware Jacobians, enabling smooth and semantically meaningful transitions across triangle boundaries. This hybrid design separates semantic modeling from geometric control, resulting in improved generalization, interpretability, and stability. Furthermore, TexAvatars captures fine-grained expression effects, including muscle-induced wrinkles, glabellar lines, and realistic mouth cavity geometry, with high fidelity. Our method achieves state-of-the-art performance under extreme pose and expression variations, demonstrating strong generalization in challenging head reenactment settings.", "AI": {"tldr": "TexAvatars\u662f\u4e00\u79cd\u6df7\u5408\u5934\u50cf\u8868\u793a\u65b9\u6cd5\uff0c\u7ed3\u5408\u4e86\u5206\u6790\u7ed1\u5b9a\u548c\u7eb9\u7406\u7a7a\u95f4\u7684\u4f18\u70b9\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5934\u50cf\u5728\u6781\u7aef\u59ff\u6001\u548c\u8868\u60c5\u4e0b\u7684\u8868\u73b0\u529b\u548c\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u6784\u5efa\u903c\u771f\u4e14\u53ef\u9a71\u52a8\u76843D\u5934\u90e8\u865a\u62df\u5f62\u8c61\u5728AR/XR\u4e2d\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u5728\u6781\u7aef\u8868\u60c5\u548c\u59ff\u6001\u4e0b\u6cdb\u5316\u80fd\u529b\u4e0d\u8db3\uff0c\u9650\u5236\u4e86\u5b9e\u9645\u5e94\u7528\u3002", "method": "TexAvatars\u7ed3\u5408\u4e86\u5206\u6790\u7ed1\u5b9a\u7684\u51e0\u4f55\u57fa\u7840\u548c\u7eb9\u7406\u7a7a\u95f4\u7684\u7a7a\u95f4\u8fde\u7eed\u6027\uff0c\u901a\u8fc7CNN\u5728UV\u7a7a\u95f4\u4e2d\u9884\u6d4b\u5c40\u90e8\u51e0\u4f55\u5c5e\u6027\uff0c\u5e76\u901a\u8fc7\u7f51\u683c\u611f\u77e5\u7684Jacobians\u9a71\u52a83D\u53d8\u5f62\u3002", "result": "\u8be5\u65b9\u6cd5\u5728\u6781\u7aef\u59ff\u6001\u548c\u8868\u60c5\u4e0b\u8868\u73b0\u51fa\u8272\uff0c\u80fd\u591f\u6355\u6349\u7ec6\u5fae\u7684\u8868\u60c5\u6548\u679c\uff0c\u5982\u808c\u8089\u5f15\u8d77\u7684\u76b1\u7eb9\u548c\u53e3\u8154\u51e0\u4f55\u7ec6\u8282\uff0c\u5b9e\u73b0\u4e86\u5148\u8fdb\u7684\u6027\u80fd\u3002", "conclusion": "TexAvatars\u901a\u8fc7\u5206\u79bb\u8bed\u4e49\u5efa\u6a21\u548c\u51e0\u4f55\u63a7\u5236\uff0c\u63d0\u5347\u4e86\u865a\u62df\u5f62\u8c61\u7684\u6cdb\u5316\u80fd\u529b\u3001\u53ef\u89e3\u91ca\u6027\u548c\u7a33\u5b9a\u6027\uff0c\u9002\u7528\u4e8e\u590d\u6742\u7684\u5934\u90e8\u91cd\u6f14\u573a\u666f\u3002"}}
{"id": "2512.20688", "categories": ["cs.GT", "cs.AI", "cs.LG", "cs.MA"], "pdf": "https://arxiv.org/pdf/2512.20688", "abs": "https://arxiv.org/abs/2512.20688", "authors": ["Stefano Grassi"], "title": "Mechanism-Based Intelligence (MBI): Differentiable Incentives for Rational Coordination and Guaranteed Alignment in Multi-Agent Systems", "comment": null, "summary": "Autonomous multi-agent systems are fundamentally fragile: they struggle to solve the Hayekian Information problem (eliciting dispersed private knowledge) and the Hurwiczian Incentive problem (aligning local actions with global objectives), making coordination computationally intractable. I introduce Mechanism-Based Intelligence (MBI), a paradigm that reconceptualizes intelligence as emergent from the coordination of multiple \"brains\", rather than a single one. At its core, the Differentiable Price Mechanism (DPM) computes the exact loss gradient $$ \\mathbf{G}_i = - \\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{x}_i} $$ as a dynamic, VCG-equivalent incentive signal, guaranteeing Dominant Strategy Incentive Compatibility (DSIC) and convergence to the global optimum. A Bayesian extension ensures incentive compatibility under asymmetric information (BIC). The framework scales linearly ($\\mathcal{O}(N)$) with the number of agents, bypassing the combinatorial complexity of Dec-POMDPs and is empirically 50x faster than Model-Free Reinforcement Learning. By structurally aligning agent self-interest with collective objectives, it provides a provably efficient, auditable and generalizable approach to coordinated, trustworthy and scalable multi-agent intelligence grounded in economic principles.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u673a\u5236\u7684\u667a\u80fd\u8303\u5f0f\uff08MBI\uff09\uff0c\u901a\u8fc7\u52a8\u6001\u53ef\u5fae\u4ef7\u683c\u673a\u5236\uff08DPM\uff09\u89e3\u51b3\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u7684\u534f\u8c03\u95ee\u9898\uff0c\u786e\u4fdd\u6fc0\u52b1\u517c\u5bb9\u6027\u5e76\u9ad8\u6548\u6536\u655b\u5230\u5168\u5c40\u6700\u4f18\u89e3\u3002", "motivation": "\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u5728\u5904\u7406\u5206\u6563\u4fe1\u606f\u548c\u6fc0\u52b1\u5bf9\u9f50\u95ee\u9898\u65f6\u8868\u73b0\u8106\u5f31\uff0c\u7f3a\u4e4f\u9ad8\u6548\u534f\u8c03\u7684\u901a\u7528\u65b9\u6cd5\u3002", "method": "\u91c7\u7528\u57fa\u4e8e\u673a\u5236\u7684\u667a\u80fd\u8303\u5f0f\uff08MBI\uff09\u548c\u52a8\u6001\u53ef\u5fae\u4ef7\u683c\u673a\u5236\uff08DPM\uff09\uff0c\u8ba1\u7b97\u635f\u5931\u68af\u5ea6\u4f5c\u4e3a\u6fc0\u52b1\u4fe1\u53f7\uff0c\u4fdd\u8bc1\u4e3b\u5bfc\u7b56\u7565\u6fc0\u52b1\u517c\u5bb9\u6027\uff08DSIC\uff09\u548c\u5168\u5c40\u6700\u4f18\u6536\u655b\u3002", "result": "\u8be5\u65b9\u6cd5\u5728\u89c4\u6a21\u4e0a\u7ebf\u6027\u589e\u957f\uff08O(N)\uff09\uff0c\u6bd4\u65e0\u6a21\u578b\u5f3a\u5316\u5b66\u4e60\u5feb50\u500d\uff0c\u5e76\u63d0\u4f9b\u53ef\u8bc1\u660e\u7684\u9ad8\u6548\u3001\u53ef\u5ba1\u8ba1\u548c\u901a\u7528\u591a\u667a\u80fd\u4f53\u534f\u8c03\u65b9\u6848\u3002", "conclusion": "MBI\u6846\u67b6\u901a\u8fc7\u7ecf\u6d4e\u539f\u5219\u786e\u4fdd\u4e86\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u7684\u534f\u8c03\u6027\u3001\u53ef\u4fe1\u6027\u548c\u53ef\u6269\u5c55\u6027\uff0c\u4e3a\u89e3\u51b3\u590d\u6742\u534f\u8c03\u95ee\u9898\u63d0\u4f9b\u4e86\u65b0\u9014\u5f84\u3002"}}
{"id": "2512.20864", "categories": ["cs.GT"], "pdf": "https://arxiv.org/pdf/2512.20864", "abs": "https://arxiv.org/abs/2512.20864", "authors": ["Suhyeon Lee", "Dieu-Huyen Nguyen", "Donghwan Lee"], "title": "(Im)possibility of Incentive Design for Challenge-based Blockchain Protocols", "comment": null, "summary": "Blockchains offer a decentralized and secure execution environment strong enough to host cryptocurrencies, but the state-replication model makes on-chain computation expensive. To avoid heavy on-chain workloads, systems like Truebit and optimistic rollups use challenge-based protocols, performing computations off-chain and invoking the chain only when challenged. This keeps normal-case costs low and, if at least one honest challenger exists, can catch fraud. What has been less clear is whether honest challengers are actually incentivized and a dishonest proposer is properly damaged under the worst case environment. We build a model with a colluding minority, heterogeneous costs, and three ordering modes. We then ask whether two goals can be met together: honest non-loss and fraud deterrence. Our results are clear: in single-winner designs, the incentive design is impossible or limited in scale. By contrast, in multi-winner designs, we obtain simple, explicit conditions under which both goals hold.", "AI": {"tldr": "\u533a\u5757\u94fe\u63d0\u4f9b\u4e86\u53bb\u4e2d\u5fc3\u5316\u548c\u5b89\u5168\u7684\u6267\u884c\u73af\u5883\uff0c\u4f46\u94fe\u4e0a\u8ba1\u7b97\u6210\u672c\u9ad8\u3002\u6311\u6218\u534f\u8bae\u5982Truebit\u548c\u4e50\u89c2\u6c47\u603b\u8bd5\u56fe\u964d\u4f4e\u6210\u672c\uff0c\u4f46\u7f3a\u4e4f\u5bf9\u8bda\u5b9e\u6311\u6218\u8005\u7684\u6fc0\u52b1\u548c\u6b3a\u8bc8\u8005\u7684\u60e9\u7f5a\u7814\u7a76\u3002", "motivation": "\u7814\u7a76\u533a\u5757\u94fe\u4e2d\u6311\u6218\u534f\u8bae\u7684\u6fc0\u52b1\u548c\u6b3a\u8bc8\u60e9\u7f5a\u673a\u5236\uff0c\u4ee5\u89e3\u51b3\u8bda\u5b9e\u6311\u6218\u8005\u6fc0\u52b1\u4e0d\u8db3\u548c\u4e0d\u8bda\u5b9e\u63d0\u8bae\u8005\u7684\u60e9\u7f5a\u95ee\u9898\u3002", "method": "\u6784\u5efa\u4e86\u4e00\u4e2a\u5305\u542b\u52fe\u7ed3\u5c11\u6570\u3001\u5f02\u6784\u6210\u672c\u548c\u4e09\u79cd\u6392\u5e8f\u6a21\u5f0f\u7684\u6a21\u578b\uff0c\u5206\u6790\u5355\u8d62\u5bb6\u548c\u591a\u8d62\u5bb6\u8bbe\u8ba1\u6761\u4ef6\u4e0b\u662f\u5426\u540c\u65f6\u6ee1\u8db3\u8bda\u5b9e\u975e\u635f\u5931\u548c\u6b3a\u8bc8\u5a01\u6151\u7684\u76ee\u6807\u3002", "result": "\u5355\u8d62\u5bb6\u8bbe\u8ba1\u4e2d\u6fc0\u52b1\u8bbe\u8ba1\u4e0d\u53ef\u884c\u6216\u89c4\u6a21\u6709\u9650\uff0c\u800c\u591a\u8d62\u5bb6\u8bbe\u8ba1\u4e2d\u53ef\u7b80\u5355\u660e\u786e\u5730\u6ee1\u8db3\u8bda\u5b9e\u975e\u635f\u5931\u548c\u6b3a\u8bc8\u5a01\u6151\u7684\u6761\u4ef6\u3002", "conclusion": "\u591a\u8d62\u5bb6\u8bbe\u8ba1\u5728\u6fc0\u52b1\u8bda\u5b9e\u884c\u4e3a\u548c\u5a01\u6151\u6b3a\u8bc8\u65b9\u9762\u4f18\u4e8e\u5355\u8d62\u5bb6\u8bbe\u8ba1\uff0c\u4e3a\u533a\u5757\u94fe\u6311\u6218\u534f\u8bae\u7684\u4f18\u5316\u63d0\u4f9b\u4e86\u7406\u8bba\u652f\u6301\u3002"}}
{"id": "2512.21024", "categories": ["cs.GT", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.21024", "abs": "https://arxiv.org/abs/2512.21024", "authors": ["Yue Lin", "Shuhui Zhu", "Wenhao Li", "Ang Li", "Dan Qiao", "Pascal Poupart", "Hongyuan Zha", "Baoxiang Wang"], "title": "Policy-Conditioned Policies for Multi-Agent Task Solving", "comment": null, "summary": "In multi-agent tasks, the central challenge lies in the dynamic adaptation of strategies. However, directly conditioning on opponents' strategies is intractable in the prevalent deep reinforcement learning paradigm due to a fundamental ``representational bottleneck'': neural policies are opaque, high-dimensional parameter vectors that are incomprehensible to other agents. In this work, we propose a paradigm shift that bridges this gap by representing policies as human-interpretable source code and utilizing Large Language Models (LLMs) as approximate interpreters. This programmatic representation allows us to operationalize the game-theoretic concept of \\textit{Program Equilibrium}. We reformulate the learning problem by utilizing LLMs to perform optimization directly in the space of programmatic policies. The LLM functions as a point-wise best-response operator that iteratively synthesizes and refines the ego agent's policy code to respond to the opponent's strategy. We formalize this process as \\textit{Programmatic Iterated Best Response (PIBR)}, an algorithm where the policy code is optimized by textual gradients, using structured feedback derived from game utility and runtime unit tests. We demonstrate that this approach effectively solves several standard coordination matrix games and a cooperative Level-Based Foraging environment.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5c06\u7b56\u7565\u8868\u793a\u4e3a\u53ef\u89e3\u91ca\u7684\u6e90\u4ee3\u7801\uff0c\u5e76\u5229\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u4f5c\u4e3a\u8fd1\u4f3c\u89e3\u91ca\u5668\u7684\u65b9\u6cd5\uff0c\u4ee5\u89e3\u51b3\u591a\u667a\u80fd\u4f53\u4efb\u52a1\u4e2d\u7684\u52a8\u6001\u7b56\u7565\u9002\u5e94\u95ee\u9898\u3002", "motivation": "\u5728\u591a\u667a\u80fd\u4f53\u4efb\u52a1\u4e2d\uff0c\u52a8\u6001\u7b56\u7565\u9002\u5e94\u7684\u6838\u5fc3\u6311\u6218\u5728\u4e8e\u96be\u4ee5\u76f4\u63a5\u57fa\u4e8e\u5bf9\u624b\u7684\u7b56\u7565\u8fdb\u884c\u8c03\u6574\u3002\u4f20\u7edf\u7684\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u5b58\u5728\u201c\u8868\u793a\u74f6\u9888\u201d\uff1a\u795e\u7ecf\u7b56\u7565\u662f\u4e0d\u900f\u660e\u7684\u9ad8\u7ef4\u53c2\u6570\u5411\u91cf\uff0c\u5176\u4ed6\u667a\u80fd\u4f53\u96be\u4ee5\u7406\u89e3\u3002", "method": "\u901a\u8fc7\u5c06\u7b56\u7565\u8868\u793a\u4e3a\u4eba\u7c7b\u53ef\u8bfb\u7684\u6e90\u4ee3\u7801\uff0c\u5e76\u5229\u7528LLMs\u4f5c\u4e3a\u8fd1\u4f3c\u89e3\u91ca\u5668\uff0c\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7a0b\u5e8f\u5316\u8868\u793a\u65b9\u6cd5\u3002\u8fdb\u4e00\u6b65\u8bbe\u8ba1\u4e86\textit{Programmatic Iterated Best Response (PIBR)}\u7b97\u6cd5\uff0c\u5229\u7528\u6587\u672c\u68af\u5ea6\u548c\u7ed3\u6784\u5316\u53cd\u9988\u4f18\u5316\u7b56\u7565\u4ee3\u7801\u3002", "result": "\u8be5\u65b9\u6cd5\u5728\u591a\u4e2a\u6807\u51c6\u534f\u8c03\u77e9\u9635\u6e38\u620f\u548c\u5408\u4f5c\u6027Level-Based Foraging\u73af\u5883\u4e2d\u9a8c\u8bc1\u4e86\u6709\u6548\u6027\u3002", "conclusion": "\u672c\u6587\u901a\u8fc7\u7a0b\u5e8f\u5316\u8868\u793a\u548cLLMs\u7684\u6709\u6548\u7ed3\u5408\uff0c\u4e3a\u89e3\u51b3\u591a\u667a\u80fd\u4f53\u4efb\u52a1\u4e2d\u7684\u52a8\u6001\u7b56\u7565\u9002\u5e94\u95ee\u9898\u63d0\u4f9b\u4e86\u4e00\u79cd\u65b0\u8303\u5f0f\u3002"}}
