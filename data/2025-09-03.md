<div id=toc></div>

# Table of Contents

- [cs.GR](#cs.GR) [Total: 13]
- [cs.PL](#cs.PL) [Total: 6]
- [cs.GT](#cs.GT) [Total: 9]


<div id='cs.GR'></div>

# cs.GR [[Back]](#toc)

### [1] [Curve-based slicer for multi-axis DLP 3D printing](https://arxiv.org/abs/2509.00040)
*Chengkai Dai,Tao Liu,Dezhao Guo,Binzhi Sun,Guoxin Fang,Yeung Yam,Charlie C. L. Wang*

Main category: cs.GR

TL;DR: 本文提出了一种基于曲线的新型切片方法，用于在DLP 3D打印中生成动态变化的平面层，解决了大悬垂区域和阶梯伪影等问题。


<details>
  <summary>Details</summary>
Motivation: 动机是解决DLP 3D打印中的关键挑战，如大悬垂区域和阶梯伪影，同时保持高分辨率和快速打印的优势。

Method: 方法将切片问题转化为优化任务，通过计算参数曲线定义切片层和模型分区，并优化曲线以满足制造目标。

Result: 通过物理实验验证，优化的曲线能够稳健地指导复杂几何体的高质量打印。

Conclusion: 结论表明该方法能够有效解决DLP打印中的问题，并实现高质量、复杂几何体的打印。

Abstract: This paper introduces a novel curve-based slicing method for generating
planar layers with dynamically varying orientations in digital light processing
(DLP) 3D printing. Our approach effectively addresses key challenges in DLP
printing, such as regions with large overhangs and staircase artifacts, while
preserving its intrinsic advantages of high resolution and fast printing
speeds. We formulate the slicing problem as an optimization task, in which
parametric curves are computed to define both the slicing layers and the model
partitioning through their tangent planes. These curves inherently define
motion trajectories for the build platform and can be optimized to meet
critical manufacturing objectives, including collision-free motion and
floating-free deposition. We validate our method through physical experiments
on a robotic multi-axis DLP printing setup, demonstrating that the optimized
curves can robustly guide smooth, high-quality fabrication of complex
geometries.

</details>


### [2] [Lightning Fast Caching-based Parallel Denoising Prediction for Accelerating Talking Head Generation](https://arxiv.org/abs/2509.00052)
*Jianzhi Long,Wenhao Sun,Rongcheng Tu,Dacheng Tao*

Main category: cs.GR

TL;DR: 该论文提出了一种针对扩散式说话头模型的加速框架，通过LightningCP和DFA技术显著提高了推理速度，同时保持了视频质量。


<details>
  <summary>Details</summary>
Motivation: 现有的扩散式说话头模型虽然能够生成高质量的视频，但由于推理速度慢，限制了实际应用。现有的通用扩散模型加速方法未能充分利用说话头生成任务的时间和空间冗余特征。

Method: 论文提出了一种任务专用框架，包括两个关键创新：LightningCP通过缓存静态特征绕过大多数模型层，并利用并行预测加速推理；DFA通过空间解耦限制注意力计算到动态前景区域，进一步提升速度。

Result: 大量实验表明，该框架在保持视频质量的同时，显著提高了推理速度。

Conclusion: 论文提出的框架有效地解决了扩散式说话头模型推理速度慢的问题，为实际应用提供了可行的解决方案。

Abstract: Diffusion-based talking head models generate high-quality, photorealistic
videos but suffer from slow inference, limiting practical applications.
Existing acceleration methods for general diffusion models fail to exploit the
temporal and spatial redundancies unique to talking head generation. In this
paper, we propose a task-specific framework addressing these inefficiencies
through two key innovations. First, we introduce Lightning-fast Caching-based
Parallel denoising prediction (LightningCP), caching static features to bypass
most model layers in inference time. We also enable parallel prediction using
cached features and estimated noisy latents as inputs, efficiently bypassing
sequential sampling. Second, we propose Decoupled Foreground Attention (DFA) to
further accelerate attention computations, exploiting the spatial decoupling in
talking head videos to restrict attention to dynamic foreground regions.
Additionally, we remove reference features in certain layers to bring extra
speedup. Extensive experiments demonstrate that our framework significantly
improves inference speed while preserving video quality.

</details>


### [3] [Evaluate Neighbor Search for Curve-based Vector Field Processing](https://arxiv.org/abs/2509.00180)
*Nguyen Phan,Guoning Chen*

Main category: cs.GR

TL;DR: 该研究评估了两种流行的邻近段搜索策略在点基向量场重建和段显著性估计任务中的表现，提出了多种测量方法来比较不同搜索策略的配置，并得出了一些新发现。


<details>
  <summary>Details</summary>
Motivation: 由于积分曲线可能无法完全代表原始流动行为且分布不均，现有的邻近段搜索策略常返回有偏差和冗余的结果。缺乏对不同搜索策略配置影响的系统性研究，本研究旨在填补这一空白。

Method: 研究结合不同距离度量评估了两种流行的邻近段搜索策略，在点基向量场重建和段显著性估计任务中进行了大量测试，并提出了平均邻近距离和均匀性等测量方法。

Result: 研究部分证实了理想邻近段配置的预期，同时揭示了一些被社区忽视的新发现。

Conclusion: 该研究为邻近段搜索策略的选择和优化提供了系统性的评估方法和见解，对后续研究具有参考价值。

Abstract: Curve-based representations, particularly integral curves, are often used to
represent large-scale computational fluid dynamic simulations. Processing and
analyzing curve-based vector field data sets often involves searching for
neighboring segments given a query point or curve segment. However, because the
original flow behavior may not be fully represented by the set of integral
curves and the input integral curves may not be evenly distributed in space,
popular neighbor search strategies often return skewed and redundant
neighboring segments. Yet, there is a lack of systematic and comprehensive
research on how different configurations of neighboring segments returned by
specific neighbor search strategies affect subsequent tasks. To fill this gap,
this study evaluates the performance of two popular neighbor search strategies
combined with different distance metrics on a point-based vector field
reconstruction task and a segment saliency estimation using input integral
curves. A large number of reconstruction tests and saliency calculations are
conducted for the study. To characterize the configurations of neighboring
segments for an effective comparison of different search strategies, a number
of measures, like average neighbor distance and uniformity, are proposed. Our
study leads to a few observations that partially confirm our expectations about
the ideal configurations of a neighborhood while revealing additional findings
that were overlooked by the community.

</details>


### [4] [3D-LATTE: Latent Space 3D Editing from Textual Instructions](https://arxiv.org/abs/2509.00269)
*Maria Parelli,Michael Oechsle,Michael Niemeyer,Federico Tombari,Andreas Geiger*

Main category: cs.GR

TL;DR: 提出了一种基于原生3D扩散模型的训练免费编辑方法，通过混合生成与源对象的3D注意力图来指导编辑合成，结合几何感知正则化引导、傅里叶域谱调制策略和3D增强细化步骤，实现了高质量的3D编辑。


<details>
  <summary>Details</summary>
Motivation: 当前基于2D先验的3D资产编辑方法存在视图不一致的编辑信号问题，导致编辑质量不如生成模型。本文旨在解决这一问题，提出直接操作3D几何的新方法。

Method: 方法包括在原生3D扩散模型的潜在空间中进行操作，通过混合3D注意力图指导编辑合成，结合几何感知正则化引导、傅里叶域谱调制策略和3D增强细化步骤。

Result: 实验结果表明，该方法在多种形状和语义操作下表现优于先前的3D编辑方法，实现了高保真、精确且稳健的编辑效果。

Conclusion: 本文提出的训练免费编辑方法通过直接操作3D几何，解决了视图不一致问题，为3D资产编辑提供了高质量的解决方案。

Abstract: Despite the recent success of multi-view diffusion models for
text/image-based 3D asset generation, instruction-based editing of 3D assets
lacks surprisingly far behind the quality of generation models. The main reason
is that recent approaches using 2D priors suffer from view-inconsistent editing
signals. Going beyond 2D prior distillation methods and multi-view editing
strategies, we propose a training-free editing method that operates within the
latent space of a native 3D diffusion model, allowing us to directly manipulate
3D geometry. We guide the edit synthesis by blending 3D attention maps from the
generation with the source object. Coupled with geometry-aware regularization
guidance, a spectral modulation strategy in the Fourier domain and a refinement
step for 3D enhancement, our method outperforms previous 3D editing methods
enabling high-fidelity, precise, and robust edits across a wide range of shapes
and semantic manipulations.

</details>


### [5] [Locality-Aware Automatic Differentiation on the GPU for Mesh-Based Computations](https://arxiv.org/abs/2509.00406)
*Ahmed H. Mahmoud,Jonathan Ragan-Kelley,Justin Solomon*

Main category: cs.GR

TL;DR: 提出了一种高性能系统，用于在三角形网格上定义的函数的自动微分（AD），通过利用网格能量函数固有的稀疏性和局部性，在GPU上实现快速的梯度和Hessian计算。


<details>
  <summary>Details</summary>
Motivation: 现有的自动微分方法在处理网格能量函数时，由于全局计算图的构建和遍历导致效率低下，无法充分利用GPU的性能。本文旨在开发一种高效的局部计算模式，减少内存流量和全局同步开销。

Method: 系统采用基于元素的前向模式微分，使得所有局部计算保持在GPU寄存器或共享内存中。与反向模式方法不同，该方法在运行时进行微分，避免了全局计算图和内存流量问题。用户可以定义局部能量项，系统负责并行评估、导数计算和稀疏Hessian组装。

Result: 在多种应用（如布料模拟、表面参数化、网格平滑和球面流形优化）中进行了测试。对于二阶导数，比优化的PyTorch实现快6.2倍；对于Hessian-向量积，快2.76倍；对于一阶导数，比Warp、JAX和Dr.JIT分别快6.38倍、2.89倍和1.98倍，且与手工编写的导数性能相当。

Conclusion: 该系统通过局部计算和高效的并行处理实现了显著的性能提升，同时简化了用户对复杂微分任务的处理，为网格能量函数的自动微分提供了一种高效且易用的解决方案。

Abstract: We present a high-performance system for automatic differentiation (AD) of
functions defined on triangle meshes that exploits the inherent sparsity and
locality of mesh-based energy functions to achieve fast gradient and Hessian
computation on the GPU. Our system is designed around per-element forward-mode
differentiation, enabling all local computations to remain in GPU registers or
shared memory. Unlike reverse-mode approaches that construct and traverse
global computation graphs, our method performs differentiation on the fly,
minimizing memory traffic and avoiding global synchronization. Our programming
model allows users to define local energy terms while the system handles
parallel evaluation, derivative computation, and sparse Hessian assembly. We
benchmark our system on a range of applications--cloth simulation, surface
parameterization, mesh smoothing, and spherical manifold optimization. We
achieve a geometric mean speedup of 6.2x over optimized PyTorch implementations
for second-order derivatives, and 2.76x speedup for Hessian-vector products.
For first-order derivatives, our system is 6.38x, 2.89x, and 1.98x faster than
Warp, JAX, and Dr.JIT, respectively, while remaining on par with hand-written
derivatives.

</details>


### [6] [LatentEdit: Adaptive Latent Control for Consistent Semantic Editing](https://arxiv.org/abs/2509.00541)
*Siyi Liu,Weiming Chen,Yushun Tang,Zhihai He*

Main category: cs.GR

TL;DR: LatentEdit是一种自适应的潜在融合框架，通过在语义重要区域选择性保留源图像特征，并在其他区域生成目标内容，实现了高质量的图像编辑，同时保持了背景相似性。


<details>
  <summary>Details</summary>
Motivation: 尽管基于扩散的图像编辑取得了显著成功，但在保持背景相似性的同时实现高质量编辑仍然具有挑战性。

Method: LatentEdit通过动态结合当前潜在代码与反向源图像的参考潜在代码，选择性保留高相似度区域的源特征，并在目标提示下生成其他区域的内容，无需复杂的注意力机制或内部模型修改。

Result: 在PIE-Bench数据集上的实验表明，LatentEdit在保真度和可编辑性之间达到了最优平衡，甚至在8-15步中超越了现有方法。

Conclusion: LatentEdit提供了一种轻量级的即插即用解决方案，显著提升了实时部署效率，适用于UNet和DiT架构。

Abstract: Diffusion-based Image Editing has achieved significant success in recent
years. However, it remains challenging to achieve high-quality image editing
while maintaining the background similarity without sacrificing speed or memory
efficiency. In this work, we introduce LatentEdit, an adaptive latent fusion
framework that dynamically combines the current latent code with a reference
latent code inverted from the source image. By selectively preserving source
features in high-similarity, semantically important regions while generating
target content in other regions guided by the target prompt, LatentEdit enables
fine-grained, controllable editing. Critically, the method requires no internal
model modifications or complex attention mechanisms, offering a lightweight,
plug-and-play solution compatible with both UNet-based and DiT-based
architectures. Extensive experiments on the PIE-Bench dataset demonstrate that
our proposed LatentEdit achieves an optimal balance between fidelity and
editability, outperforming the state-of-the-art method even in 8-15 steps.
Additionally, its inversion-free variant further halves the number of neural
function evaluations and eliminates the need for storing any intermediate
variables, substantially enhancing real-time deployment efficiency.

</details>


### [7] [IntrinsicReal: Adapting IntrinsicAnything from Synthetic to Real Objects](https://arxiv.org/abs/2509.00777)
*Xiaokang Wei,Zizheng Yan,Zhangyang Xiong,Yiming Hao,Yipeng Qin,Xiaoguang Han*

Main category: cs.GR

TL;DR: IntrinsicReal提出了一种新的域适应框架，通过双伪标签策略弥补合成数据与真实数据之间的差距，显著提升了单RGB图像的反射率估计性能。


<details>
  <summary>Details</summary>
Motivation: 当前方法主要依赖合成数据训练，直接应用于真实世界图像时存在域差距问题，导致性能不佳。

Method: IntrinsicReal采用双伪标签策略（绝对置信度阈值和相对偏好排序）对IntrinsicAnything进行微调，适应真实域。

Result: 实验结果表明，IntrinsicReal在合成和真实数据集上均达到了最先进的反射率估计效果。

Conclusion: IntrinsicReal通过创新的域适应方法有效解决了合成与真实数据间的差距问题，提升了模型性能。

Abstract: Estimating albedo (a.k.a., intrinsic image decomposition) from single RGB
images captured in real-world environments (e.g., the MVImgNet dataset)
presents a significant challenge due to the absence of paired images and their
ground truth albedos. Therefore, while recent methods (e.g., IntrinsicAnything)
have achieved breakthroughs by harnessing powerful diffusion priors, they
remain predominantly trained on large-scale synthetic datasets (e.g.,
Objaverse) and applied directly to real-world RGB images, which ignores the
large domain gap between synthetic and real-world data and leads to suboptimal
generalization performance. In this work, we address this gap by proposing
IntrinsicReal, a novel domain adaptation framework that bridges the
above-mentioned domain gap for real-world intrinsic image decomposition.
Specifically, our IntrinsicReal adapts IntrinsicAnything to the real domain by
fine-tuning it using its high-quality output albedos selected by a novel dual
pseudo-labeling strategy: i) pseudo-labeling with an absolute confidence
threshold on classifier predictions, and ii) pseudo-labeling using the relative
preference ranking of classifier predictions for individual input objects. This
strategy is inspired by human evaluation, where identifying the highest-quality
outputs is straightforward, but absolute scores become less reliable for
sub-optimal cases. In these situations, relative comparisons of outputs become
more accurate. To implement this, we propose a novel two-phase pipeline that
sequentially applies these pseudo-labeling techniques to effectively adapt
IntrinsicAnything to the real domain. Experimental results show that our
IntrinsicReal significantly outperforms existing methods, achieving
state-of-the-art results for albedo estimation on both synthetic and real-world
datasets.

</details>


### [8] [RealMat: Realistic Materials with Diffusion and Reinforcement Learning](https://arxiv.org/abs/2509.01134)
*Xilong Zhou,Pedro Figueiredo,Miloš Hašan,Valentin Deschaintre,Paul Guerrero,Yiwei Hu,Nima Khademi Kalantari*

Main category: cs.GR

TL;DR: 提出了一种基于扩散模型（RealMat）的材料生成方法，结合了合成数据和自然光照下的真实材料照片，通过强化学习进一步提升了生成材料的真实感。


<details>
  <summary>Details</summary>
Motivation: 现有的材料生成方法多基于合成数据，存在与真实材料的视觉差距；而小规模的闪光照片数据集又限制了多样性和规模。因此，研究旨在开发一种能够生成高质量且真实感强的材料的模型。

Method: 利用预训练的Stable Diffusion XL (SDXL)微调合成材料网格，再通过强化学习（RL）和真实材料图像数据集进一步优化模型，提升生成材料的真实感。

Result: 研究结果表明，RealMat相比基础模型和相关工作，能够生成更具真实感的材料图像。

Conclusion: 提出的RealMat方法通过结合合成数据与真实材料光照信息，利用强化学习优化生成过程，显著提升了材料生成的真实感，为3D内容创作提供了更高质量的素材。

Abstract: Generative models for high-quality materials are particularly desirable to
make 3D content authoring more accessible. However, the majority of material
generation methods are trained on synthetic data. Synthetic data provides
precise supervision for material maps, which is convenient but also tends to
create a significant visual gap with real-world materials. Alternatively,
recent work used a small dataset of real flash photographs to guarantee
realism, however such data is limited in scale and diversity. To address these
limitations, we propose RealMat, a diffusion-based material generator that
leverages realistic priors, including a text-to-image model and a dataset of
realistic material photos under natural lighting. In RealMat, we first finetune
a pretrained Stable Diffusion XL (SDXL) with synthetic material maps arranged
in $2 \times 2$ grids. This way, our model inherits some realism of SDXL while
learning the data distribution of the synthetic material grids. Still, this
creates a realism gap, with some generated materials appearing synthetic. We
propose to further finetune our model through reinforcement learning (RL),
encouraging the generation of realistic materials. We develop a realism reward
function for any material image under natural lighting, by collecting a
large-scale dataset of realistic material images. We show that this approach
increases generated materials' realism compared to our base model and related
work.

</details>


### [9] [Quantum Brush: A quantum computing-based tool for digital painting](https://arxiv.org/abs/2509.01442)
*João S. Ferreira,Arianna Crippa,Astryd Park,Daniel Bultrini,Pierre Fromholz,Roman Lipski,Karl Jansen,James R. Wootton*

Main category: cs.GR

TL;DR: Quantum Brush是一款开源数字绘画工具，利用量子计算生成新颖的艺术表达。


<details>
  <summary>Details</summary>
Motivation: 探索量子计算在艺术创作中的应用，通过量子效果创造独特的美学表达。

Method: 开发了四种不同的笔刷，将笔触转化为独特的量子算法，每种算法展示了量子效果的不同美学可能。

Result: 工具成功在IQM的Sirius设备上运行，证明了其在当前NISQ设备上的兼容性。

Conclusion: Quantum Brush展示了量子计算在艺术领域的潜力，为未来的量子艺术工具发展奠定了基础。

Abstract: We present Quantum Brush, an open-source digital painting tool that harnesses
quantum computing to generate novel artistic expressions. The tool includes
four different brushes that translate strokes into unique quantum algorithms,
each highlighting a different way in which quantum effects can produce novel
aesthetics. Each brush is designed to be compatible with the current noisy
intermediate-scale quantum (NISQ) devices, as demonstrated by executing them on
IQM's Sirius device.

</details>


### [10] [HodgeFormer: Transformers for Learnable Operators on Triangular Meshes through Data-Driven Hodge Matrices](https://arxiv.org/abs/2509.01839)
*Akis Nousias,Stavros Nousias*

Main category: cs.GR

TL;DR: 提出了一种基于离散外微积分的新方法，通过多头注意力机制近似Hodge矩阵，替代传统需要昂贵特征值分解的方法，用于网格分割和分类任务。


<details>
  <summary>Details</summary>
Motivation: 传统Transformer架构在图形和网格分析中依赖特征值分解等方法，计算成本高。本文旨在通过离散Hodge算子直接学习离散算子，避免复杂的预处理。

Method: 利用多头注意力机制近似Hodge矩阵（$\star_0$、$\star_1$和$\star_2$），并通过离散外微积分构建Hodge Laplacian算子，实现高效的深度学习层。

Result: 提出的方法在网格分割和分类任务中表现与现有方法相当，同时显著降低了计算成本，无需特征值分解或复杂预处理。

Conclusion: 该方法通过直接学习离散算子，提供了一种高效且性能相当的替代方案，显著提升了计算效率。

Abstract: Currently, prominent Transformer architectures applied on graphs and meshes
for shape analysis tasks employ traditional attention layers that heavily
utilize spectral features requiring costly eigenvalue decomposition-based
methods. To encode the mesh structure, these methods derive positional
embeddings, that heavily rely on eigenvalue decomposition based operations,
e.g. on the Laplacian matrix, or on heat-kernel signatures, which are then
concatenated to the input features. This paper proposes a novel approach
inspired by the explicit construction of the Hodge Laplacian operator in
Discrete Exterior Calculus as a product of discrete Hodge operators and
exterior derivatives, i.e. $(L := \star_0^{-1} d_0^T \star_1 d_0)$. We adjust
the Transformer architecture in a novel deep learning layer that utilizes the
multi-head attention mechanism to approximate Hodge matrices $\star_0$,
$\star_1$ and $\star_2$ and learn families of discrete operators $L$ that act
on mesh vertices, edges and faces. Our approach results in a
computationally-efficient architecture that achieves comparable performance in
mesh segmentation and classification tasks, through a direct learning
framework, while eliminating the need for costly eigenvalue decomposition
operations or complex preprocessing operations.

</details>


### [11] [GRMM: Real-Time High-Fidelity Gaussian Morphable Head Model with Learned Residuals](https://arxiv.org/abs/2509.02141)
*Mohit Mendiratta,Mayur Deshmukh,Kartik Teotia,Vladislav Golyanik,Adam Kortylewski,Christian Theobalt*

Main category: cs.GR

TL;DR: GRMM 是首个基于高斯 3D 可变形模型的全头模型，通过增强基础 3DMM 的几何和外观细节，实现了高保真度和交互式实时渲染。


<details>
  <summary>Details</summary>
Motivation: 传统 PCA 基于网格的 3D 可变形模型在分辨率、细节和真实感上受限，而神经体积方法虽然提高了真实感，但交互速度慢。因此，需要一种既能高保真又能实时渲染的解决方案。

Method: GRMM 在基础 3DMM 上添加几何和外观残差组件，通过粗解码器生成顶点级网格变形，细解码器控制高斯级外观，并结合轻量级 CNN 增强图像真实感。

Result: GRMM 在单目 3D 人脸重建、新视角合成和表情迁移任务中，在保真度和表情准确性方面超越了现有方法，并保持了 75 FPS 的实时渲染性能。

Conclusion: GRMM 通过残差建模和高斯 3D 可变形模型，成功实现了高保真相交互式渲染，为 3D 人脸建模和动画提供了新的解决方案。

Abstract: 3D Morphable Models (3DMMs) enable controllable facial geometry and
expression editing for reconstruction, animation, and AR/VR, but traditional
PCA-based mesh models are limited in resolution, detail, and photorealism.
Neural volumetric methods improve realism but remain too slow for interactive
use. Recent Gaussian Splatting (3DGS) based facial models achieve fast,
high-quality rendering but still depend solely on a mesh-based 3DMM prior for
expression control, limiting their ability to capture fine-grained geometry,
expressions, and full-head coverage. We introduce GRMM, the first full-head
Gaussian 3D morphable model that augments a base 3DMM with residual geometry
and appearance components, additive refinements that recover high-frequency
details such as wrinkles, fine skin texture, and hairline variations. GRMM
provides disentangled control through low-dimensional, interpretable parameters
(e.g., identity shape, facial expressions) while separately modelling residuals
that capture subject- and expression-specific detail beyond the base model's
capacity. Coarse decoders produce vertex-level mesh deformations, fine decoders
represent per-Gaussian appearance, and a lightweight CNN refines rasterised
images for enhanced realism, all while maintaining 75 FPS real-time rendering.
To learn consistent, high-fidelity residuals, we present EXPRESS-50, the first
dataset with 60 aligned expressions across 50 identities, enabling robust
disentanglement of identity and expression in Gaussian-based 3DMMs. Across
monocular 3D face reconstruction, novel-view synthesis, and expression
transfer, GRMM surpasses state-of-the-art methods in fidelity and expression
accuracy while delivering interactive real-time performance.

</details>


### [12] [Think2Sing: Orchestrating Structured Motion Subtitles for Singing-Driven 3D Head Animation](https://arxiv.org/abs/2509.02278)
*Zikai Huang,Yihan Zhou,Xuemiao Xu,Cheng Xu,Xiaofen Xing,Jing Qin,Shengfeng He*

Main category: cs.GR

TL;DR: 论文提出了Think2Sing框架，利用扩散模型和大语言模型生成基于歌词和声学的语义连贯且时间一致的3D头部动画，解决了现有语音驱动方法在唱歌动画中效果简化、情感平淡的问题。


<details>
  <summary>Details</summary>
Motivation: 唱歌驱动的3D头部动画具有广泛的应用前景，但目前的方法在情感表现和语义一致性上表现不佳，亟需一种能够捕捉歌唱中丰富情感和动态韵律的技术。

Method: 提出Think2Sing框架，结合扩散模型和大语言模型，通过‘运动字幕’作为辅助语义表示，利用新颖的‘歌唱思维链推理’和声学引导检索，实现精细控制和改善表情运动建模。

Result: 实验表明，Think2Sing在真实性、表现力和情感保真度上优于现有方法，同时支持灵活的用户可控动画编辑。

Conclusion: Think2Sing为解决唱歌驱动的3D头部动画挑战提供了一种创新解决方案，展示了其在虚拟角色创建和多模态学习中的潜力。

Abstract: Singing-driven 3D head animation is a challenging yet promising task with
applications in virtual avatars, entertainment, and education. Unlike speech,
singing involves richer emotional nuance, dynamic prosody, and lyric-based
semantics, requiring the synthesis of fine-grained, temporally coherent facial
motion. Existing speech-driven approaches often produce oversimplified,
emotionally flat, and semantically inconsistent results, which are insufficient
for singing animation. To address this, we propose Think2Sing, a
diffusion-based framework that leverages pretrained large language models to
generate semantically coherent and temporally consistent 3D head animations,
conditioned on both lyrics and acoustics. A key innovation is the introduction
of motion subtitles, an auxiliary semantic representation derived through a
novel Singing Chain-of-Thought reasoning process combined with acoustic-guided
retrieval. These subtitles contain precise timestamps and region-specific
motion descriptions, serving as interpretable motion priors. We frame the task
as a motion intensity prediction problem, enabling finer control over facial
regions and improving the modeling of expressive motion. To support this, we
create a multimodal singing dataset with synchronized video, acoustic
descriptors, and motion subtitles, enabling diverse and expressive motion
learning. Extensive experiments show that Think2Sing outperforms
state-of-the-art methods in realism, expressiveness, and emotional fidelity,
while also offering flexible, user-controllable animation editing.

</details>


### [13] [Unifi3D: A Study on 3D Representations for Generation and Reconstruction in a Common Framework](https://arxiv.org/abs/2509.02474)
*Nina Wiedemann,Sainan Liu,Quentin Leboutet,Katelyn Gao,Benjamin Ummenhofer,Michael Paulitsch,Kai Yuan*

Main category: cs.GR

TL;DR: 论文提出了一个统一的评估框架，用于比较和评估多种3D表示方法在重建和生成中的表现，重点关注质量、计算效率和泛化性能。


<details>
  <summary>Details</summary>
Motivation: 随着文本和图像生成的快速发展，3D生成的研究逐渐成为热点。然而，3D表示方法多样且分散，缺乏统一的评估标准。

Method: 研究设计了一个统一的评估框架，比较了多种3D表示方法（如体素网格、神经辐射场等），并从质量、计算效率和泛化性能等多个维度进行评估。

Result: 实验结果显示，重建误差对整体性能有显著影响，强调了评估生成和重建联合效果的重要性。研究还提供了选择合适3D模型的建议。

Conclusion: 该框架为3D生成领域提供了标准化评估方法，并支持开发更具鲁棒性和应用针对性的3D解决方案。代码已开源。

Abstract: Following rapid advancements in text and image generation, research has
increasingly shifted towards 3D generation. Unlike the well-established
pixel-based representation in images, 3D representations remain diverse and
fragmented, encompassing a wide variety of approaches such as voxel grids,
neural radiance fields, signed distance functions, point clouds, or octrees,
each offering distinct advantages and limitations. In this work, we present a
unified evaluation framework designed to assess the performance of 3D
representations in reconstruction and generation. We compare these
representations based on multiple criteria: quality, computational efficiency,
and generalization performance. Beyond standard model benchmarking, our
experiments aim to derive best practices over all steps involved in the 3D
generation pipeline, including preprocessing, mesh reconstruction, compression
with autoencoders, and generation. Our findings highlight that reconstruction
errors significantly impact overall performance, underscoring the need to
evaluate generation and reconstruction jointly. We provide insights that can
inform the selection of suitable 3D models for various applications,
facilitating the development of more robust and application-specific solutions
in 3D generation. The code for our framework is available at
https://github.com/isl-org/unifi3d.

</details>


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [14] [ChopChop: a Programmable Framework for Semantically Constraining the Output of Language Models](https://arxiv.org/abs/2509.00360)
*Shaan Nagy,Timothy Zhou,Nadia Polikarpova,Loris D'Antoni*

Main category: cs.PL

TL;DR: ChopChop是一个可编程框架，用于语义约束解码，确保语言模型生成的代码满足丰富的语义属性。


<details>
  <summary>Details</summary>
Motivation: 当前语言模型生成的代码无法保证正确性，通常违反类型安全、程序不变性或语义等价性。现有约束解码方法仅适用于浅层语法约束或依赖于脆弱的语义编码。

Method: ChopChop通过基于共归纳的形式化方法将令牌级生成与抽象程序结构推理连接起来，并将约束强制执行视为正则共数据上的可实现性问题。

Result: ChopChop在类型安全和程序等价性约束下的代码生成中表现出广泛的适用性，显著提高了生成成功率并保持了实用的解码延迟。

Conclusion: ChopChop将语义约束解码从一种小众技术转变为语言模型的系统性和原则性扩展，推动了LM驱动代码生成的进一步发展。

Abstract: Language models (LMs) can generate code, but cannot guarantee its
correctness--producing outputs that often violate type safety, program
invariants, or semantic equivalence. Constrained decoding offers a solution by
restricting generation to programs that satisfy desired properties. Yet,
existing methods are limited to shallow syntactic constraints or rely on
brittle, ad hoc encodings of semantics over token sequences.
  We present ChopChop, the first programmable framework for semantic
constrained decoding, enabling LMs to generate code that provably satisfies
rich semantic properties. ChopChop connects token-level generation with
reasoning over abstract program structures using a coinduction-based formalism
and reduces constraint enforcement to a realizability problem over regular
codata. We demonstrate ChopChop's generality through generation constrained by
type safety and program equivalence, showing how formal methods can be
seamlessly integrated into LM-driven code generation. ChopChop transforms
semantic constrained decoding from a niche technique into a systematic,
principled extension of LMs--improving success rates across models and tasks
while maintaining practical decoding latency.

</details>


### [15] [A Hoare Logic for Symmetry Properties](https://arxiv.org/abs/2509.00587)
*Vaibhav Mehta,Justin Hsu*

Main category: cs.PL

TL;DR: 论文提出了一种基于群动作的形式化方法，用于验证程序的对称性，并设计了一个Hoare风格的逻辑来验证这些属性，最终开发了一个原型工具SymVerif。


<details>
  <summary>Details</summary>
Motivation: 现有的形式化方法在验证对称性属性方面支持不足，论文旨在填补这一空白，提供一种通用的对称性验证方法。

Method: 设计了一种用于描述群动作的语法，并开发了Hoare风格的逻辑，将群动作替代传统的pre-和post-condition断言，用于验证程序的对称性。

Result: 开发了原型工具SymVerif，并在手工设计的测试用例上验证了对称性属性，发现了McLachlan_Quispel_2002模型中存在的错误。

Conclusion: 论文通过群动作和Hoare逻辑的结合，为程序对称性的形式化验证提供了一种有效的方法，并通过工具验证了其实用性。

Abstract: Many natural program correctness properties can be stated in terms of
  symmetries, but existing formal methods have little support for reasoning
  about such properties. We consider how to formally verify a broad class of
  symmetry properties expressed in terms of group actions. To specify these
  properties, we design a syntax for group actions, supporting standard
  constructions and a natural notion of entailment. Then, we develop a
  Hoare-style logic for verifying symmetry properties of imperative programs,
  where group actions take the place of the typical pre- and post-condition
  assertions. Finally, we develop a prototype tool $\mathsf{SymVerif}$, and use
  it to verify symmetry properties on a series of handcrafted benchmarks. Our
  tool uncovered an error in a model of a dynamical system described by
\citet{McLachlan_Quispel_2002}.

</details>


### [16] [Formalizing Linear Motion G-code for Invariant Checking and Differential Testing of Fabrication Tools](https://arxiv.org/abs/2509.00699)
*Yumeng He,Chandrakana Nandi,Sreepathi Pai*

Main category: cs.PL

TL;DR: 论文提出了一种新算法，将3D打印中的G代码提升为立方体集合，并定义近似点云表示，以支持CAD模型错误定位、切片器比较和网格修复工具评估。


<details>
  <summary>Details</summary>
Motivation: 传统的3D打印制造流程缺乏类似于编程语言中检查程序不变量的方法。作者希望通过引入类似的技术提升制造流程的可靠性。

Method: 提出一种算法，将G代码转换为立方体集合并定义近似点云表示，用于高效操作。开发工具GlitchFinder实现该算法。

Result: 在58个真实CAD模型上测试表明，GlitchFinder能有效识别小特征导致的切片问题、比较不同切片器的差异，并检测网格修复工具引入的错误。

Conclusion: 该算法为3D制造流程提供了一种新方法，能够在多个场景中识别和解决问题，增强了制造的可靠性。

Abstract: The computational fabrication pipeline for 3D printing is much like a
compiler - users design models in Computer Aided Design (CAD) tools that are
lowered to polygon meshes to be ultimately compiled to machine code by 3D
slicers. For traditional compilers and programming languages, techniques for
checking program invariants are well-established. Similarly, methods like
differential testing are often used to uncover bugs in compilers themselves,
which makes them more reliable. The fabrication pipeline would benefit from
similar techniques but traditional approaches do not directly apply to the
representations used in this domain. Unlike traditional programs, 3D models
exist both as geometric objects as well as machine code that ultimately runs on
the hardware. The machine code, like in traditional compiling, is affected by
many factors like the model, the slicer being used, and numerous
user-configurable parameters that control the slicing process. In this work, we
propose a new algorithm for lifting G-code (a common language used in
fabrication pipelines) by denoting a G-code program to a set of cuboids, and
then defining an approximate point cloud representation for efficiently
operating on these cuboids. Our algorithm opens up new opportunities: we show
three use cases that demonstrate how it enables error localization in CAD
models through invariant checking, quantitative comparisons between slicers,
and evaluating the efficacy of mesh repair tools. We present a prototype
implementation of our algorithm in a tool, GlitchFinder, and evaluate it on 58
real-world CAD models. Our results show that GlitchFinder is particularly
effective in identifying slicing issues due to small features, can highlight
differences in how popular slicers (Cura and PrusaSlicer) slice the same model,
and can identify cases where mesh repair tools (MeshLab and Meshmixer)
introduce new errors during repair.

</details>


### [17] [Decision Procedure for A Theory of String Sequences](https://arxiv.org/abs/2509.00948)
*Denghang Hu,Taolue Chen,Philipp Rümmer,Fu Song,Zhilin Wu*

Main category: cs.PL

TL;DR: 本文提出了一种字符串序列理论，并研究了其可满足性，证明了在直線片段下可恢复可判定性，并将其实现为工具 $\\ostrichseq$，实验验证了该方法的有效性。


<details>
  <summary>Details</summary>
Motivation: 现有SMT求解器通常仅支持通用的序列理论，而不直接支持字符串操作，如正则表达式匹配、字符串分割等，这限制了其在字符串处理程序中的应用。

Method: 通过将字符串序列编码为字符串，并将字符串序列操作对应为字符串操作，研究其可满足性，并限制在直線片段中以恢复可判定性。

Result: 实验结果表明，提出的方法在真实世界的JavaScript程序、手动制作的模板和单元测试生成的基准约束上具有高效性。

Conclusion: 通过理论分析和工具实现，本文证明了字符串序列理论在特定条件下的可判定性，并在实际应用中展示了其有效性。

Abstract: The theory of sequences, supported by many SMT solvers, can model program
data types including bounded arrays and lists. Sequences are parameterized by
the element data type and provide operations such as accessing elements,
concatenation, forming sub-sequences and updating elements. Strings and
sequences are intimately related; many operations, e.g., matching a string
according to a regular expression, splitting strings, or joining strings in a
sequence, are frequently used in string-manipulating programs. Nevertheless,
these operations are typically not directly supported by existing SMT solvers,
which instead only consider the generic theory of sequences. In this paper, we
propose a theory of string sequences and study its satisfiability. We show
that, while it is undecidable in general, the decidability can be recovered by
restricting to the straight-line fragment. This is shown by encoding each
string sequence as a string, and each string sequence operation as a
corresponding string operation. We provide pre-image computation for the
resulting string operations with respect to automata, effectively casting it
into the generic OSTRICH string constraint solving framework. We implement the
new decision procedure as a tool $\ostrichseq$, and carry out experiments on
benchmark constraints generated from real-world JavaScript programs,
hand-crafted templates and unit tests. The experiments confirm the efficacy of
our approach.

</details>


### [18] [Type-Based Incorrectness Reasoning](https://arxiv.org/abs/2509.01511)
*Zhe Zhou,Benjamin Delaware,Suresh Jagannathan*

Main category: cs.PL

TL;DR: 覆盖类型通过支持必须风格的欠近似推理，扩展了许多函数式语言中的细化类型。这种能力在基于属性的测试框架中特别有用，可以验证测试生成器的完整性和安全性。


<details>
  <summary>Details</summary>
Motivation: 本文探讨了覆盖类型与最近提出的不正确性逻辑框架之间的关联，旨在更系统地整合不正确性推理到细化类型系统中，从而为函数式程序员、程序验证者和分析工具提供新的机会。

Method: 研究覆盖类型与不正确性逻辑框架的关联，并提出了一种机制，将不正确性推理整合到表达性细化类型系统中。

Result: 研究发现覆盖类型提供的欠近似推理与不正确性逻辑框架的推理风格存在联系，并展示了这种整合为相关工具和程序员带来的潜在好处。

Conclusion: 通过深入探讨覆盖类型与不正确性逻辑的关联，论文提出了一种系统性整合方法，为函数式编程和程序分析领域提供了新的可能性。

Abstract: A coverage type generalizes refinement types found in many functional
languages with support for must-style underapproximate reasoning.
Property-based testing frameworks are one particularly useful domain where such
capabilities are useful as they allow us to verify the completeness, as well as
safety, of test generators. There is a surprising connection between the kind
of underapproximate reasoning coverage types offer and the style of reasoning
enabled by recently proposed Incorrectness Logic frameworks. In our
presentation, we propose to explore this connection more deeply, identifying
mechanisms that more systematically integrate incorrectness reasoning within an
expressive refinement type system and the opportunities that such integration
offers to functional programmers, program verifiers, and program analyzers and
related tools.

</details>


### [19] [From Traces to Program Incorrectness: A Type-Theoretic Approach](https://arxiv.org/abs/2509.02428)
*Yongwei Yuan,Zhe Zhou,Julia Belyakova,Benjamin Delaware,Suresh Jagannathan*

Main category: cs.PL

TL;DR: 提出了一种基于类型论的框架，用于分析功能性程序中与不透明库API交互时的错误行为，通过符号正则表达式（SREs）和符号有限自动机（SFAs）实现组合式推理。


<details>
  <summary>Details</summary>
Motivation: 功能性程序与不透明的库API交互时容易产生错误行为，需系统化方法识别和推理这些错误行为的类型。

Method: 使用符号正则表达式（SREs）表示库API调用的轨迹，并开发了一种新颖的类型推断算法，基于符号有限自动机（SFAs）进行组合式推理。

Result: 算法成功时，推断出的类型能够证明抽象数据类型（ADT）实现可能表现出部分指定的错误行为。

Conclusion: 该框架首次系统性实现了对基于轨迹的错误规范的近似推理，开启了轨迹引导的组合式分析新方向。

Abstract: We present a type-theoretic framework for reasoning about incorrectness in
functional programs that interact with effectful, opaque library APIs. Our
approach centers on traces -- temporally-ordered sequences of library API
invocations -- which naturally characterize both the preconditions of
individual APIs and their composite behavior. We represent these traces using
symbolic regular expressions (SREs), enabling formal specification of incorrect
abstract data type (ADT) behaviors across function boundaries. The core
contribution is a novel type inference algorithm that operates modulo specified
incorrectness properties and leverages the symbolic finite automata (SFAs)
representations of regexes for compositional reasoning of traces. When the
algorithm succeeds, the inferred types witness that an ADT implementation can
exhibit some subset of the specified incorrect behaviors. This represents the
first systematic approach to underapproximate reasoning against trace-based
incorrectness specifications, enabling a new form of trace-guided compositional
analysis.

</details>


<div id='cs.GT'></div>

# cs.GT [[Back]](#toc)

### [20] [Playing Markov Games Without Observing Payoffs](https://arxiv.org/abs/2509.00179)
*Daniel Ablin,Alon Cohen*

Main category: cs.GT

TL;DR: 本文引入并形式化了一类新的零和对称马尔可夫博弈，扩展了对称性概念，证明了在严重信息劣势下仍能通过在线学习实现稳健学习。


<details>
  <summary>Details</summary>
Motivation: 在多智能体系统中，不确定性下的优化是一个基本问题。本文旨在扩展对称性概念到马尔可夫博弈中，探索在缺乏回报观测的情况下如何进行高效竞争。

Method: 提出了一类新的零和对称马尔可夫博弈，形式化了三种不同的对称性概念，并将学习问题转化为在线学习实例，设计了多项式时间算法。

Result: 研究表明，即使缺乏回报观测，玩家仍能通过观察对手动作序列，渐近匹配对手的回报。算法适用于矩阵和马尔可夫博弈。

Conclusion: 本文扩展了在严重信息劣势下稳健学习的可能性，深化了在线学习与对抗性博弈论之间的联系。

Abstract: Optimization under uncertainty is a fundamental problem in learning and
decision-making, particularly in multi-agent systems. Previously, Feldman,
Kalai, and Tennenholtz [2010] demonstrated the ability to efficiently compete
in repeated symmetric two-player matrix games without observing payoffs, as
long as the opponents actions are observed. In this paper, we introduce and
formalize a new class of zero-sum symmetric Markov games, which extends the
notion of symmetry from matrix games to the Markovian setting. We show that
even without observing payoffs, a player who knows the transition dynamics and
observes only the opponents sequence of actions can still compete against an
adversary who may have complete knowledge of the game. We formalize three
distinct notions of symmetry in this setting and show that, under these
conditions, the learning problem can be reduced to an instance of online
learning, enabling the player to asymptotically match the return of the
opponent despite lacking payoff observations. Our algorithms apply to both
matrix and Markov games, and run in polynomial time with respect to the size of
the game and the number of episodes. Our work broadens the class of games in
which robust learning is possible under severe informational disadvantage and
deepens the connection between online learning and adversarial game theory.

</details>


### [21] [Strategyproof Mechanisms for Facility Location with Prediction Under the Maximum Cost Objective](https://arxiv.org/abs/2509.00439)
*Hau Chan,Jianan Lin,Chenhao Wang*

Main category: cs.GT

TL;DR: 研究在度量空间中基于学习的设施选址机制设计问题，利用不完美的预测优化设施选址的策略证明机制。


<details>
  <summary>Details</summary>
Motivation: 探索如何在设施选址问题中利用不完美的预测信息，设计策略证明机制以优化最大成本，并实现准确预测时的改进性能和任意预测误差时的鲁棒性。

Method: 基于预测误差设计策略证明机制，包括MinMaxP机制，并在实线和二维空间中验证其性能。此外，分析随机机制和确定性机制的组合性能。

Result: MinMaxP机制在实线上表现最优，且在二维空间中验证其性能。同时，探讨了机制的分组策略证明性。

Conclusion: 设计了一种结合预测信息的策略证明机制，在设施选址问题中实现了改进的性能和鲁棒性，为未来研究提供了方向。

Abstract: We study the mechanism design problem of facility location on a metric space
in the learning-augmented framework, where mechanisms have access to an
imperfect prediction of optimal facility locations. Our goal is to design
strategyproof (SP) mechanisms to elicit agent preferences on the facility
locations truthfully and, leveraging the given imperfect prediction, determine
the facility location that approximately minimizes the maximum cost among all
agents. In particular, we seek SP mechanisms whose approximation guarantees
depend on the prediction errors -- achieve improved guarantees when the
prediction is accurate (known as the \emph{consistency}), while still ensuring
robust worst-case performance when the prediction is arbitrarily inaccurate
(known as the \emph{robustness}).
  When the metric space is the real line, we characterize all deterministic SP
mechanisms with consistency strictly less than 2 and bounded robustness: such
mechanisms must be the MinMaxP mechanism, which returns the prediction location
if it lies between the two extreme agent locations and, otherwise, returns the
closest agent location to the prediction. We further show that, for any
prediction error $\eta\ge 0$, while MinMaxP is $(1+\min(1,
\eta))$-approximation, no deterministic SP mechanism can achieve a better
approximation. In two-dimensional spaces with the $l_p$ metric, we analyze the
approximation guarantees of a deterministic mechanism that runs MinMaxP
independently on each coordinate, as well as a randomized mechanism that
selects between two deterministic ones with specific probabilities. Finally, we
discuss the group strategyproofness of the considered mechanisms.

</details>


### [22] [Mean-payoff and Energy Discrete Bidding Games](https://arxiv.org/abs/2509.00506)
*Guy Avni,Suman Sadhukhan*

Main category: cs.GT

TL;DR: 该论文研究了离散投标游戏中的平均收益和能量目标，首次探讨了离散投标的阈值预算问题，并证明了其存在性和结构。


<details>
  <summary>Details</summary>
Motivation: 研究离散投标游戏的动机在于实际应用中投标的离散性质（如以分为单位），填补了连续投标游戏与离散投标游戏之间的研究空白。

Method: 通过分析离散投标游戏的结构，特别是阈值预算的存在性和性质，研究者证明了这类问题的复杂性属于NP和coNP。

Result: 论文首次证明了离散投标游戏中阈值预算的存在性，并揭示了其结构特性，这对于设计紧凑策略具有重要意义。

Conclusion: 离散投标游戏中的阈值预算不仅存在，而且具有明确的结构，这一发现为游戏理论中的策略设计和复杂性研究提供了新的方向。

Abstract: A \emph{bidding} game is played on a graph as follows. A token is placed on
an initial vertex and both players are allocated budgets. In each turn, the
players simultaneously submit bids that do not exceed their available budgets,
the higher bidder moves the token, and pays the bid to the lower bidder. We
focus on \emph{discrete}-bidding, which are motivated by practical applications
and restrict the granularity of the players' bids, e.g, bids must be given in
cents. We study, for the first time, discrete-bidding games with {\em
mean-payoff} and {\em energy} objectives. In contrast, mean-payoff {\em
continuous}-bidding games (i.e., no granularity restrictions) are understood
and exhibit a rich mathematical structure. The {\em threshold} budget is a
necessary and sufficient initial budget for winning an energy game or
guaranteeing a target payoff in a mean-payoff game. We first establish
existence of threshold budgets; a non-trivial property due to the concurrent
moves of the players. Moreover, we identify the structure of the thresholds,
which is key in obtaining compact strategies, and in turn, showing that finding
threshold is in \NP~and \coNP even in succinctly-represented games.

</details>


### [23] [Quantum game models for interaction-aware decision-making in automated driving](https://arxiv.org/abs/2509.01582)
*Karim Essalmi,Fernando Garrido,Fawzi Nashashibi*

Main category: cs.GT

TL;DR: 本文提出两种量子博弈模型（QG-U1和QG-G4），用于自动驾驶中的交互感知决策，相比传统方法表现更优。


<details>
  <summary>Details</summary>
Motivation: 传统自动驾驶决策方法常常忽视或过度简化与其他车辆的交互，导致车辆行为过于保守，因此需要更有效的交互模型。

Method: 提出了QG-U1和QG-G4两种量子博弈模型，结合量子力学的叠加、干涉和纠缠原理，适用于双人双策略游戏，可在标准计算机上实时运行。

Result: 在合并和环岛场景中测试，QG-G4的碰撞率更低，成功率更高；两种量子模型在某些参数设置下比传统博弈方法具有更高的期望收益。

Conclusion: 量子博弈模型在自动驾驶决策中表现出优于传统方法的潜力，尤其是QG-G4在安全性和成功率方面表现突出。

Abstract: Decision-making in automated driving must consider interactions with
surrounding agents to be effective. However, traditional methods often neglect
or oversimplify these interactions because they are difficult to model and
solve, which can lead to overly conservative behavior of the ego vehicle. To
address this gap, we propose two quantum game models, QG-U1 (Quantum Game -
Unitary 1) and QG-G4 (Quantum Game - Gates 4), for interaction-aware
decision-making. These models extend classical game theory by incorporating
principles of quantum mechanics, such as superposition, interference, and
entanglement. Specifically, QG-U1 and QG-G4 are designed for two-player games
with two strategies per player and can be executed in real time on a standard
computer without requiring quantum hardware. We evaluate both models in merging
and roundabout scenarios and compare them with classical game-theoretic methods
and baseline approaches (IDM, MOBIL, and a utility-based technique). Results
show that QG-G4 achieves lower collision rates and higher success rates
compared to baseline methods, while both quantum models yield higher expected
payoffs than classical game approaches under certain parameter settings.

</details>


### [24] [Complexity of the Existence of Constrained Secure Equilibria in Multi-Player Games](https://arxiv.org/abs/2509.01870)
*Hiroki Mizuno,Yoshiaki Takata,Hiroyuki Seki*

Main category: cs.GT

TL;DR: 探讨了多玩家非零和回合制游戏中安全均衡（SE）的可判定性和复杂性，特别是其约束条件下的存在性问题。


<details>
  <summary>Details</summary>
Motivation: 研究多玩家游戏中安全均衡的性质，作为纳什均衡的改进，其中玩家不仅考虑自身收益，也关注其他玩家的收益。

Method: 通过有限有向图模型和多玩家游戏框架，分析安全均衡的约束条件（指定某些玩家必须获胜的收益配置文件）下的存在性。

Result: 证明了安全均衡在给定约束条件下的可判定性，并探讨了其计算复杂性。

Conclusion: 安全均衡在多玩家游戏中的约束条件下是可判定的，但其复杂性仍需进一步研究。

Abstract: We consider a multi-player non-zero-sum turn-based game (abbreviated as
multi-player game) on a finite directed graph. A secure equilibrium (SE) is a
strategy profile in which no player has the incentive to deviate from the
strategy because no player can increase her own payoff or lower the payoff of
another player. SE is a promising refinement of Nash equilibrium in which a
player does not care the payoff of another player. In this paper, we discuss
the decidability and complexity of the problem of deciding whether a secure
equilibrium with constraints (a payoff profile specifying which players must
win) exists for a given multi-player game.

</details>


### [25] [Entry Barriers in Content Markets](https://arxiv.org/abs/2509.01953)
*Haiqing Zhu,Lexing Xie,Yun Kuen Cheung*

Main category: cs.GT

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: The prevalence of low-quality content on online platforms is often attributed
to the absence of meaningful entry requirements. This motivates us to
investigate whether implicit or explicit entry barriers, alongside appropriate
reward mechanisms, can enhance content quality. We present the first
game-theoretic analysis of two distinct types of entry barriers in online
content platforms. The first, a structural barrier, emerges from the collective
behaviour of incumbent content providers which disadvantages new entrants. We
show that both rank-order and proportional-share reward mechanisms induce such
a structural barrier at Nash equilibrium. The second, a strategic barrier,
involves the platform proactively imposing entry fees to discourage
participation from low-quality contributors. We consider a scheme in which the
platform redirects some or all of the entry fees into the reward pool. We
formally demonstrate that this approach can improve overall content quality.
Our findings establish a theoretical foundation for designing reward mechanisms
coupled with entry fees to promote higher-quality content and support healthier
online ecosystems.

</details>


### [26] [A Strongly Polynomial-Time Combinatorial Algorithm for the Nucleolus in Convex Games](https://arxiv.org/abs/2509.02380)
*Giacoomo Maggiorano,Alessandro Sosso,Gautier Stauffer*

Main category: cs.GT

TL;DR: 该论文重新审视了凸博弈中核仁计算的简化博弈方法，提出了首个组合式强多项式时间算法。


<details>
  <summary>Details</summary>
Motivation: 核仁是合作博弈论中的核心概念，但其计算通常为NP难问题。现有凸博弈中的多项式时间算法依赖椭球法，论文旨在探索更高效的组合式方法。

Method: 论文通过分析简化博弈方法的局限性，结合最小核心多面体结构，提出新的算法思路，实现高效核仁计算。

Result: 研究成功开发出首个组合式强多项式时间算法，有效解决了凸博弈中核仁计算的问题。

Conclusion: 论文证明了简化博弈方法在凸博弈中的有效性，并提供了高效的计算工具，为相关领域的研究提供了新思路。

Abstract: The nucleolus is a fundamental solution concept in cooperative game theory,
yet computing it is NP-hard in general. In convex games-where players' marginal
contributions grow with coalition size-the only existing polynomial-time
algorithm relies on the ellipsoid method. We re-examine a reduced game
approach, refuting a previously claimed polynomial-time implementation and
clarifying why it fails. By developing new algorithmic ideas and exploiting the
structure of least core polyhedra, we show that reduced games can in fact be
used effectively. This yields the first combinatorial and strongly polynomial
algorithm for computing the nucleolus in convex games.

</details>


### [27] [Harnessing Information in Incentive Design](https://arxiv.org/abs/2509.02493)
*Raj Kiriti Velicheti,Subhonmesh Bose,Tamer Başar*

Main category: cs.GT

TL;DR: 本研究探讨了激励设计中信息不对称的影响，表明降低信息不对称对委托人有利，并研究了通过信息设计或信息采集来缓解不确定性。


<details>
  <summary>Details</summary>
Motivation: 激励设计中，委托人面临信息租金问题，而代理人具有信息优势。本研究旨在系统地研究信息不对称对激励设计游戏的影响。

Method: 通过信息设计（代理人塑造其信念）或支付获取信息来缓解信息不对称，研究在矩阵游戏和二次高斯游戏中的效果。

Result: 研究发现，虽然引入不确定性增加了委托人的成本，但让代理人塑造其信念可能带来优势。

Conclusion: 降低信息不对称对委托人有利，而信息设计和信息采集是有效的缓解策略。

Abstract: Incentive design deals with interaction between a principal and an agent
where the former can shape the latter's utility through a policy commitment. It
is well known that the principal faces an information rent when dealing with an
agent that has informational advantage. In this work, we embark on a systematic
study of the effect of information asymmetry in incentive design games.
Specifically, we first demonstrate that it is in principal's interest to
decrease this information asymmetry. To mitigate this uncertainty, we let the
principal gather information either by letting the agent shape her belief (aka
Information Design), or by paying to acquire it. Providing solutions to all
these cases we show that while introduction of uncertainty increases the
principal's cost, letting the agent shape its belief can be advantageous. We
study information asymmetry and information acquisition in both matrix games
and quadratic Gaussian game setups.

</details>


### [28] [Selecting Interlacing Committees](https://arxiv.org/abs/2509.02519)
*Chris Dong,Martin Bullinger,Tomasz Wąs,Larry Birnbaum,Edith Elkind*

Main category: cs.GT

TL;DR: 该论文提出了一种新的委员会选举方法，通过定义两个定量指标来评估委员会如何连接选民，以避免极化委员会。虽然最大化这些指标通常属于NP难问题，但在选民-候选区间域中提供了高效算法。


<details>
  <summary>Details</summary>
Motivation: 社会极化是影响社会良好运行的主要问题，而极化的政治代表往往是其驱动力。现有的计算社会选择方法未能有效解决这一问题，因此需要新的方法来避免极化委员会的生成。

Method: 论文扩展了标准的委员会选举方法，定义了两种定量指标用于评估委员会如何连接选民。虽然最大化这些指标是NP难问题，但在选民-候选区间域中设计了高效算法。

Result: 研究发现，在选民-候选区间域中可以高效地最大化连接指标。此外，论文分析了这些目标与其他代表性目标（如卓越性、多样性和比例性）的兼容性，并提出了同时实现常数近似比的算法。

Conclusion: 通过定义和最大化连接选民的定量指标，可以有效避免极化委员会的产生。论文为选民-候选区间域提供了高效算法，并探讨了与其他代表性目标的权衡关系。

Abstract: Polarization is a major concern for a well-functioning society. Often, mass
polarization of a society is driven by polarizing political representation,
even when the latter is easily preventable. The existing computational social
choice methods for the task of committee selection are not designed to address
this issue. We enrich the standard approach to committee selection by defining
two quantitative measures that evaluate how well a given committee
interconnects the voters. Maximizing these measures aims at avoiding polarizing
committees. While the corresponding maximization problems are NP-complete in
general, we obtain efficient algorithms for profiles in the voter-candidate
interval domain. Moreover, we analyze the compatibility of our goals with other
representation objectives, such as excellence, diversity, and proportionality.
We identify trade-offs between approximation guarantees, and describe
algorithms that achieve simultaneous constant-factor approximations.

</details>
