<div id=toc></div>

# Table of Contents

- [cs.GR](#cs.GR) [Total: 2]
- [cs.PL](#cs.PL) [Total: 10]
- [cs.GT](#cs.GT) [Total: 11]


<div id='cs.GR'></div>

# cs.GR [[Back]](#toc)

### [1] [Proc3D: Procedural 3D Generation and Parametric Editing of 3D Shapes with Large Language Models](https://arxiv.org/abs/2601.12234)
*Fadlullah Raji,Stefano Petrangeli,Matheus Gadelha,Yu Shen,Uttaran Bhattacharya,Gang Wu*

Main category: cs.GR

TL;DR: Proc3D是一种生成可编辑3D模型的系统，支持实时修改，通过程序紧凑图（PCG）表示3D模型，结合自然语言提示实现高效编辑，优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 传统3D模型生成复杂且不可编辑，现有方法适应性有限，Proc3D旨在提供可编辑且支持实时修改的3D模型生成方案。

Method: Proc3D采用程序紧凑图（PCG）表示3D模型，支持通过滑块、复选框和自然语言提示进行手动或自动修改，结合GPT-4o和LLAMA-3模型实现生成。

Result: 实验显示Proc3D在编辑效率上比传统方法快400倍以上，ULIP评分提高28%，实现了文本对齐的3D模型生成和实时参数编辑。

Conclusion: Proc3D通过可编辑的3D模型生成和实时修改功能，显著提升了设计迭代的效率和准确性。

Abstract: Generating 3D models has traditionally been a complex task requiring specialized expertise. While recent advances in generative AI have sought to automate this process, existing methods produce non-editable representation, such as meshes or point clouds, limiting their adaptability for iterative design. In this paper, we introduce Proc3D, a system designed to generate editable 3D models while enabling real-time modifications. At its core, Proc3D introduces procedural compact graph (PCG), a graph representation of 3D models, that encodes the algorithmic rules and structures necessary for generating the model. This representation exposes key parameters, allowing intuitive manual adjustments via sliders and checkboxes, as well as real-time, automated modifications through natural language prompts using Large Language Models (LLMs). We demonstrate Proc3D's capabilities using two generative approaches: GPT-4o with in-context learning (ICL) and a fine-tuned LLAMA-3 model. Experimental results show that Proc3D outperforms existing methods in editing efficiency, achieving more than 400x speedup over conventional approaches that require full regeneration for each modification. Additionally, Proc3D improves ULIP scores by 28%, a metric that evaluates the alignment between generated 3D models and text prompts. By enabling text-aligned 3D model generation along with precise, real-time parametric edits, Proc3D facilitates highly accurate text-based image editing applications.

</details>


### [2] [Copy-Trasform-Paste: Zero-Shot Object-Object Alignment Guided by Vision-Language and Geometric Constraints](https://arxiv.org/abs/2601.14207)
*Rotem Gatenyo,Ohad Fried*

Main category: cs.GR

TL;DR: 该论文提出了一种零样本3D网格对齐方法，通过文本提示描述空间关系，直接优化相对位姿，结合CLIP驱动的梯度和几何感知目标，实现语义准确且物理合理的对齐。


<details>
  <summary>Details</summary>
Motivation: 研究动机是为了解决3D网格对齐问题，特别是在内容创建和场景组装中，如何利用文本提示描述空间关系，提升对齐的语义准确性和物理合理性。

Method: 方法包括直接优化相对位姿（平移、旋转和缩放），通过CLIP驱动的梯度和可微分渲染器，结合几何感知目标（软ICP变体和穿透损失）和多阶段调度策略。

Result: 实验结果优于所有基线方法，能够生成语义准确且物理合理的3D网格对齐，且无需训练新模型。

Conclusion: 结论表明，该方法通过语言和几何的结合，实现了高效的3D网格对齐，为内容创建和场景组装提供了一种强大的工具。

Abstract: We study zero-shot 3D alignment of two given meshes, using a text prompt describing their spatial relation -- an essential capability for content creation and scene assembly. Earlier approaches primarily rely on geometric alignment procedures, while recent work leverages pretrained 2D diffusion models to model language-conditioned object-object spatial relationships. In contrast, we directly optimize the relative pose at test time, updating translation, rotation, and isotropic scale with CLIP-driven gradients via a differentiable renderer, without training a new model. Our framework augments language supervision with geometry-aware objectives: a variant of soft-Iterative Closest Point (ICP) term to encourage surface attachment and a penetration loss to discourage interpenetration. A phased schedule strengthens contact constraints over time, and camera control concentrates the optimization on the interaction region. To enable evaluation, we curate a benchmark containing diverse categories and relations, and compare against baselines. Our method outperforms all alternatives, yielding semantically faithful and physically plausible alignments.

</details>


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [3] [Context-Free Grammar Inference for Complex Programming Languages in Black Box Settings](https://arxiv.org/abs/2601.12385)
*Feifei Li,Xiao Chen,Xiaoyu Sun,Xi Xiao,Shaohua Wang,Yong Ding,Sheng Wen,Qing Li*

Main category: cs.PL

TL;DR: 论文提出了Crucio方法，通过构建分解森林和分布矩阵，解决了复杂编程语言语法推断的挑战，显著提升了性能和效率。


<details>
  <summary>Details</summary>
Motivation: 现有工具如Arvada、Treevada和Kedavra在处理复杂编程语言（如C、C++和Java）的语法推断时效率低下，无法在48小时内完成任务。它们的方法存在局限性，如直接处理长输入示例或依赖原始输入的词法分析。

Method: Crucio通过构建分解森林和分布矩阵，提取短示例进行词法和语法推断，克服了现有工具的局限性。

Result: 实验表明，Crucio是唯一能在合理时间内成功推断复杂编程语言语法的方法，其非终结符数量比前人基准多23倍。在简单基准上，Crucio的平均召回率和F1分数分别提高了1.37倍、1.19倍和1.21倍、1.13倍。

Conclusion: Crucio通过创新方法显著提升复杂编程语言语法推断的效率和性能，解决了现有方法的不足。

Abstract: Grammar inference for complex programming languages remains a significant challenge, as existing approaches fail to scale to real world datasets within practical time constraints. In our experiments, none of the state-of-the-art tools, including Arvada, Treevada and Kedavra were able to infer grammars for complex languages such as C, C++, and Java within 48 hours. Arvada and Treevada perform grammar inference directly on full-length input examples, which proves inefficient for large files commonly found in such languages. While Kedavra introduces data decomposition to create shorter examples for grammar inference, its lexical analysis still relies on the original inputs. Additionally, its strict no-overgeneralization constraint limits the construction of complex grammars.
  To overcome these limitations, we propose Crucio, which builds a decomposition forest to extract short examples for lexical and grammar inference via a distributional matrix. Experimental results show that Crucio is the only method capable of successfully inferring grammars for complex programming languages (where the number of nonterminals is up to 23x greater than in prior benchmarks) within reasonable time limits. On the prior simple benchmark, Crucio achieves an average recall improvement of 1.37x and 1.19x over Treevada and Kedavra, respectively, and improves F1 scores by 1.21x and 1.13x.

</details>


### [4] [An Introduction to Razborov's Flag Algebra as a Proof System for Extremal Graph Theory](https://arxiv.org/abs/2601.12741)
*Gyeongwon Jeong,Seonghun Park,Hongseok Yang*

Main category: cs.PL

TL;DR: 介绍了Razborov的旗代数框架，该框架通过逻辑视角呈现，适用于计算机科学的多个领域。


<details>
  <summary>Details</summary>
Motivation: 为计算机科学家提供一个逻辑视角下的旗代数介绍，特别是在逻辑、编程语言和自动验证等领域，以便他们理解和应用这一数学工具。

Method: 采用逻辑的语法、语义和证明策略来呈现旗代数，重点解释了一种通过标记变体证明未标记情形不等式的策略，该策略依赖于所谓的“伴随对”概念。

Result: 通过具体例子（如Mantel定理和Goodman的Ramsey多重性边界）展示了如何在旗代数框架下符号化地执行数学论证。

Conclusion: 旗代数不仅适用于极值图论，其逻辑框架和证明策略还能为计算机科学中的形式化方法和自动验证提供新的工具和视角。

Abstract: Razborov's flag algebra forms a powerful framework for deriving asymptotic inequalities between induced subgraph densities, underpinning many advances in extremal graph theory. This survey introduces flag algebra to computer scientists working in logic, programming languages, automated verification, and formal methods. We take a logical perspective on flag algebra and present it in terms of syntax, semantics, and proof strategies, in a style closer to formal logic. One popular proof strategy derives valid inequalities by first proving inequalities in a labelled variant of flag algebra and then transferring them to the original unlabelled setting using the so-called downward operator. We explain this strategy in detail and highlight that its transfer mechanism relies on the notion of what we call an adjoint pair, reminiscent of Galois connections and categorical adjunctions, which appear frequently in work on automated verification and programming languages. Along the way, we work through representative examples, including Mantel's theorem and Goodman's bound on Ramsey multiplicity, to illustrate how mathematical arguments can be carried out symbolically in the flag algebra framework.

</details>


### [5] [A Formally Verified Procedure for Width Inference in FIRRTL](https://arxiv.org/abs/2601.12813)
*Keyin Wang,Xiaomu Shi,Jiaxiang Liu,Zhilin Wu,Taolve Chen,Fu Song,David N. Jansen*

Main category: cs.PL

TL;DR: FIRRTL是一种RTL硬件设计的中间表示语言，其位宽推断问题在主流编译器中可能失败。本文提出了一种完整的宽度推断方法，基于唯一最小解的理论结果，并在Rocq中实现和验证了其正确性。


<details>
  <summary>Details</summary>
Motivation: FIRRTL程序中许多组件的位宽未明确指定，需在编译时推断。主流编译器如firtool的InferWidths编译通道可能失败，亟需更可靠的解决方案。

Method: 研究表明，若FIRRTL程序的约束可满足，则存在唯一最小解。基于此结果，论文提出了一种完整的宽度推断方法，并在Rocq中实现并验证了其功能正确性。

Result: 从Rocq实现中提取的OCaml实现是第一个经过形式化验证的InferWidths通道。实验表明，该方法比firtool的官方InferWidths更能解决问题，且效率高。

Conclusion: 本文提出的宽度推断方法解决了FIRRTL程序中的位宽推断问题，并通过形式化验证确保了其正确性，为实际应用提供了可靠的工具。

Abstract: FIRRTL is an intermediate representation language for Register Transfer Level (RTL) hardware designs. In FIRRTL programs, the bit widths of many components are not specified explicitly and must be inferred during compilation. In mainstream FIRRTL compilers, such as the official compiler firtool, width inference is conducted by a compilation pass referred to as InferWidths, which may fail even for simple FIRRTL programs. In this paper, we thoroughly investigate the width inference problem for FIRRTL programs. We show that, if the constraints obtained from a FIRRTL program are satisfiable, there exists a unique least solution. Based on this result, we propose a complete procedure for solving the width inference problem. We implement it in the interactive theorem prover Rocq and prove its functional correctness. From the Rocq implementation, we extract an OCaml implementation, which is the first formally verified implementation of the InferWidths pass. Extensive experiments demonstrate that our approach can solve more instances than the official InferWidths pass in firtool, normally with high efficiency.

</details>


### [6] [Dependently-Typed AARA: A Non-Affine Approach for Resource Analysis of Higher-Order Programs](https://arxiv.org/abs/2601.12943)
*Han Xu,Di Wang*

Main category: cs.PL

TL;DR: 本文提出了一种名为λ_​amor}^​na的非仿射依赖类型系统，用于高阶函数程序的资源分析，解决了传统仿射类型系统在高阶程序资源预测上的不足。


<details>
  <summary>Details</summary>
Motivation: 传统仿射类型系统在处理高阶函数程序时，尤其在部分应用情况下，无法准确预测资源消耗。本文旨在解决这一问题。

Method: 通过解耦类型与资源，并采用非仿射依赖类型机制，λ_​amor}^​na系统能够更精确地分析高阶函数的资源行为。

Result: 文章形式化了λ_​amor}^​na的语法和语义，并证明了其正确性，展示了多个高阶示例以验证其表达能力和组合性。

Conclusion: λ_​amor}^​na系统通过非仿射依赖类型机制，显著提升了高阶函数程序资源分析的准确性和表达能力。

Abstract: Static resource analysis determines the resource consumption (e.g., time complexity) of a program without executing it. Among the numerous existing approaches for resource analysis, affine type systems have been one dominant approach. However, these affine type systems fall short of deriving precise resource behavior of higher-order programs, particularly in cases that involve partial applications.
  This article presents λ_\ms{amor}^\ms{na}}, a non-affine AARA-style dependent type system for resource reasoning about higher-order functional programs. The key observation is that the main issue in previous approaches comes from (i) the close coupling of types and resources, and (ii) the conflict between affine and higher-order typing mechanisms. To derive precise resource behavior of higher-order functions, λ_\ms{amor}^\ms{na}} decouples resources from types and follows a non-affine typing mechanism. The non-affine type system of λ_\ms{amor}^\ms{na}} achieves this by using dependent types, which allows expressing type-level potential functions separate from ordinary types. This article formalizes λ_\ms{amor}^\ms{na}}'s syntax and semantics, and proves its soundness, which guarantees the correctness of resource bounds. Several challenging classic and higher-order examples are presented to demonstrate the expressiveness and compositionality of λ_\ms{amor}^\ms{na}}'s reasoning capability.

</details>


### [7] [Functional Logic Program Transformations](https://arxiv.org/abs/2601.13224)
*Michael Hanus,Steven Libby*

Main category: cs.PL

TL;DR: 本文展示了如何利用函数逻辑编程的特性来实现紧凑且易于理解的程序转换方法。


<details>
  <summary>Details</summary>
Motivation: 许多工具（如编译器、分析器或验证器）需要对程序的中间表示（如抽象语法树）进行转换，而实现这些转换是一个复杂的过程。

Method: 作者提出将程序转换编写为部分定义和非确定性操作，并对比了这种方法与确定性转换方法的性能。

Result: 研究评估了在函数逻辑语言Curry及其中间表示FlatCurry中使用非确定性操作的性能开销。

Conclusion: 结果显示，尽管非确定性操作可能引入一些性能开销，但它在实现程序转换时更具表达力和灵活性。

Abstract: Many tools used to process programs, like compilers, analyzers, or verifiers, perform transformations on their intermediate program representation, like abstract syntax trees. Implementing such program transformations is a non-trivial task, since it is necessary to iterate over the complete syntax tree and apply various transformations at nodes in a tree. In this paper we show how the features of functional logic programming are useful to implement program transformations in a compact and comprehensible manner. For this purpose, we propose to write program transformations as partially defined and non-deterministic operations. Since the implementation of non-determinism usually causes some overhead compared to deterministically defined operations, we compare our approach to a deterministic transformation method. We evaluate these alternatives for the functional logic language Curry and its intermediate representation FlatCurry which is used in various analysis and verification tools and compilers.

</details>


### [8] [Reduction for Structured Concurrent Programs](https://arxiv.org/abs/2601.13341)
*Namratha Gangamreddypalli,Constantin Enea,Shaz Qadeer*

Main category: cs.PL

TL;DR: 该论文提出了一种新颖的归约技术，用于结构化并发程序，结合了并行结构顺序化和Lipton归约的扩展，支持包含递归调用的原子部分。


<details>
  <summary>Details</summary>
Motivation: 现有基于Lipton movers的可交换性推理技术难以扩展到软件系统中常用的特性（如过程和并行组合），因此需要一种更灵活的方法来解决这一挑战。

Method: 通过两种关键策略的结合：一是将并行组合替换为顺序组合的策略，二是扩展Lipton归约以支持包含递归过程调用的原子部分。这些策略可以任意组合，从而扩展了归约推理的范围。

Result: 该方法在Civl中实现，并在多个复杂案例（如快照对象、容错可线性化寄存器、FLASH缓存一致性协议和Two-Phase Commit变体）中验证了其有效性。

Conclusion: 该研究提出的统一归约技术显著提升了结构化并发程序的验证能力，解决了现有技术的局限性。

Abstract: Commutativity reasoning based on Lipton's movers is a powerful technique for verification of concurrent programs. The idea is to define a program transformation that preserves a subset of the initial set of interleavings, which is sound modulo reorderings of commutative actions. Scaling commutativity reasoning to routinely-used features in software systems, such as procedures and parallel composition, remains a significant challenge.
  In this work, we introduce a novel reduction technique for structured concurrent programs that unifies two key advances. First, we present a reduction strategy that soundly replaces parallel composition with sequential composition. Second, we generalize Lipton's reduction to support atomic sections containing (potentially recursive) procedure calls. Crucially, these two foundational strategies can be composed arbitrarily, greatly expanding the scope and flexibility of reduction-based reasoning. We implemented this technique in Civl and demonstrated its effectiveness on a number of challenging case studies, including a snapshot object, a fault-tolerant and linearizable register, the FLASH cache coherence protocol, and a non-trivial variant of Two-Phase Commit.

</details>


### [9] [Foundational VeriFast: Pragmatic Certification of Verification Tool Results through Hinted Mirroring](https://arxiv.org/abs/2601.13727)
*Bart Jacobs*

Main category: cs.PL

TL;DR: VeriFast是一款用于单线程和多线程C及Rust程序正确性模块化形式验证的工具。本文通过扩展VeriFast，使其在成功验证Rust程序后能生成Rocq证明脚本，从而提升其在安全关键领域的适用性。


<details>
  <summary>Details</summary>
Motivation: VeriFast虽然是一种强大的验证工具，但其本身并未经过形式验证，可能存在工具本身的错误导致虚假的正确性报告。因此在安全关键领域，进一步提升其可靠性至关重要。

Method: 研究者扩展了VeriFast的功能，使其在验证Rust程序时记录关键信息，并通过"提示镜像"方法将这些信息用于在Rocq中重放验证过程，生成Rocq证明脚本。

Result: 通过这一扩展，VeriFast能够生成证明程序正确性的Rocq脚本，从而显著提升了其在安全关键领域的适用性和可信度。

Conclusion: 本文提出的扩展方法成功地将VeriFast的验证能力与Rocq的形式证明能力结合起来，进一步增强了工具的可靠性，尤其是在安全关键领域的应用价值。

Abstract: VeriFast is a leading tool for the modular formal verification of correctness properties of single-threaded and multi-threaded C and Rust programs. It verifies a program by symbolically executing each function in isolation, exploiting user-annotated preconditions, postconditions, and loop invariants written in a form of separation logic, and using a separation logic-based symbolic representation of memory. However, the tool itself, written in roughly 30K lines of OCaml code, has not been formally verified. Therefore, bugs in the tool could cause it to falsely report the correctness of the input program. We here report on an early result extending VeriFast to emit, upon successful verification of a Rust program, a Rocq proof script that proves correctness of the program with respect to a Rocq-encoded axiomatic semantics of Rust. This significantly enhances VeriFast's applicability in safety-critical domains. We apply hinted mirroring: we record key information from VeriFast's symbolic execution run, and use it to direct a replay of the run in Rocq.

</details>


### [10] [Generating Functions Meet Occupation Measures: Invariant Synthesis for Probabilistic Loops (Extended Version)](https://arxiv.org/abs/2601.13991)
*Darion Haase,Kevin Batz,Adrian Gallus,Benjamin Lucien Kaminski,Joost-Pieter Katoen,Lutz Klinkenberg,Tobias Winkler*

Main category: cs.PL

TL;DR: 该论文解决了概率编程中数学精确推断的问题，提出了一种基于占用不变量的新方法，尤其适用于带循环的程序。


<details>
  <summary>Details</summary>
Motivation: 概率编程中推断程序的后验分布是一个基本但困难的任务，尤其是对于包含循环或无界递归的复杂语言。现有方法多集中于统计近似，而本文致力于解决数学精确推断的问题。

Method: 论文引入了一种称为占用不变量的技术，这种不变性与循环的占用度相关，能反映程序状态的预期访问次数。该方法通过生成函数编码自动合成占用不变量。

Result: 论文提出了基于模板的自动不变量合成方法，并通过基准测试验证了其有效性。占用不变量不仅能精确推断后验分布，还能证明程序的几乎必然终止性。

Conclusion: 论文展示了占用不变量在概率编程中的有效性，为精确推断提供了一种新工具，同时为程序终止性分析带来额外收益。

Abstract: A fundamental computational task in probabilistic programming is to infer a program's output (posterior) distribution from a given initial (prior) distribution. This problem is challenging, especially for expressive languages that feature loops or unbounded recursion. While most of the existing literature focuses on statistical approximation, in this paper we address the problem of mathematically exact inference.
  To achieve this for programs with loops, we rely on a relatively underexplored type of probabilistic loop invariant, which is linked to a loop's so-called occupation measure. The occupation measure associates program states with their expected number of visits, given the initial distribution. Based on this, we derive the notion of an occupation invariant. Such invariants are essentially dual to probabilistic martingales, the predominant technique for formal probabilistic loop analysis in the literature. A key feature of occupation invariants is that they can take the initial distribution into account and often yield a proof of positive almost sure termination as a by-product.
  Finally, we present an automatic, template-based invariant synthesis approach for occupation invariants by encoding them as generating functions. The approach is implemented and evaluated on a set of benchmarks.

</details>


### [11] [Verifying Floating-Point Programs in Stainless](https://arxiv.org/abs/2601.14059)
*Andrea Gilot,Axel Bergström,Eva Darulova*

Main category: cs.PL

TL;DR: 本文将Stainless归纳验证工具扩展至支持浮点数验证，首次为Scala的子集（包括多态、递归和高阶函数）提供自动化浮点数验证支持。


<details>
  <summary>Details</summary>
Motivation: 现有工具在支持浮点数验证时存在局限性，本文旨在填补这一空白，并为真实世界代码提供可靠的验证支持。

Method: 采用类似KeY验证器的方法，通过数学函数的公理化推理，支持Scala数学API的所有函数，并在Stainless中验证公理的正确性。

Result: 通过从GitHub真实代码提取的新基准验证，证明了该工具可以验证输出范围或特殊值缺失等规范，或在规范不成立时生成反例。

Conclusion: 本文成功扩展了Stainless工具的功能，为浮点数验证提供了有效的自动化解决方案，并在实际应用中验证了其可行性。

Abstract: We extend the Stainless deductive verifier with floating-point support, providing the first automated verification support for floating-point numbers for a subset of Scala that includes polymorphism, recursion and higher-order functions. We follow the recent approach in the KeY verifier to axiomatise reasoning about mathematical functions, but go further by supporting all functions from Scala's math API, and by verifying the correctness of the axioms against the actual implementation in Stainless itself. We validate Stainless' floating-point support on a new set of benchmarks sampled from real-world code from GitHub, showing that it can verify specifications about, e.g., ranges of output or absence of special values for most supported functions, or produce counter-examples when the specifications do not hold.

</details>


### [12] [Partial Reductions for Kleene Algebra with Linear Hypotheses](https://arxiv.org/abs/2601.14114)
*Liam Chung,Tobias Kappé*

Main category: cs.PL

TL;DR: 该论文提出了一种基于自动机的构造方法，用于自动生成Kleene代数（KA）中程序特定假设的部分归约，从而扩展了可证明的程序等价性范围。


<details>
  <summary>Details</summary>
Motivation: 虽然Kleene代数（KA）是推理程序等价性的重要工具，但它无法证明特定程序之间的等价性。传统的归约方法需要手动构造且受限于正则性约束，因此很难推广到所有表达式和假设组合。

Method: 论文提出了一种基于自动机的构造方法，能够自动生成部分归约。这些归约可以是部分的，从而实现部分完备性，适用于更广泛的表达式类别。

Result: 该方法能够自动建立比现有工作更广泛的程序等价性证明，覆盖了更多未被传统方法涵盖的情况。

Conclusion: 该研究提供了一种机械化的归约构造方法，显著扩展了Kleene代数在程序等价性证明中的应用范围，尤其是在部分完备性的情况下。

Abstract: Kleene algebra (KA) is an important tool for reasoning about general program equivalences, with a decidable and complete equational theory. However, KA cannot always prove equivalences between specific programs. For this purpose, one adds hypotheses to KA that encode program-specific knowledge. Traditionally, a map on regular expressions called a reduction then lets us lift decidability and completeness to these more expressive systems. Explicitly constructing such a reduction requires significant labour. Moreover, due to regularity constraints, a reduction may not exist for all combinations of expression and hypothesis.
  We describe an automaton-based construction to mechanically derive reductions for a wide class of hypotheses. These reductions can be partial, in which case they yield partial completeness: completeness for expressions in their domain. This allows us to automatically establish the provability of more equivalences than what is covered in existing work.

</details>


<div id='cs.GT'></div>

# cs.GT [[Back]](#toc)

### [13] [Temporal Fair Division of Indivisible Goods with Scheduling](https://arxiv.org/abs/2601.12835)
*Kui Wang Choi,Minming LI*

Main category: cs.GT

TL;DR: 研究了多轮分配下的时间公平分割问题，探讨了TEF1、TEFX及其近似α-TEFX和TMMS，并在受限设置和调度模型中分析了可能性与不可能性的边界。


<details>
  <summary>Details</summary>
Motivation: 动机源于标准设置中已知的不可能性，探究了在受限设置和引入调度的情况下，时间公平分割问题的可行性。

Method: 研究了无调度和有调度的时间公平分割方法，分析了TEF1、TEFX、α-TEFX和TMMS在不同条件下的实现能力。

Result: 发现无调度情况下通用α-TEFX无法实现常数因子，但在广义二元估值和两代理的相同天数下可实现1/2近似；调度缓冲大小为n/2时，相同天数下可达成TEF1，但TEFX和TMMS即使在调度或受限域中仍多数不可行。

Conclusion: 结果表明严格时间公平性内在困难，量化了实现近似保证所需的权衡。

Abstract: We study temporal fair division, where agents receive goods over multiple rounds and cumulative fairness is required. We investigate Temporal Envy-Freeness Up to One Good (TEF1) and Up to Any Good (TEFX), its approximation $α$-TEFX, and Temporal Maximin Share (TMMS). Motivated by known impossibilities in standard settings, we consider the model in various restricted settings and extend it by introducing scheduling.
  Our main contributions draw the boundary between possibility and impossibility. First, regarding temporal fair division without scheduling, we prove that while constant-factor $α$-TEFX is impossible in general, a $1/2$-approximation is achievable for generalized binary valuations and identical days with two agents. Second, regarding temporal fair division with scheduling, we demonstrate that a scheduling buffer of size at least $n/2$ enables TEF1 for identical days. However, we establish that TEFX and TMMS remain largely impossible even with scheduling or restricted domains. These results highlight the inherent difficulty of strict temporal fairness and quantify the trade-offs required to achieve approximation guarantees.

</details>


### [14] [The Cost of EFX: Generalized-Mean Welfare and Complexity Dichotomies with Few Surplus Items](https://arxiv.org/abs/2601.12849)
*Eugene Lim,Tzeh Yuan Neoh,Nicholas Teh*

Main category: cs.GT

TL;DR: 研究了在少量剩余物品情况下EFX分配与广义均值福利的关系，揭示了EFX在不同p值下的计算复杂度差异及其公平性代价。


<details>
  <summary>Details</summary>
Motivation: 探讨EFX公平性分配在少量剩余物品情境下与广义均值福利的交互作用及其计算复杂度。

Method: 通过理论与算法分析，研究了EFX分配在不同p值（广义均值）下的NP难解性与多项式时间算法，并量化了EFX的公平性代价。

Result: 研究表明，EFX在p>0时NP难解，而在p≤0时可多项式时间优化；公平性代价在p>0时线性增长，在p≤0时有限。

Conclusion: 揭示了EFX在少量剩余物品下的计算复杂性与福利优化的结构性对齐关系，为EFX设计与应用提供了理论指导。

Abstract: Envy-freeness up to any good (EFX) is a central fairness notion for allocating indivisible goods, yet its existence is unresolved in general. In the setting with few surplus items, where the number of goods exceeds the number of agents by a small constant (at most three), EFX allocations are guaranteed to exist, shifting the focus from existence to efficiency and computation. We study how EFX interacts with generalized-mean ($p$-mean) welfare, which subsumes commonly-studied utilitarian ($p=1$), Nash ($p=0$), and egalitarian ($p \rightarrow -\infty$) objectives. We establish sharp complexity dichotomies at $p=0$: for any fixed $p \in (0,1]$, both deciding whether EFX can attain the global $p$-mean optimum and computing an EFX allocation maximizing $p$-mean welfare are NP-hard, even with at most three surplus goods; in contrast, for any fixed $p \leq 0$, we give polynomial-time algorithms that optimize $p$-mean welfare within the space of EFX allocations and efficiently certify when EFX attains the global optimum. We further quantify the welfare loss of enforcing EFX via the price of fairness framework, showing that for $p > 0$, the loss can grow linearly with the number of agents, whereas for $p \leq 0$, it is bounded by a constant depending on the surplus (and for Nash welfare it vanishes asymptotically). Finally we show that requiring Pareto-optimality alongside EFX is NP-hard (and becomes $Σ_2^P$-complete for a stronger variant of EFX). Overall, our results delineate when EFX is computationally costly versus structurally aligned with welfare maximization in the setting with few surplus items.

</details>


### [15] [The Cost of Failure: On The Complexity of Recampaigning under Fixed Districts](https://arxiv.org/abs/2601.13246)
*Michael C. Chavrimootoo,Aidan Jeansonne*

Main category: cs.GT

TL;DR: 本文探讨了在固定选区划分下，失利政党是否通过战略性地调整竞选资源（即“重新竞选”）使其候选人获胜的可能性。


<details>
  <summary>Details</summary>
Motivation: 研究动机在于理解失利政党在选区划分固定后的策略空间，尤其是在美国两党竞争中针对12个州的选区重划问题。

Method: 论文将重新竞选建模为计算问题，通过多项式时间多一归约、分离/崩溃分析（无条件及公理充分性）以及最坏情况和参数化复杂性研究新模型。

Result: 研究结果表明，通过重新竞选策略，政党可以在一定程度上提高候选人获胜的机会，并分析了不同模型的计算复杂性。

Conclusion: 结论指出，重新竞选策略为失利政党提供了新的应对固定选区划分的工具，但其计算复杂性限制了实际可行性。

Abstract: Redistricting efforts have gathered contemporary attention in both quotidian and scholarly debates, particularly in the United States where efforts to redraw congressional districts to favor either of the two major parties in 12 states -- such as California, Texas, and Ohio -- have captured the public eye. The treatment of redistricting in computational social choice has essentially focused on the process of determining "appropriate" districts. In this work, we are interested in understanding the gamut of options left for the "losing" party, and so we consider the flip side of the problem: Given fixed/predetermined districts, can a given party still make their candidates win by strategically placing them in certain districts? We dub this as "recampaigning" to capture the intuition that a party would redirect their campaigning efforts from one district to another. We model recampaigning as a computational problem, consider natural variations of the model, and study those new models through the lens of (1) (polynomial-time many-one) interreducibilities, (2) separations/collapses (both unconditional and axiomatic-sufficient), and (3) both worst-case and parametrized complexity.

</details>


### [16] [Tight Asymptotic Bounds for Fair Division With Externalities](https://arxiv.org/abs/2601.13287)
*Frank Connor,Max Dupré la Tour,Vishnu V. Narayan,Šimon Schierreich*

Main category: cs.GT

TL;DR: 本文研究了在代理之间分配不可分割物品的问题，其中代理的偏好包含外部性。研究证明了在多项式时间内总能找到消除嫉妒的分配，且数量为$O(\sqrt{n})$，并证明了这一结果的紧性。


<details>
  <summary>Details</summary>
Motivation: 传统的公平分配模型未考虑代理之间的外部性，即在分配物品时，代理的效用不仅受自身分配到的物品影响，还可能受其他代理分配到的物品影响。此前研究虽然提出了EF1（envy-free up to one item）的放松条件，但两个核心问题尚未解决：是否存在EF1分配，以及是否存在更优的放松条件EF-$k$可以保证分配的存在性。

Method: 研究分析了带有外部性的分配问题，通过数学推导和构造性证明，确定了消除嫉妒所需的物品数量界限。作者提出了一种可以在多项式时间内找到EF-$O(\sqrt{n})$分配的方法。

Result: 研究证明，对于任何包含$n$个代理的实例，总能找到一个EF-$O(\sqrt{n})$的分配，并且可以通过多项式时间算法实现。此外，作者还证明了$Ω(\sqrt{n})$的下界，表明即使在二元估值情况下也存在紧性，从而否定了EF1分配在外部性条件下的存在性。

Conclusion: 本文解决了外部性条件下公平分配问题的核心开放问题，证明了EF-$O(\sqrt{n})$分配的存在性和紧性，同时否定了EF1分配的可行性。这一结果为带有外部性的分配问题提供了理论保证和算法支持。

Abstract: We study the problem of allocating a set of indivisible items among agents whose preferences include externalities. Unlike the standard fair division model, agents may derive positive or negative utility not only from items allocated directly to them, but also from items allocated to other agents. Since exact envy-freeness cannot be guaranteed, prior work has focused on its relaxations. However, two central questions remained open: does there always exist an allocation that is envy-free up to one item (EF1), and if not, what is the optimal relaxation EF-$k$ that can always be attained?
  We settle both questions by deriving tight asymptotic bounds on the number of items sufficient to eliminate envy. We show that for any instance with $n$ agents, an allocation that is envy-free up to $O(\sqrt{n})$ items always exists and can be found in polynomial time, and we prove a matching $Ω(\sqrt{n})$ lower bound showing that this result is tight even for binary valuations, which rules out the existence of EF1 allocations when agents have externalities.

</details>


### [17] [Bridging the Gap Between Estimated and True Regret Towards Reliable Regret Estimation in Deep Learning based Mechanism Design](https://arxiv.org/abs/2601.13489)
*Shuyuan You,Zhiqiang Zhuang,Kewen Wang,Zhe Wang*

Main category: cs.GT

TL;DR: 研究发现现有的深度学习方法（如RegretNet）低估了真实遗憾值，导致了对激励兼容性和收入的夸大宣称。为解决这一问题，论文提出了一个下界遗憾和高效的逐项遗憾近似方法，从而显著提高了遗憾估计的准确性。


<details>
  <summary>Details</summary>
Motivation: 现有基于深度学习的方法在近似最优多物品拍卖时依赖于放松激励兼容性（IC）并通过事后遗憾衡量其违反情况，但遗憾估计的真实准确性仍不明确，且现有方法存在系统性低估真实遗憾的问题。

Method: 论文通过大量实验揭示了现有方法的局限性，提出了一个遗憾的下界和高效的逐项遗憾近似方法，并在此基础上设计了一种引导性优化过程以改进遗憾估计的准确性。

Result: 实验表明，现有方法系统性低估了实际遗憾（某些模型中真实遗憾是报告值的数百倍），而提出的方法显著提高了遗憾估计的准确性并降低了计算成本。

Conclusion: 论文为深度学习拍卖机制中的激励兼容性评估提供了更可靠的基础，并强调了对先前性能宣称的重新评估的必要性。

Abstract: Recent advances, such as RegretNet, ALGnet, RegretFormer and CITransNet, use deep learning to approximate optimal multi item auctions by relaxing incentive compatibility (IC) and measuring its violation via ex post regret. However, the true accuracy of these regret estimates remains unclear. Computing exact regret is computationally intractable, and current models rely on gradient based optimizers whose outcomes depend heavily on hyperparameter choices. Through extensive experiments, we reveal that existing methods systematically underestimate actual regret (In some models, the true regret is several hundred times larger than the reported regret), leading to overstated claims of IC and revenue. To address this issue, we derive a lower bound on regret and introduce an efficient item wise regret approximation. Building on this, we propose a guided refinement procedure that substantially improves regret estimation accuracy while reducing computational cost. Our method provides a more reliable foundation for evaluating incentive compatibility in deep learning based auction mechanisms and highlights the need to reassess prior performance claims in this area.

</details>


### [18] [Concurrent Permissive Strategy Templates](https://arxiv.org/abs/2601.13500)
*Ashwani Anand,Christel Baier,Calvin Chau,Sascha Klüppelholz,Ali Mirzaei,Satya Prakash Nayak,Anne-Kathrin Schmuck*

Main category: cs.GT

TL;DR: 本文介绍了并发（宽松）策略模板（ConSTels），用于表示具有Safety、Büchi和Co-Büchi目标的并发游戏中随机获胜策略集合的新方法，支持离线合成和在线适应。


<details>
  <summary>Details</summary>
Motivation: 在多智能体交互的有限图博弈中，同步交互是许多信息物理系统（CPS）的典型特征，但目前CPS设计中对此的采用仍有限。本文旨在通过引入ConSTels，填补这一空白。

Method: 基于宽松策略模板（PeSTels）的概念，提出了ConSTels，能够紧凑地编码无限策略家族，支持离线合成的组合性和在线适应的动态调整。

Result: 通过原型工具的实验验证，ConSTels在合成和适应方面表现出潜力，能够在不冲突的情况下组合更复杂的模板，并在运行时优化性能。

Conclusion: ConSTels为并发游戏中的策略表示和适应提供了有效工具，为信息物理系统的设计和实现提供了新的可能性。

Abstract: Two-player games on finite graphs provide a rigorous foundation for modeling the strategic interaction between reactive systems and their environment. While concurrent game semantics naturally capture the synchronous interactions characteristic of many cyber-physical systems (CPS), their adoption in CPS design remains limited. Building on the concept of permissive strategy templates (PeSTels) for turn-based games, we introduce concurrent (permissive) strategy templates (ConSTels) -- a novel representation for sets of randomized winning strategies in concurrent games with Safety, Büchi, and Co-Büchi objectives. ConSTels compactly encode infinite families of strategies, thereby supporting both offline and online adaptation. Offline, we exploit compositionality to enable incremental synthesis: combining ConSTels for simpler objectives into non-conflicting templates for more complex combined objectives. Online, we demonstrate how ConSTels facilitate runtime adaptation, adjusting action probabilities in response to observed opponent behavior to optimize performance while preserving correctness. We implemented ConSTel synthesis and adaptation in a prototype tool and experimentally show its potential.

</details>


### [19] [Stochastic Dynamic Pricing of Electric Vehicle Charging with Heterogeneous User Behavior: A Stackelberg Game Framework](https://arxiv.org/abs/2601.13571)
*Yongqi Zhang,Dong Ngoduy,Li Duan,Mingchang Zhu,Zhuo Chen*

Main category: cs.GT

TL;DR: 本文提出了一种基于双层次Stackelberg博弈的随机行为异构动态定价框架，以解决电动汽车充电需求管理的时空复杂性。该方法通过多级优化和排队理论近似，显著减少了排队惩罚并提高了用户效用。


<details>
  <summary>Details</summary>
Motivation: 电动汽车的快速普及带来了充电需求时空管理的复杂性，传统动态定价模型过于简化用户行为且缺乏可扩展性，因此需要一种更现实的解决方案。

Method: 研究采用了双层次Stackelberg博弈模型，上层优化动态定价以实现全系统效用最大化，下层通过多项logit选择模型模拟分散的电动汽车用户行为，并结合排队理论近似表示拥堵效应。

Result: 通过在墨尔本Clayton地区的22个充电站的实证验证，该方法显著减少了排队惩罚并提高了用户效用，优于固定和分时定价策略。

Conclusion: 该框架为电动汽车充电管理提供了一个兼具现实性和计算效率的鲁棒、可扩展工具。

Abstract: The rapid adoption of electric vehicles (EVs) introduces complex spatiotemporal demand management challenges for charging station operators (CSOs), exacerbated by demand imbalances, behavioral heterogeneity, and system uncertainty. Traditional dynamic pricing models, often relying on deterministic EV-CS pairings and network equilibrium assumptions, frequently oversimplify user behavior and lack scalability. This study proposes a stochastic, behaviorally heterogeneous dynamic pricing framework formulated as a bi-level Stackelberg game. The upper level optimizes time-varying pricing to maximize system-wide utility, while the lower level models decentralized EV users via a multinomial logit (MNL) choice model incorporating price sensitivity, battery aging, risk attitudes, and network travel costs. Crucially, the model avoids network equilibrium constraints to enhance scalability, with congestion effects represented via queuing-theoretic approximations. To efficiently solve the resulting large-scale optimization problem, a rolling-horizon approach combining the Dynamic Probabilistic Sensitivity Analysis-guided Cross-Entropy Method (PSA-CEM) with the Method of Successive Averages (MSA) is implemented. A real-world case study in Clayton, Melbourne, validates the framework using 22 charging stations. Simulation results demonstrate that the proposed mechanism substantially reduces queuing penalties and improves user utility compared to fixed and time-of-use pricing. The framework provides a robust, scalable tool for strategic EV charging management, balancing realism with computational efficiency.

</details>


### [20] [Asymmetric regularization mechanism for GAN training with Variational Inequalities](https://arxiv.org/abs/2601.13920)
*Spyridon C. Giagtzoglou,Mark H. M. Winands,Barbara Franci*

Main category: cs.GT

TL;DR: 本文提出了一种基于非对称正则化的方法来稳定生成对抗网络（GANs）的训练过程，并将其建模为纳什均衡寻求问题。通过引入Tikhonov步长和零中心梯度惩罚，确保了训练过程的收敛性和稳定性。


<details>
  <summary>Details</summary>
Motivation: 生成对抗网络（GANs）的训练存在不稳定性和难以收敛的问题，因此需要一种有效的方法来稳定训练过程并找到纳什均衡。

Method: 本文提出了一种非对称正则化机制，结合经典的Tikhonov步长和新型的零中心梯度惩罚。在高斯-牛顿格拉姆条件下，该机制确保了正则化算子的利普希茨连续性和强单调性。

Result: 实验结果表明，即使在无法实现强单调性的情况下，非对称正则化仍能有效地使训练过程收敛到均衡点并稳定轨迹。

Conclusion: 本文的非对称正则化方法为GANs的训练提供了一种稳定的解决方案，能够有效提升训练过程的收敛性和稳定性。

Abstract: We formulate the training of generative adversarial networks (GANs) as a Nash equilibrium seeking problem. To stabilize the training process and find a Nash equilibrium, we propose an asymmetric regularization mechanism based on the classic Tikhonov step and on a novel zero-centered gradient penalty. Under smoothness and a local identifiability condition induced by a Gauss-Newton Gramian, we obtain explicit Lipschitz and (strong)-monotonicity constants for the regularized operator. These constants ensure last-iterate linear convergence of a single-call Extrapolation-from-the-Past (EFTP) method. Empirical simulations on an academic example show that, even when strong monotonicity cannot be achieved, the asymmetric regularization is enough to converge to an equilibrium and stabilize the trajectory.

</details>


### [21] [BallotRank: A Condorcet Completion Method for Graphs](https://arxiv.org/abs/2601.14015)
*Ismar Volic,Jason Douglas Todd*

Main category: cs.GT

TL;DR: BallotRank是一种基于改进PageRank算法的排名偏好聚合方法，无需阻尼即可实现Condorcet一致性，并在大量实际数据中验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 为了解决排名偏好聚合中的Condorcet一致性问题，并提供一个自然的全候选排序方法，BallotRank被设计出来。

Method: BallotRank通过改进PageRank算法，去除阻尼参数，确保在聚合排名偏好时始终能够识别Condorcet赢家。

Result: 实验验证了BallotRank在近2,000场排名选择选举和20,000多次互联网投票中均能准确识别Condorcet赢家。

Conclusion: BallotRank不仅满足了Condorcet完成方法的多个社会选择标准，还提供了完整的候选排名，是一种有效的偏好聚合方法。

Abstract: We introduce BallotRank, a ranked preference aggregation method derived from a modified PageRank algorithm. It is a Condorcet-consistent method without damping, and empirical examination of nearly 2,000 ranked choice elections and over 20,000 internet polls confirms that BallotRank always identifies the Condorcet winner at conventional values of the damping parameter. We also prove that the method satisfies many of the same social choice criteria as other well-known Condorcet completion methods, but it has the advantage of being a natural social welfare function that provides a full ranking of the candidates.

</details>


### [22] [Collective intelligence in science: direct elicitation of diverse information from experts with unknown information structure](https://arxiv.org/abs/2601.14047)
*Alexey V. Osipov,Nikolay N. Osipov*

Main category: cs.GT

TL;DR: 提出了一种基于虚拟预测市场和聊天系统的简单机制，用于高效聚合多样且不可预测的专家私人信息，以解决复杂的科学假设。


<details>
  <summary>Details</summary>
Motivation: 面对复杂的科学假设和大量分散的专家私人信息（如实验结果、AI系统输出等），需要一种高效且透明的方法来聚合这些信息。

Method: 设计了一种基于虚拟预测市场和聊天系统的机制，使专家通过聊天直接分享信息并在市场中交易，模拟真实假设的解决。

Result: 该机制能够达到均衡状态，即使无法确定最终真相或专家无法进行复杂计算，也能高效聚合信息并形成可解释的结果。

Conclusion: 通过将虚拟货币奖励转化为真实资产，这种机制为大规模协作研究提供了一种创新的资金支持方式。

Abstract: Suppose we need a deep collective analysis of an open scientific problem: there is a complex scientific hypothesis and a large online group of mutually unrelated experts with relevant private information of a diverse and unpredictable nature. This information may be results of experts' individual experiments, original reasoning of some of them, results of AI systems they use, etc. We propose a simple mechanism based on a self-resolving play-money prediction market entangled with a chat. We show that such a system can easily be brought to an equilibrium where participants directly share their private information on the hypothesis through the chat and trade as if the market were resolved in accordance with the truth of the hypothesis. This approach will lead to efficient aggregation of relevant information in a completely interpretable form even if the ground truth cannot be established and experts initially know nothing about each other and cannot perform complex Bayesian calculations. Finally, by rewarding the experts with some real assets proportionally to the play money they end up with, we can get an innovative way to fund large-scale collaborative studies of any type.

</details>


### [23] [A Minimax Perspective on Almost-Stable Matchings](https://arxiv.org/abs/2601.14195)
*Frederik Glitzner,David Manlove*

Main category: cs.GT

TL;DR: 该论文提出了一种基于最小最大原则的公平性方法来衡量和分配匹配市场中的不稳定性，以减少对少数个体的集中影响。


<details>
  <summary>Details</summary>
Motivation: 在匹配市场中，完全稳定性常难以实现，且现有方法易导致不稳定性集中在少数个体上，引发公平性和激励问题。

Method: 采用最小最大原则，目标是减少任何个体参与的最大阻塞对数，并通过计算复杂度分析和算法设计验证其可行性。

Result: 研究表明，即使仅保证每个个体最多参与一个阻塞对，也是NP完全问题；但在某些限制条件下（如偏好列表长度受限），存在多项式时间算法和近似解。

Conclusion: 论文揭示了在稳定性分配与计算可行性之间的基本权衡，为匹配市场设计提供了新的公平性视角。

Abstract: Stability is crucial in matching markets, yet in many real-world settings - from hospital residency allocations to roommate assignments - full stability is either impossible to achieve or can come at the cost of leaving many agents unmatched. When stability cannot be achieved, algorithmicists and market designers face a critical question: how should instability be measured and distributed among participants? Existing approaches to "almost-stable" matchings focus on aggregate measures, minimising either the total number of blocking pairs or the count of agents involved in blocking pairs. However, such aggregate objectives can result in concentrated instability on a few individual agents, raising concerns about fairness and incentives to deviate. We introduce a fairness-oriented approach to approximate stability based on the minimax principle: we seek matchings that minimise the maximum number of blocking pairs any agent is in. Equivalently, we minimise the maximum number of agents that anyone has justified envy towards. This distributional objective protects the worst-off agents from a disproportionate amount of instability. We characterise the computational complexity of this notion across fundamental matching settings. Surprisingly, even very modest guarantees prove computationally intractable: we show that it is NP-complete to decide whether a matching exists in which no agent is in more than one blocking pair, even when preference lists have constant-bounded length. This hardness applies to both Stable Roommates and maximum-cardinality Stable Marriage. On the positive side, we provide polynomial-time algorithms when agents rank at most two others, and present approximation algorithms and integer programs. Our results map the algorithmic landscape and reveal fundamental trade-offs between distributional guarantees and computational feasibility.

</details>
