{"id": "2602.10712", "categories": ["cs.GR", "astro-ph.EP", "astro-ph.IM"], "pdf": "https://arxiv.org/pdf/2602.10712", "abs": "https://arxiv.org/abs/2602.10712", "authors": ["Charles Constant", "Elizabeth Bates", "Santosh Bhattarai", "Marek Ziebart", "Tobias Ritschel"], "title": "Photons x Force: Differentiable Radiation Pressure Modeling", "comment": "17 pages, 19 figures", "summary": "We propose a system to optimize parametric designs subject to radiation pressure, \\ie the effect of light on the motion of objects. This is most relevant in the design of spacecraft, where radiation pressure presents the dominant non-conservative forcing mechanism, which is the case beyond approximately 800 km altitude. Despite its importance, the high computational cost of high-fidelity radiation pressure modeling has limited its use in large-scale spacecraft design, optimization, and space situational awareness applications. We enable this by offering three innovations in the simulation, in representation and in optimization: First, a practical computer graphics-inspired Monte-Carlo (MC) simulation of radiation pressure. The simulation is highly parallel, uses importance sampling and next-event estimation to reduce variance and allows simulating an entire family of designs instead of a single spacecraft as in previous work. Second, we introduce neural networks as a representation of forces from design parameters. This neural proxy model, learned from simulations, is inherently differentiable and can query forces orders of magnitude faster than a full MC simulation. Third, and finally, we demonstrate optimizing inverse radiation pressure designs, such as finding geometry, material or operation parameters that minimizes travel time, maximizes proximity given a desired end-point, minimize thruster fuel, trains mission control policies or allocated compute budget in extraterrestrial compute."}
{"id": "2602.10290", "categories": ["cs.GT", "cs.CC"], "pdf": "https://arxiv.org/pdf/2602.10290", "abs": "https://arxiv.org/abs/2602.10290", "authors": ["Colin Cleveland", "Bart de Keijzer", "Maria Polukarov"], "title": "The Complexity of Strategic Behavior in Primary Elections", "comment": "10 pages, 3 figures. Conference: AAMAS 2026", "summary": "We study the computational complexity of strategic behaviour in primary elections. Unlike direct voting systems, primaries introduce a multi-stage process in which voters first influence intra-party nominees before a general election determines the final winner. While previous work has evaluated primaries via welfare distortion, we instead examine their game-theoretic properties. We formalise a model of primaries under first-past-the-post with fixed tie-breaking and analyse voters' strategic behaviour. We show that determining whether a pure Nash equilibrium exists is $Σ_2^{\\mathbf P}$-complete, computing a best response is NP-complete, and deciding the existence of subgame-perfect equilibria in sequential primaries is PSPACE-complete. These results reveal that primaries fundamentally increase the computational difficulty of strategic reasoning, situating them as a rich source of complexity-theoretic challenges within computational social choice."}
{"id": "2602.10456", "categories": ["cs.GT", "econ.TH", "math.OC"], "pdf": "https://arxiv.org/pdf/2602.10456", "abs": "https://arxiv.org/abs/2602.10456", "authors": ["Devansh Jalota", "Matthew Tsao"], "title": "Informal and Privatized Transit: Incentives, Efficiency and Coordination", "comment": null, "summary": "Informal and privatized transit services, such as minibuses and shared auto-rickshaws, are integral to daily travel in large urban metropolises, providing affordable commutes where a formal public transport system is inadequate and other options are unaffordable. Despite the crucial role that these services play in meeting mobility needs, governments often do not account for these services or their underlying incentives when planning transit systems, which can significantly compromise system efficiency.\n  Against this backdrop, we develop a framework to analyze the incentives underlying informal and privatized transit systems, while proposing mechanisms to guide public transit operation and incentive design when a substantial share of mobility is provided by such profit-driven private operators. We introduce a novel, analytically tractable game-theoretic model of a fully privatized informal transit system with a fixed menu of routes, in which profit-maximizing informal operators (drivers) decide where to provide service and cost-minimizing commuters (riders) decide whether to use these services. Within this framework, we establish tight price of anarchy bounds which demonstrate that decentralized, profit-maximizing driver behavior can lead to bounded yet substantial losses in cumulative driver profit and rider demand served. We further show that these performance losses can be mitigated through targeted interventions, including Stackelberg routing mechanisms in which a modest share of drivers are centrally controlled, reflecting environments where informal operators coexist with public transit, and cross-subsidization schemes that use route-specific tolls or subsidies to incentivize drivers to operate on particular routes. Finally, we reinforce these findings through numerical experiments based on a real-world informal transit system in Nalasopara, India."}
{"id": "2602.10469", "categories": ["cs.GT", "cs.LG", "math.OC"], "pdf": "https://arxiv.org/pdf/2602.10469", "abs": "https://arxiv.org/abs/2602.10469", "authors": ["Zongjun Yang", "Rachitesh Kumar", "Christian Kroer"], "title": "Online Generalized-mean Welfare Maximization: Achieving Near-Optimal Regret from Samples", "comment": null, "summary": "We study online fair allocation of $T$ sequentially arriving items among $n$ agents with heterogeneous preferences, with the objective of maximizing generalized-mean welfare, defined as the $p$-mean of agents' time-averaged utilities, with $p\\in (-\\infty, 1)$. We first consider the i.i.d. arrival model and show that the pure greedy algorithm -- which myopically chooses the welfare-maximizing integral allocation -- achieves $\\widetilde{O}(1/T)$ average regret. Importantly, in contrast to prior work, our algorithm does not require distributional knowledge and achieves the optimal regret rate using only the online samples.\n  We then go beyond i.i.d. arrivals and investigate a nonstationary model with time-varying independent distributions. In the absence of additional data about the distributions, it is known that every online algorithm must suffer $Ω(1)$ average regret. We show that only a single historical sample from each distribution is sufficient to recover the optimal $\\widetilde{O}(1/T)$ average regret rate, even in the face of arbitrary non-stationarity. Our algorithms are based on the re-solving paradigm: they assume that the remaining items will be the ones seen historically in those periods and solve the resulting welfare-maximization problem to determine the decision in every period. Finally, we also account for distribution shifts that may distort the fidelity of historical samples and show that the performance of our re-solving algorithms is robust to such shifts."}
{"id": "2602.10483", "categories": ["cs.GT", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.10483", "abs": "https://arxiv.org/abs/2602.10483", "authors": ["Wei Tang", "Yifan Wang", "Mengxiao Zhang"], "title": "Pricing Query Complexity of Multiplicative Revenue Approximation", "comment": null, "summary": "We study the pricing query complexity of revenue maximization for a single buyer whose private valuation is drawn from an unknown distribution. In this setting, the seller must learn the optimal monopoly price by posting prices and observing only binary purchase decisions, rather than the realized valuations. Prior work has established tight query complexity bounds for learning a near-optimal price with additive error $\\varepsilon$ when the valuation distribution is supported on $[0,1]$. However, our understanding of how to learn a near-optimal price that achieves at least a $(1-\\varepsilon)$ fraction of the optimal revenue remains limited.\n  In this paper, we study the pricing query complexity of the single-buyer revenue maximization problem under such multiplicative error guarantees in several settings. Observe that when pricing queries are the only source of information about the buyer's distribution, no algorithm can achieve a non-trivial approximation, since the scale of the distribution cannot be learned from pricing queries alone. Motivated by this fundamental impossibility, we consider two natural and well-motivated models that provide \"scale hints\": (i) a one-sample hint, in which the algorithm observes a single realized valuation before making pricing queries; and (ii) a value-range hint, in which the valuation support is known to lie within $[1, H]$. For each type of hint, we establish pricing query complexity guarantees that are tight up to polylogarithmic factors for several classes of distributions, including monotone hazard rate (MHR) distributions, regular distributions, and general distributions."}
{"id": "2602.10524", "categories": ["cs.GT"], "pdf": "https://arxiv.org/pdf/2602.10524", "abs": "https://arxiv.org/abs/2602.10524", "authors": ["Yuqing Hou", "Yiyin Cao", "Chuangyin Dang"], "title": "Characterization and Computation of Normal-Form Proper Equilibria in Extensive-Form Games via the Sequence-Form Representation", "comment": "arXiv admin note: substantial text overlap with arXiv:2505.13827", "summary": "Normal-form proper equilibrium, introduced by Myerson as a refinement of normal-form perfect equilibrium, occupies a distinctive position in the equilibrium analysis of extensive-form games because its more stringent perturbation structure entails the sequential rationality. However, the size of the normal-form representation grows exponentially with the number of parallel information sets, making the direct determination of normal-form proper equilibria intractable. To address this challenge, we develop a compact sequence-form proper equilibrium by redefining the expected payoffs over sequences, and we prove that it coincides with the normal-form proper equilibrium via strategic equivalence. To facilitate computation, we further introduce an alternative representation by defining a class of perturbed games based on an $\\varepsilon$-permutahedron over sequences. Building on this representation, we introduce two differentiable path-following methods for computing normal-form proper equilibria. These methods rely on artificial sequence-form games whose expected payoff functions incorporate logarithmic or entropy regularization through an auxiliary variable. We prove the existence of a smooth equilibrium path induced by each artificial game, starting from an arbitrary positive realization plan and converging to a normal-form proper equilibrium of the original game as the auxiliary variable approaches zero. Finally, our experimental results demonstrate the effectiveness and efficiency of the proposed methods."}
{"id": "2602.10601", "categories": ["cs.GT", "cs.CC"], "pdf": "https://arxiv.org/pdf/2602.10601", "abs": "https://arxiv.org/abs/2602.10601", "authors": ["Katarína Cechlárová", "Ildikó Schlotter"], "title": "Necessary President in Elections with Parties", "comment": "Accepted at AAMAS 2026", "summary": "Consider an election where the set of candidates is partitioned into parties, and each party must choose exactly one candidate to nominate for the election held over all nominees. The Necessary President problem asks whether a candidate, if nominated, becomes the winner of the election for all possible nominations from other parties.\n  We study the computational complexity of Necessary President for several voting rules. We show that while this problem is solvable in polynomial time for Borda, Maximin, and Copeland$^α$ for every $α\\in [0,1]$, it is $\\mathsf{coNP}$-complete for general classes of positional scoring rules that include $\\ell$-Approval and $\\ell$-Veto, even when the maximum size of a party is two. For such positional scoring rules, we show that Necessary President is $\\mathsf{W}[2]$-hard when parameterized by the number of parties, but fixed-parameter tractable with respect to the number of voter types. Additionally, we prove that Necessary President for Ranked Pairs is $\\mathsf{coNP}$-complete even for maximum party size two, and $\\mathsf{W}[1]$-hard with respect to the number of parties; remarkably, both of these results hold even for constant number of voters."}
{"id": "2602.10679", "categories": ["cs.GT", "econ.TH"], "pdf": "https://arxiv.org/pdf/2602.10679", "abs": "https://arxiv.org/abs/2602.10679", "authors": ["Haris Aziz", "Péter Biró", "Gergely Csáji", "Tom Demeulemeester"], "title": "Smart Lotteries in School Choice: Ex-ante Pareto-Improvement with Ex-post Stability", "comment": null, "summary": "In a typical school choice application, the students have strict preferences over the schools while the schools have coarse priorities over the students based on their distance and their enrolled siblings. The outcome of a centralized admission mechanism is then usually obtained by the Deferred Acceptance (DA) algorithm with random tie-breaking. Therefore, every possible outcome of this mechanism is a stable solution for the coarse priorities that will arise with certain probability. This implies a probabilistic assignment, where the admission probability for each student-school pair is specified. In this paper, we propose a new efficiency-improving stable `smart lottery' mechanism. We aim to improve the probabilistic assignment ex-ante in a stochastic dominance sense, while ensuring that the improved random matching is still ex-post stable, meaning that it can be decomposed into stable matchings regarding the original coarse priorities. Therefore, this smart lottery mechanism can provide a clear Pareto-improvement in expectation for any cardinal utilities compared to the standard DA with lottery solution, without sacrificing the stability of the final outcome. We show that although the underlying computational problem is NP-hard, we can solve the problem by using advanced optimization techniques such as integer programming with column generation. We conduct computational experiments on generated and real instances. Our results show that the welfare gains by our mechanism are substantially larger than the expected gains by standard methods that realize efficiency improvements after ties have already been broken."}
{"id": "2602.10725", "categories": ["cs.GT"], "pdf": "https://arxiv.org/pdf/2602.10725", "abs": "https://arxiv.org/abs/2602.10725", "authors": ["Gergely Csáji", "Thánh Nguyen"], "title": "Core-Stable Kidney Exchange via Altruistic Donors", "comment": null, "summary": "Kidney exchange programs among hospitals in the United States and across European countries improve efficiency by pooling donors and patients on a centralized platform. Sustaining such cooperation requires stability. When the core is empty, hospitals or countries may withhold easily matched pairs for internal use, creating incentive problems that undermine participation and reduce the scope and efficiency of exchange.\n  We propose a method to restore core stability by augmenting the platform with altruistic donors. Although the worst-case number of required altruists can be large, we show that in realistic settings only a small number is needed. We analyze two models of the compatibility graph, one based on random graphs and the other on compatibility types. When only pairwise exchanges are allowed, the number of required altruists is bounded by the maximum number of independent odd cycles, defined as disjoint odd cycles with no edges between them. This bound grows logarithmically with market size in the random graph model and is at most one third of the number of compatibility types in the type-based model. When small exchange cycles are allowed, it suffices for each participating organization to receive a number of altruists proportional to the number of compatibility types. Finally, simulations show that far fewer altruists are needed in practice than worst-case theory suggests."}
{"id": "2602.10739", "categories": ["cs.GT", "cs.IR"], "pdf": "https://arxiv.org/pdf/2602.10739", "abs": "https://arxiv.org/abs/2602.10739", "authors": ["Dominykas Seputis", "Rajeev Verma", "Alexander Timans"], "title": "Equity by Design: Fairness-Driven Recommendation in Heterogeneous Two-Sided Markets", "comment": null, "summary": "Two-sided marketplaces embody heterogeneity in incentives: producers seek exposure while consumers seek relevance, and balancing these competing objectives through constrained optimization is now a standard practice. Yet real platforms face finer-grained complexity: consumers differ in preferences and engagement patterns, producers vary in catalog value and capacity, and business objectives impose additional constraints beyond raw relevance. We formalize two-sided fairness under these realistic conditions, extending prior work from soft single-item allocations to discrete multi-item recommendations. We introduce Conditional Value-at-Risk (CVaR) as a consumer-side objective that compresses group-level utility disparities, and integrate business constraints directly into the optimization. Our experiments reveal that the \"free fairness\" regime, where producer constraints impose no consumer cost, disappears in multi item settings. Strikingly, moderate fairness constraints can improve business metrics by diversifying exposure away from saturated producers. Scalable solvers match exact solutions at a fraction of the runtime, making fairness-aware allocation practical at scale. These findings reframe fairness not as a tax on platform efficiency but as a lever for sustainable marketplace health."}
{"id": "2602.10851", "categories": ["cs.GT", "cs.DS"], "pdf": "https://arxiv.org/pdf/2602.10851", "abs": "https://arxiv.org/abs/2602.10851", "authors": ["Frederik Glitzner"], "title": "Near-Feasible Stable Matchings: Incentives and Optimality", "comment": "EA to appear at AAMAS 2026", "summary": "Stable matching is a fundamental area with many practical applications, such as centralised clearinghouses for school choice or job markets. Recent work has introduced the paradigm of near-feasibility in capacitated matching settings, where agent capacities are slightly modified to ensure the existence of desirable outcomes. While useful when no stable matching exists, or some agents are left unmatched, it has not previously been investigated whether near-feasible stable matchings satisfy desirable properties with regard to their stability in the original instance. Furthermore, prior works often leave open deviation incentive issues that arise when the centralised authority modifies agents' capacities.\n  We consider these issues in the Stable Fixtures problem model, which generalises many classical models through non-bipartite preferences and capacitated agents. We develop a formal framework to analyse and quantify agent incentives to adhere to computed matchings. Then, we embed near-feasible stable matchings in this framework and study the trade-offs between instability, capacity modifications, and computational complexity. We prove that capacity modifications can be simultaneously optimal at individual and aggregate levels, and provide efficient algorithms to compute them. We show that different modification strategies significantly affect stability, and establish that minimal modifications and minimal deviation incentives are compatible and efficiently computable under general conditions. Finally, we provide exact algorithms and experimental results for tractable and intractable versions of these problems."}
{"id": "2602.10966", "categories": ["cs.GT", "econ.TH"], "pdf": "https://arxiv.org/pdf/2602.10966", "abs": "https://arxiv.org/abs/2602.10966", "authors": ["Mete Şeref Ahunbay", "Paul W. Goldberg", "Edwin Lock", "Panayotis Mertikopoulos", "Bary S. R. Pradelski", "Bassel Tarbush"], "title": "The Computational Intractability of Not Worst Responding", "comment": "28 pages, 2 figures", "summary": "Finding, counting, or determining the existence of Nash equilibria, where players must play optimally given each others' actions, are known to be computational intractable problems. We ask whether weakening optimality to the requirement that each player merely avoid worst responses -- arguably the weakest meaningful rationality criterion -- yields tractable solution concepts. We show that it does not: any solution concept with this minimal guarantee is ``as intractable'' as pure Nash equilibrium. In general games, determining the existence of no-worst-response action profiles is NP-complete, finding one is NP-hard, and counting them is #P-complete. In potential games, where existence is guaranteed, the search problem is PLS-complete. Computational intractability therefore stems not only from the requirement of optimality, but also from the requirement of a minimal rationality guarantee for each player. Moreover, relaxing the latter requirement gives rise to a tractability trade-off between the strength of individual rationality guarantees and the fraction of players satisfying them."}
{"id": "2602.11147", "categories": ["cs.GT"], "pdf": "https://arxiv.org/pdf/2602.11147", "abs": "https://arxiv.org/abs/2602.11147", "authors": ["Rasheed M", "Parth Desai", "Sujit Gujar"], "title": "Let Leaders Play Games: Improving Timing in Leader-based Consensus", "comment": null, "summary": "Propagation latency is inherent to any distributed network, including blockchains. Typically, blockchain protocols provide a timing buffer for block propagation across the network. In leader-based blockchains, the leader -- block proposer -- is known in advance for each slot. A fast (or low-latency) proposer may delay the block proposal in anticipation of more rewards from the transactions that would otherwise be included in the subsequent block. Deploying such a strategy by manipulating the timing is known as timing games. It increases the risk of missed blocks due to reduced time for other nodes to vote on the block, affecting the overall efficiency of the blockchain. Moreover, proposers who play timing games essentially appropriate MEV (additional rewards over transaction fees and the block reward) that would otherwise accrue to the next block, making it unfair to subsequent block proposers. We propose a double-block proposal mechanism, 2-Prop, to curtail timing games. 2-Prop selects two proposers per slot to propose blocks and confirms one of them. We design a reward-sharing policy for proposers based on how quickly their blocks propagate to avoid strategic deviations. In the induced game, which we call the Latency Game, we show that it is a Nash Equilibrium for the proposers to propose the block without delay under homogeneous network settings. Under heterogeneous network settings, we study many configurations, and our analysis shows that a faster proposer would prefer not to delay unless the other proposer is extremely slow. Thus, we show the efficacy of 2-Prop in mitigating the effect of timing games."}
{"id": "2602.11152", "categories": ["cs.GT"], "pdf": "https://arxiv.org/pdf/2602.11152", "abs": "https://arxiv.org/abs/2602.11152", "authors": ["Hamidreza Alipour", "Mohak Goyal"], "title": "Utilitarian Distortion Under Probabilistic Voting", "comment": null, "summary": "The utilitarian distortion framework evaluates voting rules by their worst-case efficiency loss when voters have cardinal utilities but express only ordinal rankings. Under the classical model, a longstanding tension exists: Plurality, which suffers from the spoiler effect, achieves optimal $Θ(m^2)$ distortion among deterministic rules, while normatively superior rules like Copeland and Borda have unbounded distortion. We resolve this tension under probabilistic voting with the Plackett-Luce model, where rankings are noisy reflections of utilities governed by an inverse temperature parameter $β$. Copeland and Borda both achieve at most $β\\frac{1+e^{-β}}{1-e^{-β}}$ distortion, independent of the number of candidates $m$, and within a factor of 2 of the lower bound for randomized rules satisfying the probabilistic Condorcet loser criterion known from prior work. This improves upon the prior $O(β^2)$ bound for Borda. These upper bounds are nearly tight: prior work establishes a $(1-o(1))β$ lower bound for Borda, and we prove a $(1-ε)β$ lower bound for Copeland for any constant $ε>0$. In contrast, rules that rely only on top-choice information fare worse: Plurality has distortion $Ω(\\min(e^β, m))$ and Random Dictator has distortion $Θ(m)$. Additional `veto' information is also insufficient to remove the dependence on $m$; Plurality Veto and Pruned Plurality Veto have distortion $Ω(β\\ln m)$. We also prove a lower bound of $(\\frac{5}{8}-ε)β$ (for any constant $ε>0$) for all deterministic finite-precision tournament-based rules, a class that includes Copeland and any rule based on pairwise comparison margins rounded to fixed precision. Our results show that the distortion framework aligns with normative intuitions once the probabilistic nature of real-world voting is taken into account."}
