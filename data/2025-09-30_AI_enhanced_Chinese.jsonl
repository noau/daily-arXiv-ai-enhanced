{"id": "2509.23336", "categories": ["cs.GR", "cs.CV"], "pdf": "https://arxiv.org/pdf/2509.23336", "abs": "https://arxiv.org/abs/2509.23336", "authors": ["Weidan Xiong", "Yongli Wu", "Bochuan Zeng", "Jianwei Guo", "Dani Lischinski", "Daniel Cohen-Or", "Hui Huang"], "title": "DiffTex: Differentiable Texturing for Architectural Proxy Models", "comment": "ACM TOG and SIGGRAPH Asia 2025 (Patent Protected); Project page:\n  https://vcc.tech/research/2025/DiffTex", "summary": "Simplified proxy models are commonly used to represent architectural\nstructures, reducing storage requirements and enabling real-time rendering.\nHowever, the geometric simplifications inherent in proxies result in a loss of\nfine color and geometric details, making it essential for textures to\ncompensate for the loss. Preserving the rich texture information from the\noriginal dense architectural reconstructions remains a daunting task,\nparticularly when working with unordered RGB photographs. We propose an\nautomated method for generating realistic texture maps for architectural proxy\nmodels at the texel level from an unordered collection of registered\nphotographs. Our approach establishes correspondences between texels on a UV\nmap and pixels in the input images, with each texel's color computed as a\nweighted blend of associated pixel values. Using differentiable rendering, we\noptimize blending parameters to ensure photometric and perspective consistency,\nwhile maintaining seamless texture coherence. Experimental results demonstrate\nthe effectiveness and robustness of our method across diverse architectural\nmodels and varying photographic conditions, enabling the creation of\nhigh-quality textures that preserve visual fidelity and structural detail.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u81ea\u52a8\u5316\u65b9\u6cd5\uff0c\u7528\u4e8e\u4ece\u65e0\u5e8fRGB\u7167\u7247\u4e2d\u4e3a\u5efa\u7b51\u4ee3\u7406\u6a21\u578b\u751f\u6210\u9ad8\u8d28\u91cf\u7684\u7eb9\u7406\u8d34\u56fe\uff0c\u4ee5\u8865\u507f\u51e0\u4f55\u7b80\u5316\u5e26\u6765\u7684\u7ec6\u8282\u635f\u5931\u3002", "motivation": "\u5efa\u7b51\u4ee3\u7406\u6a21\u578b\u7684\u51e0\u4f55\u7b80\u5316\u4f1a\u5bfc\u81f4\u7cbe\u7ec6\u989c\u8272\u548c\u51e0\u4f55\u7ec6\u8282\u7684\u4e22\u5931\uff0c\u56e0\u6b64\u9700\u8981\u7eb9\u7406\u6765\u8865\u507f\u8fd9\u4e9b\u635f\u5931\u3002\u7136\u800c\uff0c\u4ece\u65e0\u5e8fRGB\u7167\u7247\u4e2d\u4fdd\u7559\u539f\u59cb\u5bc6\u96c6\u5efa\u7b51\u91cd\u5efa\u7684\u4e30\u5bcc\u7eb9\u7406\u4fe1\u606f\u4ecd\u662f\u4e00\u4e2a\u6311\u6218\u3002", "method": "\u8be5\u65b9\u6cd5\u901a\u8fc7\u5efa\u7acbUV\u8d34\u56fe\u4e0a\u7684\u7eb9\u7406\u5355\u5143\u4e0e\u8f93\u5165\u56fe\u50cf\u4e2d\u7684\u50cf\u7d20\u4e4b\u95f4\u7684\u5bf9\u5e94\u5173\u7cfb\uff0c\u5e76\u4f18\u5316\u6df7\u5408\u53c2\u6570\uff0c\u5229\u7528\u53ef\u5fae\u5206\u6e32\u67d3\u786e\u4fdd\u5149\u5ea6\u548c\u900f\u89c6\u4e00\u81f4\u6027\uff0c\u540c\u65f6\u4fdd\u6301\u65e0\u7f1d\u7eb9\u7406\u8fde\u8d2f\u6027\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u4e0d\u540c\u5efa\u7b51\u6a21\u578b\u548c\u591a\u79cd\u6444\u5f71\u6761\u4ef6\u4e0b\u5747\u8868\u73b0\u51fa\u9ad8\u6548\u6027\u548c\u9c81\u68d2\u6027\uff0c\u80fd\u591f\u751f\u6210\u4fdd\u7559\u89c6\u89c9\u4fdd\u771f\u5ea6\u548c\u7ed3\u6784\u7ec6\u8282\u7684\u9ad8\u8d28\u91cf\u7eb9\u7406\u3002", "conclusion": "\u672c\u6587\u7684\u65b9\u6cd5\u6210\u529f\u5730\u89e3\u51b3\u4e86\u5efa\u7b51\u4ee3\u7406\u6a21\u578b\u7eb9\u7406\u751f\u6210\u7684\u6311\u6218\uff0c\u4e3a\u9ad8\u8d28\u91cf\u7eb9\u7406\u7684\u81ea\u52a8\u5316\u521b\u5efa\u63d0\u4f9b\u4e86\u6709\u6548\u5de5\u5177\u3002"}}
{"id": "2509.23489", "categories": ["cs.GR", "cs.HC", "I.3.6"], "pdf": "https://arxiv.org/pdf/2509.23489", "abs": "https://arxiv.org/abs/2509.23489", "authors": ["Ethan Chen", "Sushant Kondguli", "Carl Marshall", "Yuhao Zhu"], "title": "Modeling and Exploiting the Time Course of Chromatic Adaptation for Display Power Optimizations in Virtual Reality", "comment": "To appear in Transactions on Graphics and SIGGRAPH ASIA 2025", "summary": "We introduce a gaze-tracking--free method to reduce OLED display power\nconsumption in VR with minimal perceptual impact. This technique exploits the\ntime course of chromatic adaptation, the human visual system's ability to\nmaintain stable color perception under changing illumination. To that end, we\npropose a novel psychophysical paradigm that models how human adaptation state\nchanges with the scene illuminant. We exploit this model to compute an optimal\nilluminant shift trajectory, controlling the rate and extent of illumination\nchange, to reduce display power under a given perceptual loss budget. Our\ntechnique significantly improves the perceptual quality over prior work that\napplies illumination shifts instantaneously. Our technique can also be combined\nwith prior work on luminance dimming to reduce display power by 31% with no\nstatistical loss of perceptual quality.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u65e0\u773c\u52a8\u8ffd\u8e2a\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u5229\u7528\u8272\u9002\u5e94\u7684\u65f6\u95f4\u8fc7\u7a0b\uff0c\u51cf\u5c11VR\u4e2dOLED\u663e\u793a\u7684\u529f\u8017\uff0c\u540c\u65f6\u6700\u5c0f\u5316\u611f\u77e5\u5f71\u54cd\u3002", "motivation": "VR\u8bbe\u5907\u4e2dOLED\u663e\u793a\u7684\u9ad8\u529f\u8017\u95ee\u9898\u9700\u8981\u89e3\u51b3\uff0c\u540c\u65f6\u9700\u786e\u4fdd\u7528\u6237\u7684\u611f\u77e5\u8d28\u91cf\u4e0d\u53d7\u663e\u8457\u5f71\u54cd\u3002", "method": "\u5229\u7528\u4eba\u7c7b\u89c6\u89c9\u7cfb\u7edf\u5728\u5149\u7167\u53d8\u5316\u4e0b\u4fdd\u6301\u989c\u8272\u611f\u77e5\u7a33\u5b9a\u7684\u80fd\u529b\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u5fc3\u7406\u7269\u7406\u8303\u5f0f\uff0c\u8ba1\u7b97\u6700\u4f18\u5149\u7167\u504f\u79fb\u8f68\u8ff9\u4ee5\u51cf\u5c11\u529f\u8017\u3002", "result": "\u8be5\u65b9\u6cd5\u663e\u8457\u63d0\u9ad8\u4e86\u611f\u77e5\u8d28\u91cf\uff0c\u5e76\u53ef\u4e0e\u5176\u4ed6\u6280\u672f\u7ed3\u5408\uff0c\u51cf\u5c1131%\u7684\u663e\u793a\u529f\u8017\uff0c\u4e14\u4e0d\u5f71\u54cd\u611f\u77e5\u8d28\u91cf\u3002", "conclusion": "\u8be5\u6280\u672f\u4e3aVR\u663e\u793a\u63d0\u4f9b\u4e86\u9ad8\u6548\u7684\u8282\u80fd\u65b9\u6848\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u7528\u6237\u4f53\u9a8c\u3002"}}
{"id": "2509.23572", "categories": ["cs.GR", "cs.CV", "physics.app-ph"], "pdf": "https://arxiv.org/pdf/2509.23572", "abs": "https://arxiv.org/abs/2509.23572", "authors": ["Arjun Teh", "Delio Vicini", "Bernd Bickel", "Ioannis Gkioulekas", "Matthew O'Toole"], "title": "Automated design of compound lenses with discrete-continuous optimization", "comment": "SIGGRAPH Asia 2025, project website:\n  https://imaging.cs.cmu.edu/automated_lens_design/", "summary": "We introduce a method that automatically and jointly updates both continuous\nand discrete parameters of a compound lens design, to improve its performance\nin terms of sharpness, speed, or both. Previous methods for compound lens\ndesign use gradient-based optimization to update continuous parameters (e.g.,\ncurvature of individual lens elements) of a given lens topology, requiring\nextensive expert intervention to realize topology changes. By contrast, our\nmethod can additionally optimize discrete parameters such as number and type\n(e.g., singlet or doublet) of lens elements. Our method achieves this\ncapability by combining gradient-based optimization with a tailored Markov\nchain Monte Carlo sampling algorithm, using transdimensional mutation and\nparaxial projection operations for efficient global exploration. We show\nexperimentally on a variety of lens design tasks that our method effectively\nexplores an expanded design space of compound lenses, producing better designs\nthan previous methods and pushing the envelope of speed-sharpness tradeoffs\nachievable by automated lens design.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u53ef\u4ee5\u81ea\u52a8\u8054\u5408\u66f4\u65b0\u590d\u5408\u900f\u955c\u8bbe\u8ba1\u7684\u8fde\u7eed\u548c\u79bb\u6563\u53c2\u6570\u7684\u65b9\u6cd5\uff0c\u4ee5\u63d0\u9ad8\u5176\u9510\u5ea6\u548c\u901f\u5ea6\u6027\u80fd\u3002\u8be5\u65b9\u6cd5\u901a\u8fc7\u7ed3\u5408\u68af\u5ea6\u4f18\u5316\u548c\u9a6c\u5c14\u53ef\u592b\u94fe\u8499\u7279\u5361\u6d1b\u91c7\u6837\u7b97\u6cd5\uff0c\u5b9e\u73b0\u4e86\u5bf9\u900f\u955c\u5143\u7d20\u6570\u91cf\u548c\u7c7b\u578b\u7b49\u79bb\u6563\u53c2\u6570\u7684\u4f18\u5316\u3002", "motivation": "\u4f20\u7edf\u7684\u590d\u5408\u900f\u955c\u8bbe\u8ba1\u65b9\u6cd5\u4ec5\u901a\u8fc7\u68af\u5ea6\u4f18\u5316\u66f4\u65b0\u8fde\u7eed\u53c2\u6570\uff08\u5982\u5355\u4e2a\u900f\u955c\u5143\u7d20\u7684\u66f2\u7387\uff09\uff0c\u9700\u8981\u4e13\u5bb6\u5e72\u9884\u4ee5\u5b9e\u73b0\u62d3\u6251\u53d8\u5316\u3002\u672c\u6587\u65e8\u5728\u5b9e\u73b0\u4e00\u79cd\u65e0\u9700\u4e13\u5bb6\u5e72\u9884\u5373\u53ef\u81ea\u52a8\u4f18\u5316\u79bb\u6563\u53c2\u6570\u7684\u65b9\u6cd5\u3002", "method": "\u8be5\u65b9\u6cd5\u7ed3\u5408\u4e86\u68af\u5ea6\u4f18\u5316\u548c\u5b9a\u5236\u5316\u7684\u9a6c\u5c14\u53ef\u592b\u94fe\u8499\u7279\u5361\u6d1b\u91c7\u6837\u7b97\u6cd5\uff0c\u5229\u7528\u8de8\u7ef4\u5ea6\u7a81\u53d8\u548c\u8fd1\u8f74\u6295\u5f71\u64cd\u4f5c\u8fdb\u884c\u9ad8\u6548\u7684\u5168\u5c40\u63a2\u7d22\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u80fd\u6709\u6548\u6269\u5c55\u590d\u5408\u900f\u955c\u7684\u8bbe\u8ba1\u7a7a\u95f4\uff0c\u4ea7\u751f\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\u7684\u8bbe\u8ba1\uff0c\u5e76\u7a81\u7834\u81ea\u52a8\u900f\u955c\u8bbe\u8ba1\u4e2d\u901f\u5ea6\u4e0e\u9510\u5ea6\u6743\u8861\u7684\u6781\u9650\u3002", "conclusion": "\u672c\u6587\u7684\u65b9\u6cd5\u5728\u81ea\u52a8\u4f18\u5316\u900f\u955c\u8bbe\u8ba1\u7684\u8fde\u7eed\u548c\u79bb\u6563\u53c2\u6570\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u663e\u8457\u63d0\u5347\u4e86\u8bbe\u8ba1\u6548\u679c\uff0c\u4e3a\u81ea\u52a8\u5316\u900f\u955c\u8bbe\u8ba1\u5f00\u8f9f\u4e86\u65b0\u9014\u5f84\u3002"}}
{"id": "2509.23607", "categories": ["cs.GR", "cs.CV"], "pdf": "https://arxiv.org/pdf/2509.23607", "abs": "https://arxiv.org/abs/2509.23607", "authors": ["Xiang Tang", "Ruotong Li", "Xiaopeng Fan"], "title": "ZeroScene: A Zero-Shot Framework for 3D Scene Generation from a Single Image and Controllable Texture Editing", "comment": "16 pages, 15 figures, Project page:\n  https://xdlbw.github.io/ZeroScene/", "summary": "In the field of 3D content generation, single image scene reconstruction\nmethods still struggle to simultaneously ensure the quality of individual\nassets and the coherence of the overall scene in complex environments, while\ntexture editing techniques often fail to maintain both local continuity and\nmulti-view consistency. In this paper, we propose a novel system ZeroScene,\nwhich leverages the prior knowledge of large vision models to accomplish both\nsingle image-to-3D scene reconstruction and texture editing in a zero-shot\nmanner. ZeroScene extracts object-level 2D segmentation and depth information\nfrom input images to infer spatial relationships within the scene. It then\njointly optimizes 3D and 2D projection losses of the point cloud to update\nobject poses for precise scene alignment, ultimately constructing a coherent\nand complete 3D scene that encompasses both foreground and background.\nMoreover, ZeroScene supports texture editing of objects in the scene. By\nimposing constraints on the diffusion model and introducing a mask-guided\nprogressive image generation strategy, we effectively maintain texture\nconsistency across multiple viewpoints and further enhance the realism of\nrendered results through Physically Based Rendering (PBR) material estimation.\nExperimental results demonstrate that our framework not only ensures the\ngeometric and appearance accuracy of generated assets, but also faithfully\nreconstructs scene layouts and produces highly detailed textures that closely\nalign with text prompts.", "AI": {"tldr": "ZeroScene\u662f\u4e00\u79cd\u65b0\u578b\u7cfb\u7edf\uff0c\u5229\u7528\u5927\u578b\u89c6\u89c9\u6a21\u578b\u7684\u5148\u9a8c\u77e5\u8bc6\uff0c\u4ee5\u96f6\u6837\u672c\u65b9\u5f0f\u5b9e\u73b0\u5355\u56fe\u50cf\u52303D\u573a\u666f\u91cd\u5efa\u548c\u7eb9\u7406\u7f16\u8f91\uff0c\u540c\u65f6\u786e\u4fdd\u573a\u666f\u4e00\u81f4\u6027\u548c\u7eb9\u7406\u591a\u89c6\u89d2\u8fde\u7eed\u6027\u3002", "motivation": "\u57283D\u5185\u5bb9\u751f\u6210\u9886\u57df\uff0c\u5355\u56fe\u50cf\u573a\u666f\u91cd\u5efa\u65b9\u6cd5\u96be\u4ee5\u540c\u65f6\u4fdd\u8bc1\u590d\u6742\u73af\u5883\u4e2d\u5355\u4e2a\u8d44\u4ea7\u7684\u8d28\u91cf\u548c\u6574\u4f53\u573a\u666f\u7684\u8fde\u8d2f\u6027\uff0c\u800c\u7eb9\u7406\u7f16\u8f91\u6280\u672f\u5219\u96be\u4ee5\u7ef4\u6301\u5c40\u90e8\u8fde\u7eed\u6027\u548c\u591a\u89c6\u89d2\u4e00\u81f4\u6027\u3002", "method": "ZeroScene\u901a\u8fc7\u63d0\u53d6\u8f93\u5165\u56fe\u50cf\u7684\u7269\u4f53\u7ea72D\u5206\u5272\u548c\u6df1\u5ea6\u4fe1\u606f\u63a8\u65ad\u573a\u666f\u7a7a\u95f4\u5173\u7cfb\uff0c\u8054\u5408\u4f18\u5316\u70b9\u4e91\u76843D\u548c2D\u6295\u5f71\u635f\u5931\u4ee5\u66f4\u65b0\u7269\u4f53\u59ff\u6001\uff0c\u5b9e\u73b0\u7cbe\u786e\u573a\u666f\u5bf9\u9f50\uff0c\u5e76\u901a\u8fc7\u6269\u6563\u6a21\u578b\u7ea6\u675f\u548c\u63a9\u7801\u5f15\u5bfc\u7684\u6e10\u8fdb\u56fe\u50cf\u751f\u6210\u7b56\u7565\u652f\u6301\u7eb9\u7406\u7f16\u8f91\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cZeroScene\u4e0d\u4ec5\u80fd\u786e\u4fdd\u751f\u6210\u8d44\u4ea7\u7684\u51e0\u4f55\u548c\u5916\u89c2\u51c6\u786e\u6027\uff0c\u8fd8\u80fd\u5fe0\u5b9e\u91cd\u5efa\u573a\u666f\u5e03\u5c40\u5e76\u751f\u6210\u4e0e\u6587\u672c\u63d0\u793a\u9ad8\u5ea6\u4e00\u81f4\u7684\u8be6\u7ec6\u7eb9\u7406\u3002", "conclusion": "ZeroScene\u4e3a\u590d\u6742\u73af\u5883\u4e2d\u76843D\u5185\u5bb9\u751f\u6210\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\uff0c\u663e\u8457\u63d0\u5347\u4e86\u573a\u666f\u91cd\u5efa\u548c\u7eb9\u7406\u7f16\u8f91\u7684\u8d28\u91cf\u4e0e\u4e00\u81f4\u6027\u3002"}}
{"id": "2509.23157", "categories": ["cs.GT", "cs.LG", "cs.MA"], "pdf": "https://arxiv.org/pdf/2509.23157", "abs": "https://arxiv.org/abs/2509.23157", "authors": ["Yanqing Fu", "Chao Huang", "Chenrun Wang", "Zhuping Wang"], "title": "Grouped Satisficing Paths in Pure Strategy Games: a Topological Perspective", "comment": null, "summary": "In game theory and multi-agent reinforcement learning (MARL), each agent\nselects a strategy, interacts with the environment and other agents, and\nsubsequently updates its strategy based on the received payoff. This process\ngenerates a sequence of joint strategies $(s^t)_{t \\geq 0}$, where $s^t$\nrepresents the strategy profile of all agents at time step $t$. A widely\nadopted principle in MARL algorithms is \"win-stay, lose-shift\", which dictates\nthat an agent retains its current strategy if it achieves the best response.\nThis principle exhibits a fixed-point property when the joint strategy has\nbecome an equilibrium. The sequence of joint strategies under this principle is\nreferred to as a satisficing path, a concept first introduced in [40] and\nexplored in the context of $N$-player games in [39]. A fundamental question\narises regarding this principle: Under what conditions does every initial joint\nstrategy $s$ admit a finite-length satisficing path $(s^t)_{0 \\leq t \\leq T}$\nwhere $s^0=s$ and $s^T$ is an equilibrium? This paper establishes a sufficient\ncondition for such a property, and demonstrates that any finite-state Markov\ngame, as well as any $N$-player game, guarantees the existence of a\nfinite-length satisficing path from an arbitrary initial strategy to some\nequilibrium. These results provide a stronger theoretical foundation for the\ndesign of MARL algorithms.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7814\u7a76\u4e86\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\uff08MARL\uff09\u4e2d\u7684\u201c\u80dc\u8005\u4fdd\u6301\uff0c\u8d25\u8005\u6539\u53d8\u201d\u539f\u5219\uff0c\u8bc1\u660e\u4e86\u5728\u4efb\u4f55\u6709\u9650\u72b6\u6001\u9a6c\u5c14\u53ef\u592b\u6e38\u620f\u6216N\u73a9\u5bb6\u6e38\u620f\u4e2d\uff0c\u4ece\u4efb\u610f\u521d\u59cb\u7b56\u7565\u5230\u67d0\u4e2a\u5747\u8861\u5b58\u5728\u6709\u9650\u957f\u5ea6\u7684\u6ee1\u8db3\u8def\u5f84\u3002", "motivation": "\u7814\u7a76MARL\u4e2d\u201c\u80dc\u8005\u4fdd\u6301\uff0c\u8d25\u8005\u6539\u53d8\u201d\u539f\u5219\u7684\u9002\u7528\u6761\u4ef6\uff0c\u4e3aMARL\u7b97\u6cd5\u8bbe\u8ba1\u63d0\u4f9b\u66f4\u5f3a\u7684\u7406\u8bba\u57fa\u7840\u3002", "method": "\u901a\u8fc7\u5206\u6790\u6709\u9650\u72b6\u6001\u9a6c\u5c14\u53ef\u592b\u6e38\u620f\u548cN\u73a9\u5bb6\u6e38\u620f\u4e2d\u7684\u7b56\u7565\u5e8f\u5217\uff0c\u5efa\u7acb\u4e86\u4e00\u4e2a\u5145\u5206\u6761\u4ef6\u8bc1\u660e\u6709\u9650\u957f\u5ea6\u6ee1\u8db3\u8def\u5f84\u7684\u5b58\u5728\u6027\u3002", "result": "\u8bc1\u660e\u4e86\u5728\u4efb\u4f55\u6709\u9650\u72b6\u6001\u9a6c\u5c14\u53ef\u592b\u6e38\u620f\u6216N\u73a9\u5bb6\u6e38\u620f\u4e2d\uff0c\u4ece\u4efb\u610f\u521d\u59cb\u7b56\u7565\u5230\u67d0\u4e2a\u5747\u8861\u90fd\u5b58\u5728\u6709\u9650\u957f\u5ea6\u7684\u6ee1\u8db3\u8def\u5f84\u3002", "conclusion": "\u8be5\u7814\u7a76\u7ed3\u679c\u4e3aMARL\u7b97\u6cd5\u7684\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u66f4\u5f3a\u7684\u7406\u8bba\u652f\u6301\uff0c\u6269\u5c55\u4e86\u201c\u80dc\u8005\u4fdd\u6301\uff0c\u8d25\u8005\u6539\u53d8\u201d\u539f\u5219\u7684\u5e94\u7528\u8303\u56f4\u3002"}}
{"id": "2509.22982", "categories": ["cs.PL"], "pdf": "https://arxiv.org/pdf/2509.22982", "abs": "https://arxiv.org/abs/2509.22982", "authors": ["David M Kahn", "Jan Hoffmann", "Thomas Reps", "Jessie Grosen"], "title": "Efficient Cost Bounds with Linear Maps", "comment": null, "summary": "The Automatic Amortized Resource Analysis (AARA) derives program-execution\ncost bounds using types. To do so, AARA often makes use of cost-free types,\nwhich are critical for the composition of types and cost bounds. However,\ninferring cost-free types using the current state-of-the-art algorithm is\nexpensive due to recursive dependence on additional cost-free types.\nFurthermore, that algorithm uses a heuristic only applicable to polynomial cost\nbounds, and not, e.g., exponential bounds. This paper presents a new approach\nto these problems by representing the cost-free types of a function in a new\nway: with a linear map, which can stand for infinitely many cost-free types.\nSuch maps enable an algebraic flavor of reasoning about cost bounds (including\nnon-polynomial bounds) via matrix inequalities. These inequalities can be\nsolved with off-the-shelf linear-programming tools for many programs, so that\ntypes can always be efficiently checked and often be efficiently inferred. An\nexperimental evaluation with a prototype implementation shows that-when it is\napplicable-the inference of linear maps is exponentially more efficient than\nthe state-of-the-art algorithm.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7\u7ebf\u6027\u6620\u5c04\u8868\u793a\u51fd\u6570\u7684\u65e0\u6210\u672c\u7c7b\u578b\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u7b97\u6cd5\u5728\u65e0\u6210\u672c\u7c7b\u578b\u63a8\u65ad\u4e0a\u7684\u6548\u7387\u95ee\u9898\uff0c\u9002\u7528\u4e8e\u591a\u9879\u5f0f\u548c\u975e\u591a\u9879\u5f0f\u6210\u672c\u754c\u9650\u3002", "motivation": "\u73b0\u6709\u81ea\u52a8\u644a\u9500\u8d44\u6e90\u5206\u6790\uff08AARA\uff09\u65b9\u6cd5\u5728\u63a8\u65ad\u65e0\u6210\u672c\u7c7b\u578b\u65f6\u6548\u7387\u4f4e\u4e0b\uff0c\u4e14\u4ec5\u9002\u7528\u4e8e\u591a\u9879\u5f0f\u6210\u672c\u754c\u9650\u3002\u672c\u6587\u65e8\u5728\u901a\u8fc7\u7ebf\u6027\u6620\u5c04\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\u3002", "method": "\u91c7\u7528\u7ebf\u6027\u6620\u5c04\u8868\u793a\u51fd\u6570\u7684\u65e0\u6210\u672c\u7c7b\u578b\uff0c\u901a\u8fc7\u77e9\u9635\u4e0d\u7b49\u5f0f\u63a8\u7406\u6210\u672c\u754c\u9650\uff0c\u5e76\u5229\u7528\u73b0\u6210\u7684\u7ebf\u6027\u89c4\u5212\u5de5\u5177\u89e3\u51b3\u8fd9\u4e9b\u4e0d\u7b49\u5f0f\u3002", "result": "\u5b9e\u9a8c\u8bc4\u4f30\u8868\u660e\uff0c\u7ebf\u6027\u6620\u5c04\u63a8\u65ad\u65b9\u6cd5\u7684\u6548\u7387\u6bd4\u73b0\u6709\u7b97\u6cd5\u9ad8\u51fa\u6307\u6570\u7ea7\uff0c\u9002\u7528\u4e8e\u66f4\u591a\u7a0b\u5e8f\u3002", "conclusion": "\u7ebf\u6027\u6620\u5c04\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86\u65e0\u6210\u672c\u7c7b\u578b\u7684\u63a8\u65ad\u6548\u7387\uff0c\u6269\u5c55\u4e86\u6210\u672c\u754c\u9650\u7684\u5206\u6790\u8303\u56f4\uff0c\u7279\u522b\u662f\u975e\u591a\u9879\u5f0f\u754c\u9650\u3002"}}
{"id": "2509.23703", "categories": ["cs.GR", "cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.23703", "abs": "https://arxiv.org/abs/2509.23703", "authors": ["Zhenyu Shu", "Jian Yao", "Shiqing Xin"], "title": "DFG-PCN: Point Cloud Completion with Degree-Flexible Point Graph", "comment": null, "summary": "Point cloud completion is a vital task focused on reconstructing complete\npoint clouds and addressing the incompleteness caused by occlusion and limited\nsensor resolution. Traditional methods relying on fixed local region\npartitioning, such as k-nearest neighbors, which fail to account for the highly\nuneven distribution of geometric complexity across different regions of a\nshape. This limitation leads to inefficient representation and suboptimal\nreconstruction, especially in areas with fine-grained details or structural\ndiscontinuities. This paper proposes a point cloud completion framework called\nDegree-Flexible Point Graph Completion Network (DFG-PCN). It adaptively assigns\nnode degrees using a detail-aware metric that combines feature variation and\ncurvature, focusing on structurally important regions. We further introduce a\ngeometry-aware graph integration module that uses Manhattan distance for edge\naggregation and detail-guided fusion of local and global features to enhance\nrepresentation. Extensive experiments on multiple benchmark datasets\ndemonstrate that our method consistently outperforms state-of-the-art\napproaches.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aDFG-PCN\u7684\u81ea\u9002\u5e94\u70b9\u4e91\u8865\u5168\u6846\u67b6\uff0c\u901a\u8fc7\u7ed3\u5408\u7279\u5f81\u53d8\u5316\u548c\u66f2\u7387\u7684\u7ec6\u8282\u611f\u77e5\u5ea6\u91cf\uff0c\u7075\u6d3b\u5206\u914d\u8282\u70b9\u5ea6\u6570\uff0c\u5e76\u7ed3\u5408\u51e0\u4f55\u611f\u77e5\u56fe\u96c6\u6210\u6a21\u5757\uff0c\u663e\u8457\u63d0\u5347\u4e86\u70b9\u4e91\u8865\u5168\u6548\u679c\u3002", "motivation": "\u70b9\u4e91\u8865\u5168\u4efb\u52a1\u7531\u4e8e\u906e\u6321\u548c\u4f20\u611f\u5668\u5206\u8fa8\u7387\u9650\u5236\u5bfc\u81f4\u7684\u4e0d\u5b8c\u6574\u6027\uff0c\u4f20\u7edf\u65b9\u6cd5\u4f9d\u8d56\u4e8e\u56fa\u5b9a\u7684\u5c40\u90e8\u533a\u57df\u5212\u5206\uff08\u5982k\u8fd1\u90bb\uff09\uff0c\u65e0\u6cd5\u9002\u5e94\u5f62\u72b6\u4e0d\u540c\u533a\u57df\u7684\u51e0\u4f55\u590d\u6742\u5ea6\u4e0d\u5747\u5206\u5e03\uff0c\u5bfc\u81f4\u8868\u793a\u6548\u7387\u4f4e\u4e0b\u548c\u91cd\u6784\u6548\u679c\u4e0d\u4f73\uff0c\u7279\u522b\u662f\u5728\u7ec6\u7c92\u5ea6\u7ec6\u8282\u6216\u7ed3\u6784\u4e0d\u8fde\u7eed\u7684\u533a\u57df\u3002", "method": "\u63d0\u51fa\u4e86DFG-PCN\u6846\u67b6\uff0c\u91c7\u7528\u7ed3\u5408\u7279\u5f81\u53d8\u5316\u548c\u66f2\u7387\u7684\u7ec6\u8282\u611f\u77e5\u5ea6\u91cf\u81ea\u9002\u5e94\u5206\u914d\u8282\u70b9\u5ea6\u6570\uff0c\u5e76\u5f15\u5165\u51e0\u4f55\u611f\u77e5\u56fe\u96c6\u6210\u6a21\u5757\uff0c\u5229\u7528\u66fc\u54c8\u987f\u8ddd\u79bb\u8fdb\u884c\u8fb9\u805a\u5408\u548c\u7ec6\u8282\u5f15\u5bfc\u7684\u5c40\u90e8\u4e0e\u5168\u5c40\u7279\u5f81\u878d\u5408\u3002", "result": "\u5728\u591a\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0cDFG-PCN\u65b9\u6cd5\u5728\u70b9\u4e91\u8865\u5168\u4efb\u52a1\u4e2d\u6301\u7eed\u4f18\u4e8e\u73b0\u6709\u6700\u5148\u8fdb\u65b9\u6cd5\u3002", "conclusion": "DFG-PCN\u6846\u67b6\u901a\u8fc7\u81ea\u9002\u5e94\u8282\u70b9\u5206\u914d\u548c\u51e0\u4f55\u611f\u77e5\u7279\u5f81\u878d\u5408\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u70b9\u4e91\u8865\u5168\u4e2d\u7684\u51e0\u4f55\u5206\u5e03\u4e0d\u5747\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u8865\u5168\u6548\u679c\u3002"}}
{"id": "2509.23747", "categories": ["cs.GT", "cs.CL"], "pdf": "https://arxiv.org/pdf/2509.23747", "abs": "https://arxiv.org/abs/2509.23747", "authors": ["SeungHyun Yi", "Seungjun Yi"], "title": "Beyond Game Theory Optimal: Profit-Maximizing Poker Agents for No-Limit Holdem", "comment": null, "summary": "Game theory has grown into a major field over the past few decades, and poker\nhas long served as one of its key case studies. Game-Theory-Optimal (GTO)\nprovides strategies to avoid loss in poker, but pure GTO does not guarantee\nmaximum profit. To this end, we aim to develop a model that outperforms GTO\nstrategies to maximize profit in No Limit Holdem, in heads-up (two-player) and\nmulti-way (more than two-player) situations. Our model finds the GTO foundation\nand goes further to exploit opponents. The model first navigates toward many\nsimulated poker hands against itself and keeps adjusting its decisions until no\naction can reliably beat it, creating a strong baseline that is close to the\ntheoretical best strategy. Then, it adapts by observing opponent behavior and\nadjusting its strategy to capture extra value accordingly. Our results indicate\nthat Monte-Carlo Counterfactual Regret Minimization (CFR) performs best in\nheads-up situations and CFR remains the strongest method in most multi-way\nsituations. By combining the defensive strength of GTO with real-time\nexploitation, our approach aims to show how poker agents can move from merely\nnot losing to consistently winning against diverse opponents.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u8d85\u8d8a\u4f20\u7edf\u6e38\u620f\u7406\u8bba\u6700\u4f18\u7b56\u7565\u7684\u6a21\u578b\uff0c\u65e8\u5728\u6700\u5927\u5316\u5fb7\u5dde\u6251\u514b\u7684\u5229\u6da6\uff0c\u5c24\u5176\u662f\u5728\u5355\u6311\u548c\u591a\u73a9\u5bb6\u60c5\u5883\u4e0b\uff0c\u901a\u8fc7\u7ed3\u5408GTO\u57fa\u7840\u7b56\u7565\u548c\u5b9e\u65f6\u5bf9\u624b\u884c\u4e3a\u5206\u6790\u5b9e\u73b0\u66f4\u9ad8\u7684\u80dc\u7387\u3002", "motivation": "\u4f20\u7edf\u7684\u6e38\u620f\u7406\u8bba\u6700\u4f18\u7b56\u7565\uff08GTO\uff09\u867d\u7136\u80fd\u591f\u907f\u514d\u635f\u5931\uff0c\u4f46\u65e0\u6cd5\u4fdd\u8bc1\u6700\u5927\u5229\u6da6\u3002\u56e0\u6b64\uff0c\u4f5c\u8005\u7684\u76ee\u6807\u662f\u5f00\u53d1\u4e00\u79cd\u80fd\u591f\u5728\u5355\u6311\u548c\u591a\u73a9\u5bb6\u60c5\u5883\u4e0b\u8d85\u8d8aGTO\u7b56\u7565\u7684\u6a21\u578b\uff0c\u4ee5\u6700\u5927\u5316\u5229\u6da6\u3002", "method": "\u6a21\u578b\u9996\u5148\u901a\u8fc7\u6a21\u62df\u5927\u91cf\u6251\u514b\u624b\u724c\u5bf9\u9635\u81ea\u8eab\uff0c\u8c03\u6574\u51b3\u7b56\u76f4\u81f3\u65e0\u6cd5\u88ab\u51fb\u8d25\uff0c\u5f62\u6210\u63a5\u8fd1\u7406\u8bba\u6700\u4f73\u7b56\u7565\u7684\u57fa\u7ebf\u3002\u968f\u540e\uff0c\u6a21\u578b\u901a\u8fc7\u89c2\u5bdf\u5bf9\u624b\u884c\u4e3a\u52a8\u6001\u8c03\u6574\u7b56\u7565\u4ee5\u83b7\u53d6\u989d\u5916\u4ef7\u503c\u3002", "result": "\u7ed3\u679c\u8868\u660e\uff0c\u8499\u7279\u5361\u6d1b\u53cd\u4e8b\u5b9e\u9057\u61be\u6700\u5c0f\u5316\uff08CFR\uff09\u5728\u5355\u6311\u60c5\u5883\u4e0b\u8868\u73b0\u6700\u4f73\uff0c\u4e14\u5728\u591a\u6570\u591a\u73a9\u5bb6\u60c5\u5883\u4e2d\u4ecd\u662f\u6700\u5f3a\u65b9\u6cd5\u3002", "conclusion": "\u901a\u8fc7\u5c06GTO\u7684\u9632\u5fa1\u6027\u4e0e\u5b9e\u65f6\u5bf9\u624b\u884c\u4e3a\u5206\u6790\u76f8\u7ed3\u5408\uff0c\u8be5\u6a21\u578b\u6210\u529f\u5b9e\u73b0\u4e86\u4ece\u907f\u514d\u635f\u5931\u5230\u6301\u7eed\u51fb\u8d25\u591a\u6837\u5316\u5bf9\u624b\u7684\u8f6c\u53d8\u3002"}}
{"id": "2509.23061", "categories": ["cs.PL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.23061", "abs": "https://arxiv.org/abs/2509.23061", "authors": ["Xu Xu", "Xin Li", "Xingwei Qu", "Jie Fu", "Binhang Yuan"], "title": "Local Success Does Not Compose: Benchmarking Large Language Models for Compositional Formal Verification", "comment": null, "summary": "We introduce DafnyCOMP, a benchmark for evaluating large language models\n(LLMs) on compositional specification generation in Dafny. Unlike prior\nbenchmarks that focus on single-function tasks, DafnyCOMP targets programs\ncomposed of multiple interacting functions with data dependencies, requiring\nreasoning across component boundaries. The benchmark consists of 300\nautomatically synthesized multi-function programs. We evaluate several\nstate-of-the-art LLM families and find that, while they perform well on\nsingle-function verification, their performance drops sharply on compositional\ntasks. Analysis reveals systematic failures in cross-functional reasoning,\nincluding fragile specifications, misalignment between implementations and\nproofs, and unstable reasoning. DafnyCOMP thus provides a diagnostic tool for\nmeasuring progress toward reliable, verifiable, and compositional code\ngeneration with LLMs.", "AI": {"tldr": "DafnyCOMP\u662f\u4e00\u4e2a\u8bc4\u4f30\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728Dafny\u4e2d\u7ec4\u5408\u89c4\u8303\u751f\u6210\u80fd\u529b\u7684\u57fa\u51c6\u6d4b\u8bd5\uff0c\u4e13\u6ce8\u4e8e\u591a\u51fd\u6570\u4ea4\u4e92\u4efb\u52a1\u5e76\u63ed\u793a\u4e86\u6a21\u578b\u5728\u8de8\u529f\u80fd\u63a8\u7406\u4e2d\u7684\u7cfb\u7edf\u6027\u7f3a\u9677\u3002", "motivation": "\u73b0\u6709\u57fa\u51c6\u6d4b\u8bd5\u4e3b\u8981\u5173\u6ce8\u5355\u51fd\u6570\u4efb\u52a1\uff0c\u7f3a\u4e4f\u5bf9\u591a\u51fd\u6570\u4ea4\u4e92\u548c\u6570\u636e\u4f9d\u8d56\u7684\u8bc4\u4f30\uff0cDafnyCOMP\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\u3002", "method": "DafnyCOMP\u5305\u542b300\u4e2a\u81ea\u52a8\u5408\u6210\u7684\u591a\u51fd\u6570\u7a0b\u5e8f\uff0c\u7528\u4e8e\u8bc4\u4f30\u591a\u4e2a\u5148\u8fdb\u7684\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5bb6\u65cf\u3002", "result": "\u6a21\u578b\u5728\u5355\u51fd\u6570\u9a8c\u8bc1\u4e2d\u8868\u73b0\u826f\u597d\uff0c\u4f46\u5728\u7ec4\u5408\u4efb\u52a1\u4e2d\u6027\u80fd\u663e\u8457\u4e0b\u964d\uff0c\u8868\u73b0\u51fa\u8106\u5f31\u7684\u89c4\u8303\u3001\u5b9e\u73b0\u4e0e\u8bc1\u660e\u4e4b\u95f4\u7684\u4e0d\u4e00\u81f4\u4ee5\u53ca\u4e0d\u7a33\u5b9a\u7684\u63a8\u7406\u3002", "conclusion": "DafnyCOMP\u4e3a\u8861\u91cf\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u53ef\u9760\u3001\u53ef\u9a8c\u8bc1\u548c\u7ec4\u5408\u4ee3\u7801\u751f\u6210\u65b9\u9762\u7684\u8fdb\u5c55\u63d0\u4f9b\u4e86\u8bca\u65ad\u5de5\u5177\u3002"}}
{"id": "2509.23709", "categories": ["cs.GR", "cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.23709", "abs": "https://arxiv.org/abs/2509.23709", "authors": ["Zhenyu Shu", "Jiajun Shen", "Zhongui Chen", "Xiaoguang Han", "Shiqing Xin"], "title": "StrucADT: Generating Structure-controlled 3D Point Clouds with Adjacency Diffusion Transformer", "comment": null, "summary": "In the field of 3D point cloud generation, numerous 3D generative models have\ndemonstrated the ability to generate diverse and realistic 3D shapes. However,\nthe majority of these approaches struggle to generate controllable 3D point\ncloud shapes that meet user-specific requirements, hindering the large-scale\napplication of 3D point cloud generation. To address the challenge of lacking\ncontrol in 3D point cloud generation, we are the first to propose controlling\nthe generation of point clouds by shape structures that comprise part\nexistences and part adjacency relationships. We manually annotate the adjacency\nrelationships between the segmented parts of point cloud shapes, thereby\nconstructing a StructureGraph representation. Based on this StructureGraph\nrepresentation, we introduce StrucADT, a novel structure-controllable point\ncloud generation model, which consists of StructureGraphNet module to extract\nstructure-aware latent features, cCNF Prior module to learn the distribution of\nthe latent features controlled by the part adjacency, and Diffusion Transformer\nmodule conditioned on the latent features and part adjacency to generate\nstructure-consistent point cloud shapes. Experimental results demonstrate that\nour structure-controllable 3D point cloud generation method produces\nhigh-quality and diverse point cloud shapes, enabling the generation of\ncontrollable point clouds based on user-specified shape structures and\nachieving state-of-the-art performance in controllable point cloud generation\non the ShapeNet dataset.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u7ed3\u6784\u63a7\u5236\u7684\u70b9\u4e91\u751f\u6210\u65b9\u6cd5StrucADT\uff0c\u901a\u8fc7\u624b\u52a8\u6807\u6ce8\u70b9\u4e91\u5f62\u72b6\u7684\u76f8\u90bb\u5173\u7cfb\u6784\u5efaStructureGraph\u8868\u793a\uff0c\u5b9e\u73b0\u4e86\u7528\u6237\u6307\u5b9a\u7ed3\u6784\u7684\u53ef\u63a7\u751f\u6210\u3002", "motivation": "\u73b0\u67093D\u70b9\u4e91\u751f\u6210\u65b9\u6cd5\u867d\u80fd\u751f\u6210\u591a\u6837\u4e14\u771f\u5b9e\u7684\u5f62\u72b6\uff0c\u4f46\u7f3a\u4e4f\u5bf9\u751f\u6210\u7684\u7cbe\u786e\u63a7\u5236\uff0c\u9650\u5236\u4e86\u5176\u5927\u89c4\u6a21\u5e94\u7528\u3002", "method": "\u901a\u8fc7\u624b\u52a8\u6807\u6ce8\u70b9\u4e91\u90e8\u5206\u7684\u76f8\u90bb\u5173\u7cfb\u6784\u5efaStructureGraph\u8868\u793a\uff0c\u8bbe\u8ba1StrucADT\u6a21\u578b\uff0c\u5305\u62ecStructureGraphNet\u3001cCNF Prior\u548cDiffusion Transformer\u6a21\u5757\uff0c\u5b9e\u73b0\u7ed3\u6784\u4e00\u81f4\u7684\u70b9\u4e91\u751f\u6210\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u80fd\u5728ShapeNet\u6570\u636e\u96c6\u4e0a\u751f\u6210\u9ad8\u8d28\u91cf\u3001\u591a\u6837\u4e14\u7ed3\u6784\u53ef\u63a7\u7684\u70b9\u4e91\uff0c\u6027\u80fd\u8fbe\u5230\u5f53\u524d\u6700\u4f18\u3002", "conclusion": "\u63d0\u51fa\u7684\u7ed3\u6784\u53ef\u63a7\u70b9\u4e91\u751f\u6210\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86\u751f\u6210\u63a7\u5236\u4e0d\u8db3\u7684\u95ee\u9898\uff0c\u4e3a\u5927\u89c4\u6a21\u5e94\u7528\u63d0\u4f9b\u4e86\u53ef\u80fd\u6027\u3002"}}
{"id": "2509.24398", "categories": ["cs.GT"], "pdf": "https://arxiv.org/pdf/2509.24398", "abs": "https://arxiv.org/abs/2509.24398", "authors": ["Feipeng Zhang", "Te Wu", "Guofeng Zhang", "Long Wang"], "title": "Evolutionary hypergame dynamics: Introspection reasoning and social learning", "comment": null, "summary": "In the realm of evolutionary game theory, standard frameworks typically\npresuppose that every player possesses comprehensive knowledge and unrestricted\naccess to the entire strategy space. However, real-world human society\ninherently harbors diverse levels of knowledge, experience, and background\namong individuals. Hypergames incorporate this heterogeneity by permitting\nindividuals to differ in their access to the full strategy set, reflecting\ncognitive or informational constraints and giving rise to asymmetric strategic\ninteractions. Yet, their evolutionary consequences remain underexplored. Our\ninquiry employs prototype models featuring three available strategies, focusing\non social dilemmas involving cooperation, defection, and loner. These\nstrategies manifest cyclic dominance, akin to the well-studied\nrock-paper-scissors dynamics, a foundational model in game theory. Our study\nspans both well-mixed and spatial lattice populations, delving into the\nintricacies of learning and evolution of the strategy set within the\nevolutionary hypergame dynamics. In stark contrast to traditional evolutionary\ngame dynamics, our findings unveil nuanced and intricate phases, encompassing\nscenarios of loner dominance, coexistence of multiple strategy sets,\ncombinations of cooperation and loner dominance, and more. Remarkably, we\ndiscern that heightened rationality significantly promotes cooperative\nbehaviors.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u8fdb\u5316\u8d85\u535a\u5f08\u4e2d\u7684\u7b56\u7565\u591a\u6837\u6027\u53ca\u5176\u5f71\u54cd\uff0c\u63ed\u793a\u4e86\u4e0e\u4f20\u7edf\u8fdb\u5316\u535a\u5f08\u4e0d\u540c\u7684\u590d\u6742\u76f8\u53d8\u73b0\u8c61\uff0c\u5e76\u53d1\u73b0\u7406\u6027\u63d0\u5347\u663e\u8457\u4fc3\u8fdb\u5408\u4f5c\u884c\u4e3a\u3002", "motivation": "\u73b0\u5b9e\u793e\u4f1a\u4e2d\u4e2a\u4f53\u7684\u77e5\u8bc6\u548c\u4fe1\u606f\u83b7\u53d6\u5b58\u5728\u5f02\u8d28\u6027\uff0c\u800c\u4f20\u7edf\u8fdb\u5316\u535a\u5f08\u7406\u8bba\u5047\u8bbe\u6240\u6709\u73a9\u5bb6\u62e5\u6709\u5b8c\u6574\u7b56\u7565\u77e5\u8bc6\uff0c\u56e0\u6b64\u9700\u8981\u7814\u7a76\u8fd9\u79cd\u4e0d\u5bf9\u79f0\u6027\u5bf9\u8fdb\u5316\u52a8\u6001\u7684\u5f71\u54cd\u3002", "method": "\u7814\u7a76\u91c7\u7528\u539f\u578b\u6a21\u578b\uff0c\u5305\u542b\u5408\u4f5c\u3001\u80cc\u53db\u548c\u72ec\u884c\u4e09\u79cd\u7b56\u7565\uff0c\u6a21\u62df\u5176\u5728\u6df7\u5408\u548c\u7a7a\u95f4\u7f51\u683c\u4eba\u53e3\u4e2d\u7684\u8d85\u535a\u5f08\u52a8\u6001\u3002", "result": "\u7814\u7a76\u53d1\u73b0\u590d\u6742\u7684\u76f8\u53d8\u73b0\u8c61\uff0c\u5305\u62ec\u72ec\u884c\u8005\u4e3b\u5bfc\u3001\u591a\u7b56\u7565\u5171\u5b58\u53ca\u5408\u4f5c\u4e0e\u72ec\u884c\u7ec4\u5408\u4e3b\u5bfc\u7b49\uff0c\u4e14\u7406\u6027\u63d0\u9ad8\u663e\u8457\u589e\u5f3a\u4e86\u5408\u4f5c\u884c\u4e3a\u3002", "conclusion": "\u8fdb\u5316\u8d85\u535a\u5f08\u63ed\u793a\u4e86\u6bd4\u4f20\u7edf\u535a\u5f08\u66f4\u4e3a\u590d\u6742\u7684\u52a8\u6001\u73b0\u8c61\uff0c\u7406\u6027\u5bf9\u5408\u4f5c\u884c\u4e3a\u7684\u4fc3\u8fdb\u4f5c\u7528\u5c24\u4e3a\u91cd\u8981\u3002"}}
{"id": "2509.23229", "categories": ["cs.PL", "D.2.4; F.3.1"], "pdf": "https://arxiv.org/pdf/2509.23229", "abs": "https://arxiv.org/abs/2509.23229", "authors": ["Yawen Guan", "Cl\u00e9ment Pit-Claudel"], "title": "Fine-Grained Reasoning About Container-Internal Pointers with Logical Pinning", "comment": null, "summary": "Most separation logics hide container-internal pointers for modularity. This\nmakes it difficult to specify container APIs that temporarily expose those\npointers to the outside, and to verify programs that use these APIs. We present\nlogical pinning, a lightweight borrowing model for sequential programs that\nallows users to selectively track container-internal pointers at the logical\nlevel. Our model generalizes the magic-wand operator, making it easy to write\nand prove precise specifications, including pointer-stability properties.\nBecause it only changes how representation predicates and specifications are\nwritten, our approach is compatible with most separation logic variants. We\ndemonstrate the practicality of logical pinning by verifying small but\nrepresentative pointer-manipulating programs, and deriving more precise\nversions of common container specifications. In doing so, we show that our\napproach subsumes some well-known proof patterns, simplifies some complex\nproofs, and enables reasoning about program patterns not supported by\ntraditional specifications. All of our results are mechanized in the Rocq proof\nassistant, using the CFML library.", "AI": {"tldr": "\u903b\u8f91\u9489\u624e\u662f\u4e00\u79cd\u8f7b\u91cf\u7ea7\u501f\u7528\u6a21\u578b\uff0c\u5141\u8bb8\u9009\u62e9\u6027\u8ddf\u8e2a\u5bb9\u5668\u5185\u90e8\u6307\u9488\uff0c\u9002\u7528\u4e8e\u987a\u5e8f\u7a0b\u5e8f\uff0c\u63d0\u5347\u4e86\u5206\u79bb\u903b\u8f91\u7684\u6a21\u5757\u5316\u548c\u9a8c\u8bc1\u80fd\u529b\u3002", "motivation": "\u4f20\u7edf\u5206\u79bb\u903b\u8f91\u9690\u85cf\u5bb9\u5668\u5185\u90e8\u6307\u9488\u4ee5\u4fdd\u6301\u6a21\u5757\u5316\uff0c\u4f46\u8fd9\u4f7f\u5f97\u5728\u5bb9\u5668API\u4e34\u65f6\u66b4\u9732\u6307\u9488\u65f6\u96be\u4ee5\u8fdb\u884c\u89c4\u8303\u548c\u9a8c\u8bc1\u3002", "method": "\u63d0\u51fa\u903b\u8f91\u9489\u624e\u6a21\u578b\uff0c\u901a\u8fc7\u6982\u62ec\u9b54\u6756\u7b97\u5b50\uff08magic-wand operator\uff09\uff0c\u5141\u8bb8\u5728\u903b\u8f91\u5c42\u9762\u9009\u62e9\u6027\u8ddf\u8e2a\u5185\u90e8\u6307\u9488\uff0c\u4fdd\u6301\u4e0e\u73b0\u6709\u5206\u79bb\u903b\u8f91\u7684\u517c\u5bb9\u6027\u3002", "result": "\u9a8c\u8bc1\u4e86\u5c0f\u800c\u5177\u6709\u4ee3\u8868\u6027\u7684\u6307\u9488\u64cd\u4f5c\u7a0b\u5e8f\uff0c\u5e76\u751f\u6210\u4e86\u66f4\u7cbe\u786e\u7684\u5bb9\u5668\u89c4\u8303\uff0c\u7b80\u5316\u4e86\u67d0\u4e9b\u590d\u6742\u8bc1\u660e\uff0c\u652f\u6301\u4f20\u7edf\u89c4\u8303\u65e0\u6cd5\u5904\u7406\u7684\u7a0b\u5e8f\u6a21\u5f0f\u3002", "conclusion": "\u903b\u8f91\u9489\u624e\u4e0d\u4ec5\u6db5\u76d6\u4e86\u5df2\u77e5\u7684\u8bc1\u660e\u6a21\u5f0f\uff0c\u8fd8\u63d0\u5347\u4e86\u5206\u79bb\u903b\u8f91\u7684\u5b9e\u7528\u6027\u548c\u8868\u8fbe\u80fd\u529b\uff0c\u6240\u6709\u7ed3\u679c\u5747\u5728Rocq\u8bc1\u660e\u52a9\u624b\u4e2d\u901a\u8fc7CFML\u5e93\u5b9e\u73b0\u3002"}}
{"id": "2509.23718", "categories": ["cs.GR", "cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.23718", "abs": "https://arxiv.org/abs/2509.23718", "authors": ["Zhenyu Shu", "Jiawei Wen", "Shiyang Li", "Shiqing Xin", "Ligang Liu"], "title": "Diff-3DCap: Shape Captioning with Diffusion Models", "comment": null, "summary": "The task of 3D shape captioning occupies a significant place within the\ndomain of computer graphics and has garnered considerable interest in recent\nyears. Traditional approaches to this challenge frequently depend on the\nutilization of costly voxel representations or object detection techniques, yet\noften fail to deliver satisfactory outcomes. To address the above challenges,\nin this paper, we introduce Diff-3DCap, which employs a sequence of projected\nviews to represent a 3D object and a continuous diffusion model to facilitate\nthe captioning process. More precisely, our approach utilizes the continuous\ndiffusion model to perturb the embedded captions during the forward phase by\nintroducing Gaussian noise and then predicts the reconstructed annotation\nduring the reverse phase. Embedded within the diffusion framework is a\ncommitment to leveraging a visual embedding obtained from a pre-trained\nvisual-language model, which naturally allows the embedding to serve as a\nguiding signal, eliminating the need for an additional classifier. Extensive\nresults of our experiments indicate that Diff-3DCap can achieve performance\ncomparable to that of the current state-of-the-art methods.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faDiff-3DCap\uff0c\u4e00\u79cd\u5229\u7528\u6295\u5f71\u89c6\u56fe\u548c\u8fde\u7eed\u6269\u6563\u6a21\u578b\u8fdb\u884c3D\u5f62\u72b6\u63cf\u8ff0\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u9ad8\u65af\u566a\u58f0\u6270\u52a8\u548c\u91cd\u5efa\u6807\u6ce8\u63d0\u5347\u6027\u80fd\u3002", "motivation": "\u4f20\u7edf3D\u5f62\u72b6\u63cf\u8ff0\u65b9\u6cd5\u4f9d\u8d56\u4e8e\u6602\u8d35\u7684\u4f53\u7d20\u8868\u793a\u6216\u76ee\u6807\u68c0\u6d4b\u6280\u672f\uff0c\u4f46\u6548\u679c\u4e0d\u4f73\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u66f4\u9ad8\u6548\u4e14\u6027\u80fd\u4f18\u8d8a\u7684\u66ff\u4ee3\u65b9\u6848\u3002", "method": "Diff-3DCap\u4f7f\u7528\u6295\u5f71\u89c6\u56fe\u8868\u793a3D\u5bf9\u8c61\uff0c\u5e76\u91c7\u7528\u8fde\u7eed\u6269\u6563\u6a21\u578b\u5728\u524d\u5411\u9636\u6bb5\u5f15\u5165\u9ad8\u65af\u566a\u58f0\u6270\u52a8\u5d4c\u5165\u6807\u6ce8\uff0c\u53cd\u5411\u9636\u6bb5\u9884\u6d4b\u91cd\u5efa\u6807\u6ce8\uff0c\u540c\u65f6\u5229\u7528\u9884\u8bad\u7ec3\u7684\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u89c6\u89c9\u5d4c\u5165\u4f5c\u4e3a\u6307\u5bfc\u4fe1\u53f7\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cDiff-3DCap\u7684\u6027\u80fd\u53ef\u4e0e\u5f53\u524d\u6700\u5148\u8fdb\u65b9\u6cd5\u76f8\u5ab2\u7f8e\u3002", "conclusion": "Diff-3DCap\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u4e14\u6709\u6548\u76843D\u5f62\u72b6\u63cf\u8ff0\u65b9\u6cd5\uff0c\u901a\u8fc7\u6269\u6563\u6a21\u578b\u548c\u89c6\u89c9\u5d4c\u5165\u7684\u7ed3\u5408\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6027\u80fd\u3002"}}
{"id": "2509.24720", "categories": ["cs.GT"], "pdf": "https://arxiv.org/pdf/2509.24720", "abs": "https://arxiv.org/abs/2509.24720", "authors": ["Suyeon Choi", "Changhyun Kwon", "Seungki Min"], "title": "Dynamic Pricing of an Expiring Item under Strategic Buyers with Stochastic Arrival", "comment": null, "summary": "We study the optimal dynamic pricing of an expiring ticket or voucher, sold\nby a time-sensitive seller to strategic buyers who arrive stochastically with\nprivate values. The expiring nature creates a conflict: the seller's urgency to\nsell before expiration drives price reductions, which in turn incentivize\nbuyers to wait. We seek the seller's optimal pricing policy that resolves this\ntension. The main analytical challenge is that buyer type is two-dimensional\n(valuation and arrival time), which makes equilibrium intractable under general\nstrategies. To address this, we introduce the Value-Based Threshold (VBT)\nstrategy, a tractable framework that decouples these two dimensions. Using this\nframework, we prove equilibrium existence via an ordinary differential equation\nand provide a constructive procedure for its characterization. We then derive\nnear-optimal pricing policies for two stylized regimes: a constant price in\nthin markets and a linear discount in thick markets. Numerical frontier\nanalysis confirms these benchmarks and shows how optimal policy adapts as the\nseller's time sensitivity changes. Our findings clarify the conflict between\nquick sales and strategic waiting. Sellers facing thick markets or high time\nsensitivity benefit from linear discounts, while in thin markets a constant\nprice neutralizes buyers' incentive to wait. We also show this simple policy\nremains robust across broad conditions. For patient sellers, a quasi-auction\nschedule that maintains a high price until a sharp final drop is most effective\nin aggregating demand.", "AI": {"tldr": "\u7814\u7a76\u8fc7\u671f\u7968\u5238\u7684\u52a8\u6001\u5b9a\u4ef7\u95ee\u9898\uff0c\u89e3\u51b3\u5356\u5bb6\u7d27\u8feb\u6027\u4e0e\u4e70\u5bb6\u7b49\u5f85\u6fc0\u52b1\u7684\u51b2\u7a81\uff0c\u63d0\u51fa\u57fa\u4e8e\u4ef7\u503c\u7684\u9608\u503c\u7b56\u7565\uff08VBT\uff09\uff0c\u5e76\u9a8c\u8bc1\u7ebf\u6027\u6298\u6263\u548c\u56fa\u5b9a\u5b9a\u4ef7\u5728\u4e0d\u540c\u5e02\u573a\u6761\u4ef6\u4e0b\u7684\u9002\u7528\u6027\u3002", "motivation": "\u8fc7\u671f\u7968\u5238\u7684\u52a8\u6001\u5b9a\u4ef7\u9762\u4e34\u5356\u5bb6\u7d27\u8feb\u6027\u4e0e\u4e70\u5bb6\u7b49\u5f85\u6fc0\u52b1\u7684\u51b2\u7a81\uff0c\u73b0\u6709\u65b9\u6cd5\u96be\u4ee5\u5904\u7406\u4e70\u5bb6\u7c7b\u578b\u7684\u4e8c\u7ef4\u6027\uff08\u4f30\u503c\u548c\u5230\u8fbe\u65f6\u95f4\uff09\u3002", "method": "\u5f15\u5165\u57fa\u4e8e\u4ef7\u503c\u7684\u9608\u503c\u7b56\u7565\uff08VBT\uff09\uff0c\u5c06\u4e70\u5bb6\u7c7b\u578b\u7684\u4e8c\u7ef4\u6027\u89e3\u8026\uff0c\u5e76\u901a\u8fc7\u5e38\u5fae\u5206\u65b9\u7a0b\u8bc1\u660e\u5747\u8861\u5b58\u5728\u6027\uff0c\u63d0\u51fa\u6784\u9020\u6027\u8868\u5f81\u65b9\u6cd5\u3002", "result": "\u5728\u8584\u5e02\u573a\u4e2d\u56fa\u5b9a\u5b9a\u4ef7\u6709\u6548\uff0c\u539a\u5e02\u573a\u4e2d\u7ebf\u6027\u6298\u6263\u66f4\u4f18\uff1b\u6570\u503c\u5206\u6790\u9a8c\u8bc1\u4e86\u8fd9\u4e9b\u57fa\u51c6\uff0c\u5e76\u5c55\u793a\u4e86\u5b9a\u4ef7\u7b56\u7565\u5982\u4f55\u968f\u5356\u5bb6\u65f6\u95f4\u654f\u611f\u6027\u53d8\u5316\u3002", "conclusion": "\u7ebf\u6027\u6298\u6263\u9002\u7528\u4e8e\u539a\u5e02\u573a\u6216\u9ad8\u65f6\u95f4\u654f\u611f\u6027\u5356\u5bb6\uff0c\u56fa\u5b9a\u5b9a\u4ef7\u9002\u7528\u4e8e\u8584\u5e02\u573a\uff1b\u8010\u5fc3\u5356\u5bb6\u53ef\u91c7\u7528\u51c6\u62cd\u5356\u7b56\u7565\u4ee5\u9ad8\u6548\u805a\u96c6\u9700\u6c42\u3002"}}
{"id": "2509.25114", "categories": ["cs.PL", "cs.SC", "math.AG"], "pdf": "https://arxiv.org/pdf/2509.25114", "abs": "https://arxiv.org/abs/2509.25114", "authors": ["Erdenebayar Bayarmagnai", "Fatemeh Mohammadi", "R\u00e9mi Pr\u00e9bet"], "title": "From Affine to Polynomial: Synthesizing Loops with Branches via Algebraic Geometry", "comment": null, "summary": "Ensuring software correctness remains a fundamental challenge in formal\nprogram verification. One promising approach relies on finding polynomial\ninvariants for loops. Polynomial invariants are properties of a program loop\nthat hold before and after each iteration. Generating such invariants is a\ncrucial task in loop analysis, but it is undecidable in the general case.\nRecently, an alternative approach to this problem has emerged, focusing on\nsynthesizing loops from invariants. However, existing methods only synthesize\naffine loops without guard conditions from polynomial invariants. In this\npaper, we address a more general problem, allowing loops to have polynomial\nupdate maps with a given structure, inequations in the guard condition, and\npolynomial invariants of arbitrary form.\n  We use algebraic geometry tools to design and implement an algorithm that\ncomputes a finite set of polynomial equations whose solutions correspond to all\nnondeterministic branching loops satisfying the given invariants. Furthermore,\nwe introduce a new class of invariants for which we present a significantly\nmore efficient algorithm. In other words, we reduce the problem of synthesizing\nloops to find solutions of multivariate polynomial systems with rational\nentries. This final step is handled in our software using an SMT solver.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u66f4\u901a\u7528\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u4ee3\u6570\u51e0\u4f55\u5de5\u5177\u8bbe\u8ba1\u548c\u5b9e\u73b0\u7b97\u6cd5\uff0c\u4ece\u7ed9\u5b9a\u7684\u591a\u9879\u5f0f\u4e0d\u53d8\u91cf\u5408\u6210\u5177\u6709\u591a\u9879\u5f0f\u66f4\u65b0\u6620\u5c04\u548c\u4e0d\u7b49\u5f0f\u7684\u5faa\u73af\u3002", "motivation": "\u8f6f\u4ef6\u6b63\u786e\u6027\u9a8c\u8bc1\u662f\u5f62\u5f0f\u7a0b\u5e8f\u9a8c\u8bc1\u4e2d\u7684\u6838\u5fc3\u6311\u6218\uff0c\u800c\u591a\u9879\u5f0f\u4e0d\u53d8\u91cf\u662f\u5faa\u73af\u5206\u6790\u7684\u5173\u952e\u3002\u73b0\u6709\u65b9\u6cd5\u4ec5\u80fd\u4ece\u591a\u9879\u5f0f\u4e0d\u53d8\u91cf\u5408\u6210\u65e0\u4fdd\u62a4\u6761\u4ef6\u7684\u4eff\u5c04\u5faa\u73af\uff0c\u672c\u6587\u65e8\u5728\u89e3\u51b3\u66f4\u4e00\u822c\u7684\u5faa\u73af\u5408\u6210\u95ee\u9898\u3002", "method": "\u5229\u7528\u4ee3\u6570\u51e0\u4f55\u5de5\u5177\u8bbe\u8ba1\u5e76\u5b9e\u73b0\u7b97\u6cd5\uff0c\u751f\u6210\u4e00\u7ec4\u591a\u9879\u5f0f\u65b9\u7a0b\uff0c\u5176\u89e3\u5bf9\u5e94\u4e8e\u6ee1\u8db3\u7ed9\u5b9a\u4e0d\u53d8\u91cf\u7684\u6240\u6709\u975e\u786e\u5b9a\u6027\u5206\u652f\u5faa\u73af\u3002\u6b64\u5916\uff0c\u9488\u5bf9\u7279\u5b9a\u7c7b\u522b\u7684\u4e0d\u53d8\u91cf\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u66f4\u9ad8\u6548\u7684\u7b97\u6cd5\u3002", "result": "\u6210\u529f\u5c06\u5faa\u73af\u5408\u6210\u95ee\u9898\u8f6c\u5316\u4e3a\u6c42\u89e3\u6709\u7406\u6570\u591a\u53d8\u91cf\u591a\u9879\u5f0f\u7cfb\u7edf\u7684\u95ee\u9898\uff0c\u5e76\u901a\u8fc7SMT\u6c42\u89e3\u5668\u5b9e\u73b0\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u7684\u65b9\u6cd5\u663e\u8457\u6269\u5c55\u4e86\u5faa\u73af\u5408\u6210\u7684\u9002\u7528\u8303\u56f4\uff0c\u5e76\u4e3a\u590d\u6742\u5faa\u73af\u7684\u6b63\u786e\u6027\u9a8c\u8bc1\u63d0\u4f9b\u4e86\u65b0\u5de5\u5177\u3002"}}
{"id": "2509.23769", "categories": ["cs.GR", "cs.CV"], "pdf": "https://arxiv.org/pdf/2509.23769", "abs": "https://arxiv.org/abs/2509.23769", "authors": ["Lezhong Wang", "Shutong Jin", "Ruiqi Cui", "Anders Bjorholm Dahl", "Jeppe Revall Frisvad", "Siavash Bigdeli"], "title": "ReLumix: Extending Image Relighting to Video via Video Diffusion Models", "comment": "Project page: https://lez-s.github.io/Relumix_project/", "summary": "Controlling illumination during video post-production is a crucial yet\nelusive goal in computational photography. Existing methods often lack\nflexibility, restricting users to certain relighting models. This paper\nintroduces ReLumix, a novel framework that decouples the relighting algorithm\nfrom temporal synthesis, thereby enabling any image relighting technique to be\nseamlessly applied to video. Our approach reformulates video relighting into a\nsimple yet effective two-stage process: (1) an artist relights a single\nreference frame using any preferred image-based technique (e.g., Diffusion\nModels, physics-based renderers); and (2) a fine-tuned stable video diffusion\n(SVD) model seamlessly propagates this target illumination throughout the\nsequence. To ensure temporal coherence and prevent artifacts, we introduce a\ngated cross-attention mechanism for smooth feature blending and a temporal\nbootstrapping strategy that harnesses SVD's powerful motion priors. Although\ntrained on synthetic data, ReLumix shows competitive generalization to\nreal-world videos. The method demonstrates significant improvements in visual\nfidelity, offering a scalable and versatile solution for dynamic lighting\ncontrol.", "AI": {"tldr": "ReLumix\u662f\u4e00\u79cd\u65b0\u578b\u7684\u89c6\u9891\u91cd\u5149\u7167\u6846\u67b6\uff0c\u901a\u8fc7\u5c06\u91cd\u5149\u7167\u7b97\u6cd5\u4e0e\u65f6\u5e8f\u5408\u6210\u89e3\u8026\uff0c\u4f7f\u5f97\u4efb\u4f55\u56fe\u50cf\u91cd\u5149\u7167\u6280\u672f\u90fd\u80fd\u65e0\u7f1d\u5e94\u7528\u4e8e\u89c6\u9891\u4e2d\u3002\u8be5\u65b9\u6cd5\u901a\u8fc7\u4e24\u9636\u6bb5\u6d41\u7a0b\u5b9e\u73b0\uff1a\u827a\u672f\u5bb6\u5728\u53c2\u8003\u5e27\u4e0a\u4f7f\u7528\u559c\u6b22\u7684\u56fe\u50cf\u6280\u672f\u8fdb\u884c\u91cd\u5149\u7167\uff0c\u7136\u540e\u901a\u8fc7\u5fae\u8c03\u7684\u7a33\u5b9a\u89c6\u9891\u6269\u6563\u6a21\u578b\u5c06\u76ee\u6807\u5149\u7167\u4f20\u64ad\u5230\u6574\u4e2a\u5e8f\u5217\u4e2d\u3002", "motivation": "\u73b0\u6709\u7684\u89c6\u9891\u91cd\u5149\u7167\u65b9\u6cd5\u7f3a\u4e4f\u7075\u6d3b\u6027\uff0c\u9650\u5236\u4e86\u7528\u6237\u5bf9\u91cd\u5149\u7167\u6a21\u578b\u7684\u9009\u62e9\u3002ReLumix\u65e8\u5728\u63d0\u4f9b\u4e00\u79cd\u66f4\u7075\u6d3b\u3001\u53ef\u6269\u5c55\u7684\u89c6\u9891\u91cd\u5149\u7167\u89e3\u51b3\u65b9\u6848\u3002", "method": "ReLumix\u91c7\u7528\u4e24\u9636\u6bb5\u6d41\u7a0b\uff1a\u9996\u5148\u5728\u5355\u5e27\u53c2\u8003\u56fe\u50cf\u4e0a\u5e94\u7528\u4efb\u610f\u56fe\u50cf\u91cd\u5149\u7167\u6280\u672f\uff08\u5982\u6269\u6563\u6a21\u578b\u6216\u7269\u7406\u6e32\u67d3\u5668\uff09\uff0c\u7136\u540e\u901a\u8fc7\u5fae\u8c03\u7684\u7a33\u5b9a\u89c6\u9891\u6269\u6563\u6a21\u578b\uff08SVD\uff09\u5c06\u76ee\u6807\u5149\u7167\u4f20\u64ad\u5230\u6574\u4e2a\u89c6\u9891\u5e8f\u5217\u4e2d\u3002\u4e3a\u4e86\u786e\u4fdd\u65f6\u5e8f\u4e00\u81f4\u6027\u548c\u907f\u514d\u4f2a\u5f71\uff0c\u5f15\u5165\u4e86\u95e8\u63a7\u4ea4\u53c9\u6ce8\u610f\u529b\u673a\u5236\u548c\u65f6\u95f4\u5f15\u5bfc\u7b56\u7565\u3002", "result": "\u5c3d\u7ba1\u4ec5\u5728\u5408\u6210\u6570\u636e\u4e0a\u8bad\u7ec3\uff0cReLumix\u5728\u771f\u5b9e\u4e16\u754c\u89c6\u9891\u4e2d\u8868\u73b0\u51fa\u7ade\u4e89\u6027\u7684\u6cdb\u5316\u80fd\u529b\u3002\u8be5\u65b9\u6cd5\u5728\u89c6\u89c9\u4fdd\u771f\u5ea6\u4e0a\u8868\u73b0\u51fa\u663e\u8457\u6539\u8fdb\u3002", "conclusion": "ReLumix\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u3001\u7075\u6d3b\u7684\u89c6\u9891\u91cd\u5149\u7167\u89e3\u51b3\u65b9\u6848\uff0c\u80fd\u591f\u5728\u52a8\u6001\u5149\u7167\u63a7\u5236\u4e2d\u663e\u8457\u63d0\u5347\u89c6\u89c9\u8d28\u91cf\u3002"}}
{"id": "2509.24806", "categories": ["cs.GT"], "pdf": "https://arxiv.org/pdf/2509.24806", "abs": "https://arxiv.org/abs/2509.24806", "authors": ["Broos Maenhout", "P\u0159emysl \u0160\u016fcha", "Viktorie Valdmanov\u00e1", "Ond\u0159ej Tkadlec", "Jana Thao Rozlivkov\u00e1"], "title": "A Bilevel Approach to Integrated Surgeon Scheduling and Surgery Planning solved via Branch-and-Price", "comment": null, "summary": "In this paper, we study a multi-agent scheduling problem for organising the\noperations within the operating room department. The head of the surgeon group\nand individual surgeons are together responsible for the surgeon schedule and\nsurgical case planning. The surgeon head allocates time blocks to individual\nsurgeons, whereas individual surgeons determine the planning of surgical cases\nindependently, which might degrade the schedule quality envisaged by the\nsurgeon head. The bilevel optimisation under study seeks an optimal Nash\nequilibrium solution -- a surgeon schedule and surgical case plan that optimise\nthe objectives of the surgeon head, while ensuring that no individual surgeon\ncan improve their own objective within the allocated time blocks. We propose a\ndedicated branch-and-price that adds lazy constraints to the formulation of\nsurgeon-specific pricing problems to ensure an optimal bilevel feasible\nsolution is retrieved. In this way, the surgeon head respects the objective\nrequirements of the individual surgeons and the solution space can be searched\nefficiently. In the computational experiments, we validate the performance of\nthe proposed algorithm and its dedicated components and provide insights into\nthe benefits of attaining an equilibrium solution under different scenarios by\ncalculating the price of stability and the price of decentralisation.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u4e00\u79cd\u591a\u667a\u80fd\u4f53\u8c03\u5ea6\u95ee\u9898\uff0c\u65e8\u5728\u4f18\u5316\u624b\u672f\u5ba4\u90e8\u95e8\u5185\u7684\u5de5\u4f5c\u5b89\u6392\u3002\u901a\u8fc7\u53cc\u5c42\u4f18\u5316\u65b9\u6cd5\u627e\u5230\u6700\u4f18\u7684\u7eb3\u4ec0\u5747\u8861\u89e3\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u5206\u652f\u5b9a\u4ef7\u7b97\u6cd5\u6765\u63d0\u9ad8\u6548\u7387\u3002", "motivation": "\u624b\u672f\u5ba4\u90e8\u95e8\u7684\u64cd\u4f5c\u5b89\u6392\u9700\u8981\u5916\u79d1\u56e2\u961f\u8d1f\u8d23\u4eba\u548c\u4e2a\u4f53\u5916\u79d1\u533b\u751f\u5171\u540c\u53c2\u4e0e\u3002\u4e2a\u4f53\u5916\u79d1\u533b\u751f\u7684\u72ec\u7acb\u8ba1\u5212\u53ef\u80fd\u5f71\u54cd\u6574\u4f53\u8c03\u5ea6\u8d28\u91cf\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u65b9\u6cd5\u6765\u534f\u8c03\u4e8c\u8005\u7684\u76ee\u6807\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u5206\u652f\u5b9a\u4ef7\u7b97\u6cd5\uff0c\u901a\u8fc7\u6dfb\u52a0\u60f0\u6027\u7ea6\u675f\u786e\u4fdd\u53cc\u5c42\u4f18\u5316\u7684\u53ef\u884c\u6027\uff0c\u4ece\u800c\u9ad8\u6548\u641c\u7d22\u89e3\u7a7a\u95f4\u3002", "result": "\u901a\u8fc7\u8ba1\u7b97\u7a33\u5b9a\u4ef7\u683c\u548c\u5206\u6563\u5316\u4ef7\u683c\uff0c\u9a8c\u8bc1\u4e86\u7b97\u6cd5\u7684\u6027\u80fd\u53ca\u5176\u5728\u4e0d\u540c\u573a\u666f\u4e0b\u8fbe\u5230\u5747\u8861\u89e3\u7684\u76ca\u5904\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e0d\u4ec5\u5c0a\u91cd\u4e86\u5916\u79d1\u56e2\u961f\u8d1f\u8d23\u4eba\u548c\u4e2a\u4f53\u5916\u79d1\u533b\u751f\u7684\u76ee\u6807\u9700\u6c42\uff0c\u8fd8\u9ad8\u6548\u627e\u5230\u4e86\u6700\u4f18\u89e3\uff0c\u63d0\u5347\u4e86\u624b\u672f\u5ba4\u7684\u8c03\u5ea6\u8d28\u91cf\u3002"}}
{"id": "2509.23852", "categories": ["cs.GR", "cs.MM", "cs.RO"], "pdf": "https://arxiv.org/pdf/2509.23852", "abs": "https://arxiv.org/abs/2509.23852", "authors": ["Yiheng Huang", "Junran Peng", "Silei Shen", "Jingwei Yang", "ZeJi Wei", "ChenCheng Bai", "Yonghao He", "Wei Sui", "Muyi Sun", "Yan Liu", "Xu-Cheng Yin", "Man Zhang", "Zhaoxiang Zhang", "Chuanchen Luo"], "title": "SIG-Chat: Spatial Intent-Guided Conversational Gesture Generation Involving How, When and Where", "comment": null, "summary": "The accompanying actions and gestures in dialogue are often closely linked to\ninteractions with the environment, such as looking toward the interlocutor or\nusing gestures to point to the described target at appropriate moments. Speech\nand semantics guide the production of gestures by determining their timing\n(WHEN) and style (HOW), while the spatial locations of interactive objects\ndictate their directional execution (WHERE). Existing approaches either rely\nsolely on descriptive language to generate motions or utilize audio to produce\nnon-interactive gestures, thereby lacking the characterization of interactive\ntiming and spatial intent. This significantly limits the applicability of\nconversational gesture generation, whether in robotics or in the fields of game\nand animation production. To address this gap, we present a full-stack\nsolution. We first established a unique data collection method to\nsimultaneously capture high-precision human motion and spatial intent. We then\ndeveloped a generation model driven by audio, language, and spatial data,\nalongside dedicated metrics for evaluating interaction timing and spatial\naccuracy. Finally, we deployed the solution on a humanoid robot, enabling rich,\ncontext-aware physical interactions.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u5168\u6808\u89e3\u51b3\u65b9\u6848\uff0c\u901a\u8fc7\u97f3\u9891\u3001\u8bed\u8a00\u548c\u7a7a\u95f4\u6570\u636e\u9a71\u52a8\u7684\u751f\u6210\u6a21\u578b\uff0c\u7ed3\u5408\u4ea4\u4e92\u65f6\u673a\u548c\u7a7a\u95f4\u51c6\u786e\u6027\u7684\u8bc4\u4f30\u6307\u6807\uff0c\u63d0\u5347\u4e86\u5bf9\u8bdd\u624b\u52bf\u751f\u6210\u7684\u4ea4\u4e92\u6027\u548c\u7a7a\u95f4\u610f\u56fe\u8868\u73b0\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u4ec5\u4f9d\u8d56\u63cf\u8ff0\u6027\u8bed\u8a00\u6216\u97f3\u9891\u751f\u6210\u52a8\u4f5c\uff0c\u7f3a\u4e4f\u5bf9\u4ea4\u4e92\u65f6\u673a\u548c\u7a7a\u95f4\u610f\u56fe\u7684\u8868\u5f81\uff0c\u9650\u5236\u4e86\u5bf9\u8bdd\u624b\u52bf\u751f\u6210\u5728\u673a\u5668\u4eba\u6216\u6e38\u620f\u52a8\u753b\u9886\u57df\u7684\u5e94\u7528\u3002", "method": "\u9996\u5148\u5efa\u7acb\u4e86\u72ec\u7279\u7684\u6570\u636e\u6536\u96c6\u65b9\u6cd5\uff0c\u540c\u65f6\u6355\u83b7\u9ad8\u7cbe\u5ea6\u4eba\u4f53\u52a8\u4f5c\u548c\u7a7a\u95f4\u610f\u56fe\uff1b\u7136\u540e\u5f00\u53d1\u4e86\u7531\u97f3\u9891\u3001\u8bed\u8a00\u548c\u7a7a\u95f4\u6570\u636e\u9a71\u52a8\u7684\u751f\u6210\u6a21\u578b\uff0c\u5e76\u8bbe\u8ba1\u4e86\u8bc4\u4f30\u4ea4\u4e92\u65f6\u673a\u548c\u7a7a\u95f4\u51c6\u786e\u6027\u7684\u6307\u6807\uff1b\u6700\u540e\u5728\u7c7b\u4eba\u673a\u5668\u4eba\u4e0a\u90e8\u7f72\u4e86\u89e3\u51b3\u65b9\u6848\u3002", "result": "\u63d0\u51fa\u7684\u89e3\u51b3\u65b9\u6848\u80fd\u591f\u5b9e\u73b0\u4e30\u5bcc\u4e14\u4e0a\u4e0b\u6587\u611f\u77e5\u7684\u7269\u7406\u4ea4\u4e92\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5bf9\u8bdd\u624b\u52bf\u751f\u6210\u7684\u4ea4\u4e92\u6027\u548c\u7a7a\u95f4\u610f\u56fe\u8868\u73b0\u3002", "conclusion": "\u8be5\u7814\u7a76\u586b\u8865\u4e86\u73b0\u6709\u65b9\u6cd5\u7684\u4e0d\u8db3\uff0c\u901a\u8fc7\u5168\u6808\u89e3\u51b3\u65b9\u6848\u63d0\u5347\u4e86\u5bf9\u8bdd\u624b\u52bf\u751f\u6210\u7684\u5b9e\u7528\u6027\uff0c\u4e3a\u673a\u5668\u4eba\u3001\u6e38\u620f\u548c\u52a8\u753b\u9886\u57df\u63d0\u4f9b\u4e86\u65b0\u7684\u53ef\u80fd\u6027\u3002"}}
{"id": "2509.24849", "categories": ["cs.GT"], "pdf": "https://arxiv.org/pdf/2509.24849", "abs": "https://arxiv.org/abs/2509.24849", "authors": ["Bruno Mazorra", "Burak \u00d6z", "Christoph Schlegel", "Fei Wu"], "title": "The Free Option Problem of ePBS", "comment": null, "summary": "Ethereum's upcoming Glamsterdam upgrade introduces EIP-7732 enshrined\nProposer--Builder Separation (ePBS), which improves the block production\npipeline by addressing trust and scalability challenges. Yet it also creates a\nnew liveness risk: builders gain a short-dated ``free'' option to prevent the\nexecution payload they committed to from becoming canonical, without incurring\nan additional penalty. Exercising this option renders an empty block for the\nslot in question, thereby degrading network liveness.\n  We present the first systematic study of the free option problem. Our\ntheoretical results predict that option value and exercise probability grow\nwith market volatility, the length of the option window, and the share of block\nvalue derived from external signals such as external market prices. The\navailability of a free option will lead to mispricing and LP losses. The\nproblem would be exacerbated if Ethereum further scales and attracts more\nliquidity. Empirical estimates of values and exercise probabilities on\nhistorical blocks largely confirm our theoretical predictions. While the option\nis rarely profitable to exercise on average (0.82\\% of blocks assuming an\n8-second option time window), it becomes significant in volatile periods,\nreaching up to 6\\% of blocks on high-volatility days -- precisely when users\nmost require timely execution.\n  Moreover, builders whose block value relies heavily on CEX-DEX arbitrage are\nmore likely to exercise the option. We demonstrate that mitigation strategies\n-- shortening the option window or penalizing exercised options -- effectively\nreduce liveness risk.", "AI": {"tldr": "\u4ee5\u592a\u574a\u7684Glamsterdam\u5347\u7ea7\u5f15\u5165EIP-7732\uff08ePBS\uff09\uff0c\u6539\u5584\u533a\u5757\u751f\u4ea7\u7ba1\u9053\u7684\u4fe1\u4efb\u548c\u53ef\u6269\u5c55\u6027\u95ee\u9898\uff0c\u4f46\u4e5f\u5f15\u5165\u4e86\u65b0\u7684\u6d3b\u8dc3\u6027\u98ce\u9669\u2014\u2014\u6784\u5efa\u8005\u53ef\u4ee5\u514d\u8d39\u963b\u6b62\u5176\u627f\u8bfa\u7684\u6267\u884c\u8d1f\u8f7d\u6210\u4e3a\u89c4\u8303\uff0c\u5bfc\u81f4\u7a7a\u5757\u51fa\u73b0\u3002\u672c\u6587\u9996\u6b21\u7cfb\u7edf\u7814\u7a76\u4e86\u8fd9\u4e00\u95ee\u9898\u3002", "motivation": "\u7814\u7a76\u4ee5\u592a\u574aGlamsterdam\u5347\u7ea7\u4e2dePBS\u673a\u5236\u5e26\u6765\u7684\u65b0\u6d3b\u8dc3\u6027\u98ce\u9669\uff0c\u5373\u6784\u5efa\u8005\u53ef\u4ee5\u901a\u8fc7\u514d\u8d39\u9009\u9879\u963b\u6b62\u5176\u627f\u8bfa\u7684\u6267\u884c\u8d1f\u8f7d\u6210\u4e3a\u89c4\u8303\uff0c\u5bfc\u81f4\u7f51\u7edc\u6d3b\u8dc3\u6027\u4e0b\u964d\u3002", "method": "\u901a\u8fc7\u7406\u8bba\u5206\u6790\u548c\u5386\u53f2\u533a\u5757\u6570\u636e\u7684\u5b9e\u8bc1\u7814\u7a76\uff0c\u8bc4\u4f30\u514d\u8d39\u9009\u9879\u7684\u4ef7\u503c\u548c\u6267\u884c\u6982\u7387\uff0c\u5e76\u63a2\u8ba8\u5e02\u573a\u6ce2\u52a8\u3001\u9009\u9879\u7a97\u53e3\u957f\u5ea6\u4ee5\u53ca\u533a\u5757\u4ef7\u503c\u6765\u6e90\u5bf9\u5176\u5f71\u54cd\u3002", "result": "\u7814\u7a76\u53d1\u73b0\uff0c\u514d\u8d39\u9009\u9879\u7684\u4ef7\u503c\u548c\u6267\u884c\u6982\u7387\u53d7\u5e02\u573a\u6ce2\u52a8\u7b49\u56e0\u7d20\u5f71\u54cd\uff0c\u5728\u9ad8\u6ce2\u52a8\u65f6\u671f\u53ef\u80fd\u5f71\u54cd\u591a\u8fbe6%\u7684\u533a\u5757\u3002\u6b64\u5916\uff0c\u4f9d\u8d56CEX-DEX\u5957\u5229\u7684\u6784\u5efa\u8005\u66f4\u53ef\u80fd\u6267\u884c\u8be5\u9009\u9879\u3002\u7f13\u89e3\u7b56\u7565\u5982\u7f29\u77ed\u9009\u9879\u7a97\u53e3\u6216\u60e9\u7f5a\u6267\u884c\u9009\u9879\u53ef\u4ee5\u6709\u6548\u964d\u4f4e\u98ce\u9669\u3002", "conclusion": "\u514d\u8d39\u9009\u9879\u95ee\u9898\u5728\u9ad8\u6ce2\u52a8\u65f6\u671f\u4f1a\u663e\u8457\u5f71\u54cd\u4ee5\u592a\u574a\u7f51\u7edc\u7684\u6d3b\u8dc3\u6027\u3002\u7f13\u89e3\u63aa\u65bd\u53ef\u4ee5\u964d\u4f4e\u98ce\u9669\uff0c\u4f46\u9700\u8fdb\u4e00\u6b65\u63a2\u7d22\u4ee5\u4fdd\u8bc1\u7f51\u7edc\u7684\u7a33\u5b9a\u6027\u548c\u7528\u6237\u4f53\u9a8c\u3002"}}
{"id": "2509.24150", "categories": ["cs.GR", "cs.CV"], "pdf": "https://arxiv.org/pdf/2509.24150", "abs": "https://arxiv.org/abs/2509.24150", "authors": ["Jun-Hao Wang", "Yi-Yang Tian", "Baoquan Chen", "Peng-Shuai Wang"], "title": "Neural Visibility of Point Sets", "comment": "Accepted to SIGGRAPH Asia 2025", "summary": "Point clouds are widely used representations of 3D data, but determining the\nvisibility of points from a given viewpoint remains a challenging problem due\nto their sparse nature and lack of explicit connectivity. Traditional methods,\nsuch as Hidden Point Removal (HPR), face limitations in computational\nefficiency, robustness to noise, and handling concave regions or low-density\npoint clouds. In this paper, we propose a novel approach to visibility\ndetermination in point clouds by formulating it as a binary classification\ntask. The core of our network consists of a 3D U-Net that extracts\nview-independent point-wise features and a shared multi-layer perceptron (MLP)\nthat predicts point visibility using the extracted features and view direction\nas inputs. The network is trained end-to-end with ground-truth visibility\nlabels generated from rendered 3D models. Our method significantly outperforms\nHPR in both accuracy and computational efficiency, achieving up to 126 times\nspeedup on large point clouds. Additionally, our network demonstrates\nrobustness to noise and varying point cloud densities and generalizes well to\nunseen shapes. We validate the effectiveness of our approach through extensive\nexperiments on the ShapeNet, ABC Dataset and real-world datasets, showing\nsubstantial improvements in visibility accuracy. We also demonstrate the\nversatility of our method in various applications, including point cloud\nvisualization, surface reconstruction, normal estimation, shadow rendering, and\nviewpoint optimization. Our code and models are available at\nhttps://github.com/octree-nn/neural-visibility.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e3D U-Net\u548c\u591a\u5c42\u611f\u77e5\u673a\u7684\u65b0\u578b\u65b9\u6cd5\uff0c\u7528\u4e8e\u70b9\u4e91\u4e2d\u7684\u53ef\u89c1\u6027\u5224\u5b9a\uff0c\u76f8\u6bd4\u4f20\u7edf\u65b9\u6cd5\u5728\u7cbe\u5ea6\u548c\u8ba1\u7b97\u6548\u7387\u4e0a\u6709\u663e\u8457\u63d0\u5347\u3002", "motivation": "\u70b9\u4e91\u4f5c\u4e3a3D\u6570\u636e\u7684\u8868\u793a\u5f62\u5f0f\uff0c\u5176\u53ef\u89c1\u6027\u5224\u5b9a\u56e0\u7a00\u758f\u6027\u548c\u7f3a\u4e4f\u663e\u5f0f\u8fde\u63a5\u800c\u5177\u6709\u6311\u6218\u6027\u3002\u4f20\u7edf\u65b9\u6cd5\u5982HPR\u5728\u8ba1\u7b97\u6548\u7387\u3001\u566a\u58f0\u9c81\u68d2\u6027\u7b49\u65b9\u9762\u5b58\u5728\u5c40\u9650\u6027\u3002", "method": "\u5c06\u53ef\u89c1\u6027\u5224\u5b9a\u5efa\u6a21\u4e3a\u4e8c\u5143\u5206\u7c7b\u4efb\u52a1\uff0c\u4f7f\u75283D U-Net\u63d0\u53d6\u89c6\u70b9\u65e0\u5173\u7684\u70b9\u7279\u5f81\uff0c\u7ed3\u5408\u591a\u5c42\u611f\u77e5\u673a\u9884\u6d4b\u70b9\u53ef\u89c1\u6027\uff0c\u5e76\u901a\u8fc7\u7aef\u5230\u7aef\u8bad\u7ec3\u4f18\u5316\u6a21\u578b\u3002", "result": "\u8be5\u65b9\u6cd5\u5728\u7cbe\u5ea6\u548c\u8ba1\u7b97\u6548\u7387\u4e0a\u663e\u8457\u4f18\u4e8eHPR\uff0c\u6700\u9ad8\u53ef\u8fbe126\u500d\u7684\u52a0\u901f\uff0c\u540c\u65f6\u5bf9\u566a\u58f0\u548c\u70b9\u4e91\u5bc6\u5ea6\u53d8\u5316\u8868\u73b0\u51fa\u9c81\u68d2\u6027\uff0c\u5e76\u5728ShapeNet\u7b49\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u4e86\u6709\u6548\u6027\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u591a\u573a\u666f\u5e94\u7528\u4e2d\u5c55\u73b0\u4e86\u5e7f\u6cdb\u7684\u9002\u7528\u6027\uff0c\u5305\u62ec\u70b9\u4e91\u53ef\u89c6\u5316\u3001\u8868\u9762\u91cd\u5efa\u548c\u9634\u5f71\u6e32\u67d3\u7b49\uff0c\u4ee3\u7801\u548c\u6a21\u578b\u5df2\u516c\u5f00\u3002"}}
{"id": "2509.24946", "categories": ["cs.GT", "cs.DC"], "pdf": "https://arxiv.org/pdf/2509.24946", "abs": "https://arxiv.org/abs/2509.24946", "authors": ["Rosario Patan\u00e8", "Andrea Araldo", "Nadjib Achir", "Lila Boukhatem"], "title": "A Management Framework for Vehicular Cloudtoward Economic and Environmental Efficiency", "comment": null, "summary": "Vehicular Cloud Computing (VCC) leverages the idle computing capacity of\nvehicles to execute end-users' offloaded tasks without requiring new\ncomputation infrastructure. Despite its conceptual appeal, VCC adoption is\nhindered by the lack of quantitative evidence demonstrating its profitability\nand environmental advantages in real-world scenarios. This paper tackles the\nfundamental question: Can VCC be both profitable and sustainable? We address\nthis problem by proposing a management scheme for VCC that combines\nenergy-aware task allocation with a game-theoretic revenue-sharing mechanism.\nOur framework is the first to jointly model latency, energy consumption,\nmonetary incentives, and carbon emissions within urban mobility and 5G\ncommunication settings. The task allocation strategy maximizes the aggregate\nstakeholder utility while satisfying deadlines and minimizing energy costs. The\npayoffs are distributed via a coalitional game theory adapted to dynamic\nvehicular environments, to prevent disincentivizing participants with\npotentially negative contributions. Extensive simulations demonstrate that our\napproach supports low-latency task execution, enables effective monetization of\nvehicular resources, and reduces CO2 emissions by more than 99% compared to\nconventional edge infrastructures, making VCC a practical and sustainable\nalternative to edge computing.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u80fd\u91cf\u611f\u77e5\u4efb\u52a1\u5206\u914d\u548c\u535a\u5f08\u8bba\u6536\u5165\u5171\u4eab\u673a\u5236\u7684\u8f66\u8f7d\u4e91\u8ba1\u7b97\u7ba1\u7406\u65b9\u6848\uff0c\u65e8\u5728\u91cf\u5316\u8bc1\u660e\u5176\u5728\u76c8\u5229\u6027\u548c\u73af\u5883\u53ef\u6301\u7eed\u6027\u65b9\u9762\u7684\u4f18\u52bf\u3002", "motivation": "\u8f66\u8f7d\u4e91\u8ba1\u7b97\uff08VCC\uff09\u867d\u7136\u6982\u5ff5\u4e0a\u5177\u6709\u5438\u5f15\u529b\uff0c\u4f46\u7531\u4e8e\u7f3a\u4e4f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u8bc1\u660e\u5176\u76c8\u5229\u6027\u548c\u73af\u5883\u4f18\u52bf\u7684\u5b9a\u91cf\u8bc1\u636e\uff0c\u5176\u91c7\u7528\u53d7\u5230\u963b\u788d\u3002", "method": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ba1\u7406\u65b9\u6848\uff0c\u7ed3\u5408\u4e86\u80fd\u91cf\u611f\u77e5\u4efb\u52a1\u5206\u914d\u548c\u57fa\u4e8e\u535a\u5f08\u8bba\u7684\u6536\u5165\u5171\u4eab\u673a\u5236\uff0c\u9996\u6b21\u5728\u57ce\u533a\u79fb\u52a8\u548c5G\u901a\u4fe1\u73af\u5883\u4e2d\u5171\u540c\u5efa\u6a21\u4e86\u5ef6\u8fdf\u3001\u80fd\u8017\u3001\u8d27\u5e01\u6fc0\u52b1\u548c\u78b3\u6392\u653e\u3002", "result": "\u6a21\u62df\u7ed3\u679c\u663e\u793a\uff0c\u8be5\u65b9\u6848\u652f\u6301\u4f4e\u5ef6\u8fdf\u4efb\u52a1\u6267\u884c\uff0c\u6709\u6548\u5b9e\u73b0\u4e86\u8f66\u8f86\u8d44\u6e90\u7684\u8d27\u5e01\u5316\uff0c\u5e76\u5c06CO2\u6392\u653e\u91cf\u51cf\u5c11\u4e8699%\u4ee5\u4e0a\uff0c\u4f18\u4e8e\u4f20\u7edf\u7684\u8fb9\u7f18\u57fa\u7840\u8bbe\u65bd\u3002", "conclusion": "\u7814\u7a76\u8868\u660e\uff0c\u8f66\u8f7d\u4e91\u8ba1\u7b97\u662f\u4e00\u79cd\u65e2\u5b9e\u7528\u53c8\u53ef\u6301\u7eed\u7684\u8fb9\u7f18\u8ba1\u7b97\u66ff\u4ee3\u65b9\u6848\u3002"}}
{"id": "2509.24677", "categories": ["cs.GR", "I.3.7"], "pdf": "https://arxiv.org/pdf/2509.24677", "abs": "https://arxiv.org/abs/2509.24677", "authors": ["Xiangyu Wang", "Thomas K\u00f6hler", "Jun Lin Qiu", "Shohei Mori", "Markus Steinberger", "Dieter Schmalstieg"], "title": "NeuralPVS: Learned Estimation of Potentially Visible Sets", "comment": "SIGGRAPH Asia 2025", "summary": "Real-time visibility determination in expansive or dynamically changing\nenvironments has long posed a significant challenge in computer graphics.\nExisting techniques are computationally expensive and often applied as a\nprecomputation step on a static scene. We present NeuralPVS, the first\ndeep-learning approach for visibility computation that efficiently determines\nfrom-region visibility in a large scene, running at approximately 100 Hz\nprocessing with less than $1\\%$ missing geometry. This approach is possible by\nusing a neural network operating on a voxelized representation of the scene.\nThe network's performance is achieved by combining sparse convolution with a 3D\nvolume-preserving interleaving for data compression. Moreover, we introduce a\nnovel repulsive visibility loss that can effectively guide the network to\nconverge to the correct data distribution. This loss provides enhanced\nrobustness and generalization to unseen scenes. Our results demonstrate that\nNeuralPVS outperforms existing methods in terms of both accuracy and\nefficiency, making it a promising solution for real-time visibility\ncomputation.", "AI": {"tldr": "NeuralPVS\u662f\u4e00\u79cd\u57fa\u4e8e\u6df1\u5ea6\u5b66\u4e60\u7684\u5b9e\u65f6\u53ef\u89c1\u6027\u8ba1\u7b97\u65b9\u6cd5\uff0c\u9996\u6b21\u5728\u5927\u89c4\u6a21\u573a\u666f\u4e2d\u4ee5\u7ea6100 Hz\u7684\u901f\u5ea6\u9ad8\u6548\u8ba1\u7b97\u533a\u57df\u53ef\u89c1\u6027\uff0c\u4e14\u51e0\u4f55\u7f3a\u5931\u7387\u4f4e\u4e8e1%\u3002", "motivation": "\u5728\u8ba1\u7b97\u673a\u56fe\u5f62\u5b66\u4e2d\uff0c\u5b9e\u65f6\u786e\u5b9a\u5e7f\u9614\u6216\u52a8\u6001\u53d8\u5316\u73af\u5883\u4e2d\u7684\u53ef\u89c1\u6027\u4e00\u76f4\u662f\u4e00\u4e2a\u91cd\u5927\u6311\u6218\u3002\u73b0\u6709\u65b9\u6cd5\u8ba1\u7b97\u6210\u672c\u9ad8\uff0c\u901a\u5e38\u4f5c\u4e3a\u9759\u6001\u573a\u666f\u7684\u9884\u8ba1\u7b97\u6b65\u9aa4\u3002", "method": "NeuralPVS\u4f7f\u7528\u795e\u7ecf\u7f51\u7edc\u5bf9\u573a\u666f\u7684\u4f53\u7d20\u5316\u8868\u793a\u8fdb\u884c\u64cd\u4f5c\uff0c\u901a\u8fc7\u7ed3\u5408\u7a00\u758f\u5377\u79ef\u548c3D\u4f53\u79ef\u4fdd\u6301\u4ea4\u9519\u6570\u636e\u538b\u7f29\u6280\u672f\u5b9e\u73b0\u9ad8\u6027\u80fd\u3002\u6b64\u5916\uff0c\u8fd8\u5f15\u5165\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u6392\u65a5\u53ef\u89c1\u6027\u635f\u5931\u51fd\u6570\uff0c\u6709\u6548\u5f15\u5bfc\u7f51\u7edc\u6536\u655b\u5230\u6b63\u786e\u7684\u6570\u636e\u5206\u5e03\u3002", "result": "NeuralPVS\u5728\u51c6\u786e\u6027\u548c\u6548\u7387\u4e0a\u5747\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u51e0\u4f55\u7f3a\u5931\u7387\u4f4e\u4e8e1%\uff0c\u9002\u7528\u4e8e\u5b9e\u65f6\u53ef\u89c1\u6027\u8ba1\u7b97\u3002", "conclusion": "NeuralPVS\u662f\u4e00\u79cd\u9ad8\u6548\u4e14\u51c6\u786e\u7684\u5b9e\u65f6\u53ef\u89c1\u6027\u8ba1\u7b97\u89e3\u51b3\u65b9\u6848\uff0c\u5177\u6709\u8f83\u5f3a\u7684\u9c81\u68d2\u6027\u548c\u5bf9\u672a\u77e5\u573a\u666f\u7684\u6cdb\u5316\u80fd\u529b\u3002"}}
{"id": "2509.25150", "categories": ["cs.GT", "cs.DM", "cs.DS", "math.CO"], "pdf": "https://arxiv.org/pdf/2509.25150", "abs": "https://arxiv.org/abs/2509.25150", "authors": ["Frank Connor", "Louis-Roy Langevin", "Ndiam\u00e9 Ndiaye", "Agn\u00e8s Totschnig", "Rohit Vasishta", "Adrian Vetta"], "title": "The Popular Dimension of Matchings", "comment": "15 pages", "summary": "We study popular matchings in three classical settings: the house allocation\nproblem, the marriage problem, and the roommates problem. In the popular\nmatching problem, (a subset of) the vertices in a graph have preference\norderings over their potential matches. A matching is popular if it gets a\nplurality of votes in a pairwise election against any other matching.\nUnfortunately, popular matchings typically do not exist. So we study a natural\nrelaxation, namely popular winning sets which are a set of matchings that\ncollectively get a plurality of votes in a pairwise election against any other\nmatching. The $\\textit{popular dimension}$ is the minimum cardinality of a\npopular winning set, in the worst case over the problem class.\n  We prove that the popular dimension is exactly $2$ in the house allocation\nproblem, even if the voters are weighted and ties are allowed in their\npreference lists. For the marriage problem and the roommates problem, we prove\nthat the popular dimension is between $2$ and $3$, when the agents are weighted\nand/or their preferences orderings allow ties. In the special case where the\nagents are unweighted and have strict preference orderings, the popular\ndimension of the marriage problem is known to be exactly $1$ and we prove the\npopular dimension of the roommates problem is exactly $2$.", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u4e86\u4e09\u79cd\u7ecf\u5178\u573a\u666f\u4e2d\u7684\u53d7\u6b22\u8fce\u5339\u914d\u95ee\u9898\uff1a\u623f\u5c4b\u5206\u914d\u95ee\u9898\u3001\u5a5a\u59fb\u95ee\u9898\u548c\u5ba4\u53cb\u95ee\u9898\u3002\u5b9a\u4e49\u4e86\u4e00\u79cd\u79f0\u4e3a\u201c\u53d7\u6b22\u8fce\u83b7\u80dc\u96c6\u5408\u201d\u7684\u677e\u5f1b\u6982\u5ff5\uff0c\u5e76\u63d0\u51fa\u4e86\u201c\u53d7\u6b22\u8fce\u7ef4\u5ea6\u201d\u7684\u6700\u5c0f\u57fa\u6570\u6982\u5ff5\u3002", "motivation": "\u63a2\u8ba8\u5728\u53d7\u6b22\u8fce\u5339\u914d\u901a\u5e38\u4e0d\u5b58\u5728\u7684\u80cc\u666f\u4e0b\uff0c\u5982\u4f55\u901a\u8fc7\u5f15\u5165\u53d7\u6b22\u8fce\u83b7\u80dc\u96c6\u7684\u6982\u5ff5\u6765\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\uff0c\u5e76\u91cf\u5316\u5176\u6700\u5c0f\u89c4\u6a21\u3002", "method": "\u7814\u7a76\u623f\u5c4b\u5206\u914d\u3001\u5a5a\u59fb\u548c\u5ba4\u53cb\u95ee\u9898\u4e2d\u7684\u53d7\u6b22\u8fce\u5339\u914d\uff0c\u63d0\u51fa\u53d7\u6b22\u8fce\u7ef4\u5ea6\u7684\u5b9a\u4e49\uff0c\u5e76\u5728\u4e0d\u540c\u6761\u4ef6\u4e0b\uff08\u5982\u52a0\u6743\u4ee3\u7406\u4eba\u548c\u5141\u8bb8\u504f\u597d\u5217\u8868\u4e2d\u7684\u5e73\u5c40\uff09\u5206\u6790\u5176\u503c\u3002", "result": "\u623f\u5c4b\u5206\u914d\u95ee\u9898\u7684\u53d7\u6b22\u8fce\u7ef4\u5ea6\u4e3a2\uff0c\u5a5a\u59fb\u95ee\u9898\u548c\u5ba4\u53cb\u95ee\u9898\u7684\u53d7\u6b22\u8fce\u7ef4\u5ea6\u57282\u548c3\u4e4b\u95f4\u3002\u5728\u65e0\u6743\u91cd\u4e14\u4e25\u683c\u504f\u597d\u7684\u7279\u6b8a\u60c5\u51b5\u4e0b\uff0c\u5a5a\u59fb\u95ee\u9898\u7684\u53d7\u6b22\u8fce\u7ef4\u5ea6\u4e3a1\uff0c\u5ba4\u53cb\u95ee\u9898\u4e3a2\u3002", "conclusion": "\u53d7\u6b22\u8fce\u7ef4\u5ea6\u5728\u4e0d\u540c\u95ee\u9898\u573a\u666f\u4e2d\u6709\u4e0d\u540c\u7684\u8868\u73b0\uff0c\u4e3a\u7406\u89e3\u53d7\u6b22\u8fce\u5339\u914d\u7684\u6700\u5c0f\u89c4\u6a21\u63d0\u4f9b\u4e86\u7406\u8bba\u652f\u6301\u3002"}}
{"id": "2509.24986", "categories": ["cs.GR", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2509.24986", "abs": "https://arxiv.org/abs/2509.24986", "authors": ["Yuhan Wang", "Weikai Chen", "Zeyu Hu", "Runze Zhang", "Yingda Yin", "Ruoyu Wu", "Keyang Luo", "Shengju Qian", "Yiyan Ma", "Hongyi Li", "Yuan Gao", "Yuhuan Zhou", "Hao Luo", "Wan Wang", "Xiaobin Shen", "Zhaowei Li", "Kuixin Zhu", "Chuanlang Hong", "Yueyue Wang", "Lijie Feng", "Xin Wang", "Chen Change Loy"], "title": "Light-SQ: Structure-aware Shape Abstraction with Superquadrics for Generated Meshes", "comment": "SIGGRAPH Asia 2025. Project Page https://johann.wang/Light-SQ/", "summary": "In user-generated-content (UGC) applications, non-expert users often rely on\nimage-to-3D generative models to create 3D assets. In this context,\nprimitive-based shape abstraction offers a promising solution for UGC scenarios\nby compressing high-resolution meshes into compact, editable representations.\nTowards this end, effective shape abstraction must therefore be\nstructure-aware, characterized by low overlap between primitives, part-aware\nalignment, and primitive compactness. We present Light-SQ, a novel\nsuperquadric-based optimization framework that explicitly emphasizes\nstructure-awareness from three aspects. (a) We introduce SDF carving to\niteratively udpate the target signed distance field, discouraging overlap\nbetween primitives. (b) We propose a block-regrow-fill strategy guided by\nstructure-aware volumetric decomposition, enabling structural partitioning to\ndrive primitive placement. (c) We implement adaptive residual pruning based on\nSDF update history to surpress over-segmentation and ensure compact results. In\naddition, Light-SQ supports multiscale fitting, enabling localized refinement\nto preserve fine geometric details. To evaluate our method, we introduce\n3DGen-Prim, a benchmark extending 3DGen-Bench with new metrics for both\nreconstruction quality and primitive-level editability. Extensive experiments\ndemonstrate that Light-SQ enables efficient, high-fidelity, and editable shape\nabstraction with superquadrics for complex generated geometry, advancing the\nfeasibility of 3D UGC creation.", "AI": {"tldr": "Light-SQ\u662f\u4e00\u4e2a\u57fa\u4e8e\u8d85\u4e8c\u6b21\u66f2\u9762\u7684\u4f18\u5316\u6846\u67b6\uff0c\u901a\u8fc7SDF\u96d5\u523b\u3001\u7ed3\u6784\u5316\u5206\u533a\u548c\u81ea\u9002\u5e94\u6b8b\u5dee\u4fee\u526a\uff0c\u5b9e\u73b0\u9ad8\u6548\u3001\u9ad8\u4fdd\u771f\u4e14\u53ef\u7f16\u8f91\u7684\u5f62\u72b6\u62bd\u8c61\uff0c\u652f\u63013D\u7528\u6237\u751f\u6210\u5185\u5bb9\u7684\u521b\u5efa\u3002", "motivation": "\u5728\u7528\u6237\u751f\u6210\u5185\u5bb9\uff08UGC\uff09\u5e94\u7528\u4e2d\uff0c\u975e\u4e13\u4e1a\u7528\u6237\u4f9d\u8d56\u56fe\u50cf\u52303D\u751f\u6210\u6a21\u578b\u521b\u5efa3D\u8d44\u4ea7\u3002\u539f\u59cb\u5f62\u72b6\u62bd\u8c61\u63d0\u4f9b\u4e86\u4e00\u79cd\u538b\u7f29\u9ad8\u5206\u8fa8\u7387\u7f51\u683c\u4e3a\u7d27\u51d1\u4e14\u53ef\u7f16\u8f91\u8868\u793a\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u4f46\u9700\u8981\u7ed3\u6784\u611f\u77e5\u80fd\u529b\u4ee5\u51cf\u5c11\u91cd\u53e0\u5e76\u4fdd\u6301\u7d27\u51d1\u3002", "method": "Light-SQ\u901a\u8fc7\u4e09\u65b9\u9762\u5b9e\u73b0\u7ed3\u6784\u611f\u77e5\uff1a(a) SDF\u96d5\u523b\u8fed\u4ee3\u66f4\u65b0\u76ee\u6807\u7b26\u53f7\u8ddd\u79bb\u573a\u4ee5\u51cf\u5c11\u91cd\u53e0\uff1b(b) \u7ed3\u6784\u5316\u4f53\u79ef\u5206\u89e3\u9a71\u52a8\u7684\u5757-\u518d\u751f-\u586b\u5145\u7b56\u7565\uff1b(c) \u57fa\u4e8eSDF\u66f4\u65b0\u5386\u53f2\u7684\u81ea\u9002\u5e94\u6b8b\u5dee\u4fee\u526a\u4ee5\u907f\u514d\u8fc7\u5ea6\u5206\u5272\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cLight-SQ\u5728\u590d\u6742\u51e0\u4f55\u5f62\u72b6\u4e0a\u5b9e\u73b0\u4e86\u9ad8\u6548\u3001\u9ad8\u4fdd\u771f\u548c\u53ef\u7f16\u8f91\u7684\u5f62\u72b6\u62bd\u8c61\uff0c\u5e76\u901a\u8fc73DGen-Prim\u57fa\u51c6\u9a8c\u8bc1\u4e86\u5176\u91cd\u5efa\u8d28\u91cf\u548c\u539f\u59cb\u7ea7\u53ef\u7f16\u8f91\u6027\u3002", "conclusion": "Light-SQ\u901a\u8fc7\u7ed3\u6784\u5316\u611f\u77e5\u4f18\u5316\uff0c\u663e\u8457\u63d0\u5347\u4e863D UGC\u521b\u5efa\u7684\u53ef\u884c\u6027\uff0c\u4e3a\u590d\u6742\u51e0\u4f55\u5f62\u72b6\u63d0\u4f9b\u4e86\u7d27\u51d1\u4e14\u53ef\u7f16\u8f91\u7684\u62bd\u8c61\u8868\u793a\u3002"}}
{"id": "2509.25058", "categories": ["cs.GR", "cs.CV"], "pdf": "https://arxiv.org/pdf/2509.25058", "abs": "https://arxiv.org/abs/2509.25058", "authors": ["Jan-Niklas Dihlmann", "Arnela Killguss", "Hendrik P. A. Lensch"], "title": "CharGen: Fast and Fluent Portrait Modification", "comment": "Project page: https://chargen.jdihlmann.com/", "summary": "Interactive editing of character images with diffusion models remains\nchallenging due to the inherent trade-off between fine-grained control,\ngeneration speed, and visual fidelity. We introduce CharGen, a\ncharacter-focused editor that combines attribute-specific Concept Sliders,\ntrained to isolate and manipulate attributes such as facial feature size,\nexpression, and decoration with the StreamDiffusion sampling pipeline for more\ninteractive performance. To counteract the loss of detail that often\naccompanies accelerated sampling, we propose a lightweight Repair Step that\nreinstates fine textures without compromising structural consistency.\nThroughout extensive ablation studies and in comparison to open-source\nInstructPix2Pix and closed-source Google Gemini, and a comprehensive user\nstudy, CharGen achieves two-to-four-fold faster edit turnaround with precise\nediting control and identity-consistent results. Project page:\nhttps://chargen.jdihlmann.com/", "AI": {"tldr": "CharGen\u662f\u4e00\u79cd\u57fa\u4e8e\u6269\u6563\u6a21\u578b\u7684\u5b57\u7b26\u56fe\u50cf\u4ea4\u4e92\u7f16\u8f91\u5668\uff0c\u901a\u8fc7\u5c5e\u6027\u6ed1\u5757\u548cRepair Step\u5b9e\u73b0\u5feb\u901f\u3001\u7cbe\u7ec6\u7684\u7f16\u8f91\u63a7\u5236\u3002", "motivation": "\u5f53\u524d\u4ea4\u4e92\u5f0f\u7f16\u8f91\u5b57\u7b26\u56fe\u50cf\u9762\u4e34\u63a7\u5236\u7cbe\u7ec6\u5ea6\u3001\u751f\u6210\u901f\u5ea6\u548c\u89c6\u89c9\u4fdd\u771f\u5ea6\u7684\u6743\u8861\u95ee\u9898\uff0cCharGen\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u6311\u6218\u3002", "method": "CharGen\u7ed3\u5408\u5c5e\u6027\u6ed1\u5757\uff08Concept Sliders\uff09\u548cStreamDiffusion\u91c7\u6837\u7ba1\u9053\uff0c\u5e76\u901a\u8fc7Repair Step\u4fee\u590d\u7ec6\u8282\u635f\u5931\uff0c\u5b9e\u73b0\u5feb\u901f\u548c\u9ad8\u4fdd\u771f\u7f16\u8f91\u3002", "result": "\u4e0eInstructPix2Pix\u548cGoogle Gemini\u76f8\u6bd4\uff0cCharGen\u7f16\u8f91\u901f\u5ea6\u5feb2-4\u500d\uff0c\u63d0\u4f9b\u7cbe\u786e\u63a7\u5236\u548c\u4e00\u81f4\u6027\u7684\u7ed3\u679c\u3002", "conclusion": "CharGen\u5728\u4ea4\u4e92\u5f0f\u7f16\u8f91\u4e2d\u5b9e\u73b0\u4e86\u901f\u5ea6\u548c\u8d28\u91cf\u7684\u5e73\u8861\uff0c\u4e3a\u7528\u6237\u63d0\u4f9b\u4e86\u9ad8\u6548\u7684\u5b57\u7b26\u56fe\u50cf\u7f16\u8f91\u5de5\u5177\u3002"}}
{"id": "2509.25094", "categories": ["cs.GR", "cs.CV"], "pdf": "https://arxiv.org/pdf/2509.25094", "abs": "https://arxiv.org/abs/2509.25094", "authors": ["AmirHossein Zamani", "Bruno Roy", "Arianna Rampini"], "title": "Unsupervised Representation Learning for 3D Mesh Parameterization with Semantic and Visibility Objectives", "comment": null, "summary": "Recent 3D generative models produce high-quality textures for 3D mesh\nobjects. However, they commonly rely on the heavy assumption that input 3D\nmeshes are accompanied by manual mesh parameterization (UV mapping), a manual\ntask that requires both technical precision and artistic judgment. Industry\nsurveys show that this process often accounts for a significant share of asset\ncreation, creating a major bottleneck for 3D content creators. Moreover,\nexisting automatic methods often ignore two perceptually important criteria:\n(1) semantic awareness (UV charts should align semantically similar 3D parts\nacross shapes) and (2) visibility awareness (cutting seams should lie in\nregions unlikely to be seen). To overcome these shortcomings and to automate\nthe mesh parameterization process, we present an unsupervised differentiable\nframework that augments standard geometry-preserving UV learning with semantic-\nand visibility-aware objectives. For semantic-awareness, our pipeline (i)\nsegments the mesh into semantic 3D parts, (ii) applies an unsupervised learned\nper-part UV-parameterization backbone, and (iii) aggregates per-part charts\ninto a unified UV atlas. For visibility-awareness, we use ambient occlusion\n(AO) as an exposure proxy and back-propagate a soft differentiable AO-weighted\nseam objective to steer cutting seams toward occluded regions. By conducting\nqualitative and quantitative evaluations against state-of-the-art methods, we\nshow that the proposed method produces UV atlases that better support texture\ngeneration and reduce perceptible seam artifacts compared to recent baselines.\nOur implementation code is publicly available at:\nhttps://github.com/AHHHZ975/Semantic-Visibility-UV-Param.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65e0\u76d1\u7763\u7684\u53ef\u5fae\u5206\u6846\u67b6\uff0c\u7528\u4e8e\u81ea\u52a8\u53163D\u7f51\u683c\u53c2\u6570\u5316\uff08UV\u6620\u5c04\uff09\uff0c\u7ed3\u5408\u8bed\u4e49\u548c\u53ef\u89c1\u6027\u611f\u77e5\u76ee\u6807\uff0c\u4ee5\u51cf\u5c11\u7eb9\u7406\u751f\u6210\u4e2d\u7684\u74f6\u9888\u548c\u7f1d\u7ebf\u4f2a\u5f71\u3002", "motivation": "\u73b0\u67093D\u751f\u6210\u6a21\u578b\u4f9d\u8d56\u624b\u52a8UV\u6620\u5c04\uff0c\u8fd9\u662f\u4e00\u4e2a\u8017\u65f6\u4e14\u9700\u8981\u4e13\u4e1a\u77e5\u8bc6\u7684\u8fc7\u7a0b\uff0c\u6210\u4e3a3D\u5185\u5bb9\u521b\u4f5c\u7684\u4e3b\u8981\u74f6\u9888\u3002\u73b0\u6709\u81ea\u52a8\u65b9\u6cd5\u5ffd\u89c6\u4e86\u8bed\u4e49\u4e00\u81f4\u6027\u548c\u53ef\u89c1\u6027\u611f\u77e5\u7684\u91cd\u8981\u6027\u3002", "method": "\u91c7\u7528\u65e0\u76d1\u7763\u53ef\u5fae\u5206\u6846\u67b6\uff0c\u7ed3\u5408\u8bed\u4e49\u5206\u5272\u548c\u53ef\u89c1\u6027\u611f\u77e5\u76ee\u6807\uff08\u4f7f\u7528\u73af\u5883\u906e\u853d\u4f5c\u4e3a\u4ee3\u7406\uff09\uff0c\u81ea\u52a8\u751f\u6210UV\u56fe\u96c6\u3002", "result": "\u6240\u63d0\u65b9\u6cd5\u5728\u5b9a\u6027\u548c\u5b9a\u91cf\u8bc4\u4f30\u4e2d\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u751f\u6210\u652f\u6301\u66f4\u597d\u7eb9\u7406\u751f\u6210\u4e14\u51cf\u5c11\u7f1d\u7ebf\u4f2a\u5f71\u7684UV\u56fe\u96c6\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u6210\u529f\u5b9e\u73b0\u4e86\u81ea\u52a8\u5316UV\u53c2\u6570\u5316\uff0c\u663e\u8457\u51cf\u5c11\u4e86\u4eba\u5de5\u5e72\u9884\u7684\u9700\u6c42\uff0c\u5e76\u63d0\u5347\u4e86\u7eb9\u7406\u751f\u6210\u6548\u679c\u3002"}}
{"id": "2509.25134", "categories": ["cs.GR", "cs.CV"], "pdf": "https://arxiv.org/pdf/2509.25134", "abs": "https://arxiv.org/abs/2509.25134", "authors": ["Tomoyuki Suzuki", "Kang-Jun Liu", "Naoto Inoue", "Kota Yamaguchi"], "title": "LayerD: Decomposing Raster Graphic Designs into Layers", "comment": "ICCV 2025, Project page: https://cyberagentailab.github.io/LayerD/ ,\n  GitHub: https://github.com/CyberAgentAILab/LayerD", "summary": "Designers craft and edit graphic designs in a layer representation, but\nlayer-based editing becomes impossible once composited into a raster image. In\nthis work, we propose LayerD, a method to decompose raster graphic designs into\nlayers for re-editable creative workflow. LayerD addresses the decomposition\ntask by iteratively extracting unoccluded foreground layers. We propose a\nsimple yet effective refinement approach taking advantage of the assumption\nthat layers often exhibit uniform appearance in graphic designs. As\ndecomposition is ill-posed and the ground-truth layer structure may not be\nreliable, we develop a quality metric that addresses the difficulty. In\nexperiments, we show that LayerD successfully achieves high-quality\ndecomposition and outperforms baselines. We also demonstrate the use of LayerD\nwith state-of-the-art image generators and layer-based editing.", "AI": {"tldr": "LayerD\u662f\u4e00\u79cd\u5c06\u6805\u683c\u56fe\u5f62\u8bbe\u8ba1\u5206\u89e3\u4e3a\u53ef\u91cd\u65b0\u7f16\u8f91\u7684\u56fe\u5c42\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u8fed\u4ee3\u63d0\u53d6\u672a\u88ab\u906e\u6321\u7684\u524d\u666f\u56fe\u5c42\uff0c\u5e76\u5229\u7528\u56fe\u5f62\u8bbe\u8ba1\u4e2d\u56fe\u5c42\u901a\u5e38\u5177\u6709\u7edf\u4e00\u5916\u89c2\u7684\u5047\u8bbe\u8fdb\u884c\u4f18\u5316\u3002", "motivation": "\u5f53\u524d\u4e00\u65e6\u56fe\u5f62\u8bbe\u8ba1\u88ab\u5408\u6210\u6805\u683c\u56fe\u50cf\uff0c\u5c31\u65e0\u6cd5\u8fdb\u884c\u57fa\u4e8e\u56fe\u5c42\u7684\u7f16\u8f91\uff0c\u9650\u5236\u4e86\u8bbe\u8ba1\u7684\u7075\u6d3b\u6027\u548c\u53ef\u4fee\u6539\u6027\u3002", "method": "LayerD\u901a\u8fc7\u8fed\u4ee3\u63d0\u53d6\u672a\u88ab\u906e\u6321\u7684\u524d\u666f\u56fe\u5c42\uff0c\u5e76\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u56fe\u5c42\u5916\u89c2\u4e00\u81f4\u6027\u7684\u4f18\u5316\u65b9\u6cd5\u3002\u6b64\u5916\uff0c\u8fd8\u5f00\u53d1\u4e86\u4e00\u79cd\u8d28\u91cf\u5ea6\u91cf\u6765\u8bc4\u4f30\u5206\u89e3\u7ed3\u679c\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cLayerD\u80fd\u591f\u9ad8\u8d28\u91cf\u5730\u5b8c\u6210\u5206\u89e3\u4efb\u52a1\uff0c\u5e76\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\u3002\u5b83\u8fd8\u5c55\u793a\u4e86\u4e0e\u5148\u8fdb\u56fe\u50cf\u751f\u6210\u5668\u548c\u56fe\u5c42\u7f16\u8f91\u5de5\u5177\u7684\u7ed3\u5408\u4f7f\u7528\u3002", "conclusion": "LayerD\u901a\u8fc7\u5206\u89e3\u6805\u683c\u56fe\u50cf\u4e3a\u53ef\u7f16\u8f91\u7684\u56fe\u5c42\uff0c\u663e\u8457\u63d0\u5347\u4e86\u56fe\u5f62\u8bbe\u8ba1\u7684\u7075\u6d3b\u6027\u548c\u53ef\u4fee\u6539\u6027\uff0c\u5177\u6709\u5b9e\u9645\u5e94\u7528\u4ef7\u503c\u3002"}}
