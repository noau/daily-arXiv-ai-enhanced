<div id=toc></div>

# Table of Contents

- [cs.GR](#cs.GR) [Total: 3]
- [cs.PL](#cs.PL) [Total: 5]
- [cs.GT](#cs.GT) [Total: 7]


<div id='cs.GR'></div>

# cs.GR [[Back]](#toc)

### [1] [Wavelet-Space Super-Resolution for Real-Time Rendering](https://arxiv.org/abs/2508.16024)
*Prateek Poudel,Prashant Aryal,Kirtan Kunwar,Navin Nepal,Dinesh Bania Kshatri*

Main category: cs.GR

TL;DR: 论文提出了一种基于小波空间特征分解的神经超分辨率方法，通过分离低频和高频细节来提升纹理保留和结构一致性，显著提高了图像质量。


<details>
  <summary>Details</summary>
Motivation: 现有的RGB空间回归方法在神经超分辨率任务中难以同时保持纹理细节和结构一致性，因此作者探索了小波空间特征分解的潜力，以更好地处理不同频率的信息。

Method: 采用小波域表示，通过平稳小波变换（SWT）避免空间下采样，并预测小波系数，结合空间G缓冲和时序变形历史帧进行重建。

Result: 实验表明，使用SWT可以提升PSNR最多1.5 dB，平均减少LPIPS 17%，计算开销约为+24 ms，在高性能GPU上仍能保持实时性。

Conclusion: 小波域表示是一种有效且原理性强的方法，能够显著提升图形应用中神经超分辨率的感知质量。

Abstract: We investigate the use of wavelet-space feature decomposition in neural
super-resolution for rendering pipelines. Building on the DFASR framework, we
introduce a wavelet-domain representation that separates low- and
high-frequency details before reconstruction, enabling the network to better
preserve fine textures while maintaining structural consistency. Unlike
RGB-space regression, our approach leverages the stationary wavelet transform
(SWT) to avoid spatial down-sampling, ensuring alignment across subbands and
preserving shift invariance. The model predicts wavelet coefficients
conditioned on spatial G-buffers and temporally warped history frames, which
are then recombined through inverse wavelet synthesis. We conduct a
comprehensive ablation study across wavelet families, transform types, and
architectural variants, showing that incorporating SWT improves PSNR by up to
1.5 dB and reduces LPIPS by 17% on average, at a computational overhead of
roughly +24 ms compared to out DFASR baseline. While absolute runtimes on our
RTX 3050 mobile GPU are higher ( 141ms) than the original DFASR report on RTX
4090( 11ms), the relative overhead remains modest, suggesting that on
higher-end GPUs our method would also remain real-time capable. Taken together,
our results suggest that wavelet-domain representations are a principled and
effective way to enhance perceptual quality in neural upscaling for graphics
applications.

</details>


### [2] [Audio2Face-3D: Audio-driven Realistic Facial Animation For Digital Avatars](https://arxiv.org/abs/2508.16401)
*NVIDIA,:,Chaeyeon Chung,Ilya Fedorov,Michael Huang,Aleksey Karmanov,Dmitry Korobchenko,Roger Ribera,Yeongho Seol*

Main category: cs.GR

TL;DR: NVIDIA Audio2Face-3D 是一个音频驱动的面部动画系统，支持实时交互，并提供开源工具以帮助开发者生成逼真的面部动画。


<details>
  <summary>Details</summary>
Motivation: 为数字角色和游戏开发者提供一种高效、逼真的面部动画生成解决方案，以增强游戏角色与用户的交互体验。

Method: 系统包括数据采集、网络架构设计、重定向方法，并提供了开源网络、SDK、训练框架和示例数据集。

Result: Audio2Face-3D 实现了实时交互，并在游戏角色面部动画生成中表现出高效性和逼真性。

Conclusion: 通过开源工具和系统的实用化，Audio2Face-3D 为数字角色创作者和游戏开发者提供了强大的支持，推动了面部动画技术的发展。

Abstract: Audio-driven facial animation presents an effective solution for animating
digital avatars. In this paper, we detail the technical aspects of NVIDIA
Audio2Face-3D, including data acquisition, network architecture, retargeting
methodology, evaluation metrics, and use cases. Audio2Face-3D system enables
real-time interaction between human users and interactive avatars, facilitating
facial animation authoring for game characters. To assist digital avatar
creators and game developers in generating realistic facial animations, we have
open-sourced Audio2Face-3D networks, SDK, training framework, and example
dataset.

</details>


### [3] [Real-time 3D Light-field Viewing with Eye-tracking on Conventional Displays](https://arxiv.org/abs/2508.16535)
*Trung Hieu Pham,Chanh Minh Tran,Eiji Kamioka,Xuan Tan Phan*

Main category: cs.GR

TL;DR: 提出了一种低成本系统，使用标准2D显示器、RGB摄像头和红青色立体眼镜实现实时3D光场观看，通过实时眼球追踪动态调整显示内容，适用于教育等领域。


<details>
  <summary>Details</summary>
Motivation: 现有的3D视觉体验设备昂贵且专业，限制了其在资源受限环境中的普及和使用。

Method: 系统整合了实时眼球追踪技术，通过轻量级渲染管道从预捕获的光场数据中选择和合成立体视图，使用红青色立体眼镜生成实时更新的3D图像。

Result: 系统完全运行在CPU上，能稳定保持30 FPS的帧率，证明了其在普通消费级硬件上的可行性。

Conclusion: 该方法为教育、数字媒体等领域的互动3D应用提供了一个可访问的平台。

Abstract: Creating immersive 3D visual experiences typically requires expensive and
specialized hardware such as VR headsets, autostereoscopic displays, or active
shutter glasses. These constraints limit the accessibility and everyday use of
3D visualization technologies in resource-constrained settings. To address
this, we propose a low-cost system that enables real-time 3D light-field
viewing using only a standard 2D monitor, a conventional RGB webcam, and
red-cyan anaglyph glasses. The system integrates real-time eye-tracking to
dynamically adapt the displayed light-field image to the user's head position
with a lightweight rendering pipeline that selects and composites stereoscopic
views from pre-captured light-field data. The resulting anaglyph image is
updated in real-time, creating a more immersive and responsive 3D experience.
The system operates entirely on CPU and maintains a stable frame rate of 30
FPS, confirming its feasibility on typical consumer-grade hardware. All of
these highlight the potential of our approach as an accessible platform for
interactive 3D applications in education, digital media, and beyond.

</details>


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [4] [Correctness-Guaranteed Code Generation via Constrained Decoding](https://arxiv.org/abs/2508.15866)
*Lingxiao Li,Salar Rahili,Yiwei Zhao*

Main category: cs.PL

TL;DR: 该论文提出了一种约束解码算法，结合上下文敏感解析器动态生成语义正确的程序，适用于运行时关键领域如视频游戏和机器人技术。


<details>
  <summary>Details</summary>
Motivation: 语言模型用于代码生成时难以确保生成程序的正确性，尤其在视频游戏和机器人技术等需要一次性正确的领域，亟需一种方法保证语义和运行时的正确性。

Method: 提出了一种约束解码算法，结合动态解析器树（ToP），通过上下文敏感的语法分析和上下文信息（如变量作用域和类型约束）指导生成正确的令牌序列。

Result: 在强类型Lua变体sLua中验证了算法的有效性，成功生成了符合预设脚本API的语义正确程序，并进一步在生成游戏机制的实验中验证了运行时正确性。

Conclusion: 该方法能够生成语义和运行时正确的程序，尤其适用于需要一次性正确性的关键领域，为代码生成的可靠性提供了新的解决方案。

Abstract: Language Models (LMs) are increasingly being used for code generation, but
ensuring the correctness of generated programs remains a significant challenge.
Although imperfect code may be acceptable during software development with
human oversight, domains such as video games and robotics require one-shot
correctness for runtime-critical components. We present a constrained decoding
algorithm for generating semantically correct programs that incorporates a
context-sensitive parser, which, at each step, outputs a regular expression
that satisfies a critical non-extensible property to guide the generation of
the next token sequence that can continue to a correct program. To build such a
context-sensitive parser, we propose a framework of a dynamic tree of parsers
(ToP) during parsing, where each parser corresponds to a modular context-free
grammar enriched with contextual information such as variable scopes and type
constraints, with tree branches representing ambiguity in the future code
segment. We demonstrate our approach through sLua, a strongly typed variant of
Lua, showing that our method can generate semantically correct programs
conforming to any prescribed scripting API. We further show that, with careful
design, our semantic guarantees extend to runtime correctness, as validated in
the application of generating game mechanics for a roguelike video game.

</details>


### [5] [Automated Formal Verification of a Software Fault Isolation System](https://arxiv.org/abs/2508.15898)
*Matthew Sotoudeh,Zachary Yedidia*

Main category: cs.PL

TL;DR: 该论文通过自动形式化验证轻量级故障隔离(LFI)系统，确保SFI验证器不会因漏洞而破坏沙盒安全模型。


<details>
  <summary>Details</summary>
Motivation: 软件故障隔离(SFI)是沙盒不可信软件的常用方法，但其验证器的漏洞可能导致安全模型失效，因此需要确保验证器的正确性。

Method: 论文采用自动形式化验证技术，对轻量级故障隔离(LFI)系统进行验证，确保其验证器接受的程序不会读写沙盒区域外的内存。

Result: 研究成功验证了LFI系统验证器的正确性，证明了其能够有效防止沙盒代码访问受保护内存。

Conclusion: 通过形式化验证，论文解决了SFI验证器漏洞的安全问题，增强了SFI系统的可靠性。

Abstract: Software fault isolation (SFI) is a popular way to sandbox untrusted
software. A key component of SFI is the verifier that checks the untrusted code
is written in a subset of the machine language that guarantees it never reads
or writes outside of a region of memory dedicated to the sandbox. Soundness
bugs in the SFI verifier would break the SFI security model and allow the
supposedly sandboxed code to read protected memory. In this paper, we address
the concern of SFI verifier bugs by performing an automated formal verification
of a recent SFI system called Lightweight Fault Isolation (LFI). In particular,
we formally verify that programs accepted by the LFI verifier never read or
write to memory outside of a designated sandbox region.

</details>


### [6] [Synthesizing DSLs for Few-Shot Learning](https://arxiv.org/abs/2508.16063)
*Paul Krogmeier,P. Madhusudan*

Main category: cs.PL

TL;DR: 研究了在符号领域中为少样本学习合成领域特定语言（DSL）的问题，证明了该问题在特定语言类别下是可判定的。


<details>
  <summary>Details</summary>
Motivation: 解决在符号领域中少样本学习的问题，通过合成DSL来提高学习效率和泛化能力。

Method: 基于基础语言和少样本学习问题的实例，通过语法合成，生成能够保证训练样本解也适用于测试样本的语法。

Result: 证明了在特定语言类别（即语义可通过树自动机评估的语言）下，该问题是可判定的，且解对应的语法对应于一组规则的树。

Conclusion: 该研究为符号领域中的少样本学习提供了一种可行的DSL合成方法，并在理论上证明了其可判定性。

Abstract: We study the problem of synthesizing domain-specific languages (DSLs) for
few-shot learning in symbolic domains. Given a base language and instances of
few-shot learning problems, where each instance is split into training and
testing samples, the DSL synthesis problem asks for a grammar over the base
language that guarantees that small expressions solving training samples also
solve corresponding testing samples. We prove that the problem is decidable for
a class of languages whose semantics over fixed structures can be evaluated by
tree automata and when expression size corresponds to parse tree depth in the
grammar, and, furthermore, the grammars solving the problem correspond to a
regular set of trees. We also prove decidability results for variants of the
problem where DSLs are only required to express solutions for input learning
problems and where DSLs are defined using macro grammars.

</details>


### [7] [Leveraging Large Language Models to Detect Missed Peephole Optimizations](https://arxiv.org/abs/2508.16125)
*Zhenyang Xu,Hongxu Xu,Yongqiang Tian,Xintong Zhou,Chengnian Sun*

Main category: cs.PL

TL;DR: Lampo框架利用大型语言模型（LLMs）和严格的正确性验证工具，通过反馈驱动的迭代过程，成功发现并验证了多个LLVM中遗漏的窥孔优化。


<details>
  <summary>Details</summary>
Motivation: 窥孔优化是编译器优化中的关键类别，但由于指令集的复杂性和多样性，发现新的有效窥孔优化具有挑战性。现有方法要么扩展性不足，要么只能捕获有限的优化。

Method: Lampo框架结合LLMs的创造性代码优化能力和翻译验证工具的严格正确性验证，通过反馈驱动的迭代过程，自动发现和验证窥孔优化。

Result: 在LLVM生态系统中，Lampo平均成功检测出17个之前报告为遗漏的优化，而其他方法如Souper仅发现15个。此外，Lampo在七个月内发现了26个遗漏优化，其中15个被确认，6个已修复。

Conclusion: Lampo展示了在持续检测窥孔优化方面的强大潜力，为编译器优化领域提供了新的自动化工具。

Abstract: By replacing small, suboptimal instruction sequences within programs with a
more efficient equivalent, peephole optimization can not only directly optimize
code size and performance, but also potentially enables further transformations
in the subsequent optimization pipeline. Although peephole optimization is a
critical class of compiler optimizations, discovering new and effective
peephole optimizations is challenging as the instruction sets can be extremely
complex and diverse. Previous methods either do not scale well or can only
capture a limited subset of peephole optimizations. In this work, we leverage
Large Language Models (LLMs) to detect missed peephole optimizations. We
propose Lampo, a novel automated framework that synergistically combines the
creative but unreliable code optimization ability of LLMs with rigorous
correctness verification performed by translation validation tools, integrated
in a feedback-driven iterative process. Through a comprehensive evaluation
within LLVM ecosystems, we show that Lampo can successfully detect up to 17 out
of 25 previously reported missed optimizations in LLVM on average, and that 22
out of 25 can potentially be found by Lampo with different LLMs. For
comparison, the state-of-the-art superoptimizer for LLVM, Souper, identified 15
of them. Moreover, within seven months of development and intermittent
experiments, Lampo found 26 missed peephole optimizations, 15 of which have
been confirmed and 6 already fixed. These results demonstrate Lampo's strong
potential in continuously detecting missed peephole optimizations.

</details>


### [8] [On the Duality of Task and Actor Programming Models](https://arxiv.org/abs/2508.16522)
*Rohan Yadav,Joseph Guman,Sean Treichler,Michael Garland,Alex Aiken,Fredrik Kjolstad,Michael Bauer*

Main category: cs.PL

TL;DR: 论文探讨任务模型和角色模型在分布式和异构机器上的编程模型，展示它们是彼此的二元对偶，并提出提高任务模型性能的技术。


<details>
  <summary>Details</summary>
Motivation: 现代工作负载需求增长，任务模型和角色模型分别提供不同的开发效率与性能权衡，研究旨在探讨二者的对偶性及性能优化。

Method: 论文通过理论分析和实验验证任务模型与角色模型的对偶性，并开发技术优化任务模型的性能，应用于Realm和Legion运行时系统。

Result: 实验表明，这些技术使Realm的开销减少了1.7-5.3倍，接近角色模型的性能；Legion应用的强扩展性提升了1.3-5.0倍。

Conclusion: 任务模型和角色模型在功能和性能上具有对偶性，优化后的任务模型在不牺牲开发效率的情况下接近角色模型的性能。

Abstract: Programming models for distributed and heterogeneous machines are rapidly
growing in popularity to meet the demands of modern workloads. Task and actor
models are common choices that offer different trade-offs between development
productivity and achieved performance. Task-based models offer better
productivity and composition of software, whereas actor-based models routinely
deliver better peak performance due to lower overheads. While task-based and
actor-based models appear to be different superficially, we demonstrate these
programming models are duals of each other. Importantly, we show that this
duality extends beyond functionality to performance, and elucidate techniques
that let task-based systems deliver performance competitive with actor-based
systems without compromising productivity. We apply these techniques to both
Realm, an explicitly parallel task-based runtime, as well as Legion, an
implicitly parallel task-based runtime. We show these techniques reduce Realm's
overheads by between 1.7-5.3x, coming within a factor of two of the overheads
imposed by heavily optimized actor-based systems like Charm++ and MPI. We
further show that our techniques enable between 1.3-5.0x improved strong
scaling of unmodified Legion applications.

</details>


<div id='cs.GT'></div>

# cs.GT [[Back]](#toc)

### [9] [Ransomware Negotiation: Dynamics and Privacy-Preserving Mechanism Design](https://arxiv.org/abs/2508.15844)
*Haohui Zhang,Sirui Shen,Xinyu Hu,Chenglu Jin*

Main category: cs.GT

TL;DR: 该论文通过有限期交替出价议价模型分析了现代勒索软件攻击中攻击者与受害者之间的互动，提出了一种贝叶斯激励兼容机制，以促进快速达成公平赎金协议，并使用安全两方计算保护双方隐私。


<details>
  <summary>Details</summary>
Motivation: 勒索软件攻击造成巨大经济损失，但攻击者与受害者的谈判动态在先前研究中未得到充分探索。论文旨在填补这一空白，分析谈判对双方策略的影响，并提出解决方案以减少谈判时间和成本。

Method: 论文采用有限期交替出价议价模型分析攻击者与受害者的互动，设计了贝叶斯激励兼容机制，并使用基于混淆电路的安全两方计算实现这一机制。

Result: 分析表明，不完全信息会延长谈判时间并增加受害者成本。提出的机制能够在保护隐私的同时，促进双方快速达成公平赎金协议。

Conclusion: 论文提出了首个基于勒索软件谈判动态形式化分析的自动隐私保护谈判机制，为应对勒索软件攻击提供了新思路。

Abstract: Ransomware attacks have become a pervasive and costly form of cybercrime,
causing tens of millions of dollars in losses as organizations increasingly pay
ransoms to mitigate operational disruptions and financial risks. While prior
research has largely focused on proactive defenses, the post-infection
negotiation dynamics between attackers and victims remains underexplored. This
paper presents a formal analysis of attacker-victim interactions in modern
ransomware incidents using a finite-horizon alternating-offers bargaining game
model. Our analysis demonstrates how bargaining alters the optimal strategies
of both parties. In practice, incomplete information-attackers lacking
knowledge of victims' data valuations and victims lacking knowledge of
attackers' reservation ransoms-can prolong negotiations and increase victims'
business interruption costs. To address this, we design a Bayesian
incentive-compatible mechanism that facilitates rapid agreement on a fair
ransom without requiring either party to disclose private valuations. We
further implement this mechanism using secure two-party computation based on
garbled circuits, thereby eliminating the need for trusted intermediaries and
preserving the privacy of both parties throughout the negotiation. To the best
of our knowledge, this is the first automated, privacy-preserving negotiation
mechanism grounded in a formal analysis of ransomware negotiation dynamics.

</details>


### [10] [Data Auctions for Retrieval Augmented Generation](https://arxiv.org/abs/2508.16007)
*Minbiao Han,Seyed A. Esmaeili,Michael Albert,Haifeng Xu*

Main category: cs.GT

TL;DR: 研究生成式AI应用中RAG任务的数据销售问题，提出基于覆盖率的估值函数，设计近似算法并引入数据燃烧技术以实现激励兼容性。


<details>
  <summary>Details</summary>
Motivation: 解决数据控制和收入最大化问题，特别是在数据集只能分配给单一买家的场景下。

Method: 提出覆盖率估值函数，设计多项式时间(1-1/e)近似算法，并通过数据燃烧技术实现激励兼容性。

Result: 算法在合成和真实数据集的实验中表现优于组合拍卖基线算法。

Conclusion: 数据燃烧技术有效平衡了近似算法的效率与激励兼容性，为数据销售问题提供实用解决方案。

Abstract: We study the problem of data selling for Retrieval Augmented Generation (RAG)
tasks in Generative AI applications. We model each buyer's valuation of a
dataset with a natural coverage-based valuation function that increases with
the inclusion of more relevant data points that would enhance responses to
anticipated queries. Motivated by issues such as data control and prior-free
revenue maximization, we focus on the scenario where each data point can be
allocated to only one buyer. We show that the problem of welfare maximization
in this setting is NP-hard even with two bidders, but design a polynomial-time
$(1-1/e)$ approximation algorithm for any number of bidders. Unfortunately,
however, this efficient allocation algorithm fails to be incentive compatible.
The crux of our approach is a carefully tailored post-processing step called
\emph{data burning} which retains the $(1-1/e)$ approximation factor but
achieves incentive compatibility. Our thorough experiments on synthetic and
real-world image and text datasets demonstrate the practical effectiveness of
our algorithm compared to popular baseline algorithms for combinatorial
auctions.

</details>


### [11] [Proportional Representation in Rank Aggregation](https://arxiv.org/abs/2508.16177)
*Patrick Lederer*

Main category: cs.GT

TL;DR: 论文提出了一种名为“比例顺序博尔达规则”的简单社会福利函数，用于在排名聚合任务中确保每个输入排名的比例代表性，并引入了两种变体以满足不同公平性需求。


<details>
  <summary>Details</summary>
Motivation: 传统的排名聚合方法（社会福利函数）倾向于多数主义，无法满足比例排名的需求，因此需要设计能保证比例代表性的新方法。

Method: 论文设计了一种名为比例顺序博尔达规则的简单社会福利函数，并引入了两种变体：排名平等份额法和流量调整博尔达规则，分别满足不同的公平性条件。

Result: 提出的比例顺序博尔达规则及其变体能够满足比例代表性的公平条件，其中流量调整博尔达规则还满足更强的公平性要求。

Conclusion: 论文成功设计了一组满足比例代表性的社会福利函数，解决了传统排名聚合方法的不足，为相关领域提供了新思路。

Abstract: In rank aggregation, the task is to aggregate multiple weighted input
rankings into a single output ranking. While numerous methods, so-called social
welfare functions (SWFs), have been suggested for this problem, all of the
classical SWFs tend to be majoritarian and are thus not acceptable when a
proportional ranking is required. Motivated by this observation, we will design
SWFs that guarantee that every input ranking is proportionally represented by
the output ranking. Specifically, our central fairness condition requires that
the number of pairwise comparisons between candidates on which an input ranking
and the output ranking agree is proportional to the weight of the input
ranking. As our main contribution, we present a simple SWF called the
Proportional Sequential Borda rule, which satisfies this condition. Moreover,
we introduce two variants of this rule: the Ranked Method of Equal Shares,
which has a more utilitarian flavor while still satisfying our fairness
condition, and the Flow-adjusting Borda rule, which satisfies an even stronger
fairness condition. Many of our axioms and techniques are inspired by results
on approval-based committee voting and participatory budgeting, where the
concept of proportional representation has been studied in depth.

</details>


### [12] [Strategyproof Randomized Social Choice for Restricted Sets of Utility Functions](https://arxiv.org/abs/2508.16195)
*Patrick Lederer*

Main category: cs.GT

TL;DR: 本文提出了一种新的策略证明性概念$U$-strategyproofness，通过限制效用函数集$U$，研究了其在社会决策方案(SDSs)中的权衡，尤其是与决策性概念的关系，并探讨了其与Condorcet一致性的不兼容性。


<details>
  <summary>Details</summary>
Motivation: Gibbard(1977)的结果表明传统的策略证明性SDSs要么不公平，要么需要大量随机化。为了绕过这一负面结果，作者提出$U$-strategyproofness概念，仅限制特定效用函数集$U$下的选民无法操纵。

Method: 作者通过理论分析，研究了$U$-strategyproofness与决策性概念的权衡，例如在效用函数集$U$偏好强烈时，SDSs可以实现高决策性；并通过反例证明了$U$-strategyproofness与Condorcet一致性的不兼容性。

Result: 研究显示，当效用函数集$U$强烈偏好最优选项时，存在$U$-strategyproof SDSs能够在大多数选民支持时实现确定性选择；但$U$-strategyproofness与Condorcet一致性在最小对称条件下不兼容。

Conclusion: 本文表明，$U$-strategyproofness为规避Gibbard的负面结果提供了可能，但在特定条件下仍存在限制，尤其是与经典一致性的冲突问题。

Abstract: Social decision schemes (SDSs) map the voters' preferences over multiple
alternatives to a probability distribution over these alternatives. In a
seminal result, Gibbard (1977) has characterized the set of SDSs that are
strategyproof with respect to all utility functions and his result implies that
all such SDSs are either unfair to the voters or alternatives, or they require
a significant amount of randomization. To circumvent this negative result, we
propose the notion of $U$-strategyproofness which postulates that only voters
with a utility function in a predefined set $U$ cannot manipulate. We then
analyze the tradeoff between $U$-strategyproofness and various decisiveness
notions that restrict the amount of randomization of SDSs. In particular, we
show that if the utility functions in the set $U$ value the best alternative
much more than other alternatives, there are $U$-strategyproof SDSs that choose
an alternative with probability $1$ whenever all but $k$ voters rank it first.
On the negative side, we demonstrate that $U$-strategyproofness is incompatible
with Condorcet-consistency if the set $U$ satisfies minimal symmetry
conditions. Finally, we show that no ex post efficient and $U$-strategyproof
SDS can be significantly more decisive than the uniform random dictatorship if
the voters are close to indifferent between their two favorite alternatives.

</details>


### [13] [Limit-Computable Grains of Truth for Arbitrary Computable Extensive-Form (Un)Known Games](https://arxiv.org/abs/2508.16245)
*Cole Wyeth,Marcus Hutter,Jan Leike,Jessica Taylor*

Main category: cs.GT

TL;DR: 该论文提出了一个解决经典“真理颗粒”问题的通用方法，构造了一个包含所有可计算策略的策略类，并在已知和未知环境中证明了收敛性。


<details>
  <summary>Details</summary>
Motivation: 解决Kalai和Lehrer提出的“真理颗粒”问题，即寻找一个足够大的策略类，使得贝叶斯最优策略能够包含其中，并满足贝叶斯推断的一致性要求。

Method: 论文构造了一个广泛的策略类，包含所有可计算策略，并为每个合理的先验分布构建贝叶斯最优策略。在已知重复阶段游戏中，证明了类似[KL93a]和[KL93b]的收敛性；在未知环境中，使用Thompson采样的代理收敛到ε-纳什均衡。

Result: 证明了在已知环境中策略的收敛性，以及在未知环境中代理收敛到ε-纳什均衡。此外，还展示了解决方案可以通过计算近似。

Conclusion: 论文通过构造广泛的策略类解决了“真理颗粒”问题，并证明了在不同环境下的收敛性，为贝叶斯学习和多代理环境中的策略选择提供了理论基础。

Abstract: A Bayesian player acting in an infinite multi-player game learns to predict
the other players' strategies if his prior assigns positive probability to
their play (or contains a grain of truth). Kalai and Lehrer's classic grain of
truth problem is to find a reasonably large class of strategies that contains
the Bayes-optimal policies with respect to this class, allowing
mutually-consistent beliefs about strategy choice that obey the rules of
Bayesian inference. Only small classes are known to have a grain of truth and
the literature contains several related impossibility results. In this paper we
present a formal and general solution to the full grain of truth problem: we
construct a class of strategies wide enough to contain all computable
strategies as well as Bayes-optimal strategies for every reasonable prior over
the class. When the "environment" is a known repeated stage game, we show
convergence in the sense of [KL93a] and [KL93b]. When the environment is
unknown, agents using Thompson sampling converge to play $\varepsilon$-Nash
equilibria in arbitrary unknown computable multi-agent environments. Finally,
we include an application to self-predictive policies that avoid planning.
While these results use computability theory only as a conceptual tool to solve
a classic game theory problem, we show that our solution can naturally be
computationally approximated arbitrarily closely.

</details>


### [14] [A QoE-Driven Personalized Incentive Mechanism Design for AIGC Services in Resource-Constrained Edge Networks](https://arxiv.org/abs/2508.16251)
*Hongjia Wu,Minrui Xu,Zehui Xiong,Lin Gao,Haoyuan Pan,Dusit Niyato,Tse-Tin Chan*

Main category: cs.GT

TL;DR: 该论文提出了一种基于移动边缘计算（MEC）的多维QoE度量方法，并通过激励机制优化AIGC服务的个性化提供，显著降低了计算和通信开销。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型（LLMs）的快速发展，个性化AIGC服务的需求日益增长，但受限于用户主观需求和资源约束，服务提供商面临挑战。

Method: 开发了一种多维QoE度量方法，并提出了一种基于EPEC的QoE驱动激励机制，通过双扰动奖励优化算法实现资源分配优化。

Result: 实验结果表明，该机制将平均计算和通信开销降低了约64.9%，用户服务成本和资源消耗分别减少了66.5%和76.8%。

Conclusion: 该研究为个性化AIGC服务提供了有效的解决方案，显著提升了服务效率并降低了资源消耗。

Abstract: With rapid advancements in large language models (LLMs), AI-generated content
(AIGC) has emerged as a key driver of technological innovation and economic
transformation. Personalizing AIGC services to meet individual user demands is
essential but challenging for AIGC service providers (ASPs) due to the
subjective and complex demands of mobile users (MUs), as well as the
computational and communication resource constraints faced by ASPs. To tackle
these challenges, we first develop a novel multi-dimensional
quality-of-experience (QoE) metric. This metric comprehensively evaluates AIGC
services by integrating accuracy, token count, and timeliness. We focus on a
mobile edge computing (MEC)-enabled AIGC network, consisting of multiple ASPs
deploying differentiated AIGC models on edge servers and multiple MUs with
heterogeneous QoE requirements requesting AIGC services from ASPs. To
incentivize ASPs to provide personalized AIGC services under MEC resource
constraints, we propose a QoE-driven incentive mechanism. We formulate the
problem as an equilibrium problem with equilibrium constraints (EPEC), where
MUs as leaders determine rewards, while ASPs as followers optimize resource
allocation. To solve this, we develop a dual-perturbation reward optimization
algorithm, reducing the implementation complexity of adaptive pricing.
Experimental results demonstrate that our proposed mechanism achieves a
reduction of approximately $64.9\%$ in average computational and communication
overhead, while the average service cost for MUs and the resource consumption
of ASPs decrease by $66.5\%$ and $76.8\%$, respectively, compared to
state-of-the-art benchmarks.

</details>


### [15] [A Social Choice Analysis of Optimism's Retroactive Project Funding](https://arxiv.org/abs/2508.16285)
*Eyal Briman,Nimrod Talmon,Angela Kreitenweis,Muhammad Idrees*

Main category: cs.GT

TL;DR: Optimism Retroactive Project Funding（RetroPGF）是一个区块链项目，通过DAO管理，为对社区有价值的项目提供资金支持。研究发现当前分配系统存在不足，并提出了基于计算社会选择技术的改进方案。


<details>
  <summary>Details</summary>
Motivation: 当前RetroPGF的资金分配系统存在显著不足，特别是在资金规模庞大的情况下，需要更好的治理机制来优化资金分配。

Method: 提出采用utilitarian moving phantoms机制来改进投票流程，该机制由Freeman等人于2019年提出，旨在提高社会福利并满足策略证明性。

Result: 研究为DAO的资金机制设计提供了一个正式的框架，增强了去中心化治理和公共物品分配的讨论。

Conclusion: 通过计算社会选择技术改进的投票机制，可以显著优化RetroPGF的资金分配，适应其治理需求。

Abstract: The Optimism Retroactive Project Funding (RetroPGF) is a key initiative
within the blockchain ecosystem that retroactively rewards projects deemed
valuable to the Ethereum and Optimism communities. Managed by the Optimism
Collective, a decentralized autonomous organization (DAO), RetroPGF represents
a large-scale experiment in decentralized governance. Funding rewards are
distributed in OP tokens, the native digital currency of the ecosystem. As of
this writing, four funding rounds have been completed, collectively allocating
over 100M dollars, with an additional 1.3B dollars reserved for future rounds.
However, we identify significant shortcomings in the current allocation system,
underscoring the need for improved governance mechanisms given the scale of
funds involved.
  Leveraging computational social choice techniques and insights from
multiagent systems, we propose improvements to the voting process by
recommending the adoption of a utilitarian moving phantoms mechanism. This
mechanism, originally introduced by Freeman et al. in 2019, is designed to
enhance social welfare (using the L1 norm) while satisfying strategyproofness
-- two key properties aligned with the application's governance requirements.
Our analysis provides a formal framework for designing improved funding
mechanisms for DAOs, contributing to the broader discourse on decentralized
governance and public goods allocation.

</details>
