<div id=toc></div>

# Table of Contents

- [cs.GR](#cs.GR) [Total: 9]
- [cs.PL](#cs.PL) [Total: 2]
- [cs.GT](#cs.GT) [Total: 2]


<div id='cs.GR'></div>

# cs.GR [[Back]](#toc)

### [1] [SeHDR: Single-Exposure HDR Novel View Synthesis via 3D Gaussian Bracketing](https://arxiv.org/abs/2509.20400)
*Yiyu Li,Haoyuan Wang,Ke Xu,Gerhard Petrus Hancke,Rynson W. H. Lau*

Main category: cs.GR

TL;DR: SeHDR提出了一种新的高动态范围3D高斯飞溅方法，能够从单一曝光的低动态范围多视图图像生成HDR新视图。


<details>
  <summary>Details</summary>
Motivation: 现有方法需要不同曝光的多视图图像，这导致采集繁琐且易产生误差（如物体运动模糊和校准问题）。SeHDR旨在解决这一问题，仅需单一曝光的多视图图像即可学习HDR场景表示。

Method: SeHDR首先从单一曝光的低动态范围输入学习基础3D高斯分布，然后估计具有相同几何但不同线性颜色的3D高斯分布，并通过可微神经曝光融合技术（NeEF）将其合并成HDR高斯分布。

Result: 实验证明SeHDR优于现有方法和精心设计的基线模型，能够有效地生成高质量的HDR新视图。

Conclusion: SeHDR通过创新的方法解决了单一曝光输入学习HDR场景的难题，为高动态范围视图合成提供了高效的解决方案。

Abstract: This paper presents SeHDR, a novel high dynamic range 3D Gaussian Splatting
(HDR-3DGS) approach for generating HDR novel views given multi-view LDR images.
Unlike existing methods that typically require the multi-view LDR input images
to be captured from different exposures, which are tedious to capture and more
likely to suffer from errors (e.g., object motion blurs and
calibration/alignment inaccuracies), our approach learns the HDR scene
representation from multi-view LDR images of a single exposure. Our key insight
to this ill-posed problem is that by first estimating Bracketed 3D Gaussians
(i.e., with different exposures) from single-exposure multi-view LDR images, we
may then be able to merge these bracketed 3D Gaussians into an HDR scene
representation. Specifically, SeHDR first learns base 3D Gaussians from
single-exposure LDR inputs, where the spherical harmonics parameterize colors
in a linear color space. We then estimate multiple 3D Gaussians with identical
geometry but varying linear colors conditioned on exposure manipulations.
Finally, we propose the Differentiable Neural Exposure Fusion (NeEF) to
integrate the base and estimated 3D Gaussians into HDR Gaussians for novel view
rendering. Extensive experiments demonstrate that SeHDR outperforms existing
methods as well as carefully designed baselines.

</details>


### [2] [SGAligner++: Cross-Modal Language-Aided 3D Scene Graph Alignment](https://arxiv.org/abs/2509.20401)
*Binod Singh,Sayan Deb Sarkar,Iro Armeni*

Main category: cs.GR

TL;DR: SGAligner++是一个跨模态、语言辅助的框架，用于3D场景图对齐，解决了异构模态下部分重叠场景观测的挑战，通过统一联合嵌入空间学习实现高精度对齐。


<details>
  <summary>Details</summary>
Motivation: 当前的3D场景图对齐方法通常依赖单模态点云数据，难以应对不完整或噪声输入的问题，限制了其在机器人导航和感知应用中的效果。

Method: SGAligner++通过轻量级单模态编码器和基于注意力的融合技术，学习一个统一的联合嵌入空间，以提升场景理解能力，同时确保计算效率和可扩展性。

Result: 在真实世界数据集上的大量评估显示，SGAligner++在噪声场景重建任务中的性能优于现有最佳方法最多40%，并实现了跨模态泛化能力。

Conclusion: SGAligner++为解决异构模态和低重叠条件下的3D场景图对齐问题提供了一种高效且可扩展的解决方案，显著提升了机器人导航和感知任务的性能。

Abstract: Aligning 3D scene graphs is a crucial initial step for several applications
in robot navigation and embodied perception. Current methods in 3D scene graph
alignment often rely on single-modality point cloud data and struggle with
incomplete or noisy input. We introduce SGAligner++, a cross-modal,
language-aided framework for 3D scene graph alignment. Our method addresses the
challenge of aligning partially overlapping scene observations across
heterogeneous modalities by learning a unified joint embedding space, enabling
accurate alignment even under low-overlap conditions and sensor noise. By
employing lightweight unimodal encoders and attention-based fusion, SGAligner++
enhances scene understanding for tasks such as visual localization, 3D
reconstruction, and navigation, while ensuring scalability and minimal
computational overhead. Extensive evaluations on real-world datasets
demonstrate that SGAligner++ outperforms state-of-the-art methods by up to 40%
on noisy real-world reconstructions, while enabling cross-modal generalization.

</details>


### [3] [SceneWeaver: All-in-One 3D Scene Synthesis with an Extensible and Self-Reflective Agent](https://arxiv.org/abs/2509.20414)
*Yandan Yang,Baoxiong Jia,Shujie Zhang,Siyuan Huang*

Main category: cs.GR

TL;DR: SceneWeaver是一种反射性代理框架，通过工具迭代优化统一多样化的室内场景合成方法，利用语言模型选择生成工具，实现物理合理性和视觉真实性的提升。


<details>
  <summary>Details</summary>
Motivation: 随着Embodied AI的兴起，需要视觉逼真、物理合理且功能多样的3D环境，但现有方法在视觉细节和物理一致性上仍有不足，且难以满足复杂用户指令的需求。

Method: SceneWeaver采用语言模型规划器选择场景生成工具，通过数据驱动生成模型、视觉和LLM方法，实现语义一致性和物理合理性的迭代优化。

Result: 实验表明，SceneWeaver在物理、视觉和语义指标上优于现有方法，并能有效泛化到复杂场景和多样指令。

Conclusion: SceneWeaver标志着向通用3D环境生成迈进的一步，通过灵活的工具选择和迭代优化实现高质量场景合成。

Abstract: Indoor scene synthesis has become increasingly important with the rise of
Embodied AI, which requires 3D environments that are not only visually
realistic but also physically plausible and functionally diverse. While recent
approaches have advanced visual fidelity, they often remain constrained to
fixed scene categories, lack sufficient object-level detail and physical
consistency, and struggle to align with complex user instructions. In this
work, we present SceneWeaver, a reflective agentic framework that unifies
diverse scene synthesis paradigms through tool-based iterative refinement. At
its core, SceneWeaver employs a language model-based planner to select from a
suite of extensible scene generation tools, ranging from data-driven generative
models to visual- and LLM-based methods, guided by self-evaluation of physical
plausibility, visual realism, and semantic alignment with user input. This
closed-loop reason-act-reflect design enables the agent to identify semantic
inconsistencies, invoke targeted tools, and update the environment over
successive iterations. Extensive experiments on both common and open-vocabulary
room types demonstrate that SceneWeaver not only outperforms prior methods on
physical, visual, and semantic metrics, but also generalizes effectively to
complex scenes with diverse instructions, marking a step toward general-purpose
3D environment generation. Project website: https://scene-weaver.github.io/.

</details>


### [4] [ArtUV: Artist-style UV Unwrapping](https://arxiv.org/abs/2509.20710)
*Yuguang Chen,Xinhai Liu,Yang Li,Victor Cheung,Zhuo Chen,Dongyu Zhang,Chunchao Guo*

Main category: cs.GR

TL;DR: ArtUV是一种全自动的、端到端的方法，用于生成艺术家风格的UV展开图。它通过模拟专业UV映射过程，分为两个阶段：表面缝预测和艺术家风格的UV参数化。


<details>
  <summary>Details</summary>
Motivation: 现有的UV展开方法存在耗时、碎片化、缺乏语义性和不规则UV岛等问题，限制了其实际应用。ArtUV旨在解决这些问题，生成满足艺术家需求的UV图。

Method: ArtUV分为两个阶段：1) 使用SeamGPT预测语义上有意义的切割缝；2) 通过Auto-Encoder优化粗糙的UV图，生成艺术家风格的UV图。

Result: ArtUV在多个基准测试中表现出色，既能作为专业渲染工具的插件，也能作为独立系统快速生成高质量的UV图。

Conclusion: ArtUV通过全自动化的流程生成了语义一致且拓扑结构保留的UV图，满足了艺术家的需求，提高了实用性和编辑便捷性。

Abstract: UV unwrapping is an essential task in computer graphics, enabling various
visual editing operations in rendering pipelines. However, existing UV
unwrapping methods struggle with time-consuming, fragmentation, lack of
semanticity, and irregular UV islands, limiting their practical use. An
artist-style UV map must not only satisfy fundamental criteria, such as
overlap-free mapping and minimal distortion, but also uphold higher-level
standards, including clean boundaries, efficient space utilization, and
semantic coherence. We introduce ArtUV, a fully automated, end-to-end method
for generating artist-style UV unwrapping. We simulates the professional UV
mapping process by dividing it into two stages: surface seam prediction and
artist-style UV parameterization. In the seam prediction stage, SeamGPT is used
to generate semantically meaningful cutting seams. Then, in the
parameterization stage, a rough UV obtained from an optimization-based method,
along with the mesh, is fed into an Auto-Encoder, which refines it into an
artist-style UV map. Our method ensures semantic consistency and preserves
topological structure, making the UV map ready for 2D editing. We evaluate
ArtUV across multiple benchmarks and show that it serves as a versatile
solution, functioning seamlessly as either a plug-in for professional rendering
tools or as a standalone system for rapid, high-quality UV generation.

</details>


### [5] [SeamCrafte: Enhancing Mesh Seam Generation for Artist UV Unwrapping via Reinforcement Learning](https://arxiv.org/abs/2509.20725)
*Duoteng Xu,Yuguang Chen,Jing Li,Xinhai Liu,Xueqi Ma,Zhuo Chen,Dongyu Zhang,Chunchao Guo*

Main category: cs.GR

TL;DR: SeamCrafter是一种基于GPT风格的自回归缝线生成器，通过点云输入生成高质量缝线，显著降低UV扭曲和碎片化，提升纹理映射效果。


<details>
  <summary>Details</summary>
Motivation: 现有的缝线生成方法常常在高扭曲和碎片化之间权衡，导致纹理合成困难并影响艺术家的工作流程。

Method: SeamCrafter采用双分支点云编码器，分离并捕获拓扑和几何信息，并通过直接偏好优化（DPO）在偏好数据集上进行微调。

Result: 实验表明，SeamCrafter生成的缝线在UV扭曲和碎片化方面表现优于现有方法，同时保持了拓扑一致性和视觉保真度。

Conclusion: SeamCrafter通过创新的编码器和优化方法，有效解决了缝线生成中的关键问题，为3D表面分割和纹理映射提供了高效解决方案。

Abstract: Mesh seams play a pivotal role in partitioning 3D surfaces for UV
parametrization and texture mapping. Poorly placed seams often result in severe
UV distortion or excessive fragmentation, thereby hindering texture synthesis
and disrupting artist workflows. Existing methods frequently trade one failure
mode for another-producing either high distortion or many scattered islands. To
address this, we introduce SeamCrafter, an autoregressive GPT-style seam
generator conditioned on point cloud inputs. SeamCrafter employs a dual-branch
point-cloud encoder that disentangles and captures complementary topological
and geometric cues during pretraining. To further enhance seam quality, we
fine-tune the model using Direct Preference Optimization (DPO) on a preference
dataset derived from a novel seam-evaluation framework. This framework assesses
seams primarily by UV distortion and fragmentation, and provides pairwise
preference labels to guide optimization. Extensive experiments demonstrate that
SeamCrafter produces seams with substantially lower distortion and
fragmentation than prior approaches, while preserving topological consistency
and visual fidelity.

</details>


### [6] [ARMesh: Autoregressive Mesh Generation via Next-Level-of-Detail Prediction](https://arxiv.org/abs/2509.20824)
*Jiabao Lei,Kewei Shi,Zhihao Liang,Kui Jia*

Main category: cs.GR

TL;DR: 本文提出了一种渐进式自回归方法生成3D网格，通过从粗到细的方式逐步增加几何细节，避免了传统按字典顺序生成网格的局限性。


<details>
  <summary>Details</summary>
Motivation: 传统自回归网格生成模型按字典顺序逐面构建网格，无法有效捕获符合人类感知的几何结构，因此需要更自然的生成方式。

Method: 将网格简化算法视为从细到粗的过程，并基于Transformer模型开发了一种从单点开始、逐步添加几何细节的自回归网格生成方法。

Result: 实验表明，该渐进式方法不仅能直观控制生成质量和时间消耗，还支持网格细化和编辑等应用。

Conclusion: 渐进式网格生成方法在生成质量和应用灵活性上优于传统方法，为3D建模提供了新的可能性。

Abstract: Directly generating 3D meshes, the default representation for 3D shapes in
the graphics industry, using auto-regressive (AR) models has become popular
these days, thanks to their sharpness, compactness in the generated results,
and ability to represent various types of surfaces. However, AR mesh generative
models typically construct meshes face by face in lexicographic order, which
does not effectively capture the underlying geometry in a manner consistent
with human perception. Inspired by 2D models that progressively refine images,
such as the prevailing next-scale prediction AR models, we propose generating
meshes auto-regressively in a progressive coarse-to-fine manner. Specifically,
we view mesh simplification algorithms, which gradually merge mesh faces to
build simpler meshes, as a natural fine-to-coarse process. Therefore, we
generalize meshes to simplicial complexes and develop a transformer-based AR
model to approximate the reverse process of simplification in the order of
level of detail, constructing meshes initially from a single point and
gradually adding geometric details through local remeshing, where the topology
is not predefined and is alterable. Our experiments show that this novel
progressive mesh generation approach not only provides intuitive control over
generation quality and time consumption by early stopping the auto-regressive
process but also enables applications such as mesh refinement and editing.

</details>


### [7] [ArchGPT: Understanding the World's Architectures with Large Multimodal Models](https://arxiv.org/abs/2509.20858)
*Yuze Wang,Luo Yang,Junyi Wang,Yue Qi*

Main category: cs.GR

TL;DR: ArchGPT是一种多模态建筑视觉问答(VQA)模型，通过可扩展的数据构建管道生成高质量的建筑专用VQA注释，并提供了Arch-300K数据集，用于改善VR/MR/AR系统在建筑领域的通用性。


<details>
  <summary>Details</summary>
Motivation: 现有VR/MR/AR系统在建筑领域的应用通常依赖于硬编码注释和任务特定交互，缺乏通用性和可扩展性。ArchGPT旨在通过高质量数据集和多模态模型解决这一问题。

Method: 通过多阶段流程构建Arch-300K数据集，包括从Wikimedia Commons等来源筛选建筑图像，利用LLM引导的文本验证和知识蒸馏生成可靠问答对。随后基于数据集对ShareGPT4V-7B进行监督微调，得到ArchGPT模型。

Result: ArchGPT能够提供更高质量的视觉问答能力，Arch-300K数据集包含约315,000个图像-问答三元组，显著提升了建筑领域VQA任务的性能和通用性。

Conclusion: ArchGPT及其数据构建管道的提出，为建筑领域的沉浸式技术和知识传播提供了更高效和可扩展的解决方案。

Abstract: Architecture embodies aesthetic, cultural, and historical values, standing as
a tangible testament to human civilization. Researchers have long leveraged
virtual reality (VR), mixed reality (MR), and augmented reality (AR) to enable
immersive exploration and interpretation of architecture, enhancing
accessibility, public understanding, and creative workflows around architecture
in education, heritage preservation, and professional design practice. However,
existing VR/MR/AR systems are often developed case-by-case, relying on
hard-coded annotations and task-specific interactions that do not scale across
diverse built environments. In this work, we present ArchGPT, a multimodal
architectural visual question answering (VQA) model, together with a scalable
data-construction pipeline for curating high-quality, architecture-specific VQA
annotations. This pipeline yields Arch-300K, a domain-specialized dataset of
approximately 315,000 image-question-answer triplets. Arch-300K is built via a
multi-stage process: first, we curate architectural scenes from Wikimedia
Commons and filter unconstrained tourist photo collections using a novel
coarse-to-fine strategy that integrates 3D reconstruction and semantic
segmentation to select occlusion-free, structurally consistent architectural
images. To mitigate noise and inconsistency in raw textual metadata, we propose
an LLM-guided text verification and knowledge-distillation pipeline to generate
reliable, architecture-specific question-answer pairs. Using these curated
images and refined metadata, we further synthesize formal analysis
annotations-including detailed descriptions and aspect-guided conversations-to
provide richer semantic variety while remaining faithful to the data. We
perform supervised fine-tuning of an open-source multimodal backbone
,ShareGPT4V-7B, on Arch-300K, yielding ArchGPT.

</details>


### [8] [Marching Neurons: Accurate Surface Extraction for Neural Implicit Shapes](https://arxiv.org/abs/2509.21007)
*Christian Stippel,Felix Mujkanovic,Thomas Leimkühler,Pedro Hermosilla*

Main category: cs.GR

TL;DR: 提出了一种从神经隐式函数中解析提取表面的新方法，通过深度优先遍历策略高效跟踪编码表面，实现了高准确性和竞争速度。


<details>
  <summary>Details</summary>
Motivation: 准确的表面几何表示在3D视觉计算中至关重要。现有的隐式表示转换方法（如Marching Cubes算法）由于固定和有限的分辨率导致不准确性，因此需要一种更高效且准确的方法。

Method: 提出了一种新颖的解析方法，利用深度优先遍历策略高效跟踪神经隐式函数中编码的表面，无需空间离散化，且适用于并行计算和大规模神经网络架构。

Result: 生成的网格忠实捕捉了网络的完整几何信息，实现了前所未有的准确性，同时在多样化的形状和网络架构中保持了竞争性的速度。

Conclusion: 该方法为神经隐式函数的表面提取提供了高精度和效率的解决方案，克服了传统方法的局限性。

Abstract: Accurate surface geometry representation is crucial in 3D visual computing.
Explicit representations, such as polygonal meshes, and implicit
representations, like signed distance functions, each have distinct advantages,
making efficient conversions between them increasingly important. Conventional
surface extraction methods for implicit representations, such as the widely
used Marching Cubes algorithm, rely on spatial decomposition and sampling,
leading to inaccuracies due to fixed and limited resolution. We introduce a
novel approach for analytically extracting surfaces from neural implicit
functions. Our method operates natively in parallel and can navigate large
neural architectures. By leveraging the fact that each neuron partitions the
domain, we develop a depth-first traversal strategy to efficiently track the
encoded surface. The resulting meshes faithfully capture the full geometric
information from the network without ad-hoc spatial discretization, achieving
unprecedented accuracy across diverse shapes and network architectures while
maintaining competitive speed.

</details>


### [9] [CHARM: Control-point-based 3D Anime Hairstyle Auto-Regressive Modeling](https://arxiv.org/abs/2509.21114)
*Yuze He,Yanning Zhou,Wang Zhao,Jingwen Ye,Yushi Bai,Kaiwen Xiao,Yong-Jin Liu,Zhongqian Sun,Wei Yang*

Main category: cs.GR

TL;DR: CHARM是一种用于动漫发型建模的新型参数化表示和生成框架，通过控制点参数化和自回归生成模型实现高效且高质量的建模。


<details>
  <summary>Details</summary>
Motivation: 传统的发型建模方法专注于真实头发，使用基于线或体积的表示方法，而动漫发型具有高度风格化和分段结构化的几何特征，现有技术难以处理。

Method: CHARM提出了一种紧凑、可逆的控制点参数化表示，每个发卡由一系列控制点表示，并通过自回归生成框架从输入图像或点云生成动漫发型。

Result: 实验表明，CHARM在重建精度和生成质量上均达到最先进的性能，提供了一个表达性强且可扩展的解决方案。

Conclusion: CHARM通过创新的参数化表示和生成框架，成功解决了动漫发型建模的高效性和准确性挑战，并构建了一个大规模数据集AnimeHair支持训练和评估。

Abstract: We present CHARM, a novel parametric representation and generative framework
for anime hairstyle modeling. While traditional hair modeling methods focus on
realistic hair using strand-based or volumetric representations, anime
hairstyle exhibits highly stylized, piecewise-structured geometry that
challenges existing techniques. Existing works often rely on dense mesh
modeling or hand-crafted spline curves, making them inefficient for editing and
unsuitable for scalable learning. CHARM introduces a compact, invertible
control-point-based parameterization, where a sequence of control points
represents each hair card, and each point is encoded with only five geometric
parameters. This efficient and accurate representation supports both
artist-friendly design and learning-based generation. Built upon this
representation, CHARM introduces an autoregressive generative framework that
effectively generates anime hairstyles from input images or point clouds. By
interpreting anime hairstyles as a sequential "hair language", our
autoregressive transformer captures both local geometry and global hairstyle
topology, resulting in high-fidelity anime hairstyle creation. To facilitate
both training and evaluation of anime hairstyle generation, we construct
AnimeHair, a large-scale dataset of 37K high-quality anime hairstyles with
separated hair cards and processed mesh data. Extensive experiments demonstrate
state-of-the-art performance of CHARM in both reconstruction accuracy and
generation quality, offering an expressive and scalable solution for anime
hairstyle modeling. Project page: https://hyzcluster.github.io/charm/

</details>


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [10] [Dual-Language General-Purpose Self-Hosted Visual Language and new Textual Programming Language for Applications](https://arxiv.org/abs/2509.20426)
*Mahmoud Samir Fayed*

Main category: cs.PL

TL;DR: PWCT2是一个双语言（阿拉伯语/英语）、通用的、自我托管的可视化编程语言，通过Ring文本编程语言开发，提供了36倍的代码生成速度和20倍的存储效率，并能将Ring代码转换为可视化代码。


<details>
  <summary>Details</summary>
Motivation: 通用可视化编程语言（VPL）如PWCT通常通过文本编程语言开发和改进，限制了其发展。因此，需要一个自我托管的VPL，能够通过自身进行开发。

Method: 首先设计了Ring文本编程语言，用于开发PWCT2。Ring具有动态类型和轻量级实现，支持语法定制和领域特定语言的创建。PWCT2通过Ring开发，包含了92,000行Ring代码和394个可视化组件。

Result: PWCT2实现了36倍的代码生成速度提升，存储需求减少了20倍，并能自我托管。通过Steam平台分发给用户，获得了积极的反馈和使用记录。

Conclusion: PWCT2成功地实现了自我托管的高效可视化编程语言开发，并通过用户反馈验证了其潜力，为进一步研究和开发提供了基础。

Abstract: Most visual programming languages (VPLs) are domain-specific, with few
general-purpose VPLs like Programming Without Coding Technology (PWCT). These
general-purpose VPLs are developed using textual programming languages and
improving them requires textual programming. In this thesis, we designed and
developed PWCT2, a dual-language (Arabic/English), general-purpose,
self-hosting visual programming language. Before doing so, we specifically
designed a textual programming language called Ring for its development. Ring
is a dynamically typed language with a lightweight implementation, offering
syntax customization features. It permits the creation of domain-specific
languages through new features that extend object-oriented programming,
allowing for specialized languages resembling Cascading Style Sheets (CSS) or
Supernova language. The Ring Compiler and Virtual Machine are designed using
the PWCT visual programming language where the visual implementation is
composed of 18,945 components that generate 24,743 lines of C code, which
increases the abstraction level and hides unnecessary details. Using PWCT to
develop Ring allowed us to realize several issues in PWCT, which led to the
development of the PWCT2 visual programming language using the Ring textual
programming language. PWCT2 provides approximately 36 times faster code
generation and requires 20 times less storage for visual source files. It also
allows for the conversion of Ring code into visual code, enabling the creation
of a self-hosting VPL that can be developed using itself. PWCT2 consists of
approximately 92,000 lines of Ring code and comes with 394 visual components.
PWCT2 is distributed to many users through the Steam platform and has received
positive feedback, On Steam, 1772 users have launched the software, and the
total recorded usage time exceeds 17,000 hours, encouraging further research
and development.

</details>


### [11] [Efficient Symbolic Computation vis Hash Consing](https://arxiv.org/abs/2509.20534)
*Bowen Zhu,Aayush Sabharwal,Songchen Tan,Yingbo Ma,Alan Edelman,Christopher Rackauckas*

Main category: cs.PL

TL;DR: 本文介绍了在JuliaSymbolics中首次集成哈希一致化技术，通过全局弱引用哈希表消除冗余表达式，显著提升了内存效率和计算性能。


<details>
  <summary>Details</summary>
Motivation: 解决符号计算系统中的内存效率问题，特别是表达式膨胀导致的性能下降，以提升经典计算机代数和AI驱动的数学推理工具的性能。

Method: 在JuliaSymbolics中集成哈希一致化技术，使用全局弱引用哈希表对表达式进行规范化并消除重复存储。

Result: 性能显著提升：符号计算速度提高3.2倍，内存使用减少2倍，代码生成快5倍，函数编译快10倍，数值评估快100倍。

Conclusion: 哈希一致化技术在扩展符号计算中具有重要作用，并为未来与e-graphs集成以增强AI驱动管道的等价感知表达式共享奠定基础。

Abstract: Symbolic computation systems suffer from memory inefficiencies due to
redundant storage of structurally identical subexpressions, commonly known as
expression swell, which degrades performance in both classical computer algebra
and emerging AI-driven mathematical reasoning tools. In this paper, we present
the first integration of hash consing into JuliaSymbolics, a high-performance
symbolic toolkit in Julia, by employing a global weak-reference hash table that
canonicalizes expressions and eliminates duplication. This approach reduces
memory consumption and accelerates key operations such as differentiation,
simplification, and code generation, while seamlessly integrating with Julia's
metaprogramming and just-in-time compilation infrastructure. Benchmark
evaluations across different computational domains reveal substantial
improvements: symbolic computations are accelerated by up to 3.2 times, memory
usage is reduced by up to 2 times, code generation is up to 5 times faster,
function compilation up to 10 times faster, and numerical evaluation up to 100
times faster for larger models. While certain workloads with fewer duplicate
unknown-variable expressions show more modest gains or even slight overhead in
initial computation stages, downstream processing consistently benefits
significantly. These findings underscore the importance of hash consing in
scaling symbolic computation and pave the way for future work integrating hash
consing with e-graphs for enhanced equivalence-aware expression sharing in
AI-driven pipelines.

</details>


<div id='cs.GT'></div>

# cs.GT [[Back]](#toc)

### [12] [Efficient Kernelized Learning in Polyhedral Games Beyond Full-Information: From Colonel Blotto to Congestion Games](https://arxiv.org/abs/2509.20919)
*Andreas Kontogiannis,Vasilis Pollatos,Gabriele Farina,Panayotis Mertikopoulos,Ioannis Panageas*

Main category: cs.GT

TL;DR: 论文研究了在多面体博弈中高效学习粗相关均衡（CCE）的问题，针对行动集规模较大的博弈（如Blotto博弈和拥塞博弈），提出了一种基于核化框架的算法，显著提升了部分信息下的学习效率。


<details>
  <summary>Details</summary>
Motivation: 多面体博弈因其行动集规模庞大且具有组合结构，现有学习方法在部分信息下的计算复杂度较高，难以实际应用。论文旨在解决这一问题，提升学习CCE的效率。

Method: 通过构建基于核化（kernelization）的框架，并将其应用到Blotto博弈、图拟阵博弈和网络拥塞博弈中，提出了一种计算高效的基于收益的学习算法。

Result: 论文提出的算法显著提升了在多面体博弈中学习CCE的运行时间，优于现有方法。

Conclusion: 核化框架为多面体博弈中的CCE学习提供了一种高效的解决方案，尤其在部分信息场景下表现突出。

Abstract: We examine the problem of efficiently learning coarse correlated equilibria
(CCE) in polyhedral games, that is, normal-form games with an exponentially
large number of actions per player and an underlying combinatorial structure.
Prominent examples of such games are the classical Colonel Blotto and
congestion games. To achieve computational efficiency, the learning algorithms
must exhibit regret and per-iteration complexity that scale polylogarithmically
in the size of the players' action sets. This challenge has recently been
addressed in the full-information setting, primarily through the use of
kernelization. However, in the case of the realistic, but mathematically
challenging, partial-information setting, existing approaches result in
suboptimal and impractical runtime complexity to learn CCE. We tackle this
limitation by building a framework based on the kernelization paradigm. We
apply this framework to prominent examples of polyhedral games -- namely the
Colonel Blotto, graphic matroid and network congestion games -- and provide
computationally efficient payoff-based learning algorithms, which significantly
improve upon prior works in terms of the runtime for learning CCE in these
settings.

</details>


### [13] [A Category Theoretic Approach to Approximate Game Theory](https://arxiv.org/abs/2509.20932)
*Neil Ghani*

Main category: cs.GT

TL;DR: 使用范畴论开发了一种全新的近似博弈论方法，研究在多智能体系统中如何做出近似最优决策。


<details>
  <summary>Details</summary>
Motivation: 博弈论研究多智能体系统中的决策问题，而近似博弈论关注的是在不确定性下如何做出近似最优决策。这在实践中非常重要，因为精确解可能难以或无法计算。

Method: 首先研究"选择函数"，开发了一个简单而鲁棒的近似均衡模型，并探讨了选择函数的代数性质及其组合结构。随后将此方法应用于更高级的Open Games模型。

Result: 成功开发了基于选择函数的近似均衡模型，并扩展到了Open Games模型中。

Conclusion: 范畴论为近似博弈论提供了新的理论框架，能够有效处理不确定性下的决策问题。

Abstract: This paper uses category theory to develop an entirely new approach to
approximate game theory. Game theory is the study of how different agents
within a multi-agent system take decisions. At its core, game theory asks what
an optimal decision is in a given scenario. Thus approximate game theory asks
what is an approximately optimal decision in a given scenario. This is
important in practice as -- just like in much of computing -- exact answers
maybe too difficult to compute or even impossible to compute given inherent
uncertainty in input.
  We consider first "Selection Functions" which are functions and develop a
simple yet robust model of approximate equilibria. We develop the algebraic
properties of approximation wrt selection functions and also relate
approximation to the compositional structure of selection functions. We then
repeat this process successfully for Open Games -- a more advanced model of
game theory.

</details>
