{"id": "2509.00040", "categories": ["cs.GR", "cs.RO"], "pdf": "https://arxiv.org/pdf/2509.00040", "abs": "https://arxiv.org/abs/2509.00040", "authors": ["Chengkai Dai", "Tao Liu", "Dezhao Guo", "Binzhi Sun", "Guoxin Fang", "Yeung Yam", "Charlie C. L. Wang"], "title": "Curve-based slicer for multi-axis DLP 3D printing", "comment": null, "summary": "This paper introduces a novel curve-based slicing method for generating\nplanar layers with dynamically varying orientations in digital light processing\n(DLP) 3D printing. Our approach effectively addresses key challenges in DLP\nprinting, such as regions with large overhangs and staircase artifacts, while\npreserving its intrinsic advantages of high resolution and fast printing\nspeeds. We formulate the slicing problem as an optimization task, in which\nparametric curves are computed to define both the slicing layers and the model\npartitioning through their tangent planes. These curves inherently define\nmotion trajectories for the build platform and can be optimized to meet\ncritical manufacturing objectives, including collision-free motion and\nfloating-free deposition. We validate our method through physical experiments\non a robotic multi-axis DLP printing setup, demonstrating that the optimized\ncurves can robustly guide smooth, high-quality fabrication of complex\ngeometries."}
{"id": "2509.00052", "categories": ["cs.GR", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2509.00052", "abs": "https://arxiv.org/abs/2509.00052", "authors": ["Jianzhi Long", "Wenhao Sun", "Rongcheng Tu", "Dacheng Tao"], "title": "Lightning Fast Caching-based Parallel Denoising Prediction for Accelerating Talking Head Generation", "comment": null, "summary": "Diffusion-based talking head models generate high-quality, photorealistic\nvideos but suffer from slow inference, limiting practical applications.\nExisting acceleration methods for general diffusion models fail to exploit the\ntemporal and spatial redundancies unique to talking head generation. In this\npaper, we propose a task-specific framework addressing these inefficiencies\nthrough two key innovations. First, we introduce Lightning-fast Caching-based\nParallel denoising prediction (LightningCP), caching static features to bypass\nmost model layers in inference time. We also enable parallel prediction using\ncached features and estimated noisy latents as inputs, efficiently bypassing\nsequential sampling. Second, we propose Decoupled Foreground Attention (DFA) to\nfurther accelerate attention computations, exploiting the spatial decoupling in\ntalking head videos to restrict attention to dynamic foreground regions.\nAdditionally, we remove reference features in certain layers to bring extra\nspeedup. Extensive experiments demonstrate that our framework significantly\nimproves inference speed while preserving video quality."}
{"id": "2509.00180", "categories": ["cs.GR", "cs.CG"], "pdf": "https://arxiv.org/pdf/2509.00180", "abs": "https://arxiv.org/abs/2509.00180", "authors": ["Nguyen Phan", "Guoning Chen"], "title": "Evaluate Neighbor Search for Curve-based Vector Field Processing", "comment": "12 pages, 17 figures", "summary": "Curve-based representations, particularly integral curves, are often used to\nrepresent large-scale computational fluid dynamic simulations. Processing and\nanalyzing curve-based vector field data sets often involves searching for\nneighboring segments given a query point or curve segment. However, because the\noriginal flow behavior may not be fully represented by the set of integral\ncurves and the input integral curves may not be evenly distributed in space,\npopular neighbor search strategies often return skewed and redundant\nneighboring segments. Yet, there is a lack of systematic and comprehensive\nresearch on how different configurations of neighboring segments returned by\nspecific neighbor search strategies affect subsequent tasks. To fill this gap,\nthis study evaluates the performance of two popular neighbor search strategies\ncombined with different distance metrics on a point-based vector field\nreconstruction task and a segment saliency estimation using input integral\ncurves. A large number of reconstruction tests and saliency calculations are\nconducted for the study. To characterize the configurations of neighboring\nsegments for an effective comparison of different search strategies, a number\nof measures, like average neighbor distance and uniformity, are proposed. Our\nstudy leads to a few observations that partially confirm our expectations about\nthe ideal configurations of a neighborhood while revealing additional findings\nthat were overlooked by the community."}
{"id": "2509.00269", "categories": ["cs.GR", "cs.CV"], "pdf": "https://arxiv.org/pdf/2509.00269", "abs": "https://arxiv.org/abs/2509.00269", "authors": ["Maria Parelli", "Michael Oechsle", "Michael Niemeyer", "Federico Tombari", "Andreas Geiger"], "title": "3D-LATTE: Latent Space 3D Editing from Textual Instructions", "comment": null, "summary": "Despite the recent success of multi-view diffusion models for\ntext/image-based 3D asset generation, instruction-based editing of 3D assets\nlacks surprisingly far behind the quality of generation models. The main reason\nis that recent approaches using 2D priors suffer from view-inconsistent editing\nsignals. Going beyond 2D prior distillation methods and multi-view editing\nstrategies, we propose a training-free editing method that operates within the\nlatent space of a native 3D diffusion model, allowing us to directly manipulate\n3D geometry. We guide the edit synthesis by blending 3D attention maps from the\ngeneration with the source object. Coupled with geometry-aware regularization\nguidance, a spectral modulation strategy in the Fourier domain and a refinement\nstep for 3D enhancement, our method outperforms previous 3D editing methods\nenabling high-fidelity, precise, and robust edits across a wide range of shapes\nand semantic manipulations."}
{"id": "2509.00179", "categories": ["cs.GT", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.00179", "abs": "https://arxiv.org/abs/2509.00179", "authors": ["Daniel Ablin", "Alon Cohen"], "title": "Playing Markov Games Without Observing Payoffs", "comment": null, "summary": "Optimization under uncertainty is a fundamental problem in learning and\ndecision-making, particularly in multi-agent systems. Previously, Feldman,\nKalai, and Tennenholtz [2010] demonstrated the ability to efficiently compete\nin repeated symmetric two-player matrix games without observing payoffs, as\nlong as the opponents actions are observed. In this paper, we introduce and\nformalize a new class of zero-sum symmetric Markov games, which extends the\nnotion of symmetry from matrix games to the Markovian setting. We show that\neven without observing payoffs, a player who knows the transition dynamics and\nobserves only the opponents sequence of actions can still compete against an\nadversary who may have complete knowledge of the game. We formalize three\ndistinct notions of symmetry in this setting and show that, under these\nconditions, the learning problem can be reduced to an instance of online\nlearning, enabling the player to asymptotically match the return of the\nopponent despite lacking payoff observations. Our algorithms apply to both\nmatrix and Markov games, and run in polynomial time with respect to the size of\nthe game and the number of episodes. Our work broadens the class of games in\nwhich robust learning is possible under severe informational disadvantage and\ndeepens the connection between online learning and adversarial game theory."}
{"id": "2509.00360", "categories": ["cs.PL", "D.3.0"], "pdf": "https://arxiv.org/pdf/2509.00360", "abs": "https://arxiv.org/abs/2509.00360", "authors": ["Shaan Nagy", "Timothy Zhou", "Nadia Polikarpova", "Loris D'Antoni"], "title": "ChopChop: a Programmable Framework for Semantically Constraining the Output of Language Models", "comment": null, "summary": "Language models (LMs) can generate code, but cannot guarantee its\ncorrectness--producing outputs that often violate type safety, program\ninvariants, or semantic equivalence. Constrained decoding offers a solution by\nrestricting generation to programs that satisfy desired properties. Yet,\nexisting methods are limited to shallow syntactic constraints or rely on\nbrittle, ad hoc encodings of semantics over token sequences.\n  We present ChopChop, the first programmable framework for semantic\nconstrained decoding, enabling LMs to generate code that provably satisfies\nrich semantic properties. ChopChop connects token-level generation with\nreasoning over abstract program structures using a coinduction-based formalism\nand reduces constraint enforcement to a realizability problem over regular\ncodata. We demonstrate ChopChop's generality through generation constrained by\ntype safety and program equivalence, showing how formal methods can be\nseamlessly integrated into LM-driven code generation. ChopChop transforms\nsemantic constrained decoding from a niche technique into a systematic,\nprincipled extension of LMs--improving success rates across models and tasks\nwhile maintaining practical decoding latency."}
{"id": "2509.00406", "categories": ["cs.GR"], "pdf": "https://arxiv.org/pdf/2509.00406", "abs": "https://arxiv.org/abs/2509.00406", "authors": ["Ahmed H. Mahmoud", "Jonathan Ragan-Kelley", "Justin Solomon"], "title": "Locality-Aware Automatic Differentiation on the GPU for Mesh-Based Computations", "comment": null, "summary": "We present a high-performance system for automatic differentiation (AD) of\nfunctions defined on triangle meshes that exploits the inherent sparsity and\nlocality of mesh-based energy functions to achieve fast gradient and Hessian\ncomputation on the GPU. Our system is designed around per-element forward-mode\ndifferentiation, enabling all local computations to remain in GPU registers or\nshared memory. Unlike reverse-mode approaches that construct and traverse\nglobal computation graphs, our method performs differentiation on the fly,\nminimizing memory traffic and avoiding global synchronization. Our programming\nmodel allows users to define local energy terms while the system handles\nparallel evaluation, derivative computation, and sparse Hessian assembly. We\nbenchmark our system on a range of applications--cloth simulation, surface\nparameterization, mesh smoothing, and spherical manifold optimization. We\nachieve a geometric mean speedup of 6.2x over optimized PyTorch implementations\nfor second-order derivatives, and 2.76x speedup for Hessian-vector products.\nFor first-order derivatives, our system is 6.38x, 2.89x, and 1.98x faster than\nWarp, JAX, and Dr.JIT, respectively, while remaining on par with hand-written\nderivatives."}
{"id": "2509.00439", "categories": ["cs.GT"], "pdf": "https://arxiv.org/pdf/2509.00439", "abs": "https://arxiv.org/abs/2509.00439", "authors": ["Hau Chan", "Jianan Lin", "Chenhao Wang"], "title": "Strategyproof Mechanisms for Facility Location with Prediction Under the Maximum Cost Objective", "comment": "To appear in ECAI 2025", "summary": "We study the mechanism design problem of facility location on a metric space\nin the learning-augmented framework, where mechanisms have access to an\nimperfect prediction of optimal facility locations. Our goal is to design\nstrategyproof (SP) mechanisms to elicit agent preferences on the facility\nlocations truthfully and, leveraging the given imperfect prediction, determine\nthe facility location that approximately minimizes the maximum cost among all\nagents. In particular, we seek SP mechanisms whose approximation guarantees\ndepend on the prediction errors -- achieve improved guarantees when the\nprediction is accurate (known as the \\emph{consistency}), while still ensuring\nrobust worst-case performance when the prediction is arbitrarily inaccurate\n(known as the \\emph{robustness}).\n  When the metric space is the real line, we characterize all deterministic SP\nmechanisms with consistency strictly less than 2 and bounded robustness: such\nmechanisms must be the MinMaxP mechanism, which returns the prediction location\nif it lies between the two extreme agent locations and, otherwise, returns the\nclosest agent location to the prediction. We further show that, for any\nprediction error $\\eta\\ge 0$, while MinMaxP is $(1+\\min(1,\n\\eta))$-approximation, no deterministic SP mechanism can achieve a better\napproximation. In two-dimensional spaces with the $l_p$ metric, we analyze the\napproximation guarantees of a deterministic mechanism that runs MinMaxP\nindependently on each coordinate, as well as a randomized mechanism that\nselects between two deterministic ones with specific probabilities. Finally, we\ndiscuss the group strategyproofness of the considered mechanisms."}
{"id": "2509.00587", "categories": ["cs.PL"], "pdf": "https://arxiv.org/pdf/2509.00587", "abs": "https://arxiv.org/abs/2509.00587", "authors": ["Vaibhav Mehta", "Justin Hsu"], "title": "A Hoare Logic for Symmetry Properties", "comment": "Accepted to OOPSLA '25", "summary": "Many natural program correctness properties can be stated in terms of\n  symmetries, but existing formal methods have little support for reasoning\n  about such properties. We consider how to formally verify a broad class of\n  symmetry properties expressed in terms of group actions. To specify these\n  properties, we design a syntax for group actions, supporting standard\n  constructions and a natural notion of entailment. Then, we develop a\n  Hoare-style logic for verifying symmetry properties of imperative programs,\n  where group actions take the place of the typical pre- and post-condition\n  assertions. Finally, we develop a prototype tool $\\mathsf{SymVerif}$, and use\n  it to verify symmetry properties on a series of handcrafted benchmarks. Our\n  tool uncovered an error in a model of a dynamical system described by\n\\citet{McLachlan_Quispel_2002}."}
{"id": "2509.00541", "categories": ["cs.GR", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2509.00541", "abs": "https://arxiv.org/abs/2509.00541", "authors": ["Siyi Liu", "Weiming Chen", "Yushun Tang", "Zhihai He"], "title": "LatentEdit: Adaptive Latent Control for Consistent Semantic Editing", "comment": "Accepted by PRCV 2025", "summary": "Diffusion-based Image Editing has achieved significant success in recent\nyears. However, it remains challenging to achieve high-quality image editing\nwhile maintaining the background similarity without sacrificing speed or memory\nefficiency. In this work, we introduce LatentEdit, an adaptive latent fusion\nframework that dynamically combines the current latent code with a reference\nlatent code inverted from the source image. By selectively preserving source\nfeatures in high-similarity, semantically important regions while generating\ntarget content in other regions guided by the target prompt, LatentEdit enables\nfine-grained, controllable editing. Critically, the method requires no internal\nmodel modifications or complex attention mechanisms, offering a lightweight,\nplug-and-play solution compatible with both UNet-based and DiT-based\narchitectures. Extensive experiments on the PIE-Bench dataset demonstrate that\nour proposed LatentEdit achieves an optimal balance between fidelity and\neditability, outperforming the state-of-the-art method even in 8-15 steps.\nAdditionally, its inversion-free variant further halves the number of neural\nfunction evaluations and eliminates the need for storing any intermediate\nvariables, substantially enhancing real-time deployment efficiency."}
{"id": "2509.00506", "categories": ["cs.GT", "cs.FL", "cs.MA"], "pdf": "https://arxiv.org/pdf/2509.00506", "abs": "https://arxiv.org/abs/2509.00506", "authors": ["Guy Avni", "Suman Sadhukhan"], "title": "Mean-payoff and Energy Discrete Bidding Games", "comment": null, "summary": "A \\emph{bidding} game is played on a graph as follows. A token is placed on\nan initial vertex and both players are allocated budgets. In each turn, the\nplayers simultaneously submit bids that do not exceed their available budgets,\nthe higher bidder moves the token, and pays the bid to the lower bidder. We\nfocus on \\emph{discrete}-bidding, which are motivated by practical applications\nand restrict the granularity of the players' bids, e.g, bids must be given in\ncents. We study, for the first time, discrete-bidding games with {\\em\nmean-payoff} and {\\em energy} objectives. In contrast, mean-payoff {\\em\ncontinuous}-bidding games (i.e., no granularity restrictions) are understood\nand exhibit a rich mathematical structure. The {\\em threshold} budget is a\nnecessary and sufficient initial budget for winning an energy game or\nguaranteeing a target payoff in a mean-payoff game. We first establish\nexistence of threshold budgets; a non-trivial property due to the concurrent\nmoves of the players. Moreover, we identify the structure of the thresholds,\nwhich is key in obtaining compact strategies, and in turn, showing that finding\nthreshold is in \\NP~and \\coNP even in succinctly-represented games."}
{"id": "2509.00699", "categories": ["cs.PL"], "pdf": "https://arxiv.org/pdf/2509.00699", "abs": "https://arxiv.org/abs/2509.00699", "authors": ["Yumeng He", "Chandrakana Nandi", "Sreepathi Pai"], "title": "Formalizing Linear Motion G-code for Invariant Checking and Differential Testing of Fabrication Tools", "comment": null, "summary": "The computational fabrication pipeline for 3D printing is much like a\ncompiler - users design models in Computer Aided Design (CAD) tools that are\nlowered to polygon meshes to be ultimately compiled to machine code by 3D\nslicers. For traditional compilers and programming languages, techniques for\nchecking program invariants are well-established. Similarly, methods like\ndifferential testing are often used to uncover bugs in compilers themselves,\nwhich makes them more reliable. The fabrication pipeline would benefit from\nsimilar techniques but traditional approaches do not directly apply to the\nrepresentations used in this domain. Unlike traditional programs, 3D models\nexist both as geometric objects as well as machine code that ultimately runs on\nthe hardware. The machine code, like in traditional compiling, is affected by\nmany factors like the model, the slicer being used, and numerous\nuser-configurable parameters that control the slicing process. In this work, we\npropose a new algorithm for lifting G-code (a common language used in\nfabrication pipelines) by denoting a G-code program to a set of cuboids, and\nthen defining an approximate point cloud representation for efficiently\noperating on these cuboids. Our algorithm opens up new opportunities: we show\nthree use cases that demonstrate how it enables error localization in CAD\nmodels through invariant checking, quantitative comparisons between slicers,\nand evaluating the efficacy of mesh repair tools. We present a prototype\nimplementation of our algorithm in a tool, GlitchFinder, and evaluate it on 58\nreal-world CAD models. Our results show that GlitchFinder is particularly\neffective in identifying slicing issues due to small features, can highlight\ndifferences in how popular slicers (Cura and PrusaSlicer) slice the same model,\nand can identify cases where mesh repair tools (MeshLab and Meshmixer)\nintroduce new errors during repair."}
{"id": "2509.00777", "categories": ["cs.GR", "cs.CV"], "pdf": "https://arxiv.org/pdf/2509.00777", "abs": "https://arxiv.org/abs/2509.00777", "authors": ["Xiaokang Wei", "Zizheng Yan", "Zhangyang Xiong", "Yiming Hao", "Yipeng Qin", "Xiaoguang Han"], "title": "IntrinsicReal: Adapting IntrinsicAnything from Synthetic to Real Objects", "comment": null, "summary": "Estimating albedo (a.k.a., intrinsic image decomposition) from single RGB\nimages captured in real-world environments (e.g., the MVImgNet dataset)\npresents a significant challenge due to the absence of paired images and their\nground truth albedos. Therefore, while recent methods (e.g., IntrinsicAnything)\nhave achieved breakthroughs by harnessing powerful diffusion priors, they\nremain predominantly trained on large-scale synthetic datasets (e.g.,\nObjaverse) and applied directly to real-world RGB images, which ignores the\nlarge domain gap between synthetic and real-world data and leads to suboptimal\ngeneralization performance. In this work, we address this gap by proposing\nIntrinsicReal, a novel domain adaptation framework that bridges the\nabove-mentioned domain gap for real-world intrinsic image decomposition.\nSpecifically, our IntrinsicReal adapts IntrinsicAnything to the real domain by\nfine-tuning it using its high-quality output albedos selected by a novel dual\npseudo-labeling strategy: i) pseudo-labeling with an absolute confidence\nthreshold on classifier predictions, and ii) pseudo-labeling using the relative\npreference ranking of classifier predictions for individual input objects. This\nstrategy is inspired by human evaluation, where identifying the highest-quality\noutputs is straightforward, but absolute scores become less reliable for\nsub-optimal cases. In these situations, relative comparisons of outputs become\nmore accurate. To implement this, we propose a novel two-phase pipeline that\nsequentially applies these pseudo-labeling techniques to effectively adapt\nIntrinsicAnything to the real domain. Experimental results show that our\nIntrinsicReal significantly outperforms existing methods, achieving\nstate-of-the-art results for albedo estimation on both synthetic and real-world\ndatasets."}
{"id": "2509.01582", "categories": ["cs.GT", "cs.RO"], "pdf": "https://arxiv.org/pdf/2509.01582", "abs": "https://arxiv.org/abs/2509.01582", "authors": ["Karim Essalmi", "Fernando Garrido", "Fawzi Nashashibi"], "title": "Quantum game models for interaction-aware decision-making in automated driving", "comment": "8 pages, 8 figures, submitted to ICAR 2025", "summary": "Decision-making in automated driving must consider interactions with\nsurrounding agents to be effective. However, traditional methods often neglect\nor oversimplify these interactions because they are difficult to model and\nsolve, which can lead to overly conservative behavior of the ego vehicle. To\naddress this gap, we propose two quantum game models, QG-U1 (Quantum Game -\nUnitary 1) and QG-G4 (Quantum Game - Gates 4), for interaction-aware\ndecision-making. These models extend classical game theory by incorporating\nprinciples of quantum mechanics, such as superposition, interference, and\nentanglement. Specifically, QG-U1 and QG-G4 are designed for two-player games\nwith two strategies per player and can be executed in real time on a standard\ncomputer without requiring quantum hardware. We evaluate both models in merging\nand roundabout scenarios and compare them with classical game-theoretic methods\nand baseline approaches (IDM, MOBIL, and a utility-based technique). Results\nshow that QG-G4 achieves lower collision rates and higher success rates\ncompared to baseline methods, while both quantum models yield higher expected\npayoffs than classical game approaches under certain parameter settings."}
{"id": "2509.00948", "categories": ["cs.PL", "cs.FL"], "pdf": "https://arxiv.org/pdf/2509.00948", "abs": "https://arxiv.org/abs/2509.00948", "authors": ["Denghang Hu", "Taolue Chen", "Philipp Rümmer", "Fu Song", "Zhilin Wu"], "title": "Decision Procedure for A Theory of String Sequences", "comment": "21 pages, 2 tables, APLAS 2025", "summary": "The theory of sequences, supported by many SMT solvers, can model program\ndata types including bounded arrays and lists. Sequences are parameterized by\nthe element data type and provide operations such as accessing elements,\nconcatenation, forming sub-sequences and updating elements. Strings and\nsequences are intimately related; many operations, e.g., matching a string\naccording to a regular expression, splitting strings, or joining strings in a\nsequence, are frequently used in string-manipulating programs. Nevertheless,\nthese operations are typically not directly supported by existing SMT solvers,\nwhich instead only consider the generic theory of sequences. In this paper, we\npropose a theory of string sequences and study its satisfiability. We show\nthat, while it is undecidable in general, the decidability can be recovered by\nrestricting to the straight-line fragment. This is shown by encoding each\nstring sequence as a string, and each string sequence operation as a\ncorresponding string operation. We provide pre-image computation for the\nresulting string operations with respect to automata, effectively casting it\ninto the generic OSTRICH string constraint solving framework. We implement the\nnew decision procedure as a tool $\\ostrichseq$, and carry out experiments on\nbenchmark constraints generated from real-world JavaScript programs,\nhand-crafted templates and unit tests. The experiments confirm the efficacy of\nour approach."}
{"id": "2509.01134", "categories": ["cs.GR", "cs.CV"], "pdf": "https://arxiv.org/pdf/2509.01134", "abs": "https://arxiv.org/abs/2509.01134", "authors": ["Xilong Zhou", "Pedro Figueiredo", "Miloš Hašan", "Valentin Deschaintre", "Paul Guerrero", "Yiwei Hu", "Nima Khademi Kalantari"], "title": "RealMat: Realistic Materials with Diffusion and Reinforcement Learning", "comment": "11 pages, 11 figures", "summary": "Generative models for high-quality materials are particularly desirable to\nmake 3D content authoring more accessible. However, the majority of material\ngeneration methods are trained on synthetic data. Synthetic data provides\nprecise supervision for material maps, which is convenient but also tends to\ncreate a significant visual gap with real-world materials. Alternatively,\nrecent work used a small dataset of real flash photographs to guarantee\nrealism, however such data is limited in scale and diversity. To address these\nlimitations, we propose RealMat, a diffusion-based material generator that\nleverages realistic priors, including a text-to-image model and a dataset of\nrealistic material photos under natural lighting. In RealMat, we first finetune\na pretrained Stable Diffusion XL (SDXL) with synthetic material maps arranged\nin $2 \\times 2$ grids. This way, our model inherits some realism of SDXL while\nlearning the data distribution of the synthetic material grids. Still, this\ncreates a realism gap, with some generated materials appearing synthetic. We\npropose to further finetune our model through reinforcement learning (RL),\nencouraging the generation of realistic materials. We develop a realism reward\nfunction for any material image under natural lighting, by collecting a\nlarge-scale dataset of realistic material images. We show that this approach\nincreases generated materials' realism compared to our base model and related\nwork."}
{"id": "2509.01870", "categories": ["cs.GT", "cs.FL"], "pdf": "https://arxiv.org/pdf/2509.01870", "abs": "https://arxiv.org/abs/2509.01870", "authors": ["Hiroki Mizuno", "Yoshiaki Takata", "Hiroyuki Seki"], "title": "Complexity of the Existence of Constrained Secure Equilibria in Multi-Player Games", "comment": "5 pages, 1 figure, this is a longer version of a short paper\n  submitted to IEICE Transactions on Information and Systems", "summary": "We consider a multi-player non-zero-sum turn-based game (abbreviated as\nmulti-player game) on a finite directed graph. A secure equilibrium (SE) is a\nstrategy profile in which no player has the incentive to deviate from the\nstrategy because no player can increase her own payoff or lower the payoff of\nanother player. SE is a promising refinement of Nash equilibrium in which a\nplayer does not care the payoff of another player. In this paper, we discuss\nthe decidability and complexity of the problem of deciding whether a secure\nequilibrium with constraints (a payoff profile specifying which players must\nwin) exists for a given multi-player game."}
{"id": "2509.01511", "categories": ["cs.PL"], "pdf": "https://arxiv.org/pdf/2509.01511", "abs": "https://arxiv.org/abs/2509.01511", "authors": ["Zhe Zhou", "Benjamin Delaware", "Suresh Jagannathan"], "title": "Type-Based Incorrectness Reasoning", "comment": null, "summary": "A coverage type generalizes refinement types found in many functional\nlanguages with support for must-style underapproximate reasoning.\nProperty-based testing frameworks are one particularly useful domain where such\ncapabilities are useful as they allow us to verify the completeness, as well as\nsafety, of test generators. There is a surprising connection between the kind\nof underapproximate reasoning coverage types offer and the style of reasoning\nenabled by recently proposed Incorrectness Logic frameworks. In our\npresentation, we propose to explore this connection more deeply, identifying\nmechanisms that more systematically integrate incorrectness reasoning within an\nexpressive refinement type system and the opportunities that such integration\noffers to functional programmers, program verifiers, and program analyzers and\nrelated tools."}
{"id": "2509.01442", "categories": ["cs.GR", "cs.ET", "cs.MM", "physics.soc-ph", "quant-ph"], "pdf": "https://arxiv.org/pdf/2509.01442", "abs": "https://arxiv.org/abs/2509.01442", "authors": ["João S. Ferreira", "Arianna Crippa", "Astryd Park", "Daniel Bultrini", "Pierre Fromholz", "Roman Lipski", "Karl Jansen", "James R. Wootton"], "title": "Quantum Brush: A quantum computing-based tool for digital painting", "comment": null, "summary": "We present Quantum Brush, an open-source digital painting tool that harnesses\nquantum computing to generate novel artistic expressions. The tool includes\nfour different brushes that translate strokes into unique quantum algorithms,\neach highlighting a different way in which quantum effects can produce novel\naesthetics. Each brush is designed to be compatible with the current noisy\nintermediate-scale quantum (NISQ) devices, as demonstrated by executing them on\nIQM's Sirius device."}
{"id": "2509.01953", "categories": ["cs.GT", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.01953", "abs": "https://arxiv.org/abs/2509.01953", "authors": ["Haiqing Zhu", "Lexing Xie", "Yun Kuen Cheung"], "title": "Entry Barriers in Content Markets", "comment": null, "summary": "The prevalence of low-quality content on online platforms is often attributed\nto the absence of meaningful entry requirements. This motivates us to\ninvestigate whether implicit or explicit entry barriers, alongside appropriate\nreward mechanisms, can enhance content quality. We present the first\ngame-theoretic analysis of two distinct types of entry barriers in online\ncontent platforms. The first, a structural barrier, emerges from the collective\nbehaviour of incumbent content providers which disadvantages new entrants. We\nshow that both rank-order and proportional-share reward mechanisms induce such\na structural barrier at Nash equilibrium. The second, a strategic barrier,\ninvolves the platform proactively imposing entry fees to discourage\nparticipation from low-quality contributors. We consider a scheme in which the\nplatform redirects some or all of the entry fees into the reward pool. We\nformally demonstrate that this approach can improve overall content quality.\nOur findings establish a theoretical foundation for designing reward mechanisms\ncoupled with entry fees to promote higher-quality content and support healthier\nonline ecosystems."}
{"id": "2509.02428", "categories": ["cs.PL"], "pdf": "https://arxiv.org/pdf/2509.02428", "abs": "https://arxiv.org/abs/2509.02428", "authors": ["Yongwei Yuan", "Zhe Zhou", "Julia Belyakova", "Benjamin Delaware", "Suresh Jagannathan"], "title": "From Traces to Program Incorrectness: A Type-Theoretic Approach", "comment": null, "summary": "We present a type-theoretic framework for reasoning about incorrectness in\nfunctional programs that interact with effectful, opaque library APIs. Our\napproach centers on traces -- temporally-ordered sequences of library API\ninvocations -- which naturally characterize both the preconditions of\nindividual APIs and their composite behavior. We represent these traces using\nsymbolic regular expressions (SREs), enabling formal specification of incorrect\nabstract data type (ADT) behaviors across function boundaries. The core\ncontribution is a novel type inference algorithm that operates modulo specified\nincorrectness properties and leverages the symbolic finite automata (SFAs)\nrepresentations of regexes for compositional reasoning of traces. When the\nalgorithm succeeds, the inferred types witness that an ADT implementation can\nexhibit some subset of the specified incorrect behaviors. This represents the\nfirst systematic approach to underapproximate reasoning against trace-based\nincorrectness specifications, enabling a new form of trace-guided compositional\nanalysis."}
{"id": "2509.01839", "categories": ["cs.GR", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2509.01839", "abs": "https://arxiv.org/abs/2509.01839", "authors": ["Akis Nousias", "Stavros Nousias"], "title": "HodgeFormer: Transformers for Learnable Operators on Triangular Meshes through Data-Driven Hodge Matrices", "comment": "12 pages, 11 figures, 9 tables", "summary": "Currently, prominent Transformer architectures applied on graphs and meshes\nfor shape analysis tasks employ traditional attention layers that heavily\nutilize spectral features requiring costly eigenvalue decomposition-based\nmethods. To encode the mesh structure, these methods derive positional\nembeddings, that heavily rely on eigenvalue decomposition based operations,\ne.g. on the Laplacian matrix, or on heat-kernel signatures, which are then\nconcatenated to the input features. This paper proposes a novel approach\ninspired by the explicit construction of the Hodge Laplacian operator in\nDiscrete Exterior Calculus as a product of discrete Hodge operators and\nexterior derivatives, i.e. $(L := \\star_0^{-1} d_0^T \\star_1 d_0)$. We adjust\nthe Transformer architecture in a novel deep learning layer that utilizes the\nmulti-head attention mechanism to approximate Hodge matrices $\\star_0$,\n$\\star_1$ and $\\star_2$ and learn families of discrete operators $L$ that act\non mesh vertices, edges and faces. Our approach results in a\ncomputationally-efficient architecture that achieves comparable performance in\nmesh segmentation and classification tasks, through a direct learning\nframework, while eliminating the need for costly eigenvalue decomposition\noperations or complex preprocessing operations."}
{"id": "2509.02380", "categories": ["cs.GT"], "pdf": "https://arxiv.org/pdf/2509.02380", "abs": "https://arxiv.org/abs/2509.02380", "authors": ["Giacoomo Maggiorano", "Alessandro Sosso", "Gautier Stauffer"], "title": "A Strongly Polynomial-Time Combinatorial Algorithm for the Nucleolus in Convex Games", "comment": null, "summary": "The nucleolus is a fundamental solution concept in cooperative game theory,\nyet computing it is NP-hard in general. In convex games-where players' marginal\ncontributions grow with coalition size-the only existing polynomial-time\nalgorithm relies on the ellipsoid method. We re-examine a reduced game\napproach, refuting a previously claimed polynomial-time implementation and\nclarifying why it fails. By developing new algorithmic ideas and exploiting the\nstructure of least core polyhedra, we show that reduced games can in fact be\nused effectively. This yields the first combinatorial and strongly polynomial\nalgorithm for computing the nucleolus in convex games."}
{"id": "2509.02141", "categories": ["cs.GR", "cs.CV"], "pdf": "https://arxiv.org/pdf/2509.02141", "abs": "https://arxiv.org/abs/2509.02141", "authors": ["Mohit Mendiratta", "Mayur Deshmukh", "Kartik Teotia", "Vladislav Golyanik", "Adam Kortylewski", "Christian Theobalt"], "title": "GRMM: Real-Time High-Fidelity Gaussian Morphable Head Model with Learned Residuals", "comment": "Project page: https://mohitm1994.github.io/GRMM/", "summary": "3D Morphable Models (3DMMs) enable controllable facial geometry and\nexpression editing for reconstruction, animation, and AR/VR, but traditional\nPCA-based mesh models are limited in resolution, detail, and photorealism.\nNeural volumetric methods improve realism but remain too slow for interactive\nuse. Recent Gaussian Splatting (3DGS) based facial models achieve fast,\nhigh-quality rendering but still depend solely on a mesh-based 3DMM prior for\nexpression control, limiting their ability to capture fine-grained geometry,\nexpressions, and full-head coverage. We introduce GRMM, the first full-head\nGaussian 3D morphable model that augments a base 3DMM with residual geometry\nand appearance components, additive refinements that recover high-frequency\ndetails such as wrinkles, fine skin texture, and hairline variations. GRMM\nprovides disentangled control through low-dimensional, interpretable parameters\n(e.g., identity shape, facial expressions) while separately modelling residuals\nthat capture subject- and expression-specific detail beyond the base model's\ncapacity. Coarse decoders produce vertex-level mesh deformations, fine decoders\nrepresent per-Gaussian appearance, and a lightweight CNN refines rasterised\nimages for enhanced realism, all while maintaining 75 FPS real-time rendering.\nTo learn consistent, high-fidelity residuals, we present EXPRESS-50, the first\ndataset with 60 aligned expressions across 50 identities, enabling robust\ndisentanglement of identity and expression in Gaussian-based 3DMMs. Across\nmonocular 3D face reconstruction, novel-view synthesis, and expression\ntransfer, GRMM surpasses state-of-the-art methods in fidelity and expression\naccuracy while delivering interactive real-time performance."}
{"id": "2509.02493", "categories": ["cs.GT", "cs.MA", "cs.SY", "eess.SY", "math.OC"], "pdf": "https://arxiv.org/pdf/2509.02493", "abs": "https://arxiv.org/abs/2509.02493", "authors": ["Raj Kiriti Velicheti", "Subhonmesh Bose", "Tamer Başar"], "title": "Harnessing Information in Incentive Design", "comment": "Initial Version", "summary": "Incentive design deals with interaction between a principal and an agent\nwhere the former can shape the latter's utility through a policy commitment. It\nis well known that the principal faces an information rent when dealing with an\nagent that has informational advantage. In this work, we embark on a systematic\nstudy of the effect of information asymmetry in incentive design games.\nSpecifically, we first demonstrate that it is in principal's interest to\ndecrease this information asymmetry. To mitigate this uncertainty, we let the\nprincipal gather information either by letting the agent shape her belief (aka\nInformation Design), or by paying to acquire it. Providing solutions to all\nthese cases we show that while introduction of uncertainty increases the\nprincipal's cost, letting the agent shape its belief can be advantageous. We\nstudy information asymmetry and information acquisition in both matrix games\nand quadratic Gaussian game setups."}
{"id": "2509.02278", "categories": ["cs.GR", "cs.AI", "cs.MM"], "pdf": "https://arxiv.org/pdf/2509.02278", "abs": "https://arxiv.org/abs/2509.02278", "authors": ["Zikai Huang", "Yihan Zhou", "Xuemiao Xu", "Cheng Xu", "Xiaofen Xing", "Jing Qin", "Shengfeng He"], "title": "Think2Sing: Orchestrating Structured Motion Subtitles for Singing-Driven 3D Head Animation", "comment": null, "summary": "Singing-driven 3D head animation is a challenging yet promising task with\napplications in virtual avatars, entertainment, and education. Unlike speech,\nsinging involves richer emotional nuance, dynamic prosody, and lyric-based\nsemantics, requiring the synthesis of fine-grained, temporally coherent facial\nmotion. Existing speech-driven approaches often produce oversimplified,\nemotionally flat, and semantically inconsistent results, which are insufficient\nfor singing animation. To address this, we propose Think2Sing, a\ndiffusion-based framework that leverages pretrained large language models to\ngenerate semantically coherent and temporally consistent 3D head animations,\nconditioned on both lyrics and acoustics. A key innovation is the introduction\nof motion subtitles, an auxiliary semantic representation derived through a\nnovel Singing Chain-of-Thought reasoning process combined with acoustic-guided\nretrieval. These subtitles contain precise timestamps and region-specific\nmotion descriptions, serving as interpretable motion priors. We frame the task\nas a motion intensity prediction problem, enabling finer control over facial\nregions and improving the modeling of expressive motion. To support this, we\ncreate a multimodal singing dataset with synchronized video, acoustic\ndescriptors, and motion subtitles, enabling diverse and expressive motion\nlearning. Extensive experiments show that Think2Sing outperforms\nstate-of-the-art methods in realism, expressiveness, and emotional fidelity,\nwhile also offering flexible, user-controllable animation editing."}
{"id": "2509.02519", "categories": ["cs.GT"], "pdf": "https://arxiv.org/pdf/2509.02519", "abs": "https://arxiv.org/abs/2509.02519", "authors": ["Chris Dong", "Martin Bullinger", "Tomasz Wąs", "Larry Birnbaum", "Edith Elkind"], "title": "Selecting Interlacing Committees", "comment": null, "summary": "Polarization is a major concern for a well-functioning society. Often, mass\npolarization of a society is driven by polarizing political representation,\neven when the latter is easily preventable. The existing computational social\nchoice methods for the task of committee selection are not designed to address\nthis issue. We enrich the standard approach to committee selection by defining\ntwo quantitative measures that evaluate how well a given committee\ninterconnects the voters. Maximizing these measures aims at avoiding polarizing\ncommittees. While the corresponding maximization problems are NP-complete in\ngeneral, we obtain efficient algorithms for profiles in the voter-candidate\ninterval domain. Moreover, we analyze the compatibility of our goals with other\nrepresentation objectives, such as excellence, diversity, and proportionality.\nWe identify trade-offs between approximation guarantees, and describe\nalgorithms that achieve simultaneous constant-factor approximations."}
{"id": "2509.02474", "categories": ["cs.GR", "cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.02474", "abs": "https://arxiv.org/abs/2509.02474", "authors": ["Nina Wiedemann", "Sainan Liu", "Quentin Leboutet", "Katelyn Gao", "Benjamin Ummenhofer", "Michael Paulitsch", "Kai Yuan"], "title": "Unifi3D: A Study on 3D Representations for Generation and Reconstruction in a Common Framework", "comment": null, "summary": "Following rapid advancements in text and image generation, research has\nincreasingly shifted towards 3D generation. Unlike the well-established\npixel-based representation in images, 3D representations remain diverse and\nfragmented, encompassing a wide variety of approaches such as voxel grids,\nneural radiance fields, signed distance functions, point clouds, or octrees,\neach offering distinct advantages and limitations. In this work, we present a\nunified evaluation framework designed to assess the performance of 3D\nrepresentations in reconstruction and generation. We compare these\nrepresentations based on multiple criteria: quality, computational efficiency,\nand generalization performance. Beyond standard model benchmarking, our\nexperiments aim to derive best practices over all steps involved in the 3D\ngeneration pipeline, including preprocessing, mesh reconstruction, compression\nwith autoencoders, and generation. Our findings highlight that reconstruction\nerrors significantly impact overall performance, underscoring the need to\nevaluate generation and reconstruction jointly. We provide insights that can\ninform the selection of suitable 3D models for various applications,\nfacilitating the development of more robust and application-specific solutions\nin 3D generation. The code for our framework is available at\nhttps://github.com/isl-org/unifi3d."}
